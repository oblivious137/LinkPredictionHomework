W00-1401 " Certain generation applications may profit from the use of stochastic methods. In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models. In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment. This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects. To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment. The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development. 1 Introdu  "
W00-1402 "<NoAbstract>"
W00-1403 "<NoAbstract>"
W00-1404 " The use of XML-based authoring tools is swiftly becoming a standard in the world of technical documentation. An XML document is a mixture of structure (the tags) and surface (text between the tags). The structure reflects the choices made by the author during the top-down stepwise refinement of the document under control of a DTD grammar. These choices are typically choices of meaning which are independent of the language in which the document is rendered, and can be seen as a kind of interlingua for the class of documents which is modeled by the DTD. Based on this remark, we advocate a radicalization of XML authoring, where the semantic content of the document is accounted for exclusively in terms of choice structures, and where appropriate rendering/realization mechanisms are responsible for producing the surface, possibly in several languages simultaneously. In this view, XML authoring has strong connections to natural language generation and text authoring. We describe the IG (Interaction Grammar) formalism, an extension of DTD's which permits powerful linguistic manipulations, and show its application to the production of multilingual versions of a certain class of pharmaceutical documents. "
W00-1405 " Extensively annotated bilingual parallel corpora can be exploited to feed editing tools that integrate the processes of document composition and translation. Here we discuss the architecture of an interactive editing tool that, on top of techniques common to most Translation Memory-based systems, applies the potential of SGML's DTDs to guide the process of bilingual document generation. Rather than employing just simple task-oriented mark-up, we selected a set of TEI's highly complex and versatile collection of tags to help disclose the underlying logical structure of documents in the test-corpus. DTDs were automatically induced and later integrated in the editing tool to provide the basic scheme for new documents. 1 Introdu  "
W00-1406 " We describe a mechanism which generates rebuttals to a user's rejoinders in the context of arguments generated from Bayesian networks. This mechanism is implemented in an interactive argumentation system. Given an argument generated by the system and an interpretation of a user's rejoinder, the generation of the rebuttal takes into account the intended effect of the user's rejoinder, determined on a model of the user's beliefs, and its actual effect, determined on a model of the system's beliefs. We consider three main rebuttal strategies: refute the user's rejoinder, strengthen the argument goal, and dismiss the user's line of reasoning. 1 I  "
W00-1407 "<NoAbstract>"
W00-1408 " During argumentation, people persuade their audience using a variety of strategies, e.g., hypothetical reasoning, reasoning by cases and ordinary premise-to-goal arguments. In this paper, we offer an operational definition of the conditions for pursuing these strategies, and incorporate into a Bayesian argument-generation system a mechanism for proposing applicable argumentation strategies, generating specific arguments based on these strategies, and selecting a final argument. 1 Intro  "
W00-1409 "Stuhlsatzenhausweg 3 D-66123 Saarbriicken, Germany {becker, kilger, lopez, poller}@dfki, de Abstract Based on our experiences in VERBMOBIL, a large scale speech-to-speech translation system, we identify two types of problems that a generation component must address in a realistic implementation and present relevant examples. As an extension to the architecture of a translation system, we present a module for robustness preprocessing on the interface between translation and generation. "
W00-1410 " The RAGS project aims to define a reference architecture for Natural Language Generation (NLG) systems. Currently the major part of this architecture consists of a set of datatype definitions for specifying the input and output formats for modules within NLG systems. In this paper we describe our efforts to reinterpret an existing NLG system in terms of these definitions. The system chosen was the Caption Generation System. 2. Which aspects of the RAGS repertoire would : .... .... .... -,.= ., ~,~,aemaltybe'requireti~ftrr~strch~a-~reinterpretation; which would be unnecessary and which additions to the RAGS repertoire would be motivated. 1 Introduct  "
W00-1411 " This paper describes an implemented system which uses centering theory for planning of coherent texts and choice of referring expressions. We argue that text and sentence planning need to be driven in part by the goal of maintaining referential continuity and thereby facilitating pronoun resolution: obtaining a favourable ordering of clauses, and of arguments within clauses, is likely to increase opportunities for non-ambiguous pronoun use. Centering theory provides the basis for such an integrated approach. Generating coherent texts according to centering theory is treated as a constraint satisfaction problem. 1 Introdu  "
W00-1412 " In this paper we present a psycholinguistically motivated architecture and its prototypical implementation for an incremental conceptualizer, which monitors dynamic changes in the world and simultaneously generates warnings for (possibly) safety-critical developments. It does so by conceptualizing events and building up a hierarchical knowledge representation of the perceived states of affairs. If it detects a safety problem, it selects suitable elements from the representation for a warning, brings them into an appropriate order, and generates incremental preverbal messages (propositional structures) from them, which can be taken by a subsequent component to encode them linguistically. 1 Intro  "
W00-1413 " When a lexical item is selected in the language production process, it needs to be explained why none of its superordinates gets selected instead, since their applicability conditions are fulfilled all the same. This question has received much attention in cognitive modelling and not as much in other branches of NLG. This paper describes the various approaches taken, discusses the reasons why they are so different, and argues that production models using symbolic representations should make a distinction between conceptual and lexical hierarchies, which can be organized along fixed levels as studied in (some branches of) lexical semantics. 1 Intro  "
W00-1414 "New York, NY 10027, USA shaw, kathy*cs, columbia, edu , :, ~.*~ . Abstract In this paper, we describe how quantifiers can be generated in a text generation system. By taking advantage of discourse and ontological information, quantified expressions can replace entities in a text, making the text more fluent and concise. In addition to avoiding ambiguities between distributive and collective readings in universal quantification generation, we will also show how different scope orderings between universal and existential quantitiers will result in different quantified expressions in our algorithm. "
W00-1415 " It is not a rare phenomenon for human written text to use non-restrictive NP modifiers to express essential pieces of information or support the situation presented in the main proposition containing the NP, for example, \"Private Eye, which couldn't afford the libel payment, had been threatened with closure.\" (from Wall Street Journal) Yet no previous research in NLG investigates this in detail. This paper describes corpus analysis and a psycholinguistic experiment regarding the acceptability of using non-restrictive NP modifiers to express semantic relations that might normally be signalled by 'because' and 'then'. The experiment tests several relevant factors and enables us to accept or reject a number of hypotheses. The results are incorporated into an NLG system based on a Genetic Algorithm. 1 Introdu  "
W00-1416 " A range of research has explored the problem of generating referring expressions that uniquely identify a single entity from the shared context. But what about expressions that identify sets of entities? In this paper, I adapt recent semantic research on plural descriptions--using covers to abstract collective and distributive readings and using sets of assignments to represent dependencies among references--to describe a search problem for set-identifying expressions that largely mirrors the search problem for singular referring expressions. By structuring the search space only in terms of the words that can be added to the description, the proposal defuses potential combinatorial explosions that might otherwise arise with reference to sets. 1 Introdu  "
W00-1417 " We present a new approach to paratactic content aggregation in the context of generating hypertext summaries of OLAP and data mining discoveries. Two key properties make this approach innovative and interesting: (1) it encapsulates aggregation inside the sentence planning component, and (2) it relies on a domain independent algorithm working on a data structure that abstracts from lexical and syntactic knowledge.  "
W00-1418 "nburgh. ..... ~.:D.eparl~me~t.nf: Compulzercience~ ~Otago University: Abstract This paper outlines a text generation system suited to a large class of information sources, relational databases. We focus on one aspect of the problem: the additional information which needs to be specified to produce reasonable text quality when generating from relational databases. We outline how databases need to be prepared, and then describe various types of domain semantics which can be used to improve text qualify. "
W00-1419 " This paper argues for looking at Controlled Languages (CL) from a Natural Language Generation (NLG) perspective. We show that CLs are used in a normative environment in which different textual modules can be identified, each having its own set of rules constraining the text. These rules can be used as a basis for natural language generation. These ideas were tested in a proof of concept generator for the domain of aircraft maintenance manuals. 1 Wha  "
W00-1420 " This paper describes a novel functionality of the VERBMOBIL system, a large scale translation system designed for spontaneously spoken multilingual negotiation dialogues. The task is the on-demand generation of dialogue scripts and result summaries of dialogues. We focus on summary generation and show how the relevant data are selected from the dialogue memory and how they are packed into an appropriate abstract representation. Finally, we demonstrate how the existing generation module of VERBMOBIL was extended to produce multilingual and result summaries from these representations. 1 Introduction   "
W00-1421 "Karlstr. 2, 69117 Heidelberg, Germany {endriss, klabunde}@j anus. gs. uni-heidelberg, de Abstract Word order and accent placement are the primary linguistic means to indicate focus/background structures in German. This paper presents a pipelined architecture for the generation of German monologues with contextually appropriate word order and accent placements for the realization of focus/background structures. Our emphasis is on the sentence planner that extends the respective propositional contents with discourse-relational features and decides which part will be focused. Such an enriched semantic input for an HPSG-based formulator allows word order variations and the placement of prenucleus and nucleus accents. Word order is realized by grammatical competition based on linear precedence (LP) rules which are based on the discourserelational features. Accent placement is realized by a syntax-driven focus principle that determines the focus exponent and possible bearers of prenucleus accents within the syntactically realized focus, the so-called focus domain. 1 Focus and word order determination as sentence planning tasks This paper addresses aspects of the control of intonation belonging to the area of sentence planning [Beale et al., 1998: Wanner and Hovy, 1996]. In many languages, intonation can reflect pragmatically motivated conceptual decisions. In particular, focus/background structures (FBSs) reflect the speaker's beliefs of the listener's information state. Since FBSs are realized in German primarily by word order dependent accent placements, focus planning and word order determination are subtasks of sentence planning. Due to the complex interactions anaong the various subtasks of sentence planning [Hovy and Wan* The research reported in this paper is funded by the DFG (German Science Foundation) in the priority program \"Language Production\" under grant lie 146T/3-I. The authors would like to thank the three anonymous referees for helpful comments and suggestions. All remaining mistakes are, of course. OUF o~vn. her, 1996] proposed a blackboard-based sentence planner instead of a pipelined architecture. However, we will demonstrate as a byproduct of our approach to focus planning that in some cases the complexity of interactions can be realized by a traditional top-down expansion process. The intertwined clause-internal organization of focus planning and word order determination for the realization of FBSs in German is obtainable by hierarchical planning. In what follows, we will first give examples of the interplay of focus and word order in German. We will present the architecture of our NLG system that realizes FBSs and describe in more detail hierarchical sentence planning for FBSs. By means of some examples we are finally showing how word order dependent focus assignment works. "
W00-1422 " We present a new approach to enriching underspecified representations of content to be realized as text. Our approach uses an attribute grammar to propagate missing information where needed in a tree that represents the text to be realized. This declaratively-specified grammar mediates between application-produced output and the input to a generation system and, as a consequence, can easily augment an existing generation system. Endapplications that use this approach can produce high quality text without a fine-grained specification of the text to be realized, thereby reducing the burden to the application. Additionally, representations used by the generator are compact, because values that can be constructed from the constraints encoded by the grammar will be propagated where necessary. This approach is more flexible than defaulting or making a statistically good choice because it can deal with long-distance dependencies (such as gaps and reflexive pronouns). Our approach differs from other approaches that use attribute grammars in that we use the grammar to enrich the representations of the content to be realized, rather than to generate the text itself. We illustrate the approach with examples from our template-based textrealizer, YAG. "
W00-1423 " We describe the generation of communicative actions in an implemented embodied conversational agent. Our agent plans each utterance so that multiple communicative goals may be realized opportunistically by a composite action including not only speech but also coverbal gesture that fits the context and the ongoing speech in ways representative of natural human conversation. We accomplish this by reasoning from a grammar which describes gesture declaratively in terms of its discourse function, semantics and synchrony with speech. 1 Introdu  "
W00-1424 " This paper deals with the generation of definite (i.e., uniquely referring) descriptions containing semantically vague expressions ('large', 'small', etc.). Firstly, the paper proposes a semantic analysis of vague descriptions that does justice to the contextdependent meaning of the vague expressions in them. Secondly, the paper shows how this semantic analysis can be implemented using a modification of the Dale and Reiter (1995) algorithm for the generation of referring expressions. A notable feature of the new algorithm is that, unlike Dale and Reiter (1995), it covers plural as well as singular NPs. This algorithm has been implemented in an experimental NLG program using ProFIT. The paper concludes by formulating some pragmatic constraints that could allow a generator to choose between different semantically correct descriptions. 1 Intro  "
W00-1425 " In natural language generation, different generation tasks often interact with each other in a complex way. We think that how to resolve the complex interactions inside and between tasks is more important to the generation of a coherent text than how to model each individual factor. This paper focuses on the interaction between aggregation and text planning, and tries to explore what preferences exist among the features considered by the two tasks. The preferences are implemented in two generation systems, namely ILEX-TS and a text planner using a Genetic Algorithm. The evaluation emphasises the second implementation and shows that capturing these preferences properly can lead to coherent text. 1 Discourse coh  "
W00-1426 "<NoAbstract>"
W00-1427 " In practical natural language generation systems it is often advantageous to have a separate component that deals purely with morphological processing. We present such a component: a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required. We describe how this morphological generator is used in a prototype system for automatic simplification of English newspaper text, and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application. 1 Introduction  "
W00-1428 " This paper presents the integration of a largescale, reusable lexicon for generation with the FUF/SURGE unification-based syntactic realizer. The lexicon was combined from multiple existing resources in a semi-automatic process. The integration is a multi-step unification process. This integration allows the reuse of lexical, syntactic, and semantic knowledge encoded in the lexicon in the development of lexical chooser module in a generation system. The lexicon also brings other benefits to a generation system: for example, the ability to generate many lexical and syntactic paraphrases and the ability to avoid non-grammatical output. 1 Introdu  "
W00-1429 " We describe the knowledge acquisition (KA) techniques used to build the STOP system, especially sorting and think-aloud protocols. That is, we describe the ways in which we interacted with domain experts to determine appropriate user categories, schemas, detailed content rules, and so forth for STOP. Informal evaluations of these techniques suggest that they had some benefit, but perhaps were most successful as a source of insight and hypotheses, and should ideally have been supplemented by other techniques when deciding on the specific rules and knowledge incorporated into STOP. 1 Intro  "
W00-1430 " When generating utterances, humans may choose among a number of alternative sentence forms expressing the same propositional content. The context determines these decisions to a large extent. This paper presents a strategy to allow for such context-sensitive variation when generating text for a wearable, advice giving device. Several dimensions of context feed a model of the heater's attention space, which, in terms of Information Structure Theory, determines the form of the sentence to be generated. "
W00-1431 "Thomson-CSF/LCR Talana, Univ. Paris 7 February 6 th ,2000 Abstract 2 Presentation of GTAG \" ............. We+will discuss the:somi-reeursiveaigorithm for ....... -+-+,~he~GTAG.ovmalismdescrihes.th~domain_ text generation, as defined for the GTAG model used to specify the input of the generator, formalism, and its implementation in the CLEF project. We will show how to use iexical choice constraints and properties of the LTAG grammar to minimize the backtracking of the semirecursive algorithm. "
W00-1432 " In this paper we describe a neural networks approach to generation. The task is to generate sentences with hotel-information from a structured database. The system is inspired by Karen Kukich's ANA, but expands on it by adding generality in the form of language independence in representations and lexical look-up. Introduction In the growing field of intelligent communication (web-browsers, dialogue systems, etc.) the need for a flexible generator has become more important (e.g. Hovy & Lin, 1999). NLG is usually seen as a two-stage process where the planning component takes care of the inter-sentential content planning, while the surface realisation component transforms the content representation into a string of words. Interactions between the two components have called for the micro-planning stage to be postulated in the middle, but still the rule-based pipeline architecture has problems with sequential rules and their two-way relations. Statistical approaches have been developed, and seem to provide flexibility to generation tasks. The approach taken in this thesis, however, explores generation as .a .classification task whereby the representation that describes the intended meaning of the utterance is ultimately to be classified into an appropriate surface form. Although the task as such is a complex one, the approach allows its decomposition into a series of smaller classification tasks tbrmulated as input-output mappings rather than step-wise rules. One of the goals of the thesis is to study the ways generation could .be broken down into suitable sub-classification tasks so as to enhance flexibility in the generation process in general. Artificial neural networks are a classification technique that is robust and resistant to noisy input, and learns to classify inputs on the basis of training examples, without specific rules that describe how the classification is to be done. There is not much research into using ANN's for generation, the main reason being long training times. Two notable exceptions are Kukich (1987) and Ward (1997), both argue in favour of NN's robustness, but at the same time point out problems with scalability. We believe that with improved computer facilities that shorten the training time, this new way of looking at generation as a classification task constitutes an interesting approach to generation. We have chosen Kukich's approach, as our application domain is to generate utterances from structured databases. This paper is structured as follows; we first discuss the general model. The second part briefly describes neural networks. We continue with describing a possible implementation of the model, and finally we draw some conclusions and point to future challenges.  "
W00-1433 "Rochester;:N'Y 14~27 ...... stent~cs, rochester, edu Abstract In this paper we report on several issues arising out of a first attempt to annotate task-oriented spoken dialog for rhetorical structure using Rhetorical Structure Theory. We discuss an annotation scheme we are developing to resolve the difficulties we have encountered. "
W00-1434 "h. Abstract RSTTool is a graphical tool for annotating a text in terms of its rhetorical structure. The demonstration will show the various interfaces of the tool, focusing on its ease of use. "
W00-1435 " We will demonstrate the ILEX system, a system which dynamically generates descriptions of database objects for the web, adapting the description to the discourse context and user type. Among other improvements in version 3, the system now generates from relational databases, and this demonstration will focus on this ability. We will also show how incremental extensions to the domain semantics improve the quality of the text produced. 1 Intro  "
W00-1436 "<NoAbstract>"
W00-1437 "<NoAbstract>"
W00-1438 "Newark, DE 19711 { silber, mccoy} @cis. udel. edu  Abstract ........ We present a system which uses lexical chains as an intermediate representation for automatic text summarization. This system builds on previous research by implementing a lexical chain extraction algorithm in linear time. The system is reasonably domain independent and takes as input any text or HTML document. The system outputs a short summary based on the most salient concepts from the original document. The length of the extracted summary can be either controlled automatically, or manually based on length or percentage of compression. While still under development, the system provides useful summaries which compare well in information content to human generated summaries. Additionally, the system provides a robust test bed for future summary generation research. "
W00-1439 "<NoAbstract>"
W00-1602 " Context free grammars parse faster than TFS grammars, but have disadvantages. On our test TFS grammar, precompilation into CFG results in a speedup of 16 times for parsing without taking into account additional mechanisms for increasing parsing efficiency. A formal overview is given of precompilation and parsing. Modifications to ALE rules permit a closure over the rules from the lexicon, and analysis leading to a fast treatment of semantic structure. The closure algorithm, and retrieval of full semantic structure are described. Introduction Head Driven Phrase Structure Grammar (HPSG), Pollard and Sag (1994) is expressed in Typed Feature Structures (TFSs). Context Free Grammar (CFG) without features supports much faster parsing, but a TFS grammar has many advantages. Fast parsing can be obtained by precompiling a CFG approximation, with TFSs converted into CF near-equivalents. CFG parsing eliminates impossible trees, and TFS unification over the remainder eliminates more, and instantiates path values. Our method treats slashes separately in a precompiled table, and careful allocation of categories to TFSs makes TFS unification unnecessary: instead skeleton semantic structures are formed in parsing, and full structures retrieved afterwards. A prototype precompiler and fast parser 1 were built in Prolog, and tested with an HPSG grammar of English by Matheson (1996),   1 Downloadable code on http://www.cs.york.ac.uk/~johnb or http://www.soft.net.uk/research/hsppar.htm .   http://www.soft.net.uk/research/hsppar.htm . written in the ALE formalism, by Carpenter and Penn (1996). This has the 6 schemas and 5 principles of HPSG, with 184 lexemes. A complex sentence,kim can believe sandy can expect sandy to persuade kim to promise sandy to try to expect sandy to persuade kim to promise kim to give sandy a happy happy book, parsed in 3.3s. with retrieval, 5.6 times faster than with TFSs, Brown and Manandhar (2000). An 11 word sentence was 18 times faster at 87ms., or 16 times counting retrieval.  "
W00-1605 " In this paper, we identify syntactic lexical ambiguity and sentence complexity as factors that contribute to parsing complexity in fully lexicalized grammar formalisms such as Lexicalized Tree Adjoining Grammars. We also report on experiments that explore the effects of these factors on parsing complexity. We discuss how these constraints can be exploited in improving efficiency of parsers for such grammar formalisms. 1 Int  "
W00-1701 "Abstract This paper describes the semantic annotations we are performing on the CallHome Japanese corpus of spontaneous, unscripted telephone conversations (LDC, 1996). Our annotations include (i) semantic classes for all nouns and verbs; (ii) verb senses for all main verbs; and (iii) relations between main verbs and their complements in the same utterance. Our semantic tagset is taken from NTTs Goi-Taikei semantic lexicon and ontology (Ikehara et al., 1997). A pilot study demonstrates that the verb sense tagging can be eciently performed by native Japanese speakers using computergenerated HTML forms, and that good interannotator reliability can be obtained in the right conditions. "
W00-1702 " The most effective paradigm for word sense disambiguation, supervised learning, seems to be stuck because of the knowledge acquisition bottleneck. In this paper we take an in-depth study of the performance of decision lists on two publicly available corpora and an additional corpus automatically acquired from the Web, using the fine-grained highly polysemous senses in WordNet. Decision lists are shown a versatile state-of-the-art technique. The experiments reveal, among other facts, that SemCor can be an acceptable (0.7 precision for polysemous words) starting point for an all-words system. The results on the DSO corpus show that for some highly polysemous words 0.7 precision seems to be the current state-of-the-art limit. On the other hand, independently constructed hand-tagged corpora are not mutually useful, and a corpus automatically acquired from the Web is shown to fail.  WordNet. Decision lists are shown a versatile state-of-the-art technique. The experiments reveal, among other facts, that SemCor can be an acceptable (0.7 precision for polysemous words) starting point for an all-words system. The results on the DSO corpus show that for some highly polysemous words 0.7 precision seems to be the current state-of-the-art limit. On the other hand, independently constructed hand-tagged corpora are not mutually useful, and a corpus automatically acquired from the Web is shown to fail. "
W00-1704 "okyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan Abstract Corpus annotation is now a key topic for all areas of natural language processing (NLP) and information extraction (IE) which employ supervised learning. With the explosion of results in molecular-biology there is an increased need for IE to extract knowledge to support database building and to search intelligently for information in online journal collections. To support this we are building a corpus of annotated abstracts taken from National Library of Medicines MEDLINE database. In this paper we report on this new corpus, its ontological basis, and our experience in designing the annotation scheme. Experimental results are shown for inter-annotator agreement and comments are made on methodological considerations. "
W00-1705 " We are annotating a corpus with information relevant to discourse entity realization, and especially the information needed to decide which type of NP to use. The corpus is being used to study correlations between NP type and certain semantic or discourse features, to evaluate hand-coded algorithms, and to train statistical models. We report on the development of our annotation scheme, the problems we have encountered, and the results obtained so far. 1 MOTIV  "
W00-1706 "<NoAbstract>"
W00-1707 " Various kinds of video recordings have discourse structures. Therefore, it is important to determine how video segments are combined and what kind of coherence relations they are connected with. In this paper, we propose a method for estimating the discourse structure of video news reports by analyzing the discourse structure of their transcripts. 1  "
W00-1709 "Abstract This paper proposes an easy and simple method for constructing a super-structure on the Web which provides current Web contents with new value and new means of use. The super-structure is based on external annotations to Web documents. We have developed a system for any user to annotate any element of any Web document with additional information. We have also developed a proxy that transcodes requested contents by considering annotations assigned to them. In this paper, we classify annotations into three categories. One is linguistic annotation which helps the transcoder understand the semantic structure of textual elements. The second is commentary annotation which helps the transcoder manipulate non-textual elements such as images and sounds. The third is multimedia annotation, which is a combination of the above two types. All types of annotation are described using XML, and correspondence between annotations and document elements is defined using URLs and XPaths. We call the entire process semantic transcoding because we deal with the deep semantic content of documents with annotations. The current semantic transcoding process mainly handles text and video summarization, language translation, and speech synthesis of documents including images. "
W01-0801 "Extended Abstract Owen Rambow AT&T Labs  Research Florham Park , NJ, USA rambow@research.att.com In computational linguistics, the 1990s were characterized by the rapid rise to prominence of corpus-based methods in natural language understanding (NLU). These methods include statistical and machine-learning and approaches. In natural language generation (NLG), in the mean time, there was little work using statistical and machine learning approaches. Some researchers felt that the kind of ambiguities that appeared to profit from corpus-based approaches in NLU did not exist in NLG: if the input is adequately specified, then all the rules that map to a correct output can also be explicitly specified. However, this paper will argue that this view is not correct, and NLG can and does profit from corpusbased methods. The resistance to corpus-based approaches in NLG may have more to do with the fact that in many NLG applications (such as report or description generation) the output to be generated is extremely limited. As is the case with NLU, if the language is limited, hand-crafted methods are adequate and successful. Thus, it is not a surprise that the first use of corpus-based techniques, at ISI (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) was motivated by the use of NLG not in traditional NLG applications, but in machine translation, in which the range of output language is (potentially) much larger. In fact, the situations in NLU and NLG do not actually differ with respect to the notion of ambiguity. Though it is not a trivial task, we can fully specify a grammar such that the generated text is not ungrammatical. But the problem for NLG is not specifying a grammar, but determining which part of the grammar to use: to give a simple example, a give-event can be generated with the double-object frame (give Mary a book) or with a prepositional object (give a book to Mary). We can easily specify the syntax of these two constrictions. What we need to know is when to choose which. But the situation is exactly the same in NLU: the problem is knowing which grammar rules to use when during analysis. Thus, just as the mapping from input to output is ambiguous in NLU, it is ambiguous in NLG, not because the grammar is wrong, but because it leaves too many options. The difference is that in NLG, different outputs differ not in whether they are correct (as is the case in NLU), but in whether they are appropriate or felicitous in a given context. Thus, the need for corpus-based approaches is less apparent. Determining which linguistic forms are appropriate in what contexts is a hard task. The introspective grammaticality judgment that (perhaps) is legitimate in the study of syntax is methodologically suspect in the study of language use in context, and most work in linguistic pragmatics is in fact corpus-based, such as Princes work using the Watergate transcripts and similar corpora (Prince, 1981). Thus, it is clear that the role of corpus-based methods in NLG is not to displace traditional methods, but rather to accelerate them. If indeed corpus-based methods are necessary in any case, we may as well use automated procedures for discovering regularities; we no longer need to use multi-colored pencils to mark up paper copies. For the researcher, there is enough left to do: the corpus-based techniques still require linguistic research in order to determine which features to code for (i.e., what linguistic phenomena to count). To the extent that corpus-based methods fail currently, it is largely because we are substituting easily codable features for those that are more difficult to code, or because we are simply coding the wrong features. It is not because there is some hidden truth which traditional linguistic methodologies have access to but corpusbased methods do not, because they are not in fact in opposition to each other. Finally, the emphasis on evaluation that the corpus-based techniques in NLU have brought with them have often aroused animosity in the NLG community. Evaluation is necessary for development purposes when using corpus-based techniques: it is easy to generate many different hypotheses, and we need to be able to choose among them. Since this is crucial, increased attention needs to be paid to evaluation in generation (Bangalore et al., 2000; Rambow et al., 2001). But again, the situation is in fact not different from a traditional linguistic methodology: theories about language use in context need to be defeasible on empirical grounds and hence need to be evaluated against a corpus. Of course, the choice of evaluation corpus is an important one, and the costs associated with compiling and annotating corpora can greatly impact the choice of evaluation corpus and hence the evaluation. In conclusion, NLG has nothing to fear from corpus-based methods. Instead, the NLG community can continue to provide a test-bed for linguists to exercise their theories (to a much greater extent than can NLU). The difference is that everyone can now start using computers. "
W01-0802 " In this paper we describe a two-stage model for content determination in systems that summarise time series data.Thefirststageinvolvesbuildinga qualitativeoverviewofthedataset,and the second involves using this overview,togetherwiththeactualdata, to produce summaries of the timeseriesdata.Thismodelisbasedonour observations of how human experts summarisetime-seriesdata. 1  "
W01-0803 " In this paper, the issue of document structuring is addressed. To achieve this task, we advocate that Segmented Discourse Representation Theory (SDRT) is a most expressive discourse framework. Then we sketch a discourse planning mechanism which aims at producing as many paraphrastic document structures as possible from a set of factual data encoded into a logical form. 1 Introdu  "
W01-0804 " We examine the principle of coextensivity which underlies current algorithms for the generation of referring expressions, and investigate to what extent the principle allows these algorithms to be generalized. The discussion focusses on the generation of complex Boolean descriptions and sentence aggregation. 1 Logic in  "
W01-0805 "<NoAbstract>"
W01-0806 " We present an algorithm which improves the efficiency of a search for the optimally aggregated paragraph which summarises a flat structured input specification. We model the space of possible paraphrases of possible paragraphs as the space of sequences of compositions of a set of tree-adjoining grammar (TAG) elementary trees. Our algorithm transforms this to a set with equivalent paraphrasing power but better computational properties. Also, it identifies an explicit mapping between input propositions and their possible surface realisations. 1 Introductio  "
W01-0807 "<NoAbstract>"
W01-0808 "Abstract This paper presents an overview of a robust, broad-coverage, and application-independent natural language generation system. It demonstrates how the different language generation components function within a multilingual Machine Translation (MT) system, using the languages that we are currently working on (English, Spanish, Japanese, and Chinese). Section 1 provides a system description. Section 2 focuses on the generation components and their core set of rules. Section 3 describes an additional layer of generation rules included to address applicationspecific issues. Section 4 provides a brief description of the evaluation method and results for the MT system of which our generation components are a part. "
W01-0809 " This paper presents the implementation of the Vietnamese generation module in ITS3, a multilingual machine translation (MT) system based on the Government & Binding (GB) theory. Despite well-designed generic mechanisms of the system, it turned out that the task of generating Vietnamese posed non-trivial problems. We therefore had to deviate from the generic code and make new design and implementation in many important cases. By developing corresponding bilingual lexicons, we obtained prototypes of French-Vietnamese and English-Vietnamese MT, the former being the first known prototype of this kind. Our experience suggests that in a principle-based generation system, the parameterized modules, which contain language-specific and lexicalized properties, deserve more attention, and the generic mechanisms should be flexible enough to facilitate the integration of these modules.  the implementation of the Vietnamese generation module in ITS3, a multilingual machine translation (MT) system based on the Government & Binding (GB) theory. Despite well-designed generic mechanisms of the system, it turned out that the task of generating Vietnamese posed non-trivial problems. We therefore had to deviate from the generic code and make new design and implementation in many important cases. By developing corresponding bilingual lexicons, we obtained prototypes of French-Vietnamese and English-Vietnamese MT, the former being the first known prototype of this kind. Our experience suggests that in a principle-based generation system, the parameterized modules, which contain language-specific and lexicalized properties, deserve more attention, and the generic mechanisms should be flexible enough to facilitate the integration of these modules. "
W01-0810 " We propose a multilingual approach to characterizing word order at the clause level as a means to realize information structure. We illustrate the problem with three languages which differ in the degree of word order freedom they exhibit: Czech, a free word order language in which word order variation is pragmatically determined; English, a fixed word order language in which word order is primarily grammatically determined; and German, a language which is between Czech and English on the scale of word order freedom. Our work is theoretically rooted in previous work on information structuring and word order in the Prague School framework as well as on the systemic-functional notion of Theme. The approach we present has been implemented in KPML. "
W01-0811 " The aim of this talk is to show to what extent the work on text generation by computer (TGBC) does not address some of the fundamental problems people struggle with when generating language (TGBP). We will substantiate this claim by taking two tasks on which a lot of research has been carried out during the last 15 years: discourse planning and lexicalisation.  "
W01-0812 " A relatively self-contained subtask of natural language generation is sentence realization: the process of generating a grammatically correct sentence from an abstract semantic / logical representation. We propose a method where sentence realization is carried out using a simplified (context free) version of a large analysis grammar, combined with a statistical language model from the full (context sensitive) version of the same grammar. The statistical model provides a measure of the probability of syntactic substructures, derived from the analysis of a corpus with the full grammar, and is used to guide both subsequent analysis and generation.  "
W01-0813 " The task of creating indicative summaries that help a searcher decide whether to read a particular document is a difficult task. This paper examines the indicative summarization task from a generation perspective, by first analyzing its required content via published guidelines and corpus analysis. We show how these summaries can be factored into a set of document features, and how an implemented content planner uses the topicality document feature to create indicative multidocument query-based summaries. 1 Introdu  "
W01-0814 "<NoAbstract>"
W01-1601 "<NoAbstract>"
W01-1602 "<NoAbstract>"
W01-1603 " We have developed a discourse level tagging tool for spoken dialogue corpus using machine learning methods. As discourse level information, we focused on dialogue act, relevance and discourse segment. In dialogue act tagging, we have implemented a transformation-based learning procedure and resulted in 70% accuracy in open test. In relevance and discourse segment tagging, we have implemented a decision-tree based learning procedure and resulted in about 75% and 72% accuracy respectively. 1 Introduct  "
W01-1604 " Since van der Sandt and Geurts have put forward and extensively applied the notion of a fundamental identity of presupposition and anaphora, something like a universal consensus seems to have developed that this view is basically correct. Supposing that it is, and further supposing that it entails an empirical hypothesis, there are a number of facts that have so far remained unaccounted for. This paper presents some of these facts and argues that more differentiated notions of anaphora and presupposition may well be more fruitful for further research in the semantics-pragmatics interface. Introduction Since van der Sandt & Geurts (1991), van der Sandt (1992) and Geurts (1995, 1999) have put forward the notion of a fundamental identity of presupposition and anaphora, a fairly universal consensus seems to have developed that this view is basically correct. Assuming that this notion is intended to have empirical consequences I shall refer to this view in the following as the 'Presupposition is Anaphora Hypothesis', for short: the PIA hypothesis. There is surely no doubt that phenomena of presupposition and anaphora are not unrelated. Understanding anaphora quite conventionally as a way of resuming a previously established reference more or less involves the assumption that this reference has actually been established beforehand and in this sense is presupposed. In this rough and general sense anaphora is presuppositional. Conversely, in presupposing that one or the other proposition is true, a relation is established to something that either was said before, follows from something that was said before, or at least could reasonably have been said before; and this relation may in some rough and general sense be called anaphorical. Now Geurts and van der Sandt do not stay at a rough and general level, but turn these relations between anaphora and presupposition into a venerable theory of presupposition that yields certain technical advantages for the treatment of presupposition when it is implemented in (a variety of) Discourse Representation Theory (van der Sandt (1992) and Geurts (1995, 1999)). I shall argue below that despite this technical progress the PIA hypothesis obscures both the notion of presupposition and the notion of anaphora. The Procrustean relationship actually harms our understanding of both phenomena. At the same time, I shall argue, the Hypothesis is empirically wrong with respect to linguistic data. I shall start with the latter.  "
W01-1605 " We describe our experience in developing a discourse-annotated corpus for community-wide use. Working in the framework of Rhetorical Structure Theory, we were able to create a large annotated resource with very high consistency, using a well-defined methodology and protocol. This resource is made publicly available through the Linguistic Data Consortium to enable researchers to develop empirically grounded, discourse-specific applications.  "
W01-1606 " The development of spoken dialogue systems is often limited by the performance of their speech recognition component. The impact of speech recognition errors on dialogue systems is often studied at the global level of task completion. In this paper, we carry an empirical study on the consequences of speech recognition errors on a fully-implemented dialogue prototype, based on a speech acts formalisms. We report the impact of speech recognition errors on speech act identification and discuss how standard control mechanisms can participate to robustness by assisting the user in repairing the consequences of speech recognition errors. Introduction The development of spoken dialogue systems is faced with limitations in speech recognition technologies that make recognition errors a recurring problem for any dialogue system. Several studies have shown little correlation between speech recognition scores and user satisfaction, or the ability to complete the tasks underlying spoken dialogue [Yankelovich et al., 1995] [Dybkjaer et al., 1997], suggesting that a certain level of errors should not prevent spoken dialogue systems from being successful. However, most of the studies on speech recognition errors have concentrated either on parsing incomplete utterances or on global dialogue robustness, i.e. at task completion level [Allen et al., 1996] [Stromback and Jonsson, 1998] [Brandt-Pook et al., 1996]. In this paper, we investigate the impact of speech recognition errors on a fullyimplemented prototype for a task-oriented dialogue system. This system supports a conversational character for Interactive Television and is based on a speech acts formalism. We report a first empirical study on the consequences of speech recognition errors on the identification of speech acts, and the conditions under which the system can be robust to those errors. 1  "
W01-1607 "Bedford, MA 01730 USA {cdoran,aberdeen,laurie,lynette}@mitre.org Abstract While researchers have many intuitions about the differences between humancomputer and human-human interactions, most of these have not previously been subject to empirical scrutiny. This work presents some initial experiments in this direction, with the ultimate goal being to use what we learn to improve computer dialogue systems. Working with data from the air travel domain, we identified a number of striking differences between the human-human and human-computer interactions. "
W01-1608 "<NoAbstract>"
W01-1609 "USA {fry,mginzton,peters,bzack,ponbarry}@csli.stanford.edu Abstract This paper describes an application of state-of-the-art spoken language technology (OAA/Gemini/Nuance) to a new problem domain: engaging students in automated tutorial dialogues in order to evaluate and improve their performance in a training simulator. 1 Introduction Shipboard damage control refers to the task of containing the effects of fire, explosions, hull breaches, flooding, and other critical events that can occur aboard Naval vessels. The high-stakes, high-stress nature of this task, together with limited opportunities for real-life training, make damage control an ideal target for AI-enabled educational technologies like training simulators and tutoring systems. This paper describes the spoken dialogue system we developed for automated critiquing of student performance on a damage control training simulator. The simulator is DCTrain (Bulitko and Wilkins, 1999), an immersive, multimedia training environment for damage control. DC-Trains training scenarios simulate a mixture of physical phenomena (e.g., fire, flooding) and personnel issues (e.g., casualties, communications, standardized procedures). Our current tutoring system is restricted fire damage scenarios only, and in particular to the twelve fire scenarios available in DC-Train version 2.5, but in future versions we plan to support postsession critiques for all of the damage phenomena that will be modeled by DC-Train 4.0: fire, flooding, missile damage, and wall or firemain ruptures. 2 Previous Work Eliciting self-explanation from a student has been shown to be a highly effective tutoring method (Chi et al., 1994). For this reason, a number of automated tutoring systems currently use NLP techniques to engage students in reflective dialogues. Three notable examples are the medical Circsim tutor (Zhou et al., 1999); the Basic Electricity and Electronics (BE&E) tutor (Ros  e et al., 1999); and the computer literacy AutoTutor (WiemerHastings et al., 1999). Our system shares several features with these three tutoring systems: A knowledge base Our system encodes all domain knowledge relevant to supporting intelligent tutoring feedback into a structure called an Expert Session Summary (Section 4). These expert summaries encode causal relationships between events on the ship as well as the proper and improper responses to shipboard crises. Tutoring strategies In our system, as in those above, the flow of dialogue is controlled by (essentially) a finite-state transition network (Fig. 1). An interpretation component In our system, the students speech is recognized and parsed into logical forms (Section 3). A dialogue manager inspects the current dialogue information state to determine how best to incorporate each new utterance into the dialogue (Lemon et al., 2001). Prompt student review of actions Correct students report Prompt for reflection on START END continue\" \"OK, lets event N... Summary of damage main points Review performance students Evaluate reflections Correct students \"You handled this one well\" event 1 of damage Summary Brief summary of session errors Figure 1: Post-session dialogue move graph (simplified) However, an important difference is that the three systems above are entirely textbased, whereas ours is a spoken dialogue system. Our speech interface offers greater naturalness than keyboard-based input. In this respect, our system is similar to cove (Roberts, 2000), a training simulator for conning Navy ships that uses speech to interact with the student. But whereas cove uses short conversational exchanges to coach the student during the simulation, our system engages in extended tutorial dialogues after the simulation has ended. Besides being more natural, spoken language systems are also better suited to multimodal interactions (viz., one can point and click while talking but not while typing). An additional significant difference between our system and a number of other automated tutoring systems is our use of deep processing techniques. While other systems utilize shallow statistical approaches like Latent Semantic Analysis (e.g. AutoTutor), our system utilizes Gemini, a symbolic grammar. This approach enables us to provide precise and reliable meaning representations. "
W01-1610 "Abstract This paper deals with user corrections and aware sites of system errors in the TOOT spoken dialogue system. We rst describe our corpus, and give details on our procedure to label corrections and aware sites. Then, we show that corrections and aware sites exhibit some prosodic and other properties which set them apart from `normal' utterances. It appears that some correction types, such as simple repeats, are more likely to be correctly recognized than other types, such a s p a r a phrases. We also present evidence that system dialogue strategy aects users' choice of correction type, suggesting that strategy-speciic methods of detecting or coaching users on corrections may be useful. Aware sites tend to be shorter than other utterances, and are also more difcult to recognize correctly for the ASR system. "
W01-1611 "<NoAbstract>"
W01-1612 " We present a tool for the annotation of anaphoric and bridging relations in a corpus of written texts. Based on differences as well as similarities between these phenomena, we define an annotation scheme. We then implement the scheme within an annotation tool and demonstrate its use. 1 Int  "
W01-1613 "<NoAbstract>"
W01-1614 " "
W01-1615 "<NoAbstract>"
W01-1616 "<NoAbstract>"
W01-1617 "<NoAbstract>"
W01-1618 "<NoAbstract>"
W01-1619 "<NoAbstract>"
W01-1620 "<NoAbstract>"
W01-1621 "<NoAbstract>"
W01-1622 "A 2 RT, Dept. Language and Speech, Nijmegen University Erasmusplein 1 6525 HT Nijmegen, The Netherlands {janienke.sturm | f.wang | b.cranen}@let.kun.nl Abstract This paper describes a prototype of a multimodal railway information system that was built by extending an existing speech-only system. The purpose of the extensions is to alleviate a number of shortcomings of speech-only interfaces. "
W01-1623 "Abstract This paper addresses recent results on Mandarin spoken dialogues and introduces the collection of a large Mandarin conversational dialogue corpus. In the context of data processing, principles of transcription are proposed and accordingly a transcription tool is specifically developed for Mandarin spoken conversations. Introduction Large speech corpora have become indispensable for current linguistic research and information science applications dealing with spoken data (Gibbon et al. 1997). Concretely, they provide real phonetic data and empirical data-driven knowledge on linguistic features of spoken language. The corpus presented here is composed of conversational dialogues. Conversations contain a considerable variety of linguistic phenomena as well as phonetic-acoustic variations. Furthermore, they open up a wide range of research issues such as dialogue acts, turn-taking, lexical use of spoken language and prosodic use in conversation. From a diachronic point of view, such a large dialogue corpus archives the contemporary daily conversational use of a given language. 1 General Issues on Mandarin Dialogues In the following, issues on Mandarin dialogues relevant to spontaneous dialogue annotation are summarized and discussed. It includes lexical distribution, discourse markers, turn-taking and prosodic characterization. 1.1 Lexical Distribution in Spoken Mandarin Results presented by Tseng (2001) show that speakers of Mandarin adopt some 30 words for building core structures of utterances in conversation, independently of individual speakers. All subjects used these words more than three times. The occurrences of these 30 core words make up about 80% of the overall tokens in conversation. Interestingly but also expected in conversational dialogues, the distribution of token frequency across all subjects is highly symmetric (Tseng 2001). For instance, verbs is located, is, that is, say, want and have were frequently used, so were pronouns s/he, you and I. The negation dont have was a high-frequency word, so were words right, this/these and that/those. Grammatical particles as well as discourse particles were also among the core words. 1.2 Discourse Markers It is now well known that what differentiates written texts from spontaneous speech most is the use of discourse particles. Among the core words, eleven words were discourse particles, or they were used as discourse markers. In the literature, there is still no consistent definition for discourse markers (Hirschberg and Litman 1993). Discourse markers can be defined as follows: elements whose original semantic meaning tends to decrease and their use in spoken discourse becomes more pragmatic and indicative of discourse structuring are discourse markers. In addition to several adverbs and determiners, discourse particles can also be categorized as discourse markers. They are very often observed in Mandarin spoken conversations as mentioned in Tseng (2001) and Clancy et al. (1996). In Tseng (2001), each subject used on average 1.6 discourse particles per turn. This result leads to the consideration, if there is a need to add special categories for discourse particles or particle-like words for spoken Mandarin. Discourse particles were found to have different and specific discourse use in conversation. Namely, there exist discourse particles appearing preferably in turn-beginning position and some other discourse particles may exclusively mark the location of repairs. Regarding the small size of data used in Tseng (2001), it is one of the reasons why the ongoing project is necessary for research of Mandarin spontaneous conversations. 1.3 Taking Turns in Dialogues In spontaneous conversation, turn-taking usually takes place arbitrarily to the extent that every individual interacts differently with the others under different circumstances. Thus, how to annotate overlapping sequences is one of the essential tasks in developing annotation systems. In Mandarin conversation, there are words preferably used in turn-initial position (Tseng 2001, Chui 2000). They normally have their own discourse-related pragmatic function associated with their positioning in utterances. Similarly, how to mark up turn-initial positions is also directly connected with the annotation convention. 1.4 Prosody in Spoken Mandarin Lexical tones are typically characteristic of spoken Mandarin. The interaction of lexical tones and the other prosodic means such as stress and intonation are related to a number of research issues, particularly in conversation. Falling tones may not show falling tendency anymore, when the associated words are used for specific discourse functions such as for indicating hesitation or the beginning of a turn (Tseng 2001). "
W01-1624 "<NoAbstract>"
W01-1625 " This paper presents a study of the effects of syntax and melodic configuration on turntaking in Southern British English. Using dialogue materials, two perception experiments were carried out. In the first, subjects heard dialogue fragments in which syntactic completeness and melodic contour were systematically varied, and were asked whether they expected a subsequent turn exchange or not. In the second, subjects were presented with short speaker exchanges taken from the same material, and asked whether they thought the first speaker had intended to cede the turn or not. The results suggest that syntactic completion or non-completion is the main factor in predicting turn-taking behaviour. Only one melodic contour, the high level tone H* %, appears to operate as a turn holding device, regardless of whether the utterance is grammatically complete or not. The results of this study were found to be similar to those of a study of Dutch turn-taking. I  "
W01-1626 "<NoAbstract>"
W01-1627 "Manchester M13 9PL U.K. mary@cs.man.ac.uk Abstract Dialogue analysis is widely used in oncology for training health professionals in communication skills. Parameters and tagsets have been developed independently of work in natural language processing. In relation to emergent standards in NLP, syntactic tagging is minimal, semantics is domain-specific, pragmatics is comparable, and the analysis of cognitive affect is richly developed. We suggest productive directions for convergence. "
W01-1628 " In this paper we show that interruptions are important elements in the interactive character of discourse and in the resolution of issues of cognitive uncertainty and planning. By representing discourse graphically, we also show that interruptions are part of the local and global coherence that is brought about through the systematic phrase-to-phrase prosodic patterns of discourse. The specific pitch height of the interruption varies with the expression of emotion, signals of attention-getting, and signals of competitiveness. These prosodic forms are potentially usable in spoken dialogue systems to provide intelligent responding systems that are responsive to human motivations in dialogues.  1 Introduction: Interruptions and Dialogue  "
W02-0201 "A {blaylock,james,ferguson}@cs.rochester.edu Abstract Most dialogue architectures are either pipelined or, if agent-based, are restricted to a pipelined flowof-information. The TRIPS dialogue architecture is agent-based and asynchronous, with several layers of information flow. We present this architecture and the synchronization issues we encountered in building a truly distributed, agentbased dialogue architecture. "
W02-0202 " This paper describes the prosodic transcription of a corpus of Hong Kong English and some preliminary findings on the communicative role of intonation in Hong Kong English.  "
W02-0203 "<NoAbstract>"
W02-0204 "<NoAbstract>"
W02-0205 " Recently developed markup tools for dialogue work are quite sophisticated and require considerable knowledge and overhead, but older tools do not support XML standoff markup, the current annotation style of choice. For the DIAG-NLP project we have created a lightweight but modern markup tool that can be configured and used by the working NLP researcher. Introdu  "
W02-0206 " We present the motivation for and design of an experiment to evaluate the usefulness of cross-media cues, phrases such as 'See Figure 1'.  "
W02-0207 " Recent work on natural language processing systems is aimed at more conversational, context-adaptive systems in multiple domains. An important requirement for such a system is the automatic detection of the domain and a domain consistency check of the given speech recognition hypotheses. We report a pilot study addressing these tasks, the underlying data collection and investigate the feasibility of annotating the data reliably by human annotators. 1 Intro  "
W02-0208 "<NoAbstract>"
W02-0209 "<NoAbstract>"
W02-0210 ", Finland {krista|jkuusist}@james.hut.fi Abstract Technological development has made computer interaction more common and also commercially feasible, and the number of interactive systems has grown rapidly. At the same time, the systems should be able to adapt to various situations and various users, so as to provide the most efficient and helpful mode of interaction. The aim of the Interact project is to explore natural human-computer interaction and to develop dialogue models which will allow users to interact with the computer in a natural and robust way. The paper describes the innovative goals of the project and presents ways that the Interact system supports adaptivity on different system design and interaction management levels. "
W02-0211 " The Why-Atlas tutoring system presents students with qualitative physics questions and encourages them to explain their answers via natural language. Although there are inexpensive techniques for analyzing explanations, we claim that better understanding is necessary for use within tutoring systems. In this paper we describe how Why-Atlas creates and utilizes a proof-based representation of student essays. We describe how it creates the proof given the output of sentence-level understanding, how it uses the proofs to give students feedback, some preliminary runtime measures, and the work we are currently doing to derive additional benefits from a proof-based approach for tutoring applications. 1 Introductio  "
W02-0212 " The paper deals with conditional responses of the form Not if c/Yes if c in reply to a question ?q in the context of information-seeking dialogues. A conditional response is triggered if the obtainability of q depends on whether c holds: The response indicates a possible need to find alternative solutions, opening a negotiation in the dialogue. The paper discusses the conditions under which conditional responses are appropriate, and proposes a uniform approach to their generation and interpretation. 1  "
W02-0213 " This paper presents work on using Bayesian networks for the dialogue act recognition module of a dialogue system for Dutch dialogues. The Bayesian networks can be constructed from the data in an annotated dialogue corpus. For two series of experiments using different corpora but the same annotation scheme recognition results are presented and evaluated. 1 Intro  "
W02-0214 "Abstract In humancomputer interaction systems using natural language, the recognition of the topic from users utterances is an important task. We examine two different perspectives to the problem of topic analysis needed for carrying out a successful dialogue. First, we apply selforganized document maps for modeling the broader subject of discourse based on the occurrence of content words in the dialogue context. On a Finnish corpus of 57 dialogues the method is shown to work well for recognizing subjects of longer dialogue segments, whereas for individual utterances the subject recognition history should perhaps be taken into account. Second, we attempt to identify topically relevant words in the utterances and thus locate the old information (topic words) and new information (focus words). For this we define a probabilistic model and compare different methods for model parameter estimation on a corpus of 189 dialogues. Moreover, the utilization of information regarding the position of the word in the utterance is found to improve the results. "
W02-0215 "Abstract In this paper, we give an account of a simple kind of collaborative negotiative dialogue. We also sketch a formalization of this account and discuss its implementation in a dialogue system. "
W02-0216 "We explain dialogue management techniques for collaborative activities with humans, involving multiple concurrent tasks. Conversational context for multiple concurrent activities is represented using a Dialogue Move Tree and an Activity Tree which support multiple interleaved threads of dialogue about different activities and their execution status. We also describe the incremental message selection, aggregation, and generation method employed in the system. "
W02-0217 " We show how Bayesian networks and related probabilistic methods provide an efficient way of capturing the complex balancing of different factors that determine interpretation and generation in dialogue. As a case study, we show how a probabilistic approach can be used to model anaphora resolution in dialogue 1 . 1 Intro  "
W02-0218 " Dialogue Macrogame Theory is designed to enable analysis of particular natural dialogues. Some dialogues can be analyzed with DMT; some cannot. Where it fits, DMT gives a partial technical characterization of the classes of dialogues represented in the analyses. This paper is focused on presenting the elements of the theory. Other presentations are expected to describe its application and validation. This paper introduces Dialogue Macrogame Theory, a method for describing the organization of certain kinds of dialogues. Dialogue Macrogame Theory (DMT) is a successor to a theory sometimes called Dialogue Game Theory, developed in the 1970s and 1980s at USCInformation Sciences Institute (ISI). DMT is able to describe substantially more dialogues than its predecessor, and it identifies kinds of mechanisms not included in the predecessor. DMT is a step toward accounting for the coherence of entire dialogues. In the late 1970s a research team at USCInformation Sciences Institute (ISI) studied natural dialogues with particular interest in applying the results to human-computer interaction. The team produced a series of reports (Mann 1979) (Mann, Carlisle et al. 1977) (Mann, Carlisle et al. 1975), a published paper by Levin and Moore (Levin and Moore 1977) and later another paper, entitled Dialogue Games: Conventions of Human Interaction (Mann 1988). These publications arose out of the study of many natural dialogues. The final theoretical summary was presented in the form of a technical method for representing the short range and long range dialogue structures. Recent work has attempted to apply this set of structures to additional dialogues. This effort has identified a number of deficiencies of the framework, which have led to making a fresh start rather than adjustment. The major structures in DMT are based on intentions which are imputed to dialogue participants. The focus of this paper is on mechanisms. Dialogue Macrogames are defined. Another class of mechanisms, called Unilaterals, is also described. A DMT analysis is presented. The analyzed dialogue is an excerpt (41 turns) of actual dialogue from the Apollo 13 mission, from the emergency period after the explosion. DMT is then related to another dialogue analysis method (Carletta, Isard et al. 1997). DMT is an exercised framework, meaning that it has been applied to dialogues from a diversity of situations. These include various emergency communications, tutoring, administrative interactions, online human computer help, medical interviews, laboratory conversational tasks, courtroom questioning of witnesses and hostage negotiation. The paper reports work in progress, and also indicates likely courses of further development. The central constructs in the prior work were called Dialogue Games, a name inspired by terminology of Wittgenstein (Wittgenstein 1973) but with only a distant family resemblance to his usage. Since the end of that work this term has been used in many other ways that are not technically related to the ISI usage. In addition, the terms dialogue act, conversational act, and conversational game are in wide use (Carlson Philadelphia, July 2002, pp. 129-141. Association for Computational Linguistics. Proceedings of the Third SIGdial Workshop on Discourse and Dialogue, 1983) (Kreutel and Matheson 2001) (Traum 1994; Poesio and Traum 1998; Traum 1999) . To simply resume using the old terminology now would invite confusion and misunderstanding. So this paper introduces a new term, Dialogue Macrogame, which represents some structures that resemble the dialogue games of the predecessor model. In this paper, Dialogue Macrogame may be abbreviated to dialogue game or even game. The term dialogue represents two party immediate interaction. 1 Studies of dialogue from various points of view are numerous. There are dozens of technical fields with the word communication in their names (Craig 1999). Many of them study dialogue. There are structural views, communication views and many others. Linguistics tends to produce structurally oriented studies, but not exclusively. Even restricting attention to studies of dialogue coherence, there are many radically different viewpoints. Conversational Coherence: Form, Structure and Strategy (Craig and Tracy 1983) is a particularly relevant collection, now somewhat dated but representing ideas that persist in the wider literature. In this book, and in the wider literature as well, the distinction between coherence and cohesion is often not made. Studies of coherence are often really about abstract cohesive devices, in the sense of (Halliday and Hasan 1976). Some studies (Ellis 1983; Goldberg 1983) assume that coherence is produced by design, by appropriate use of cohesive devices. In some studies coherence is equated to topic continuity or to the appropriate use of topic shifting devices (Crow 1983; Sigman 1983). In others it is seen as conformity to expressive rules. Grice is often interpreted as believing that conversational coherence is based on rule (or maxim) following (Grice 1975). Still others see coherence as an identifiable outcome of rule governed (social or linguistic) behavior (Goldberg 1983). Hawes defends his views against this idea (Hawes 1983). Some studies of coherence in dialogue assume that people pursue a tacit goal of being coherent, in addition to any other goals, when they interact. It is a benefit which they seek. (Hopper attributes this orientation to interpreters of language, but not producers (Hopper 1983).) Others see coherence as an obligation that is attached to interaction. It is an added duty. (Sanders sees this view as widely accepted, and defends his views against it (Sanders 1983).) Some studies equate coherence with propositional consistency, see (Goldberg 1983) for citations. Others see coherence as a kind of summary impression that is a side effect of understanding an interaction, an understanding that is enabled by the processes that ordinarily govern interaction (Sanders 1983). The (Craig and Tracy 1983) book incorporates an admirable attempt to make the various approaches comparable. All of the chapter authors were given one particular 30 minute dialogue (included in the volume) and told to relate their approaches to it. In a volume summary, apropos of this paper, the editors note that Conversationalists goals must play a central role in any adequate explanation of discourse production and interpretation in conversations. p. 22. Global approaches to coherence, ones that attempt to address entire texts or dialogues, are often associated with some notion of genre or tradition, such as Rummelhardts story grammars (Rumelhart 1975) or Schank and Abelsons scripts (Schank and Abelson 1977). (DMT addresses whole dialogues, but without resemblance to those approaches.) There is more recent progress in many aspects of understanding dialogue. A rich array of formal approaches has been built on the Discourse Representation Theory of Kamp and colleagues (Kamp and Reyle 1993) (Traum 1994). Agency theory, along with various vigorous efforts to develop data annotation methods, are also producing insightful views of natural dialogue. 1 Sometimes DMT structures can be applied to interactions that have more than two participants. Even so, dialogue macrogame theory would have to be significantly augmented in order to become a sound representation scheme for multiparty interaction. Knowing of these complications, we focus on two party interaction as a research tactic. Clearly there is no consensus on the nature of coherence in dialogue. Although comparing views is typically difficult, as researchers we always find some views more credible than others. We may be able to make certain alternatives more distinct by an analogy concerning oral dialogue. What is the status of breathing? Do people breathe in dialogue because they believe it will make the dialogue more beneficial? Or is there a duty to breathe in dialogue? Is breathing simply following a tradition, or an attempt to perform smoothly? Are there rules of dialogue that would, for example, make a dialogue ill-formed if it did not involve breathing? Or is breathing regulated by processes that interact with the speaking processes? DMT is designed following assumptions that most resemble this latter alternative. Instead, we seek to find a set of theories that can jointly account for the coherence of dialogues that arise in different kinds of situations. We hope that the set will be small, but also that the set of theories will be very informative. DMT is designed to be one such theory. Size limitations and incomplete development prevent this paper from describing all of the significant aspects of DMT. We choose to describe only the elements which are part of DMT analysis. In future presentations we expect to describe dialogue analysis methods, framework validation, some form/framework relationships, and relationships to prior methods. We also expect to provide a corpus of analyzed dialogues. This work puts a high value on expanding the coverage of the set of theories mentioned above. This is in effect a personal preference for placing a high value on breadth rather than depth or precision. 3 R  "
W02-0219 "<NoAbstract>"
W02-0220 "<NoAbstract>"
W02-0221 " While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and humanhuman dialogues, their utility is limited by the huge effort involved in handlabelling dialogues with a dialogue act labelling scheme. In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues. We report results for training and testing an automatic classifier to label the information providers utterances in spoken human-computer and human-human dialogues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags. We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora, and the CMU human-human corpus in the travel planning domain. Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available. "
W02-0222 "<NoAbstract>"
W02-0223 "<NoAbstract>"
W02-0224 " This paper reports an exploratory study of the grounding styles of older dyads, namely, the characteristic ways in which they mutually agree to have shared a piece of information in dialogue. On the basis of Traums classification of grounding acts, we conducted an exploratory comparison of dialogue data on older and younger dyads, and found that a fairly clear contrast holds mainly in the types of acknowledgement utterances used by the two groups. We will discuss the implications of this contrast, concerning how some of the negative stereotypes about conversations with older people may arise from this difference in grounding styles.  "
W02-0225 " Dialogue Acts (DAs) which explicitly ensure mutual understanding are frequent in dialogues between cancer patients and health professionals. We present examples, and argue that this arises from the healthcritical nature of these dialogues. 1 Bac  "
W02-0226 " Why do few working spoken dialogue systems make use of dialogue models in their dialogue management? We find out the causes and propose a generic dialogue model. It promises to bridge the gap between practical dialogue management and (pattern-based) dialogue model through integrating interaction patterns with the underling tasks and modeling interaction patterns via utterance groups using a high level construct different from dialogue act. 1 I  "
W02-0301 " We explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition. To make the SVM training with the available largest corpus  the GENIA corpus  tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information. In addition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning. Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy. The proposed new features also contribute to improve the accuracy. We compare our SVMbased recognition system with a system using Maximum Entropy tagging method. 1 Introdu  "
W02-0302 " Current information extraction efforts in the biomedical domain tend to focus on finding entities and facts in structured databases or MEDLINE abstracts. We apply a gene and protein name tagger trained on Medline abstracts (ABGene) to a randomly selected set of full text journal articles in the biomedical domain. We show the effect of adaptations made in response to the greater heterogeneity of full text.  "
W02-0303 " We studied contrast and variability in a corpus of gene names to identify potential heuristics for use in performing entity identification in the molecular biology domain. Based on our findings, we developed heuristics for mapping weakly matching gene names to their official gene names. We then tested these heuristics against a large body of Medline abstracts, and found that using these heuristics can increase recall, with varying levels of precision. Our findings also underscored the importance of good information retrieval and of the ability to disambiguate between genes, proteins, RNA, and a variety of other referents for performing entity identification with high precision.  "
W02-0304 " We propose two internal methods for accenting unknown words, which both learn on a reference set of accented words the contexts of occurrence of the various accented forms of a given letter. One method is adapted from POS tagging, the other is based on finite state transducers. We show experimental results for letter e on the French version of the Medical Subject Headings thesaurus. With the best training set, the tagging method obtains a precision-recall breakeven point of 84.24.4% and the transducer method 83.84.5% (with a baseline at 64%) for the unknown words that contain this letter. A consensus combination of both increases precision to 92.03.7% with a recall of 75%. We perform an error analysis and discuss further steps that might help improve over the current performance. 1 Introduct  "
W02-0305 " This paper describes the basic philosophy and implementation of MPLUS (M+), a robust medical text analysis tool that uses a semantic model based on Bayesian Networks (BNs). BNs provide a concise and useful formalism for representing semantic patterns in medical text, and for recognizing and reasoning over those patterns. BNs are noise-tolerant, and facilitate the training of M+. 1 2 Introduction In the field of medical informatics, computerized tools are being developed that depend on databases of clinical information. These include alerting systems for improved patient care, data mining systems for quality assurance and research, and diagnostic systems for more complex medical decision support. These systems require data that is appropriately structured and coded. Since a large portion of the information stored in patient databases is in the form of free text, manually coding this information in a format accessible to these tools can be time consuming and expensive. In recent years, natural language processing (NLP) methodologies have been studied as a means of automating this task. There have been many projects involving automated medical language analysis, including deciphering pathology reports (Smart and Roux, 1995), physical exam findings (Lin et al., 1991), and radiology reports (Friedman et al., 1994; Ranum, 1989; Koehler, 1998). M+ is the latest in a line of NLP tools developed at LDS Hospital in Salt Lake City, Utah. Its predecessors include SPRUS (Ranum, 1989) and SymText (Koehler, 1998). These tools have been used in the realm of radiology reports, admitting diagnoses (Haug et al., 1997), radiology utilization review (Fiszman, 2002) and syndromic detection (Chapman et al., 2002). Some of the character of these tools derives from common characteristics of radiology reports, their initial target domain. Because of the off-the-cuff nature of radiology dictation, a report will frequently contain text that is telegraphic or otherwise not well formed grammatically. Our desire was not only to take advantage of phrasal structure to discover semantic patterns in text, but also to be able to infer those patterns from lexical and contextual cues when necessary. Most NLP systems capable of semantic analysis employ representational formalisms with ties to classical logic, including semantic grammars (Friedman et al., 1994), unificationbased semantics (Moore, 1989), and description logics (Romacker and Hahn, 2000). M+ and its predecessors employ Bayesian Networks (Pearl, 1988), a methodology outside this tradition. This study discusses the philosophy and implementation of M+, and attempts to show how Bayesian Networks can be useful in medical text analysis. The M+ Semantic Model 2  "
W02-0306 "<NoAbstract>"
W02-0307 " We describe our use of an existing resource, the Mouse Anatomical Nomenclature, to improve a symbolic interface to anatomically-indexed gene expression data. The goal is to reduce user effort in specifying anatomical structures of interest and increase precision and recall. 1 Int  "
W02-0308 "Objectives: To automatically extend downwardsanexistingbiomedicalterminology using a corpus and both lexical andterminologicalknowledge.Methods: Adjectival modifiers are removed from terms extracted from the corpus (three million noun phrases extracted from MEDLINE), and demodified terms are searched for in the terminology (UMLS Metathesaurus,restrictedtodisordersand procedures). A phrase from MEDLINE becomesacandidatetermintheMetathesaurus if the following two requirements are met: 1) a demodified term created fromthisphraseisfoundintheterminologyand2)themodifiersremovedtocreate the demodified term also modify existingtermsfromtheterminology,fora given semantic category. A manual review of asampleofcandidatetermswas performed. Results: Out of the 3 million simple phrases randomly extracted from MEDLINE, 125,000 new terms were identified for inclusion in the UMLS. 83%ofthe1000termsreviewedmanually were associated with a relevant UMLS concept. Discussion: The limitations of thisapproacharediscussed,aswellasadaptationandgeneralizationissues. "
W02-0309 " Document retrieval in languages with a rich and complex morphology  particularly in terms of derivation and (single-word) composition  suffers from serious performance degradation with the stemming-only query-term-to-text-word matching paradigm. We propose an alternative approach in which morphologically complex word forms are segmented into relevant subwords (such as stems, named entities, acronyms), and subwords constitute the basic unit for indexing and retrieval. We evaluate our approach on a large biomedical document collection. 1  "
W02-0310 " We describe the use of clinical data present in the medical record to determine the relevance of research evidence from literature databases. We studied the effect of using automated knowledge approaches as compared to physicians selection of articles, when using a traditional information retrieval system. Three methods were evaluated. The first method identified terms and their semantics and relationships in the patients record to build a map of the record, which was represented in conceptual graph notation. This approach was applied to data in an individuals medical record and used to score citations retrieved using a graph matching algorithm. The second method identified associations between terms in the medical record, assigning them semantic types and weights based on the co-occurrence of these associations in citations of biomedical literature. The method was applied to data in an individuals medical record and used to score citations. The last method combined the first two. The results showed that physicians agreed better with each other than with the automated methods. However, we found a significant positive relation between physicians selection of abstracts and two of the methods. We believe the results encourage the use of clinical data to determine the relevance of medical literature to the care of individual patients.  "
W02-0311 "  Information Extraction (IE), defined as the activity to extract structured knowledge from unstructured text sources, offers new opportunities for the exploitation of biological information contained in the vast amounts of scientific literature. But while IE technology has received increasing attention in the area of molecular biology, there have not been many examples of IE systems successfully deployed in end-user applications. We describe the development of PASTAWeb, a WWWbased interface to the extraction output of PASTA, an IE system that extracts protein structure information from MEDLINE abstracts. Key characteristics of PASTAWeb are the seamless integration of the PASTA extraction results (templates) with WWWbased technology, the dynamic generation of WWW content from static data and the fusion of information extracted from multiple documents. 1 Introduction  (IE), defined as the activity to extract structured knowledge from unstructured text sources, offers new opportunities for the exploitation of biological information contained in the vast amounts of scientific literature. But while IE technology has received increasing attention in the area of molecular biology, there have not been many examples of IE systems successfully deployed in end-user applications. We describe the development of PASTAWeb, a WWWbased interface to the extraction output of PASTA, an IE system that extracts protein structure information from MEDLINE abstracts. Key characteristics of PASTAWeb are the seamless integration of the PASTA extraction results (templates) with WWWbased technology, the dynamic generation of WWW content from static data and the fusion of information extracted from multiple documents. "
W02-0701 " To achieve translation technology that is adequate for speech-to-speech translation (S2S), this paper introduces a new attempt named Corpus-Centered Computation, (abbreviated to C 3 and pronounced c-cube). As opposed to conventional approaches adopted by machine translation systems for written language, C 3 places corpora at the center of the technology. For example, translation knowledge is extracted from corpora, translation quality is gauged by referring to corpora and the corpora themselves are normalized by paraphrasing or filtering. High-quality translation has been demonstrated in the domain of travel conversation, and the prospects of this approach are promising due to the benefits of synergistic effects.  "
W02-0702 " In this paper, we propose a topic detection method using a dialogue history for selecting a scene in the automatic interpretation system (Ikeda et al., 2002). The method uses a k-nearest neighbor method for the algorithm, automatically clusters target topics into smaller topics grouped by similarity, and incorporates dialogue history weighted in terms of time to detect and track topics on spoken phrases. From the evaluation of detection performance using test corpus comprised of realistic spoken dialogue, the method has shown to perform better with clustering incorporated, and combined with time-weighted dialogue history of three sentences, gives detection accuracy of 77.0%.  "
W02-0703 " In this paper, we describe a novel approach to spoken language analysis for translation, which uses a combination of grammar-based phrase-level parsing and automatic classification. The job of the analyzer is to produce a shallow semantic interlingua representation for spoken task-oriented utterances. The goal of our hybrid approach is to provide accurate real-time analyses while improving robustness and portability to new domains and languages.  "
W02-0704 "<NoAbstract>"
W02-0705 " In this paper we compare the performance of two methods for speech translation. One is a statistical dependency transduction model using head transducers, the other a case-based transduction model involving a lexical similarity measure. Examples of translated utterance transcriptions are used in training both models, though the case-based model also uses semantic labels classifying the source utterances. The main conclusion is that while the two methods provide similar translation accuracy under the experimental conditions and accuracy metric used, the statistical dependency transduction method is significantly faster at computing translations. "
W02-0706 "Speech-to-speech translation can be approached using finite state models and several ideas borrowed from automatic speech recognition. The models can be Hidden Markov  Hidden Markov Models for the accoustic part, language models for the source language and finite state transducers for the transfer between the source and target language. A serial architecture would use the Hidden Markov and the language models for recognizing input utterance and the transducer for finding the translation. An integrated architecture, on the other hand, would integrate all the models in a single network where the search process takes place. The output of this search process is the target word sequence associated to the optimal path. In both architectures, HMMs can be trained from a source-language speech corpus, and the translation model can be learned automatically from a parallel text training corpus. The experiments presented here correspond to speech-input translations from Spanish to English and from Italian to English, in applications involving the interaction (by telephone) of a customer with the front-desk of a hotel. "
W02-0707 " This paper evaluates a direct speech translation Method with waveforms using the Inductive Learning method for short conversation. The method is able to work without conventional speech recognition and speech synthesis because syntactic expressions are not needed for translation in the proposed method. We focus only on acoustic characteristics of speech waveforms of source and target languages without obtaining character strings from utterances. This speech translation method can be utilized for any language because the system has no processing dependent on an individual character of a specific language. Therefore, we can utilize the speech of a handicapped person who is not able to be treated by conventional speech recognition systems, because we do not need to segment the speech into phonemes, syllables, or words to realize speech translation. Our method is realized by learning translation rules that have acoustic correspondence between two languages inductively. In this paper, we deal with a translation between Japanese and English. 1 Introduction  "
W02-0708 " In this paper we compare two interlingua representations for speech translation. The basis of this paper is a distributional analysis of the C-star II and Nespole databases tagged with interlingua representations. The C-star II database has been partially re-tagged with the Nespole interlingua, which enables us to make comparisons on the same data with two types of interlinguas and on two types of data (Cstar II and Nespole) with the same interlingua. The distributional information presented in this paper show that the Nespole interlingua maintains the language-independence and simplicity of the C-star II speech-actbased approach, while increasing semantic expressiveness and scalability. "
W02-0709 " In this paper, we propose a novel paradigm for the Chinese-to-English speech-to-speech (S2S) translation, which is interactive under the guidance of dialogue management. In this approach, the input utterance is first pre-processed and then serially translated by the template-based translator and the interlingua based translator. The dialogue management mechanism (DMM) is employed to supervise the interactive analysis for disambiguation of the input. The interaction is led by the system, so the system always acts on its own initiative in the interactive procedure. In this approach, the complicated semantic analysis is not involved. 1  "
W02-0710 "<NoAbstract>"
W02-0711 "<NoAbstract>"
W02-0712 " This paper proposes an automatic interpretation system that integrates freestyle sentence translation and parallel text based translation. Free-style sentence translation accepts natural language sentences and translates them by machine translation. Parallel text based translation provides a proper translation for a sentence in the parallel text by referring to a corresponding translation of the sentence and supplements free-style sentence translation. We developed a prototype of an automatic interpretation system for Japanese overseas travelers with parallel text based translation using 9206 parallel bilingual sentences prepared in task-oriented manner. Evaluation results show that the parallel text based translation covers 72% of typical utterances for overseas travel and the user can easily find an appropriate sentence from a natural utterance for 64% of typical travelers tasks. This indicates that the user can benefit from reliable translation based on parallel text for fundamental utterances necessary for overseas travel. "
W02-0713 " Examples from chat interaction are presented to demonstrate that machine translation of written interaction shares many problems with translation of spoken interaction. The potential for common solutions to the problems is illustrated by describing operations that normalize and tag input before translation. Segmenting utterances into small translation units and processing short turns separately are also motivated using data from chat.  "
W02-0714 "<NoAbstract>"
W02-0715 " We propose a test set selection method to sensitively evaluate the performance of a speech translation system. The proposed method chooses the most sensitive test sentences by removing insensitive sentences iteratively. Experiments are conducted on the ATR-MATRIX speech translation system, developed at ATR Interpreting Telecommunications Research Laboratories. The results show the effectiveness of the proposed method. According to the results, the proposed method can reduce the test set size to less than 40% of the original size while improving evaluation reliability. Introduction The translation paired comparison method precisely measures the capability of a speech translation system. In this method, native speakers compare a systems translation and the translations, made by examinees who have various TOEIC scores. The method requires two human costs: the data collection of examinees translations and the comparison by native speakers. In this paper, we propose a test set size reduction method that reduces the number of test set utterances. The method chooses the most sensitive test utterances by removing the most insensitive utterances iteratively. In section 2, the translation paired comparison method is described. Section 3 explains the proposed method. In section 4, evaluation results for ATR-MATRIX are shown. Section 5 discusses the experimental results. In section 6, we state our conclusions. Translation paired comparison method The translation paired comparison method (Sugaya, 2000) is an effective evaluation method for precisely measuring the capability of a speech translation system. In this section, a description of the method is given.  "
W02-0716 " General characteristics of a pragmatic metric for the production evaluation of speech-to-speech translations are discussed. While these characteristics constrain the space of allowable metrics, infinite definition space remains from which to select and define any particular metric. The recommended characteistics are drawn from the authors experience as primary developer of a text-based translation quality metric used in a production environment. The primary contribution is that of strict category ordering and two meta-rules that reduce the variance in assignment of errors to categories. 1 Intro  "
W02-0717 "<NoAbstract>"
W02-2101 " We explore how machine learning can be employed to learn rulesets for the traditional modules of content planning and surface realization. Our approach takes advantage of semantically annotated corpora to induce preferences for content planning and constraints on realizations of these plans. We applied this methodology to an annotated corpus of indicative summaries to derive constraint rules that can assist in generating summaries for new, unseen material. 1 Int  "
W02-2102 " The parsing community has long recognized the importance of lexicalized models of syntax. By contrast, these models do not appear to have had an impact on the statistical NLG community. To prove their importance in NLG, we show that a lexicalized model of syntax improves the performance of a statistical text compression system, and show results that suggest it would also improve the performances of an MT application and a pure natural language generation system. 1 Intro  "
W02-2103 " This paper describes a general-purpose sentence generation system that can achieve both broad scale coverage and high quality while aiming to be suitable for a variety of generation tasks. We measure the coverage and correctness empirically using a section of the Penn Treebank corpus as a test set. We also describe novel features that help make the generator flexible and easier to use for a variety of tasks. To our knowledge, this is the first empirical measurement of coverage reported in the literature, and the highest reported measurements of correctness. 1 Introdu  "
W02-2104 " In this paper we describe two parallel experiments on the integration of machine learning (ML) methods into the Spanish and Japanese rule-based sentence realization modules developed at Microsoft Research. The paper explores the use of decision trees (DT) for the lexical selection of the copula in Spanish and the insertion of a locative postposition in Japanese. We show that it is possible to machine-learn the contexts for these two non-trivial linguistic phenomena with high accuracy. 1 Introdu  "
W02-2105 " We present an overview of Amalgam, a sentence realization module that combines machine-learned and knowledgeengineered components to produce natural language sentences from logical form inputs. We describe the decomposition of the task of sentence realization into a linguistically informed series of steps, with particular attention to the linguistic issues that arise in German. We report on the evaluation of component steps and of the overall system. 1 Int  "
W02-2106 "<NoAbstract>"
W02-2107 " In spoken language applications such as conversation systems where not only the speech waveforms but also the content of the speech (the text) need to be generated automatically, a Concept-to-Speech (CTS) system is needed. In this paper, we address several issues on designing a speech corpus to facilitate an instance-based integrated CTS framework. Both the instance-based CTS generation approach and the corpus design process have not been addressed systematicallyinpreviousresearches.  "
W02-2108 " We present a framework for handling emotional variations in a speech-based natural language generation system for use in the MRE virtual training environment. The system is a first step toward addressing issues in emotion-based modeling of verbal communicative behavior. We cast the problem of emotion-based generation as a distance minimization task, in which the system chooses between multiple valid realizations for a given input based on the emotional distance of each realization from the speakers attitude toward that input. We discuss evaluations of the system and future work that includes modeling personality and empathy within the same framework.  "
W02-2109 " Developing an embodied conversational agent that is able to exhibit a human-like behavior while communicating with other virtual or human agents requires enriching a typical NLG architecture. The purpose of this paper is to describe our efforts in this direction and to illustrate our approach to the generation of an Agent that shows a personality, a social intelligence and is able to react emotionally to events occurring in the environment, consistently with her goals and with the context in which the conversation takes place.  "
W02-2110 " Recent work on evaluation of spoken dialogue systems indicates that better algorithms are needed for the presentation of complex information in speech. Current dialogue systems often rely on presenting sets of options and their attributes sequentially. This places a large memory burden on users, who have to remember complex trade-offs between multiple options and their attributes. To address these problems we build on previous work using multiattribute decision theory to devise speech-planning algorithms that present usertailored summaries, comparisons and recommendations that allow users to focus on critical differences between options and their attributes. We discuss the differences between speech and text planning that result from the particular demands of the speech situation. 1 Introductio  "
W02-2111 " This paper explores the feasibility of implementing an evolutionary algorithm for text structuring using the heuristic of continuity as a fitness function, chosen over other more complicated metrics of text coherence. Using MCGONAGALL (Manurung et al., 2000) as our experimental platform, we show that by employing an elitist strategy for stochastic search it is possible to quickly reach the global optimum of minimal violations of continuity. 1 Backgroun  "
W02-2112 " In this paper, we present a novel technique to learn a tree-like structure for a content planner from an aligned corpus of semantic inputs and corresponding, humanproduced, outputs. We apply a stochastic search mechanism with a two-level fitness function. As a first stage, we use high level order constraints to quickly discard unpromising planners. As a second stage, alignments between regenerated text and human output are employed. We evaluate our approach by using the existing symbolic planner in our system as a gold standard, obtaining a 66% improvement over a random baseline in just 20 generations of genetic search. 1 Introductio  "
W02-2113 "<NoAbstract>"
W02-2114 " Aggregation is typically treated in NLG as a local optimization measure, and methods exist only for building conjoined expressions with 'and'. In contrast to that, solutions to logical problems are characterized by regularly occurring commonalities, including complete subsets of possible value combinations and alternatives. In order to address constellations of this kind, we extend current aggregation techniques, envisioning high degrees of condensation. In particular, we define novel constructs that can express sets of propositions with highly regular variations on slot values concisely, including special forms of disjunctions. Our methods enable the generation of expressions with semantically complex operators, such as 'vice-versa' and 'each', and they support various aspects in interpreting solutions produced by formal systems, such as highlighting commonalities among and differences across solution parts, supporting the inspection of dependencies and variations, and the discovery of flaws in problem specifications. 1 Introductio  "
W02-2115 " This paper argues that algorithms for the generation of referring expressions should aim to make it easy for hearers and readers to find the referent of the expressions that are generated. To illustrate this claim, an algorithm is described for the generation of expressions that refer across a hierarchically ordered domain, and which takes search effort into account by adding logically redundant information. To support the ideas underlying the algorithm, a psycholinguistic experiment is presented that confirms readers preference for the generated, logically redundant expressions over non-redundant alternatives. 1 Intro  "
W02-2116 " We added a sentence planning component to an existing ITS that teaches students how to troubleshoot mechanical systems. We evaluated the original version of the system and the enhanced one via a user study in which we collected performance, learning and usability metrics. We show that on the whole the enhanced system is better than the original one. We discuss how to use the binomial cumulative distribution to assess cumulative effects. 1  "
W02-2117 " This paper presents an evaluation of the instructional text generated by Isolde, an authoring tool for technical writers that automates the production of procedural on-line help. The evaluation compares the effectiveness of the instructional text produced by Isolde with that of professionally authored instructions, such as MS Word Help. The results suggest that the documentation produced by Isolde is of comparable quality to similar texts found in commercial manuals. 1 I  "
W02-2118 "<NoAbstract>"
W02-2119 " Giving an adequate general definition of the input to natural language generation (NLG), and hence to NLG itself, is a notoriously difficult problem, practically, theoretically and even methodologically. In this paper, we describe our recent experiences of implementing an NLG component of a larger question-answering system, and trying to understand and resolve some of these problems in this context. We examine the whole lifetime of an answer, from internal data structure to final expression as text, and look for characteristics of the processing which might help identify where NLG really begins. On the basis of this analysis we propose some principles to inform discussions on the scope of NLG as an individuated enterprise. "
W02-2120 " Previous research has shown that certain discourse conditions are necessary for the felicitous use of non-canonical syntactic forms like topicalization, left-dislocation, and clefts. However, the distribution of these forms does not correlate one-to-one with the presence of these conditions, and a system that generates these statisticallyrare forms based only on these conditions will overgenerate. Instead, a generation algorithm must be based on additional communicative goals that can be achieved through the use of these forms. Based on a corpus study, I present three types of communicative goals that speakers achieve through the use of non-canonical syntax. 1 Intro  "
W02-2121 "<NoAbstract>"
W02-2122 " We describe some of the complications involved in expressing the technique of induction when automatically generating textual versions of formal mathematical proofs produced by a theorem proving system, and describe our approach to this problem. The pervasiveness of induction within mathematical proofs makes its effective generation vital to readable proof texts. Our focus is on planning texts involving induction. Our efforts are driven by a corpus of human-produced proof texts, incorporating both regularities within this corpus and the formal structure of induction into coherent text plans. 1  "
W02-2123 " In this paper we present an architecture for generating texts that vary in the emphasis put on conciseness, readability and the marking of particularly salient items. We abandon the traditional pipeline architecture, and use an integrated approach which makes the search for an optimum text explicit, taking into account both inter-sentential and intra-sentential features. We describe a context sensitive scoring system which can relate surface properties to a deeper representational level. We show how this approach can be used in generating paragraph length texts, optimised against various criteria. 1 Int  "
W02-2124 "<NoAbstract>"
W03-1301 "Machine-learning based entity extraction requires a large corpus of annotated training to achieve acceptable results. However, the cost of expert annotation of relevant data, coupled with issues of inter-annotator variability, makes it expensive and time-consuming to create the necessary corpora. We report here on a simple method for the automatic creation of large quantities of imperfect training data for a biological entity (gene or protein) extraction system. We used resources available in the FlyBase model organism database; these resources include a curated lists of genes and the articles from which the entries were drawn, together a synonym lexicon. We applied simple pattern matching to identify gene names in the associated abstracts and filtered these entities using the list of curated entries for the article. This process created a data set that could be used to train a simple Hidden Markov Model (HMM) entity tagger. The results from the HMM tagger were comparable to those reported by other groups (F-measure of 0.75). This method has the advantage of being rapidly transferable to new domains that have similar existing resources. "
W03-1302 "Abstract This paper describes techniques for unsupervised word sense disambiguation of English and German medical documents using UMLS. We present both monolingual techniques which rely only on the structure of UMLS, and bilingual techniques which also rely on the availability of parallel corpora. The best results are obtained using relations between terms given by UMLS, a method which achieves 74% precision, 66% coverage for English and 79% precision, 73% coverage for German on evaluation corpora and over 83% coverage over the whole corpus. The success of this technique for German shows that a lexical resource giving relations between concepts used to index an English document collection can be used for high quality disambiguation in another language. "
W03-1303 " In this paper we present an approach to term classification based on verb complementation patterns. The complementation patterns have been automatically learnt by combining information found in a corpus and an ontology, both belonging to the biomedical domain. The learning process is unsupervised and has been implemented as an iterative reasoning procedure based on a partial order relation induced by the domain-specific ontology. First, term recognition was performed by both looking up the dictionary of terms listed in the ontology and applying the C/NC-value method. Subsequently, domain-specific verbs were automatically identified in the corpus. Finally, the classes of terms typically selected as arguments for the considered verbs were induced from the corpus and the ontology. This information was used to classify newly recognised terms. The precision of the classification method reached 64%. 1 Introductio  "
W03-1304 " Named entity recognition is a fundamental task in biological relationship mining. This paper employs protein collocates extracted from a biological corpus to enhance the performance of protein name recognizers. Yapex and KeX are taken as examples. The precision of Yapex is increased from 70.90% to 81.94% at the low expense of recall rate (i.e., only decrease 2.39%) when collocates are incorporated. We also integrate the results proposed by Yapex and KeX, and employs collocates to filter the merged results. Because the candidates suggested by these two systems may be inconsistent, i.e., overlap in partial, one of them is considered as a basis. The experiments show that Yapex-based integration is better than KeX-based integration.  "
W03-1305 " Using SVMs for named entity recognition, we are often confronted with the multi-class problem. Larger as the number of classes is, more severe the multiclass problem is. Especially, one-vs-rest method is apt to drop the performance by generating severe unbalanced class distribution. In this study, to tackle the problem, we take a two-phase named entity recognition method based on SVMs and dictionary; at the first phase, we try to identify each entity by a SVM classifier and post-process the identified entities by a simple dictionary look-up; at the second phase, we try to classify the semantic class of the identified entity by SVMs. By dividing the task into two subtasks, i.e. the entity identification and the semantic classification, the unbalanced class distribution problem can be alleviated. Furthermore, we can select the features relevant to each task and take an alternative classification method according to the task. The experimental results on the GENIA corpus show that the proposed method is effective not only in the reduction of training cost but also in performance improvement: the identification performance is about 79.9(F  = 1), the semantic classification accuracy is about 66.5(F  = 1). "
W03-1306 "Dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches. However, dictionary based approaches have two serious problems: (1) a large number of false recognitions mainly caused by short names. (2) low recall due to spelling variation. In this paper, we tackle the former problem by using a machine learning method to filter out false positives. We also present an approximate string searching method to alleviate the latter problem. Experimental results using the GENIA corpus show that the filtering using a naive Bayes classifier greatly improves precision with slight loss of recall, resulting in a much better F-score. "
W03-1307 " In this paper, we explore how to adapt a general Hidden Markov Model-based named entity recognizer effectively to biomedical domain. We integrate various features, including simple deterministic features, morphological features, POS features and semantic trigger features, to capture various evidences especially for biomedical named entity and evaluate their contributions. We also present a simple algorithm to solve the abbreviation problem and a rule-based method to deal with the cascaded phenomena in biomedical domain. Our experiments on GENIA V3.0 and GENIA V1.1 achieve the 66.1 and 62.5 F-measure respectively, which outperform the previous best published results by 8.1 F-measure when using the same training and testing data. 1  "
W03-1308 " Support Vector Machines have achieved state of the art performance in several classification tasks. In this article we apply them to the identification and semantic annotation of scientific and technical terminology in the domain of molecular biology. This illustrates the extensibility of the traditional named entity task to special domains with extensive terminologies such as those in medicine and related disciplines. We illustrate SVMs capabilities using a sample of 100 journal abstracts texts taken from the {human, blood cell, transcription factor} domain of MEDLINE. Approximately 3400 terms are annotated and the model performs at about 74% F-score on cross-validation tests. A detailed analysis based on empirical evidence shows the contribution of various feature sets to performance. "
W03-1309 " We explore the use of morphological analysis as preprocessing for protein name tagging. Our method finds protein names by chunking based on a morpheme, the smallest unit determined by the morphological analysis. This helps to recognize the exact boundaries of protein names. Moreover, our morphological analyzer can deal with compounds. This offers a simple way to adapt name descriptions from biomedical resources for language processing. Using GENIA corpus 3.01, our method attains f-score of 70 points for protein molecule names, and 75 points for protein names including molecules, families and domains. 1 Int  "
W03-1310 " We describe our work in progress on natural language analysis in medical questionanswering in the context of a broader medical text-retrieval project. We analyze the limitations in the medical domain of the technologies that have been developed for general question-answering systems, and describe an alternative approach whose organizing principle is the identification of semantic roles in both question and answer texts that correspond to the fields of PICO format. 1 Motiv  "
W03-1311 " Natural language processing (NLP) is critical for improvement of the healthcare process because it has the potential to encode the vast amount of clinical data in textual patient reports. Many clinical applications require coded data to function appropriately, such as decision support and quality assurance applications. However, in order to be applicable in the clinical domain, performance of the NLP systems must be adequate. A valuable clinical application is the detection of infectious diseases, such as surveillance of healthcare-associated pneumonia in newborns (e.g. neonates) because it produces significant rates of morbidity and mortality, and manual surveillance of respiratory infection in these patients is a challenge. Studies have already demonstrated that automated surveillance using NLP tools is a useful adjunct to manual clinical management, and is an effective tool for infection control practitioners. This paper presents a study aimed at evaluating the feasibility of an NLP-based electronic clinical monitoring system to identify healthcare-associated pneumonia in neonates. We estimated sensitivity, specificity, and positive predictive value by comparing the detection with clinicians judgments and our results demonstrated that the automated method was indeed feasible. Sensitivity (recall) was 87.5%, and specificity (true negative rates) was 94.1%. "
W03-1312 "<NoAbstract>"
W03-1313 " It is well known that standardising the annotation of language resources significantly raises their potential, as it enables re-use and spurs the development of common technologies. Despite the fact that increasingly complex linguistic information is being added to biomedical texts, no standard solutions have so far been proposed for their encoding. This paper describes a standardised XML tagset (DTD) for annotated biomedical corpora and other resources, which is based on the Text Encoding Initiative Guidelines P4, a general and parameterisable standard for encoding language resources. We ground the discussion in the encoding of the GENIA corpus, which currently contains 2,000 abstracts taken from the MEDLINE database, and has almost 100,000 hand-annotated terms marked for semantic class from the accompanying ontology. The paper introduces GENIA and TEI and implements a TEI parametrisation and conversion for the GENIA corpus. A number of aspects of biomedical language are discussed, such as complex tokenisation, prevalence of contractions and complex terms, and the linkage and encoding of ontologies. "
W03-1314 " Objectives: To explore the phenomenon of adjectival modification in biomedical discourse across two genres: the biomedical literature and patient records. Methods: Adjectival modifiers are removed from phrases extracted from two corpora (three million noun phrases extracted from MEDLINE, on the one hand, and clinical notes from the Mayo Clinic, on the other). The original phrases, the adjectives extracted, and the resulting demodified phrases are compared across the two corpora after normalization. Quantitative comparisons (frequency of occurrence) are performed on the whole domain. Qualitative comparisons are performed on the two subdomains (disorders and procedures). Results: Although the average number of adjectives per phrase is equivalent in the two corpora (1.4), there are more adjective types in MAYO than in MEDLINE for disorders and procedures. For disorder phrases, the 38% of adjective types common to the two corpora account for 85% of the occurrences. The predominance of adjectives in one corpus is analyzed. Discussion: Potential applications of this approach are discussed, namely terminology acquisition, information retrieval, and genre characterization. "
W03-1315 " The classification task is an integral part of named entity extraction. This task has not received much attention in the biomedical setting, partly due to the fact that protein name recognition has been the focus of the majority of the work in this field. We study this problem and focus on different sources of information that can be utilized for the classification task and investigate the extent of their contributions for classification in this domain. However, while developing a specific algorithm for the classification of the names is not our main focus, we make use of some simple techniques to investigate different sources of information and verify our intuitions about their usefulness. 1 Intro  "
W03-1801 "<NoAbstract>"
W03-1802 " Term extraction systems are now an integral part of the compiling of specialized dictionaries and updating of term banks. In this paper, we present a term detection approach that discovers, structures, and infers conceptual relationships between terms for French. Conceptual relationships are deduced from specific types of term variations, morphological and syntagmatic, and are expressed through lexical functions. The linguistic precision of the conceptual structuring through morphological variations is of 95 %. 1 Introdu  "
W03-1803 " The translation of compound nouns is a major issue in machine translation due to their frequency of occurrence and high productivity. Various shallow methods have been proposed to translate compound nouns, notable amongst which are memory-based machine translation and word-to-word compositional machine translation. This paper describes the results of a feasibility study on the ability of these methods to translate Japanese and English noun-noun compounds. 1 Introduct  "
W03-1804 " This paper describes an implementation to compute positional ngram statistics (i.e. Frequency and Mutual Expectation) based on masks, suffix array-based data structures and multidimensional arrays. Positional ngrams are ordered sequences of words that represent continuous or discontinuous substrings of a corpus. In particular, the positional ngram model has shown successful results for the extraction of discontinuous collocations from large corpora. However, its computation is heavy. For instance, 4.299.742 positional ngrams (n=1..7) can be generated from a 100.000-word size corpus in a seven-word size window context. In comparison, only 700.000 ngrams would be computed for the classical ngram model. It is clear that huge efforts need to be made to process positional ngram statistics in reasonable time and space. Our solution shows O(h(F) N log N) time complexity where N is the corpus size and h(F) a function of the window context. 1 Int  "
W03-1805 "<NoAbstract>"
W03-1806 " This paper describes an original hybrid system that extracts multiword unit candidates from part-of-speech tagged corpora. While classical hybrid systems manually define local part-ofspeech patterns that lead to the identification of well-known multiword units (mainly compound nouns), our solution automatically identifies relevant syntactical patterns from the corpus. Word statistics are then combined with the endogenously acquired linguistic information in order to extract the most relevant sequences of words. As a result, (1) human intervention is avoided providing total flexibility of use of the system and (2) different multiword units like phrasal verbs, adverbial locutions and prepositional locutions may be identified. The system has been tested on the Brown Corpus leading to encouraging results. "
W03-1807 " Automatic extraction of multiword expressions (MWE) presents a tough challenge for the NLP community and corpus linguistics. Although various statistically driven or knowledge-based approaches have been proposed and tested, efficient MWE extraction still remains an unsolved issue. In this paper, we present our research work in which we tested approaching the MWE issue using a semantic field annotator. We use an English semantic tagger (USAS) developed at Lancaster University to identify multiword units which depict single semantic concepts. The Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) built in Sheffield was used to evaluate our approach. In our evaluation, this approach extracted a total of 4,195 MWE candidates, of which, after manual checking, 3,792 were accepted as valid MWEs, producing a precision of 90.39% and an estimated recall of 39.38%. Of the accepted MWEs, 68.22% or 2,587 are low frequency terms, occurring only once or twice in the corpus. These results show that our approach provides a practical solution to MWE extraction. 1 Introduction  "
W03-1808 " In this paper we investigate the phenomenon of verb-particle constructions, discussing their characteristics and their availability for use with NLP systems. We concentrate in particular on the coverage provided by some electronic resources. Given the constantly growing number of verb-particle combinations, possible ways of extending the coverage of the available resources are investigated, taking into account regular patterns found in some productive combinations of verbs and particles. We discuss, in particular, the use of Levins (1993) classes of verbs as a means to obtain productive verb-particle constructions, and discuss the issues involved in adopting such an approach. 1 Introdu  "
W03-1809 "<NoAbstract>"
W03-1810 "<NoAbstract>"
W03-1811 "<NoAbstract>"
W03-1812 "This paper presents a constructioninspecific model of multiword expression decomposability based on latent semantic analysis. We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability. We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in "
W03-1901 " This paper describes the outline of a linguistic annotation framework under development by ISO TC37 SC WG1-1. This international standard provides an architecture for the creation, annotation, and manipulation of linguistic resources and processing software. The goal is to provide maximum flexibility for encoders and annotators, while at the same time enabling interchange and re-use of annotated linguistic resources. We describe here the outline of the standard for the purposes of enabling annotators to begin to explore how their schemes may map into the framework. 1 Introdu  "
W03-1902 " This work presents the data model we adopted for annotating coreference. Our data model includes different levels of annotation, such as part-of-speech, syntax and discourse. We compare our encoding schemes to the abstract XML encoding being proposed as standard. We also present our tool for coreference resolution that handles our data model. 1 Int  "
W03-1903 " We propose an ontology-based framework for linguistic annotation of written texts. We argue that linguistic annotation can be actually considered a special case of semantic annotation with regard to an ontology such as pursued within the context of the Semantic Web. Furthermore, we present CREAM, a semantic annotation framework, as well as its concrete implementation OntoMat and show how they can be used for the purpose of linguistic annotation. We demonstrate the value of our framework by applying it to the annotation of anaphoric relations in written texts. 1 Introduct  "
W03-1904 " This paper describes FrameNet (Lowe et al., 1997; Baker et al., 1998; Fillmore et al., 2002), an online lexical resource for English based on the principles of frame semantics (Fillmore, 1977a; Fillmore, 1982; Fillmore and Atkins, 1992), and considers the FrameNet database in reference to the proposed ISO model for linguistic annotation of language resources (ISO TC37 SC4 )(ISO, 2002; Ide and Romary, 2001b). We provide a data category specification for frame semantics and FrameNet annotations in an RDF-based language. More specifically, we provide a DAML+OIL markup for lexical units, defined as a relation between a lemma and a semantic frame, and frame-to-frame relations, namely Inheritance and Subframes. The paper includes simple examples of FrameNet annotated sentences in an XML/RDF format that references the project-specific data category specification. 2 Frame Sem  "
W03-1905 " In this paper we describe the overall model for MILE lexical entries and provide an instantiation of the model in RDF/OWL. This work has been done with an eye toward the goal of creating a web-based registry of lexical data categories and enabling the description of lexical information by establishing relations among them, and/or using predefined objects that may reside at various locations on the web. It is also assumed that using OWL specifications to enhance specifications of the ontology of lexical objects will eventually enable the exploitation of inferencing engines to retrieve and possibly create lexical information on the fly, as suited to particular contexts. As such, the model and RDF instantiation provided here are in line with the goals of ISO TC37 SC4, and should be fully mappable to the proposed pivot. 1  "
W03-2101 "Keywords: graphics, understanding, discourse, plan-based models Information graphics that appear in newspapers and magazines generally have a message that the viewer is intended to recognize. This paper argues that understanding such information graphics is a discourse-level problem. In particular, it requires assimilating information from multiple knowledge sources to recognize the intended message of the graphic, just as recognizing intention in text does. Moreover, when an article is composed of text and graphics, the intended message of the information graphic (its discourse intention) must be integrated into the discourse structure of the surrounding text and contributes to the overall discourse intention of the article. This paper describes how we extend plan-based techniques that have been used for understanding traditional discourse to the understanding of information graphics. This work is part of a project to develop an interactive natural language system that provides sight-impaired users with access to information graphics. "
W03-2102 " In this paper we present a detailed scheme for annotating expressions of opinions, beliefs, emotions, sentiment and speculation (private states) in the news and other discourse. We explore inter-annotator agreement for individual private state expressions, and show that these low-level annotations are useful for producing higher-level subjective sentence annotations. "
W03-2103 " This paper describes the results of corpus and experimental investigation into the factors that affect the way clarification questions in dialogue are interpreted, and the way they are responded to. We present some results from an investigation using the BNC which show some general correlations between clarification request type, likelihood of answering, answer type and distance between question and answer. We then describe a new experimental technique for integrating manipulations into text-based synchronous dialogue, and give more specific results concerning the effect of word category and level of grounding on interpretation and response type. 1 Introductio  "
W03-2104 " We describe an information-theoretic argument-interpretation mechanism embedded in an interactive system. Our mechanism receives as input an argument entered through a web interface. It generates candidate interpretations in terms of its underlying knowledge representation  a Bayesian network, and applies the Minimum Message Length principle to select the best candidate. The results of our preliminary evaluations are encouraging, with the system generally producing plausible interpretations of users arguments. Keywords: Minimum message length, discourse interpretation, Bayesian networks. 1 Introd  "
W03-2105 " This paper proposes a general theory of conversational inferences which distinguishes two kinds of inferences: the hard way and the easy way. The theory accounts for a wider range of non-literal utterance meanings than Gricean and relevance theories and is motivated by the types of utterances in which the hearer fails to infer nonliteral meanings. 1  "
W03-2106 " We present an overview of a comprehensive formal theory of the interpretation of sentential fragments, which has as components an empirically validated taxonomy, an analysis of the syntax and compositional semantics of fragments, and a formalisation of their contextual interpretation. We also briefly describe an implementation of this theory, and quantify the potential practical use of handling fragments in dialogue systems. 1 I  "
W03-2107 "We realize a telephone-based collaborative natural language dialogue system. Since natural language involves very various expressions, a large number of VoiceXML scripts need to be prepared to handle all possible input patterns. We realize flexible dialogue management for various user utterances by generating VoiceXML scripts dynamically. Moreover, we address appropriate user modeling in order to generate cooperative responses to each user. Specifically, we set up three dimensions of user models: skill level to the system, knowledge level on the target domain and the degree of hastiness. The models are automatically derived by decision tree learning using real dialogue data collected by the system. Experimental evaluation shows that the cooperative responses adapted to individual users serve as good guidance for novice users without increasing the dialogue duration for skilled users. Keywords: spoken dialogue system, user model, VoiceXML, cooperative responses, dialogue strategy "
W03-2108 " Chat system has gained popularity as a tool for real-time conversation. However, standard chat systems have problems due to lack of timing information. To tackle this problem, we have built a system which has the following functions: 1) function of making typing state visible; 2) floor holding function at the start of typing. The evaluation results show that the sys-tem with each new function significantly increases the number of turns, which indicates the effectiveness of the new functions for smooth communication. The survey results showed that the system with the function of making typing state visible significantly different from that without them concerning 1) easiness of adjusting the timing of utterances and smoothness of conversations, and 2) easiness of using the system. 1 Introduct  "
W03-2109 " Recently the technology for speech recognition and language processing for spoken dialogue systems has been improved, and speech recognition systems and dialogue systems have been developed to the extent of practical usage. In order to become more practical, not only those fundamental techniques but also the techniques of portability and expansibility should be developed. In our previous research, we demonstrated the portability of the speech recognition module to a developed portal spoken dialogue system. And we constructed a dialogue strategy design tool of dialogue script for controlling the dialogue strategy. In this paper, we report a highly portable interpreter using a commercial electronic dictionary. We apply this to three domains/tasks and confirm the validity of the interpreter for each domain/task. Keywords: spoken dialogue system, robust interpreter, portability, dialogue script 1 Introduction  "
W03-2110 "<NoAbstract>"
W03-2111 " This paper describes a method for bootstrapping a Reinforcement Learningbased dialog manager using a Wizard-ofOz trial. The state space and action set are discovered through the annotation, and an initial policy is generated using a Supervised Learning algorithm. The method is tested and shown to create an initial policy which performs significantly better and with less effort than a handcrafted policy, and can be generated using a small number of dialogs. 1 Intro  "
W03-2112 " This paper proposes a new framework for a spoken dialogue system based on dialogue examples between human subjects and the Wizard of OZ (WOZ) system. Using this framework and a model of information retrieval dialogue, a spoken dialogue system for retrieving shop information while driving in a car has been designed. The system refers to the dialogue examples to find an example that is suitable for generating a query or a reply. The authors have also constructed a large-scale dialogue database using a WOZ system, which enables efficient collection of dialogue examples. 1 Introduction  "
W03-2113 " In this paper we present implications for development of dialogue systems, based on an evaluation of the system BIRDQUEST which combine dialogue interaction with information extraction. A number of issues detected during the evaluation concerning primarily dialogue management, and domain knowledge representation and use are presented and discussed. 1 Int  "
W03-2114 " We present evidence for the importance of low-level phenomena in dialogue interaction and use this to motivate a multi-layered approach to dialogue processing. We describe an architecture that separates content-level communicative processes from interaction-level phenomena (such as feedback, grounding, turn-management), and provide details of specific implementations of a number of such phenomena. 1 Intro  "
W03-2115 " In this paper we present a contextual extension to ONTOSCORE, a system for scoring sets of concepts on the basis of an ontology. We apply the contextually enhanced system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence. We conducted several annotation experiments and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses (both with and without discourse context). We also showed, that annotators can reliably identify the overall best hypothesis from a given n-best list. While the original ONTOSCORE system correctly assigns the highest score to 84.06% of the corpus, the inclusion of the conceptual context increases the number of correct classifications to yield 86.76%, given a baseline of 63.91% in both cases. 1 Introduction  "
W03-2116 " This paper reports a tool which assists the user in annotating a video corpus and enables the user to search for a semantic or pragmatic structure in a GDA tagged corpus. An XQL format is allowed for search patterns as well as a plain phrase. This tool is capable of generating a GDA timestamped corpus from a video file manually. It will be publicly available for academic purposes. 1 Introdu  "
W03-2117 " We present a light-weight tool for the annotation of linguistic data on multiple levels. It is based on the simplification of annotations to sets of markables having attributes and standing in certain relations to each other. We describe the main features of the tool, emphasizing its simplicity, customizability and versatility. 1 Introduct  "
W03-2118 " We describe a coding scheme for machine translation of spoken taskoriented dialogue. The coding scheme covers two levels of speaker intention  domain independent speech acts and domain dependent domain actions. Our database contains over 14,000 tagged sentences in English, Italian, and German. We argue that domain actions, and not speech acts, are the relevant discourse unit for improving translation quality. We also show that, although domain actions are domain specific, the approach scales up to large domains without an explosion of domain actions and can be coded with high inter-coder reliability across research sites. Furthermore, although the number of domain actions is on the order of ten times the number of speech acts, sparseness is not a problem for the training of classifiers for identifying the domain action. We describe our work on developing high accuracy speech act and domain action classifiers, which is the core of the source language analysis module of our NESPOLE machine translation system. 1 Introduction  "
W03-2119 " This paper reports an investigation of the turn-taking functions of drawings in graphical communication. Based on the examination of dialogue data collected that involve collaborative drawing interactions, as well as spoken dialogue interactions, in a joint problem solving task, we found that drawing turns, in which the drawer presents a piece of information to her partner through drawing, had almost the same turn-keeping effects as speech turns.  "
W03-2120 " Annotation of discourse phenomena is a notoriously difficult task which cannot be carried out without the help of annotation tools. In this paper we present a Perspicuous and Adjustable Links Annotator (PALinkA), a tool successfully used in several of our projects. We also briefly describe three types of discourse annotations applied using the tool.  "
W03-2121 " This paper describes a dialogue management system in which an attempt is made to factor out a declarative theory of context updates in dialogue from a procedural theory of generating and interpreting utterances in dialogue. 1 Bac  "
W03-2122 "<NoAbstract>"
W03-2123 " The DIPPER architecture is a collection of software agents for prototyping spoken dialogue systems. Implemented on top of the Open Agent Architecture (OAA), it comprises agents for speech input and output, dialogue management, and further supporting agents. We define a formal syntax and semantics for the DIPPER information state update language. The language is independent of particular programming languages, and incorporates procedural attachments for access to external resources using OAA. 1 Intro  "
W03-2124 " A key challenge for users and designers of spoken language systems is determining the form of the commands that the system can recognize. Using more than 60 hours of interactions, we quantitatively analyze the acquisition of system vocabulary by novice users. We contrast the longitudinal performance of long-term novice users with both expert system developers and guest users. We find that novice users successfully learn the form of system requests, achieving a significant decrease in ill-formed utterances. However, the working vocabulary on which novice users converge is significantly smaller than that of expert users, and their rate of speech recognition errors remains higher. Finally, we observe that only 50% of each users small vocabulary is shared with any other, indicating the importance of the flexibility of a conversational interface that allows users to converge to their own preferred vocabulary. Keyword s Spoken Language System; NoviceExpert; Lexical Entrainment "
W03-2125 " We present a demonstration of a prototype system aimed at providing support with procedural tasks for astronauts on board the International Space Station. Current functionality includes navigation within the procedure, previewing steps, requesting a list of images or a particular image, recording voice notes and spoken alarms, setting parameters such as audio volume. Dialogue capabilities include handling spoken corrections for an entire dialogue move, reestablishing context in response to a user request, responding to user barge-in, and help on demand. The current system has been partially reimplemented for better efficiency and in response to feedback from astronauts and astronaut training personnel. Added features include visual and spoken step previewing, and spoken correction of dialogue moves. The intention is to introduce the system into astronaut training as a prelude to flight on board the International Space Station. 1 Introductio  "
W03-2126 " This paper presents results of using belief functions to rank the list of candidate information provided in a noisy dialogue input. The information under consideration is the intended task to be performed and the information provided for the completion of the task. As an example, we use the task of information access in a multi-domain dialogue system. Currently, the system contains knowledge of ten different domains. Callers calling in are greeted with an open-ended How may I help you? prompt (Thomson and Wisowaty, 1999; Chu-Carroll and Carpenter, 1999; Gorin et al., 1997). After receiving a reply from the caller, we extract word evidences from the recognized utterances. By using transferable belief model (TBM), we in turn determine the task that the caller intends to perform as well as any information provided. "
W03-2127 " Communication behaviour is affected by emotion. Here we discuss how dialogue is affected by participants emotion and how expressions of emotion are manifested in its content. Keywords: Dialogue, Emotions, Annotation  "
W04-0201 "<NoAbstract>"
W04-0202 " In this paper, we present a preliminary version of COOPML, a language designed for annotating cooperative discourse. We investigate the different linguistic marks that identify and characterize the different forms of cooperativity found in written texts from FAQs, Forums and emails. 1 Wha  "
W04-0203 " Like speakers of any natural language, speakers of English potentially have many different word orders in which to encode a single meaning. One key factor in speakers use of certain non-canonical word orders in English is their ability to contribute information about syntactic and semantic discourse relations. Explicit annotation of discourse relations is a difficult and subjective task. In order to measure the correlations between different word orders and various discourse relations, this project utilizes a model in which discourse relations are approximated using a set of lower-level linguistic features, which are more easily and reliably annotated than discourse relations themselves. The featural model provides statistical evidence for the claim that speakers use non-canonicals to communicate information about discourse structure. 1 Introdu  "
W04-0204 " In this paper we describe the (annotation) tools underlying two automatic techniques to analyse the meaning and use of backward causal connectives in large Dutch newspaper corpora. With the help of these techniques, Latent Semantic Analysis and Thematic Text Analysis, the contexts of more than 14,000 connectives were studied. We will focus here on the methods of analysis and on the fairly straightforward (annotation) tools needed to perform the semantic analyses, i.e. POS-tagging, lemmatisation and a thesaurus-like thematic dictionary. 1 Int  "
W04-0205 "<NoAbstract>"
W04-0206 " We present discourse-level annotation of newspaper texts in German and English, as part of an ongoing project aimed at investigating information structure from a cross-linguistic perspective. Rather than annotating some specific notion of information structure, we propose a theory-neutral annotation of basic features at the levels of syntax, prosody and discourse, using treebank data as a starting point. Our discourse-level annotation scheme covers properties of discourse referents (e.g., semantic sort, delimitation, quantification, familiarity status) and anaphoric links (coreference and bridging). We illustrate what investigations this data serves and discuss some integration issues involved in combining different levels of stand-off annotations, created by using different tools. 1 Int  "
W04-0207 " Most research on automated categorization of documents has concentrated on the assignment of one or many categories to a whole text. However, new applications, e.g. in the area of the Semantic Web, require a richer and more fine-grained annotation of documents, such as detailed thematic information about the parts of a document. Hence we investigate the automatic categorization of text segments of scientific articles with XML markup into 16 topic types from a text type structure schema. A corpus of 47 linguistic articles was provided with XML markup on different annotation layers representing text type structure, logical document structure, and grammatical categories. Six different feature extraction strategies were applied to this corpus and combined in various parametrizations in different classifiers. The aim was to explore the contribution of each type of information, in particular the logical structure features, to the classification accuracy. The results suggest that some of the topic types of our hierarchy are successfully learnable, while the features from the logical structure layer had no particular impact on the results. "
W04-0208 " Getting a machine to understand human narratives has been a classic challenge for NLP and AI. This paper proposes a new representation for the temporal structure of narratives. The representation is parsimonious, using temporal relations as surrogates for discourse relations. The narrative models, called Temporal Discourse Models, are treestructured, where nodes include abstract events interpreted as pairs of time points and where the dominance relation is expressed by temporal inclusion. Annotation examples and challenges are discussed, along with a report on progress to date in creating annotated corpora. 1 Introduction Getting a machine to understand human narratives has been a classic challenge for NLP and AI. Central to all narratives is the notion of time and the unfolding of events. When we understand a story, in addition to understanding other aspects such as plot, characters, goals, etc., we are able to understand the order of happening of events. A given text may have multiple stories; when we understand such a text, we are able to tease apart these distinct stories. Thus, understanding the story from a text involves building a global model of the sequences of events in the text, as well as the structure of nested stories. We refer to such models as Temporal Discourse Models (TDMs). Currently, while we have informal descriptions of the structure of narratives, e.g., (Bell 1999), we lack a precise understanding of this aspect of discourse. What sorts of structural configurations are observed? What formal characteristics do they have? For syntactic processing of natural languages, we have, arguably, answers to similar questions. However, for discourse, we have hardly begun to ask the questions. One of the problems here is that most of the information about narrative structure is implicit in the text. Thus, while linguistic information in the form of tense, aspect, temporal adverbials and discourse markers is often present, people use commonsense knowledge to fill in information. Consider a simple discourse: Yesterday Holly was running a marathon when she twisted her ankle. David had pushed her. Here, aspectual information indicates that the twisting occurred during the running, while tense suggests that the pushing occurs before the twisting. Commonsense knowledge also suggests that the pushing caused the twisting. We can see that even for interpreting such relatively simple discourses, a system might require a variety of sources of linguistic knowledge, including knowledge of tense, aspect, temporal adverbials, discourse relations, as well as background knowledge. Of course, other inferences are clearly possible, e.g., that the running stopped after the twisting, but when viewed as defaults, these latter inferences seem to be more easily violated. The need for commonsense inferences has motivated computational approaches that are domainspecific, using hand-coded knowledge (e.g., Asher and Lascarides 2003, Hitzeman et al. 1995). A number of theories have postulated the existence of various discourse relations that relate elements in the text to produce a global model of discourse, e.g., (Mann and Thompson 1988), (Hobbs 1985), (Hovy 1990) and others. In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses. In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993). Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic. This is due to the fact that discourse markers may be absent (i.e., implicit) or ambiguous; but more importantly, because in many cases the precise nature of these discourse relations is unclear. Although (Marcu et In addition to T1, we also have the temporal ordering constraints C1: {Eb < Ec, Ec < Ea, Ea < Ed}. These are represented separately from the tree. A TDM is thus a pairing of tree structures and temporal constraints. More precisely, a Temporal Discourse Model for a text is a pair , where T is a rooted, unordered, directed tree with nodes N = E  A, where E is the set of events mentioned in the text and A is a set of abstract events, and a parent-child ordering relation,  (temporal inclusion). A non-leaf node can be textually mentioned or abstract. Nodes also have a set of atomic-valued features. Note that the tree is temporally unordered left to right. C is a set of temporal ordering constraints using the ordering relation, < (temporal precedence) as well as (for states, clarified below) minimal restri  "
W04-0209 "<NoAbstract>"
W04-0210 " The GNOME corpus was created to study the discourse and semantic properties of discourse entities that affect their realization and interpretation, and particularly salience. We discuss what information was annotated and the methods we followed. 1 Introduction The GNOME corpus was created to study the aspects of discourse that appear to affect generation, especially salience (Pearson et al., 2000; Poesio and Di Eugenio, 2001; Poesio and Nissim, 2001; Poesio et al., 2004b). Particular attention was paid to the factors affecting the generation of pronouns (Pearson et al., 2000; Henschel et al., 2000), demonstratives (Poesio and Nygren-Modjeska, To appear) possessives (Poesio and Nissim, 2001) and definites in general (Poesio, 2004a). These results, and the annotated corpus, were used in the development of both symbolic and statistical natural language generation algorithms for sentence planning (Poesio, 2000a; Henschel et al., 2000; Cheng et al., 2001), aggregation (Cheng, 2001) and text planning (Karamanis, 2003). The empirical side of the project involved both psychological experiments and corpus annotation, based on a scheme based on the MATE proposals, as well as on a detailed annotation manual (Poesio, 2000b), the reliability of whose instructions was tested by extensive experiments (Poesio, 2000a). More recently, the corpus has also been used to develop and evaluate anaphora resolution systems, with a special focus on the resolution of bridging references (Poesio, 2003; Poesio and Alexandrov-Kabadjov, 2004; Poesio et al., 2004a) Although the results of the studies using the GNOME corpus mentioned above have been published in a number of papers, and although a detailed annotation manual was written and has been available on the Web for a few years (Poesio, 2000b), none of the previously published papers discusses in detail the goals of the annotation and the methodology that was f  "
W04-0211 " 1 In this paper, we describe how the LIDAS System (Linguistic Discourse Analysis System), a discourse parser built as an implementation of the Unified Linguistic Discourse Model (U-LDM) uses information from sentential syntax and semantics along with lexical semantic information to build the Open Right Discourse Parse Tree (DPT) that serves as a representation of the structure of the discourse (Polanyi et al., 2004; Thione 2004a,b). More specifically, we discuss how discourse segmentation, sentence-level discourse parsing, and text-level discourse parsing depend on the relationship between sentential syntax and discourse. Specific discourse rules that use syntactic information are used to identify possible attachment points and attachment relations for each Basic Discourse Unit to the DPT. 1 Introduct  "
W04-0212 " The Penn Discourse TreeBank (PDTB) is a new resource built on top of the Penn Wall Street Journal corpus, in which discourse connectives are annotated along with their arguments. Its use of standoff annotation allows integration with a stand-off version of the Penn TreeBank (syntactic structure) and PropBank (verbs and their arguments), which adds value for both linguistic discovery and discourse modeling. Here we describe the PDTB and some experiments in linguistic discovery based on the PDTB alone, as well as on the linked PTB and PDTB corpora. 1 Intro  "
W04-0213 "Abstract A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. "
W04-0214 " We describe a method for annotating spoken dialog corpora using both automatic and manual annotation. Our semi-automated method for corpus development results in a corpus combining rich semantics, discourse information and reference annotation, and allows us to explore issues relating these. 1 Intro  "
W04-0215 " In this paper, we introduce LiveTree, a core component of LIDAS, the Linguistic Discourse Analysis System for automatic discourse parsing with the Unified Linguistic Discourse Model (U-LDM) (X et al, 2004). LiveTree is an integrated workbench for supervised and unsupervised creation, storage and manipulation of the discourse structure of text documents under the U-L DM. The LiveTree environment provides tools for manual and automatic U-LDM segmentation and discourse parsing. Document management, grammar testing, manipulation of discourse structures and creation and editing of discourse relations are also supported. "
W04-0301 "Abstract The goal of this paper is to explore some consequences of the dichotomy between competence and performance from the point of view of incrementality. We introduce a TAGbased formalism that encodes a strong notion of incrementality directly into the operations of the formal system. A left-associative operation is used to build a lexicon of extended elementary trees. Extended elementary trees allow derivations in which a single fully connected structure is mantained through the course of a leftto-right word-by-word derivation. In the paper, we describe the consequences of this view for semantic interpretation, and we also evaluate some of the computational consequences of enlarging the lexicon in this way. "
W04-0302 " This paper proposes a method for evaluating the validity of partial parse trees constructed in incremental parsing. Our method is based on stochastic incremental parsing, and it incrementally evaluates the validity for each partial parse tree on a wordby-word basis. In our method, incremental parser returns partial parse trees at the point where the validity for the partial parse tree becomes greater than a threshold. Our technique is effective for improving the accuracy of incremental parsing. 1 Intro  "
W04-0303 " This talk will present several issues related to incremental (left-to-right) beam-search parsing of natural language using generative or discriminative models, either individually or in combination. The first part of the talk will provide background in incremental top-down and (selective) left-corner beamsearch parsing algorithms, and in stochastic models for such derivation strategies. Next, the relative benefits and drawbacks of generative and discriminative models with respect to heuristic pruning and search will be discussed. A range of methods for using multiple models during incremental parsing will be detailed. Finally, we will discuss the potential for effective use of fast, finite-state processing, e.g. partof-speech tagging, to reduce the parsing search space without accuracy loss. POS-tagging is shown to improve efficiency by as much as 20-25 percent with the same accuracy, largely due to the treatment of unknown words. In contrast, an islands-of-certainty approach, which quickly annotates labeled bracketing over low-ambiguity word sequences, is shown to provide little or no efficiency gain over the existing beam-search. The basic parsing approach that will be described in this talk is stochastic incremental top-down parsing, using a beam-search to prune the search space. Grammar induction occurs from an annotated treebank, and non-local features are extracted from each derivation to enrich the stochastic model. Left-corner grammar and tree transforms can be applied to the treebank or the induced grammar, either fully or selectively, to change the derivation order while retaining the same underlying parsing algorithm. This approach has been shown to be accurate, relatively efficient, and robust using both generative and discriminative models (Roark, 2001; Roark, 2004; Collins and Roark, 2004). The key to effective beam-search parsing is comparability of analyses when the pruning is done. If two competing parses are at different points in their respective derivations, e.g. one is near the end of the derivation and another is near the beginning, then it will be difficult to evaluate which of the two is likely to result in a better parse. With a generative model, comparability can be accomplished by the use of a look-ahead statistic, which estimates the amount of probability mass required to extend a given derivation to include the word(s) in the look-ahead. Every step in the derivation decreases the probability of the derivation, but also takes the derivation one step closer to attaching to the look-ahead. For good parses, the look-ahead statistic should increase with each step of the derivation, ensuring a certain degree of comparability among competing parses with the same look-ahead. Beam-search parsing using an unnormalized discriminative model, as in Collins and Roark (2004), requires a slightly different search strategy than the original generative model described in Roark (2001; 2004). This alternate search strategy is closer to the approach taken in Costa et al. (2001; 2003), in that it enumerates a set of possible ways of attaching the next word before evaluating with the model. This ensures comparability for models that do not have the sort of behavior described above for the generative models, rendering look-ahead statistics difficult to estimate. This approach is effective, although somewhat less so than when a look-ahead statistic is used. A generative parsing model can be used on its own, and it was shown in Collins and Roark (2004) that a discriminative parsing model can be used on its own. Most discriminative parsing approaches, e.g. (Johnson et al., 1999; Collins, 2000; Collins and Duffy, 2002), are re-ranking approaches, in which another model (typically a generative model) presents a relatively small set of candidates, which are then re-scored using a second, discriminatively trained model. There are other ways to combine a generative and discriminative model apart from waiting for the former to provide a set of completed candidates to the latter. For example, the scores can be used simultaneously; or the generative model can present candidates to the discriminative model at intermediate points in the string, rather than simply at the end. We discuss these options and their potential benefits. Finally, we discuss and present a preliminary evaluation of the use of rapid finite-state tagging to reduce the parsing search space, as was done in (Ratnaparkhi, 1997; Ratnaparkhi, 1999). When the parsing algorithm is integrated with model training, such efficiency improvements can be particularly important. POS-tagging using a simple bi-tag model improved parsing efficiency by nearly 25 percent without a loss in accuracy, when 1.2 tags per word were produced on average by the tagger. Producing a single tag sequence for each string resulted in further speedups, but at the loss of 1-2 points of accuracy. We show that much, but not all, of the speedup from POS-tagging is due to more constrained tagging of unknown words. In a second set of trials, we make use of what we are calling syntactic collocations, i.e. collocations that are (nearly) unambiguously associated with a particular syntactic configuration. For example, a chain of auxiliaries in English will always combine in a particular syntactic configuration, modulo noise in the annotation. In our approach, the labeled bracketing spanning the sub-string is treated as a tag for the sequence. A simple, finite-state method for finding such collocations, and an efficient longest match algorithm for labeling strings will be presented. The labeled-bracketing tags are integrated with the parse search as follows: when a derivation reaches the first word of such a collocation, the remaining words are attached in the given configuration. This has the effect of extending the look-ahead beyond the collocation, as well as potentially reducing the amount of search required to extend the derivations to include the words in the collocation. However, while POS-tagging improved efficiency, we find that using syntactic collocations does not, indicating that islands-of-certainty approaches are not what is needed from shallow processing; rather genuine dis-ambiguation of the sort provided by the POS-tagger. "
W04-0304 " We present a general architecture for incremental interaction between modules in a speech-tointention continuous understanding dialogue system. This architecture is then instantiated in the form of an incremental parser which receives suitability feedback on NP constituents from a reference resolution module. Oracle results indicate that perfect NP suitability judgments can provide a labelled-bracket error reduction of as much as 42% and an efficiency improvement of 30%. Preliminary experiments in which the parser incorporates feedback judgments based on the set of referents found in the discourse context achieve a maximum error reduction of 9.3% and efficiency gain of 4.6%. The parser is also able to incrementally instantiate the semantics of underspecified pronouns based on matches from the discourse context. These results suggest that the architecture holds promise as a platform for incremental parsing supporting continuous understanding. 1 Introductio  "
W04-0305 "Abstract To support incremental interpretation, any model of human sentence processing must not only process the sentence incrementally, it must to some degree restrict the number of analyses which it produces for any sentence prefix. Deterministic parsing takes the extreme position that there can only be one analysis for any sentence prefix. Experiments with an incremental statistical parser show that performance is severely degraded when the search for the most probable parse is pruned to only the most probable analysis after each prefix. One method which has been extensively used to address the difficulty of deterministic parsing is lookahead, where information about a bounded number of subsequent words is used to decide which analyses to pursue. We simulate the effects of lookahead by summing probabilities over possible parses for the lookahead words and using this sum to choose which parse to pursue. We find that a large improvement is achieved with one word lookahead, but that more lookahead results in relatively small additional improvements. This suggests that one word lookahead is sufficient, but that other modifications to our left-corner parsing model could make deterministic parsing more effective. "
W04-0306 " We define a new learning task, minimum average lookahead grammar induction, with strong potential implications for incremental parsing in NLP and cognitive models. Our thesis is that a suitable learning bias for grammar induction is to minimize the degree of lookahead required, on the underlying tenet that language evolution drove grammars to be efficiently parsable in incremental fashion. The input to the task is an unannotated corpus, plus a nondeterministic constraining grammar that serves as an abstract model of environmental constraints confirming or rejecting potential parses. The constraining grammar typically allows ambiguity and is itself poorly suited for an incremental parsing model, since it gives rise to a high degree of nondeterminism in parsing. The learning task, then, is to induce a deterministic LR (k) grammar under which it is possible to incrementally construct one of the correct parses for each sentence in the corpus, such that the average degree of lookahead needed to do so is minimized. This is a significantly more difficult optimization problem than merely compiling LR (k) grammars, since k is not specified in advance. Clearly, na  ve approaches to this optimization can easily be computationally infeasible. However, by making combined use of GLR ancestor tables and incremental LR table construction methods, we obtain an O(n 3 + 2 m ) greedy approximation algorithm for this task that is quite efficient in practice. "
W04-0307 "<NoAbstract>"
W04-0308 "Abstract Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework. However, we also show that it is possible to minimize the number of structures that require nonincremental processing by choosing an optimal parsing algorithm. This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text. When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9%. "
W04-0309 " When an incremental parser gets the next word, its expectations about upcoming grammatical structures can change. When a word greatly constrains these grammatical expectations, uncertainty is reduced. This elimination of possibilities constitutes information processing work. Formalizing this notion of information processing work yields a complexity metric that predicts human repetition accuracy scores across a systematic class of linguistic phenomena, the Accessibility Hierarchy of relativizable grammatical relations. 1 Introductio  "
W04-0310 "Abstract It is a well-known intuition that human sentence understanding works in an incremental fashion, with a seemingly constant update of the interpretation through the left-to-right processing of a string. Such intuitions are backed up by experimental evidence dating from at least as far back as Marslen-Wilson (1973), showing that under many circumstances, interpretations are indeed updated very quickly. From a parsing point of view it is interesting to consider the structure-building processes that might underlie incremental interpretation what kinds of partial structures are built during sentence processing, and with what timecourse? In this talk I will give an overview of the stateof-the-art of experimental psycholinguistic research, paying particular attention to the timecourse of structure-building. The discussion will focus on a new line of research (some as yet unpublished) in which syntactic phenomena such as binding relations (e.g., Sturt, 2003) and unbounded dependencies (e.g., Aoshima, Phillips, & Weinberg, in press) are exploited to make a very direct test of the availability of syntactic structure over time. The experimental research will be viewed from the perspective of a space of computational models, which make different predictions about time-course of structure building. One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized Left Corner Parsing (Demers, 1977), algorithms can be characterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its righthand side. Another relevant dimension is represented by the type of grammar formalism that is assumed. For example, with bottom-up parsing algorithms, the degree to which structurebuilding is delayed in right-branching structures depends heavily on whether we employ a traditional phrase-structure formalism with rigid constituency, or a cateogorial formalism with flexible constituency (e.g., Steedman, 2000). I will argue that the evidence is incompatible with models which predict systematic delays in the construction of syntactic structure. In particular, I will argue against both head-driven strategies (e.g., Mulders, 2002), and purely bottom-up parsing strategies, even when flexible constituency is employed. Instead, I will argue that to capture the data in the most parsimonious way, we should turn our attention to those models in which a fully connected syntactic structure is maintained throughout the processing of a string. "
W04-0311 "Abstract The inherent robustness of a system might be an important prerequisite for an incremental parsing model to the effect that grammaticality requirements on full sentences may be suspended or allowed to be violated transiently. However, we present additional means that allow the grammarian to model prefix-analyses by altering a grammar for non-incremental parsing in a controlled way. This is done by introducing underspecified dependency edges that model the expected relation between already seen and yet unseen words during parsing. Thus the basic framework of weighted constraint dependency parsing is extended by the notion of dynamic dependency parsing. "
W04-0312 " Standard grammar formalisms are defined without reflection of the incremental and serial nature of language processing, and incrementality must therefore be reflected by independently defined parsing and/or generation techniques. We argue that this leads to a poor setup for modelling dialogue, with its rich speaker-hearer interaction, and instead propose context-based parsing and generation models defined in terms of an inherently incremental grammar formalism (Dynamic Syntax), which allow a straightforward model of otherwise problematic dialogue phenomena such as shared utterances, ellipsis and alignment. 1 Introductio  "
W04-0401 " We propose a statistical measure for the degree of acceptability of light verb constructions, such as take a walk, based on their linguistic properties. Our measure shows good correlations with human ratings on unseen test data. Moreover, we find that our measure correlates more strongly when the potential complements of the construction (such as walk, stroll, or run) are separated into semantically similar classes. Our analysis demonstrates the systematic nature of the semi-productivity of these constructions. 1 Lig  "
W04-0402 " Some particular classes of lexical paraphrases such as verb alteration and compound noun decomposition can be handled by a handful of general rules and lexical semantic knowledge. In this paper, we attempt to capture the regularity underlying these classes of paraphrases, focusing on the paraphrasing of Japanese light-verb constructions (LVCs). We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements. We also propose a refinement of an existing LCS dictionary. Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases. 1 Introduct  "
W04-0404 " We present a method for compositionally translating noun-noun (NN) compounds, using a word-level bilingual dictionary and syntactic templates for candidate generation, and corpus and dictionary statistics for selection. We propose a support vector learning-based method employing target language corpus and bilingual dictionary data, and evaluate it over a English   Japanese machine translation task. We show the proposed method to be superior to previous methods and also robust over low-frequency NN compounds. 1 I  "
W04-0405 "<NoAbstract>"
W04-0406 " This paper describes an algorithm that can be used to improve the quality of multiword expressions extracted from documents. We measure multiword expression quality by the usefulness of a multiword expression in helping ontologists build knowledge maps that allow users to search a large document corpus. Our stopword based algorithm takes ngrams extracted from documents, and cleans them up to make them more suitable for building knowledge maps. Running our algorithm on large corpora of documents has shown that it helps to increase the percentage of useful terms from 40% to 70%  with an eight-fold improvement observed in some cases. 1 Introduc  "
W04-0407 " This paper describes the representation of Basque Multiword Lexical Units and the automatic processing of Multiword Expressions. After discussing and stating which kind of multiword expressions we consider to be processed at the current stage of the work, we present the representation schema of the corresponding lexical units in a generalpurpose lexical database. Due to its expressive power, the schema can deal not only with fixed expressions but also with morphosyntactically flexible constructions. It also allows us to lemmatize word combinations as a unit and yet to parse the components individually if necessary. Moreover, we describe HABIL, a tool for the automatic processing of these expressions, and we give some evaluation results. This work must be placed in a general framework of written Basque processing tools, which currently ranges from the tokenization and segmentation of single words up to the syntactic tagging of general texts. 1  "
W04-0408 " We propose to model multiword expressions as dependency subgraphs, and realize this idea in the grammar formalism of Extensible Dependency Grammar (XDG). We extend XDG to lexicalize dependency subgraphs, and show how to compile them into simple lexical entries, amenable to parsing and generation with the existing XDG constraint solver. 1 Introduct  "
W04-0409 " This paper describes a multi-word expression processor for preprocessing Turkish text for various language engineering applications. In addition to the fairly standard set of lexicalized collocations and multi-word expressions such as named-entities, Turkish uses a quite wide range of semi-lexicalized and non-lexicalized collocations. After an overview of relevant aspects of Turkish, we present a description of the multi-word expressions we handle. We then summarize the computational setting in which we employ a series of components for tokenization, morphological analysis, and multi-word expression extraction. We finally present results from runs over a large corpus and a small gold-standard corpus. 1 I  "
W04-0410 " This paper presents on-going research on the building of an electronic dictionary of frozen sentences of European Portuguese. It will focus on the problems arising from the description of their formal variation in view of natural language processing 1 I  "
W04-0411 " Multiword Expressions present a challenge for language technology, given their flexible nature. Each type of multiword expression has its own characteristics, and providing a uniform lexical encoding for them is a difficult task to undertake. Nonetheless, in this paper we present an architecture for the lexical encoding of these expressions in a database, that takes into account their flexibility. This encoding extends in a straightforward manner the one required for simplex (single) words, and maximises the information contained for them in the description of multiwords. 1 Introduct  "
W04-0412 "Abstract The growing amount of textual information available electronically has increased the need for high performance retrieval. The use of phrases was long seen as a natural way to improve retrieval performance over the common document models that ignore the sequential aspect of word occurrences in documents, considering them as bags of words. However, both statistical and syntactical phrases showed disappointing results for large document collections. In this paper we present a recent type of multi-word expressions in the form of Maximal Frequent Sequences (Ahonen-Myka, 1999). Mined phrases rather than statistical or syntactical phrases, their main strengths are to form a very compact index and to account for the sequentiality and adjacency of meaningful word co-occurrences, by allowing for a gap between words. We introduce a method for using these phrases in information retrieval and present our experiments. They show a clear improvement over the well-known technique of extracting frequent word pairs. "
W04-1201 " In this paper, we present a named entity recognition system in the biomedical domain, called PowerBioNE. In order to deal with the special phenomena in the biomedical domain, various evidential features are proposed and integrated through a Hidden Markov Model (HMM). In addition, a Support Vector Machine (SVM) plus sigmoid is proposed to resolve the data sparseness problem in our system. Finally, we present two post-processing modules to deal with the cascaded entity name and abbreviation phenomena. Evaluation shows that our system achieves the F-measure of 69.1 and 71.2 on the 23 classes of GENIA V1.1 and V3.0 respectively. In particular, our system achieves the F-measure of 77.8 on the protein class of GENIA V3.0. It shows that our system outperforms the best published system on GENIA V1.1 and V3.0.  "
W04-1202 " The aim of this study is to investigate the relationships between citations and the scientific argumentation found in the abstract. We extracted citation lists from a set of 3200 full-text papers originating from a narrow domain. In parallel, we recovered the corresponding MEDLINE records for analysis of the argumentative moves. Our argumentative model is founded on four classes: PURPOSE, METHODS, RESULTS, and CONCLUSION. A Bayesian classifier trained on explicitly structured MEDLINE abstracts generates these argumentative categories. The categories are used to generate four different argumentative indexes. A fifth index contains the complete abstract, together with the title and the list of Medical Subject Headings (MeSH) terms. To appraise the relationship of the moves to the citations, the citation lists were used as the criteria for determining relatedness of articles, establishing a benchmark. Our results show that the average precision of queries with the PURPOSE and CONCLUSION features is the highest, while the precision of the RESULTS and METHODS features was relatively low. A linear weighting combination of the moves is proposed, which significantly improves retrieval of related articles.  "
W04-1203 "Abstract In this paper, we present an evaluation of the Link Grammar parser on a corpus consisting of sentences describing protein-protein interactions. We introduce the notion of an interaction subgraph, which is the subgraph of a dependency graph expressing a protein-protein interaction. We measure the performance of the parser for recovery of dependencies, fully correct linkages and interaction subgraphs. We analyze the causes of parser failure and report specific causes of error, and identify potential modifications to the grammar to address the identified issues. We also report and discuss the effect of an extension to the dictionary of the parser. "
W04-1204 " Although there have been many research projects to extract protein pathways, most such information still exists only in the scientific literature, usually written in natural languages and defying data mining efforts. We present a novel and robust approach for extracting protein-protein interactions from the literature. Our method uses a dynamic programming algorithm to compute distinguishing patterns by aligning relevant sentences and key verbs that describe protein interactions. A matching algorithm is designed to extract the interactions between proteins. Equipped only with a protein name dictionary, our system achieves a recall rate of about 80.0% and a precision rate of about 80.5%. 1 Int  "
W04-1205 " Information extraction (IE) in the biomedical domain is now regarded as an essential technique for the dynamic management of factual information contained in archived journal articles and abstract collections. We aim to provide a technique serving as a basis for pinpointing and organizing factual information related to experimental results. In this paper, we enhance the idea proposed in (Mizuta and Collier, 2004); annotating articles in terms of rhetorical zones with shallow nesting. We give a qualitative analysis of the zone identification (ZI) process in biology articles. Specifically, we illustrate the linguistic and other features of each zone based on our investigation of articles selected from four major online journals. We also discuss controversial cases and nested zones, and ZI using multiple features. In doing so, we provide a stronger theoretical and practical support for our framework toward automatic ZI. 1  "
W04-1206 "Abstract The tagging of biological entities, and in particular gene and protein names, is an essential step in the analysis of textual information in Molecular Biology and Biomedicine. The problem is harder than was originally thought because of the highly dynamic nature of the research area, in which new genes and their functions are constantly being discovered, and because of the lack of commonly accepted standards. An impressive collection of techniques has been used to detect protein and gene names in the last fourfive years, ranging from typical NLP to purely bioinformatics approaches. We explore here the relationship between protein/gene names and expressions used to characterize protein/gene function. These expressions are captured in a collection of patterns derived from an original set of manually derived expressions, extended to cover lexical variants and filtered with known cases of association patterns/ names. Applying these patterns to a large collection of curated sentences, we found a significant number of patterns with a very strong tendency to appear only in sentences in which a protein/gene name is simultaneously present. This approach is part of a larger effort to incorporate contextual information so as to make biological information less ambiguous. "
W04-1207 " T h i s p a p e r g i v e s a n o v e r v i e w o f t h e C a d e r i g e p r o j e c t . T h i s p r o j e c t i n v o l v e s t e a m s f r o m d i f f e r e n t a r e a s ( b i o l o g y , m a c h i n e l e a r n i n g , n a t u r a l l a n g u a g e p r o c e s s i n g ) i n o r d e r t o d e v e l o p h i g h l e v e l a n a l y s i s t o o l s f o r e x t r a c t i n g s t r u c t u r e d i n f o r m a t i o n f r o m b i o l o g i c a l b i b l i o g r a p h i c a l d a t a b a s e s , e s p e c i a l l y M e d l i n e . T h e p a p e r g i v e s a n o v e r v i e w o f t h e a p p r o a c h a n d c o m p a r e s i t t o t h e s t a t e o f t h e a r t . 1 Introduction D e v e l o p m e n t s i n b i o l o g y a n d b i o m e d i c i n e a r e r e p o r t e d i n l a r g e b i b l i o g r a p h i c a l d a t a b a s e s e i t h e r f o c u s e d o n a s p e c i f i c s p e c i e s ( e . g . F l y b a s e , s p e c i a l i z e d o n Drosophilia Menogaster) o r n o t ( e . g . M e d l i n e ) . T h i s t y p e o f i n f o r m a t i o n s o u r c e s i s c r u c i a l f o r b i o l o g i s t s b u t t h e r e i s a l a c k o f t o o l s t o e x p l o r e t h e m a n d e x t r a c t r e l e v a n t i n f o r m a t i o n . W h i l e r e c e n t n a m e d e n t i t y r e c o g n i t i o n t o o l s h a v e g a i n e d a c e r t a i n s u c c e s s o n t h e s e d o m a i n s , e v e n t b a s e d I n f o r m a t i o n E x t r a c t i o n ( I E ) i s s t i l l a c h a l l e n g e . T h e C a d e r i g e p r o j e c t a i m s a t d e s i g n i n g a n d i n t e g r a t i n g N a t u r a l L a n g u a g e P r o c e s s i n g ( N L P ) a n d M a c h i n e L e a r n i n g ( M L ) t e c h n i q u e s t o e x p l o r e , a n a l y z e a n d e x t r a c t t a r g e t e d i n f o r m a t i o n i n b i o l o g i c a l t e x t u a l d a t a b a s e s . W e p r o m o t e a c o r p u s b a s e d a p p r o a c h f o c u s i n g o n t e x t p r e a n a l y s i s a n d n o r m a l i z a t i o n : i t i s i n t e n d e d t o d r a i n o u t t h e l i n g u i s t i c v a r i a t i o n d i m e n s i o n , a s m o s t a s p o s s i b l e . A c t u a l l y , t h e M U C ( 1 9 9 5 ) c o n f e r e n c e s h a v e d e m o n s t r a t e d t h a t e x t r a c t i o n i s m o r e e f f i c i e n t w h e n p e r f o r m e d o n n o r m a l i z e d t e x t s . T h e e x t r a c t i o n p a t t e r n s a r e t h u s e a s i e r t o a c q u i r e o r l e a r n , m o r e a b s t r a c t a n d e a s i e r t o m a i n t a i n B e y o n d e x t r a c t i o n p a t t e r n s , i t i s a l s o p o s s i b l e t o a c q u i r e f r o m t h e c o r p u s , v i a M L m e t h o d s , a p a r t o f t h e k n o w l e d g e n e c e s s a r y f o r t e x t n o r m a l i z a t i o n a s s h o w n h e r e . T h i s p a p e r g i v e s a n o v e r v i e w o f c u r r e n t r e s e a r c h a c t i v i t i e s a n d a c h i e v e m e n t s o f t h e C a d e r i g e p r o j e c t . T h e p a p e r f i r s t p r e s e n t s o u r a p p r o a c h a n d c o m p a r e s i t w i t h t h e o n e d e v e l o p e d i n t h e f r a m e w o r k o f a s i m i l a r p r o j e c t c a l l e d G e n i a ( C o l l i e r et al. 1 9 9 9 ) . W e t h e n p r o p o s e a n a c c o u n t o f C a d e r i g e t e c h n i q u e s o n v a r i o u s f i l t e r i n g a n d n o r m a l i z a t i o n t a s k s , n a m e l y , s e n t e n c e f i l t e r i n g , r e s o l u t i o n o f n a m e d e n t i t y s y n o n y m y , s y n t a c t i c p a r s i n g , a n d o n t o l o g y l e a r n i n g . F i n a l l y , w e s h o w h o w e x t r a c t i o n p a t t e r n s c a n b e l e a r n e d f r o m n o r m a l i z e d a n d a n n o t a t e d d o c u m e n t s , a l l a p p l i e d t o b i o l o g i c a l t e x t s . 2 Description of our approach I n t h i s s e c t i o n , w e g i v e s o m e d e t a i l s a b o u t t h e m o t i v a t i o n s a n d c h o i c e s o f i m p l e m e n t a t i o n . W e t h e n b r i e f l y c o m p a r e o u r a p p r o a c h w i t h t h e o n e o f t h e G e n i a p r o j e c t . 43 2.1 Pro  "
W04-1208 "Hinxton, Cambridge CB10 1SD, UK {kirsch,rebholz}@ebi.ac.uk Abstract Biological databases contain facts from scientific literature, which have been curated by hand to ensure high quality. Curation is timeconsuming and can be supported by information extraction methods. We present a server which identifies biological facts in scientific text and presents the annotation to the curator. Such facts are: UniProt, UMLS and GO terminology, identification of gene and protein names, mutations and protein-protein interactions. UniProt, UMLS and GO concepts are automatically linked to the original source. The module for mutations is based on syntax patterns and the one for protein-protein interactions on NLP. All modules work independently of each other in single threads and are combined in a pipeline to ensure proper meta data integration. For fast response time the modules are distributed on a Linux cluster. The server is at present available to curation teams of biomedical data and will be opened to the public in the future. Contents "
W04-1209 " In the biological domain, extracting newly discovered functional features from the massive literature is a major challenging issue. To automatically annotate Gene References into Function (GeneRIF) in a new literature is the main goal of this paper. We tried to find GRIF words in a training corpus, and then applied these informative words to annotate the GeneRIFs in abstracts with several different weighting schemes. The experiments showed that the Classic Dice score is at most 50.18%, when the weighting schemes proposed in the paper (Hou et al., 2003) were adopted. In contrast, after employing Support Vector Machines (SVMs) and the definition of classes proposed by Jelier et al. (2003), the score greatly improved to 56.86% for Classic Dice (CD). Adopting the same features, SVMs demonstrated advantage over the Naive Bayes Classifier. Finally, the combination of the former two models attained a score of 59.51% for CD.  "
W04-1210 "<NoAbstract>"
W04-1211 " This paper presents a project whose main goal is to construct a corpus of clinical text manually annotated for part-of-speech information. We describe and discuss the process of training three domain experts to perform linguistic annotation. We list some of the challenges as well as encouraging results pertaining to inter-rater agreement and consistency of annotation. We also present preliminary experimental results indicating the necessity for adapting state-of-the-art POS taggers to the sublanguage domain of medical text.  "
W04-1212 " The accelerating growth in biomedical literature has stimulated activity on automated classification of and information extraction from this literature. The work described here attempts to improve on an earlier classification study associating biological articles to GO codes. It demonstrates the need, under particular assumptions, for more access to full text articles and for the use of Part-of-Speech tagging. 1 I  "
W04-1213 "01-8430, Japan  Abstract We describe here the JNLPBA shared task of bio-entity recognition using an extended version of the GENIA version 3 named entity corpus of MEDLINE abstracts. We provide background information on the task and present a general discussion of the approaches taken by participating systems. "
W04-1215 " Named entity recognition is a fundamental task in biomedical data mining. Multiple -class annotation is more challenging than single cla ss annotation. In this paper, we took a single word classification approach to dealing with the multiple -class annotation problem using Support Vector Machines (SVMs). Word attributes, results of existing gene/protein name taggers, context, and other information are important features for classification. During training, the size of training data and the distribution of named entities are considered. The preliminary results showed that the approach might be feasible when more training data is used to alleviate the data imbalance problem. 1  "
W04-1216 " Although there exists a huge number of biomedical texts online, there is a lack of tools good enough to help people get information or knowledge from them. Named entity Recognition (NER) becomes very important for further processing like information retrieval, information extraction and knowledge discovery. We introduce a Hidden Markov Model (HMM) for NER, with a word similarity-based smoothing. Our experiment shows that the word similarity-based smoothing can improve the performance by using huge unlabeled data. While many systems have laboriously hand-coded rules for all kinds of word features, we show that word similarity is a potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information.  "
W04-1217 " We describe a machine learning system for the recognition of names in biomedical texts. The system makes extensive use of local and syntactic features within the text, as well as external resources including the web and gazetteers. It achieves an Fscore of 70% on the Coling 2004 NLPBA/BioNLP shared task of identifying five biomedical named entities in the GENIA corpus. 1 Intro  "
W04-1218 " In this paper, we report the adaptation of a named entity recognition (NER) system to the biomedical domain in order to participate in the Shared Task Bio-Entity Recognition. The system is originally developed for German NER that shares characteristics with the biomedical task. To facilitate adaptability, the system is knowledge-poor and utilizes unlabeled data. Investigating the adaptability of the single components and the enhancements necessary, we get insights into the task of bioentity recognition. 1 Intro  "
W04-1219 " In this paper, we present a named entity recognition system in the biomedical domain. In order to deal with the special phenomena in the biomedical domain, various evidential features are proposed and integrated through a Hidden Markov Model (HMM). In addition, a Support Vector Machine (SVM) plus sigmoid is proposed to resolve the data sparseness problem in our system. Besides the widely used lexical-level features, such as word formation pattern, morphological pattern, out-domain POS and semantic trigger, we also explore the name alias phenomenon, the cascaded entity name phenomenon, the use of both a closed dictionary from the training corpus and an open dictionary from the database term list SwissProt and the alias list LocusLink, the abbreviation resolution and indomain POS using the GENIA corpus. 1  "
W04-1220 "Two classifiers -Support Vector Machine (SVM) and Conditional Random Fields (CRFs) are applied here for the recognition of biomedical named entities. According to their different characteristics, the results of two classifiers are merged to achieve better performance. We propose an automatic corpus expansion method for SVM and CRF to overcome the shortage of the annotated training data. In addition, we incorporate a keyword-based post-processing step to deal with the remaining problems such as assigning an appropriate named entity tag to the word/phrase containing parentheses. "
W04-2301 "<NoAbstract>"
W04-2302 " Until recently, surface generation in dialogue systems has served the purpose of simply providing a backend to other areas of research. The generation component of such systems usually consists of templates and canned text, providing inflexible, unnatural output. To make matters worse, the resources are typically specific to the domain in question and not portable to new tasks. In contrast, domainindependent generation systems typically require large grammars, full lexicons, complex collocational information, and much more. Furthermore, these frameworks have primarily been applied to text applications and it is not clear that the same systems could perform well in a dialogue application. This paper explores the feasibility of adapting such systems to create a domain-independent generation component useful for dialogue systems. It utilizes the domain independent semantic form of The Rochester Interactive Planning System (TRIPS) with a domain independent stochastic surface generation module. We show that a written text language model can be used to predict dialogue utterances from an overgenerated word forest. We also present results from a human oriented evaluation in an emergency planning domain. "
W04-2304 " This paper presents the NICE fairy-tale game system, in which adults and children can interact with various animated characters in a 3D world. Computer games is an interesting application for spoken and multimodal dialogue systems. Moreover, for the development of future computer games, multimodal dialogue has the potential to greatly enrichen the users experience. In this paper, we also present some requirements that have to be fulfilled to successfully integrate spoken dialogue technology with a computer game application.  "
W04-2305 " This paper describes a machine learning approach to classifying n-best speech recognition hypotheses as either correctly or incorrectly recognised. The learners are trained on a combination of acoustic confidence features and move evaluation scores in a chess-playing scenario. The results show significant improvements over sharp baselines that use confidence rejection thresholds for classification. 1 Intro  "
W04-2306 "GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project, which has two main objectives: First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort, and, second, the demonstration of the platforms efficiency through the development of two different applications based on this platform: EG-Banking, a voice-portal for highquality interactions for bank customers, and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction. "
W04-2307 "<NoAbstract>"
W04-2308 " The paper gives an overview of repair sequences used in Estonian spoken information dialogues. 62 calls for information, travel bureaus, shops or outpatients departments are analysed. Several repair types are considered. Our further aim is to develop a dialogue system which can interact with the user in Estonian following the norms and rules of humanhuman communication 1 Introdu  "
W04-2309 " In this paper we hypothesise that Denial of Expectation (DofE) across turns in dialogue signalled by but can involve a range of different expectations, i.e., not just causal expectations, as argued in the literature. We will argue for this hypothesis and outline a methodology to distinguish the relations these denied expectations convey. Finally we will demonstrate the practical utility of this hypothesis by showing how it can improve generation of appropriate responses to DofE and decrease the likelihood of misunderstandings based on incorrectly interpreting these underlying cross-speaker relations. 1 Introdu  "
W04-2310 " Anaphora resolution for dialogues is a difficult problem because of the several kinds of complex anaphoric references generally present in dialogic discourses. It is nevertheless a critical first step in the processing of any such discourse. In this paper, we describe a system for anaphora resolution in multi-person dialogues. This system aims to bring together a wide array syntactic, semantic and world knowledge based techniques used for anaphora resolution. In this system, the performance of the heuristics is optimized for specific dialogues using genetic algorithms, which relieves the programmer of hand-crafting the weights of these heuristics. In our system, we propose a new technique based on the use of anaphora chains to enable resolution of a large variety of anaphors, including plural anaphora and cataphora. 1 Intro  "
W04-2311 " Surface realization in statistical natural language generation is based on the idea that when there are many ways to say the same thing, the most frequent option based on corpus counts is the best. Based on data from English and Finnish, we argue instead that all options are not equivalent, and the most frequent one can be incoherent in some contexts. A statistical NLG system where word order choice is based only on frequency counts of forms cannot capture the contextually-appropriate use of word order. We describe an alternative method for word order selection and show how it outperforms a frequency-only approach. 1 Int  "
W04-2312 "The development of conversational multidomain spoken dialogue systems poses new challenges for the reliable processing of less restricted user utterances. Unlike in controlled and restricted dialogue systems a simple oneto-one mapping from words to meanings is no longer feasible here. In this paper two different approaches to the resolution of lexical ambiguities are applied to a multi-domain corpus of speech recognition output produced from spontaneous utterances in a spoken dialogue system. The resulting evaluations show that all approaches yield significant gains over the majority class baseline performance of .68, i.e. fmeasures of .79 for the knowledge-driven approach and .86 for the supervised learning approach. "
W04-2313 " This article discusses the detection of discoursemarkers(DM)indialogtranscriptions, by human annotators and by automated means. After a theoretical discussion of the definitionofDMsandtheirrelevancetonatural language processing, we focus on therole of like as a DM. Results from experiments withhumanannotatorsshowthatdetectionof DMs is adifficult butreliabletask, whichrequiresprosodicinformationfromsoundtracks. Then,severaltypesoffeaturesaredefinedfor automatic disambiguation of like: collocations, part-of-speech tags and duration-based features.Decision-treelearningshowsthatfor like, nearly 70% precision can be reached, with near 100% recall, mainly using collocation filters. Similarresultshold for well, with about91%precisionat100%recall. 1 Introdu  "
W04-2314 " Building natural language spoken dialog systems requires large amounts of human transcribed and labeled speech utterances to reach useful operational service performances. Furthermore, the design of such complex systems consists of several manual steps. The User Experience (UE) expert analyzes and defines by hand the system core functionalities: the system semantic scope (call-types) and the dialog manager strategy which will drive the human-machine interaction. This approach is extensive and error prone since it involves several non-trivial design decisions that can only be evaluated after the actual system deployment. Moreover, scalability is compromised by time, costs and the high level of UE know-how needed to reach a consistent design. We propose a novel approach for bootstrapping spoken dialog systems based on reuse of existing transcribed and labeled data, common reusable dialog templates and patterns, generic language and understanding models, and a consistent design process. We demonstrate that our approach reduces design and development time while providing an effective system without any application specific data. "
W04-2315 " The Speech Graffiti interface is designed to be a portable, transparent interface for spoken language interaction with simple machines and information servers. Because it is a subset language, users must learn and adhere to the constraints of the language. We conducted a user study to determine habitability and found that more than 80% of utterances were Speech Graffiti-grammatical, suggesting that the language is acceptably learnable and usable for most users. We also analyzed deviations from grammaticality and found that natural language input accounted for the most deviations from Speech Graffiti. The results will suggest changes to the interface and can also inform design choices in other speech interfaces. 1 I  "
W04-2316 "<NoAbstract>"
W04-2317 " The paper is about the issue of addressing in multi-party dialogues. Analysis of addressing behavior in face to face meetings results in the identification of several addressing mechanisms. From these we extract several utterance features and features of non-verbal communicative behavior of a speaker, like gaze and gesturing, that are relevant for observers to identify the participants the speaker is talking to. A method for the automatic prediction of the addressee of speech acts is discussed. 1 Int  "
W04-2318 " Theories of discourse structure hypothesize a hierarchical structure of discourse segments, typically tree-structured. While substantial work has been done on identifying and automatically recognizing the textual and prosodic correlates of discourse structure in monologue, comparable cues for dialogue or multiparty conversation, and in particular humancomputer dialogue remain relatively less studied. In this paper, we explore prosodic cues to discourse segmentation in humancomputer dialogue. Using data drawn from 60 hours of interactions with a voice-only conversational spoken language system, we identify pitch and intensity features that signal segment boundaries. Specifically, based on 473 pairs of segment-final and segmentinitiating utterances, we find significant increases for segment-initial utterances in maximum pitch, average pitch, and average intensity, while segment-final utterances show significantly lower minimum pitch. These results suggest that even in the artificial environment of human-computer dialogue, prosodic cues robustly signal discourse segment structure, comparably to the contrastive uses of pitch and amplitude identified in natural monologues. Keywords Dialogue Systems, Discourse structure, Prosody in understanding "
W04-2319 "<NoAbstract>"
W04-2320 " This talk introduces new research that works towards an overarching model of natural face-to-face conversation about spatially-located actions in the world, and then uses that model to implement a trustworthy embodied conversational agent to guide users' ongoing, real-world activities away from the desktop. Past research has demonstrated that the relationship between verbal and nonverbal behavior exists at the level of intonational phrases, conversational turns, discourse units, and the negotiation of reference to objects and actions, that mental representations of shared space are structured in such a way as to allow participants in a dialogue to draw on them, that dialogue systems must be based on models of coordination and collaboration, and that users are willing to engage in persistent, natural, trusting conversation with embodied conversational systems. In this talk, these diverse strands of research are brought together in the service of a single underlying modality-independent model of action and language, non-verbal behaviors and words, production and comprehension that can lead to a physically-located, spatially-aware, collaborative embodied conversational agent.  "
W04-2321 " This paper describes an interpretation and decision strategy that minimizes interpretation errors and perform dialogue actions which may not depend on the hypothesized concepts only, but also on confidence of what has been recognized. The concepts introduced here are applied in a system which integrates language and interpretation models into Stochastic Finite State Transducers (SFST). Furthermore, acoustic, linguistic and semantic confidence measures on the hypothesized word sequences are made available to the dialogue strategy. By evaluating predicates related to these confidence measures, a decision tree automatically learn a decision strategy for rescoring a n-best list of candidates representing a users utterance. The different actions that can be then performed are chosen according to the confidence scores given by the tree. "
W04-2322 "<NoAbstract>"
W04-2323 " Human annotation of discourse corpora typically results in segmentation hierarchies that vary in their degree of agreement. This paper presents several techniques for unifying multiple discourse annotations into a single hierarchy, deemed a gold standard  the segmentation that best captures the underlying linguistic structure of the discourse. It proposes and analyzes methods that consider the level of embeddedness of a segmentation as well as methods that do not. A corpus containing annotated hierarchical discourses, the Boston Directions Corpus, was used to evaluate the goodness of each technique, by comparing the similarity of the segmentation it derives to the original annotations in the corpus. Several metrics of similarity between hierarchical segmentations are computed: precision/recall of matching utterances, pairwise inter-reliability scores (   ), and non-crossing-brackets. A novel method for unification that minimizes conflicts among annotators outperforms methods that require consensus among a majority for the   and precision metrics, while capturing much of the structure of the discourse. When high recall is preferred, methods requiring a majority are preferable to those that demand full consensus among annotators. "
W04-2324 " I show that the semantic structure for discourses, understood as a dependency representation, can be mathematically characterized as DAGs, but these DAGs present heavy structural constraints. The argumentation is based on a simple case, i.e. discourses with three clauses and two discourse connectives. I show that only four types of DAGs are needed for these discourses. 1 I  "
W04-2325 " We do two things in this paper. First, we present a model of possible causes for requesting clarifications in dialogue, i.e., we classify types of non-understandings that lead to clarifications. For this we make more precise the models of communication of (Clark, 1996) and (Allwood, 1995), relating them to an independently motivated theory of discourse semantics, SDRT (Asher and Lascarides, 2003). As we show, the lack of such a model is a problem for extant analyses of clarification moves. Second, we combine this model with an extended notion of confidence score that combines speech recognition confidence with different kinds of semantic and pragmatic confidence, and argue that the resulting processing model can produce a more natural clarification and confirmation behaviour than that of current dialogue systems. We close with a description of an experimental implementation of the model. "
W04-2326 " We present an annotation scheme for student emotions in tutoring dialogues. Analyses of our scheme with respect to interannotator agreement and predictive accuracy indicate that our scheme is reliable in our domain, and that our emotion labels can be predicted with a high degree of accuracy. We discuss issues concerning the implementation of emotion prediction and adaptation in the computer tutoring dialogue system we are developing. 1 Intro  "
W04-2327 "e United Kingdom Abstract In the five years since it was proposed, the MATE scheme for anaphoric annotation has been used in a variety of annotation projects, and the resulting corpora have been used to study both anaphora resolution and NL generation. Annotation tools inspired by the proposals have been used in some of these projects. In this paper we discuss these first experiences with the scheme, some lessons that have been learned, and suggest a few modifications. "
W04-2701 "<NoAbstract>"
W04-2702 " Mapping between syntax and semantics is one of the most promising research topics in corpus annotation. This paper deals with the implementation of an semi-automatic transformation from a syntactically-tagged corpus into a semantic-tagged one. The method has been experimentally applied to a 1600-sentence treebank (the UAM Spanish Treebank). Results of evaluation are provided as well as prospective work in comparing syntax and semantics in written and spoken annotated corpora. 1 I  "
W04-2703 " This paper describes a new, large scale discourse-level annotation project  the Penn Discourse TreeBank (PDTB). We present an approach to annotating a level of discourse structure that is based on identifying discourse connectives and their arguments. The PDTB is being built directly on top of the Penn TreeBank and Propbank, thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms. We provide a detailed preliminary analysis of inter-annotator agreement  both the level of agreement and the types of inter-annotator variation. 1 Int  "
W04-2704 " The PropBank project is creating a corpus of text annotated with information about basic semantic propositions. PropBank I (Kingsbury & Palmer, 2002) added a layer of predicateargument information, or semantic roles, to the syntactic structures of the English Penn Treebank. This paper presents an overview of the second phase of PropBank Annotation, PropBank II, which is being applied to English and Chinese, and includes (Neodavidsonian) eventuality variables, nominal references, sense tagging, and connections to the Penn Discourse Treebank (PDTB), a project for annotating discourse connectives and their arguments. 1 Intro  "
W04-2705 " This paper describes NomBank, a project that will provide argument structure for instances of common nouns in the Penn Treebank II corpus. NomBank is part of a larger effort to add additional layers of annotation to the Penn Treebank II corpus. The University of Pennsylvanias PropBank, NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text. This paper describes the NomBank project in detail including its specifications and the process involved in creating the resource. 1 Introdu  "
W04-2706 " The requirements of the depth and precision of annotation vary for different intended uses of the corpus but it has been commonly accepted nowadays that the standard annotations of surface structure are only the first steps in a more ambitious research program, aiming at a creation of advanced resources for most different systems of natural language processing and for testing and further enrichment of linguistic and computational theories. Among the several possible directions in which we believe the standard annotation systems should go (and in some cases already attempt to go) beyond the POS tagging or shallow syntactic annotations, the following four are characterized in the present contribution: (i) predicateargument representation of the underlying syntactic relations as basically corresponding to a rooted tree that can be univocally linearized, (ii) the inclusion of the information structure using very simple means (the left-to-right order of the nodes and three attribute values), (iii) relating this underlying structure (rendering the linguistic meaning, i.e. the semantically relevant counterparts of the grammatical means of expression) to certain central aspects of referential semantics (reference assignment and coreferential relations), and (iv) handling of word sense disambiguation. The first three issues are documented in the present paper on the basis of our experience with the development of the structure and scenario of the Prague Dependency Treebank which provides for syntactico-semantic annotation of large text segments from the Czech National Corpus and which is based on a solid theoretical framework. "
W04-2707 "<NoAbstract>"
W04-2708 " The Prague Czech-English Dependency Treebank (PCEDT) is a new syntactically annotated Czech-English parallel resource. The Penn Treebank has been translated to Czech, and its annotation automatically transformed into dependency annotation scheme. The dependency annotation of Czech is done from plain text by automatic procedures. A small subset of corresponding Czech and English sentences has been annotated by humans. We discuss some of the problems we have experienced during the automatic transformation between annotation schemes and hint at some of the difficulties to be tackled by potential guidelines for dependency annotation of English. 1 Introduction The Prague Czech-English Dependency Treebank (PCEDT) is a project of creating a Czech-English syntactically annotated parallel corpus motivated by research in the field of machine translation. Parallel data are needed for designing, training, and evaluation of both statistical and rule-based machine translation systems. Since Czech is a language with relatively high degree of word-order freedom, and its sentences contain certain syntactic phenomena, such as discontinuous constituents (non-projective constructions), which cannot be straightforwardly handled using the annotation scheme of Penn Treebank (Marcus et al., 1993; Linguistic Data Consortium, 1999), based on phrase-structure trees, we decided to adopt for the PCEDT the dependency-based annotation scheme of the Prague Dependency Treebank  PDT (Linguistic Data Consortium, 2001). The PDT is annotated on three levels: morphological layer (lowest), analytic layer (middle)  surface syntactic annotation, and tectogrammatical layer (highest)  level of linguistic meaning. Dependency trees, representing the sentence structure as concentrated around the verb and its valency, are used for the analytical and tectogrammatical levels, as proposed by Functional Generative Description (Sgall et al., 1986). In Section 2, we describe  "
W04-2709 " This paper describes a multi-site project to annotate six sizable bilingual parallel corpora for interlingual content. After presenting the background and objectives of the effort, we describe the data set that is being annotated, the interlingua representation language used, an interface environment that supports the annotation task and the annotation process itself. We will then present a preliminary version of our evaluation methodology and conclude with a summary of the current status of the project along with a number of issues which have arisen. 1 Introduction This paper describes a multi-site National Science Foundation project focusing on the annotation of six sizable bilingual parallel corpora for interlingual content with the goal of providing a significant data set for improving knowledge-based approaches to machine translation (MT) and a range of other Natural Language Processing (NLP) applications. The project participants include the Computing Research Laboratory at NMSU, the Language Technologies Institute at CMU, the Information Science Institute at USC, UMIACS at the University of Maryland, the MI  "
W04-2710 " High-quality lexical resources are needed to both train and evaluate Word Sense Disambiguation (WSD) systems. The problem of ambiguity persists even in limited domains, thus the necessity for wide-coverage inventories of senses (dictionaries) and corpora sense-tagged to them. WordNet has been used extensively for WSD, for both its broad coverage and its large network of semantic relations. In this paper, we present a report on the state of our current endeavor to increase the connectivity of WordNet through sense-tagging the glosses, the result of which will be to create a more integrated lexical resource. 1 Int  "
W04-2901 "<NoAbstract>"
W04-2902 " In this paper we report on our recent efforts to collect a corpus of spoken lecture material that will enable research directed towards fast, accurate, and easy access to lecture content. Thus far, we have collected a corpus of 270 hours of speech from a variety of undergraduate courses and seminars. We report on an initial analysis of the spontaneous speech phenomena present in these data and the vocabulary usage patterns across three courses. Finally, we examine language model perplexities trained from written and spoken materials, and describe an initial recognition experiment on one course. 1 I  "
W04-2903 " This paper reports our on-going efforts to exploit multiple features derived from an audio stream using source material such as broadcast news, teleconferences, and meetings. These features are derived from algorithms including automatic speech recognition, automatic speech indexing, speaker identification, prosodic and audio feature extraction. We describe our research prototype  the Audio Hot Spotting System  that allows users to query and retrieve data from multimedia sources utilizing these multiple features. The system aims to accurately find segments of user interest, i.e., audio hot spots within seconds of the actual event. In addition to spoken keywords, the system also retrieves audio hot spots by speaker identity, word spoken by a specific speaker, a change of speech rate, and other non-lexical features, including applause and laughter. Finally, we discuss our approach to semantic, morphological, phonetic query expansion to improve audio retrieval performance and to access cross-lingual dat a. "
W04-2904 "Abstract When evaluating wordspotting systems, one normally compares receiver operating characteristic curves and different measures of accuracy. However, there are many other factors that are relevant to the systems usability for searching speech. In this paper, we discuss both measures of quality for confidence scores and propose algorithms for producing scores that are optimal with respect to these criteria. "
W04-2905 " In this paper we highlight the problems that arise due to variations of spellings of names that occur in text, as a result of which links between two pieces of text where the same name is spelt differently may be missed. The problem is particularly pronounced in the case of ASR text. We propose the use of approximate string matching techniques to normalize names in order to overcome the problem. We show how we could achieve an improvement if we could tag names with reasonable accuracy in ASR. 1 I  "
W04-2906 " Automatic topic segmentation, separation of a discourse stream into its constituent stories or topics, is a necessary preprocessing step for applications such as information retrieval, anaphora resolution, and summarization. While significant progress has been made in this area for text sources and for English audio sources, little work has been done in automatic segmentation of other languages using both text and acoustic information. In this paper, we focus on exploiting both textual and prosodic features for topic segmentation of Mandarin Chinese. As a tone language, Mandarin presents special challenges for applicability of intonation-based techniques, since the pitch contour is also used to establish lexical identity. However, intonational cues such as reduction in pitch and intensity at topic boundaries and increase in duration and pause still provide significant contrasts in Mandarin Chinese. We first build a decision tree classifier that based only on prosodic information achieves boundary classification accuracy of 89-95.8% on a large standard test set. We then contrast these results with a simple text similarity-based classification scheme. Finally we build a merged classifier, finding the best effectiveness for systems integrating text and prosodic cues. "
W04-3001 " An effective way of representing the meaning of a utterance is with frame structures in which a type of sentence is represented by a set of property/value slots. Properties can types of verbs and cases and values are extracted from a sentence and should respect constraints represented by case relations and selectional restrictions involving word senses organized in type hierarchies. Properties and values can be obtained as the output of Stochastic Finite State Transducers (SFST) based on property specific language models combined with generic n-gam models. In this way, sentence interpretation and recognition are carried out by the same search process. LM adaptation can be performed by dynamically modifying the probability of each SFST based on system expectations. Phrases accepted by different SFSTs may share words, especially if different SFST recognize constituents of the same frame. For this reason, search for the most likely interpretation has to consider promising (possibly overlapping) hypotheses generated by SFSTs and the best combination of them into an acceptable semantic structure. Using different types of acoustic confidence measures and indices of consistency, it is possible to evaluate the probability that each semantic component that has been hypothesized is correct. These probabilities can be used by the dialogue strategy to decide about specific clarification and confirmation actions. SFSTs can be constructed using semi-automatic learning procedures, including the manual analysis of a limited number of cases followed by the automatic generation of examples by analogy or the retrieval of analogous examples from existing corpora of data. Strategies for clarification and confirmation actions can be learned using classification and regression trees.  "
W04-3002 " This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU). The first stage of this SLU utilizes a weighted finite-state transducer (WFST)-based parser, which encodes the regular grammar of concepts to be extracted. The proposed method improves the regular grammar model by incorporating a well-known n-gram semantic tagger. This hybrid model thus enhances the syntax of n-gram outputs while providing robustness against speech-recognition errors. With applications to a Thai hotel reservation domain, it is shown to outperform both individual models at every stage of the SLU system. Under the probabilistic WFST framework, the use of N-best hypotheses from the speech recognizer instead of the 1best can further improve performance requiring only a small additional processing time. 1 Introductio  "
W04-3003 " Spoken language understanding is a critical component of automated customer service applications. Creating effective SLU models is inherently a data driven process and requires considerable human intervention. We describe an interactive system for speech data mining. Using data visualization and interactive speech analysis, our system allows a User Experience (UE) expert to browse and understand data variability quickly. Supervised machine learning techniques are used to capture knowledge from the UE expert. This captured knowledge is used to build an initial SLU model, an annotation guide, and a training and testing system for the labelers. Our goal is to shorten the time to market by increasing the efficiency of the process and to improve the quality of the call types, the call routing, and the overall application. 1 Introduction  "
W04-3004 " This paper introduces a method that generates simulated multimodal input to be used in testing multimodal system implementations, as well as to build statistically motivated multimodal integration modules. The generation of such data is inspired by the fact that true multimodal data, recorded from real usage scenarios, is difficult and costly to obtain in large amounts. On the other hand, thanks to operational speech-only dialogue system applications, a wide selection of speech/text data (in the form of transcriptions, recognizer outputs, parse results, etc.) is available. Taking the textual transcriptions and converting them into multimodal inputs in order to assist multimodal system development is the underlying idea of the paper. A conceptual framework is established which utilizes two input channels: the original speech channel and an additional channel called Virtual Modality. This additional channel provides a certain level of abstraction to represent non-speech user inputs (e.g., gestures or sketches). From the transcriptions of the speech modality, pre-defined semantic items (e.g., nominal location references) are identified, removed, and replaced with deictic references (e.g., here, there). The deleted semantic items are then placed into the Virtual Modality channel and, according to external parameters (such as a pre-defined user population with various deviations), temporal shifts relative to the instant of each corresponding deictic reference are issued. The paper explains the procedure followed to create Virtual Modality data, the details of the speech-only database, and results based on a multimodal city information and navigation application. "
W04-3005 "<NoAbstract>"
W04-3006 " This paper describes our research on both the detection and subsequent resolution of recognition errors in spoken dialogue systems. The paper consists of two major components. The first half concerns the design of the error detection mechanism for resolving city names in our MERCURY flight reservation system, and an investigation of the behavioral patterns of users in subsequent subdialogues involving keypad entry for disambiguation. An important observation is that, upon a request for keypad entry, users are frequently unresponsive to the extent of waiting for a time-out or hanging up the phone. The second half concerns a pilot experiment investigating the feasibility of replacing the solicitation of a keypad entry with that of a speak-and-spell entry. A novelty of our work is the introduction of a speech synthesizer to simulate the user, which facilitates development and evaluation of our proposed strategy. We have found that the speak-and-spell strategy is quite effective in simulation mode, but it remains to be tested in real user dialogues. "
W04-3007 "<NoAbstract>"
W04-3008 "<NoAbstract>"
W04-3009 " Speech interface is often required in many application environments such as telephonebased information retrieval, car navigation systems, and user-friendly interfaces, but the low speech recognition rate makes it difficult to extend its application to new fields. Several approaches to increase the accuracy of the recognition rate have been researched by error correction of the recognition results, but previous approaches were mainly lexical-oriented ones in post error correction. We suggest an improved syllable-based model and a new semantic-oriented approach to correct both semantic and lexical errors, which is also more accurate for especially domain-specific speech error correction. Through extensive experiments using a speech-driven in-vehicle telematics information retrieval, we demonstrate the superior performance of our approach and some advantages over previous lexical-oriented approaches. "
W04-3011 "<NoAbstract>"
W05-0301 "<NoAbstract>"
W05-0302 "Abstract Many recent annotation efforts for English have focused on pieces of the larger problem of semantic annotation, rather than initially producing a single unified representation. This paper discusses the issues involved in merging four of these efforts into a unified linguistic structure: PropBank, NomBank, the Discourse Treebank and Coreference Annotation undertaken at the University of Essex. We discuss resolving overlapping and conflicting annotation as well as how the various annotation schemes can reinforce each other to produce a representation that is greater than the sum of its parts. "
W05-0303 " This paper reports on the SYN-RA (SYNtax-based Reference Annotation ) project, an on-going project of annotating German newspaper texts with referential relations. The project has developed an inventory of anaphoric and coreference relations for German in the context of a unified, XML-based annotation scheme for combining morphological, syntactic, semantic, and anaphoric information. The paper discusses how this unified annotation scheme relates to other formats currently discussed in the literature, in particular the annotation graph model of Bird and Liberman (2001) and the pie-in-thesky scheme for semantic annotation. 1 Introduction  "
W05-0304 " We describe a parallel annotation approach for PubMed abstracts. It includes both entity/relation annotation and a treebank containing syntactic structure, with a goal of mapping entities to constituents in the treebank. Crucial to this approach is a modification of the Penn Treebank guidelines and the characterization of entities as relation components, which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events. 1 Introduct  "
W05-0305 " The annotations of the Penn Discourse Treebank (PDTB) include (1) discourse connectives and their arguments, and (2) attribution of each argument of each connective and of the relation it denotes. Because the PDTB covers the same text as the Penn TreeBank WSJ corpus, syntactic and discourse annotation can be compared. This has revealed significant differences between syntactic structure and discourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and finally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation. "
W05-0306 " We investigated of the characteristics of in-text causal relations. We designed causal relation tags. With our designed tag set, three annotators annotated 750 Japanese newspaper articles. Then, using the annotated corpus, we investigated the causal relation instances from some viewpoints. Our quantitative study shows that what amount of causal relation instances are present, where these relation instances are present, and which types of linguistic expressions are used for expressing these relation instances in text. 1  "
W05-0307 " We present a framework for the integrated analysis of the textual and prosodic characteristics of information structure in the Switchboard corpus of conversational English. Information structure describes the availability, organisation and salience of entities in a discourse model. We present standards for the annotation of information status (old, mediated and new), and give guidelines for annotating information structure, i.e. theme/rheme and background/kontrast. We show that information structure in English can only be analysed concurrently with prosodic prominence and phrasing. This annotation, using stand-off XML in NXT, can help establish standards for the annotation of information structure in discourse. 1 Introduction We pre  "
W05-0308 " This paper describes extensions to a corpus annotation scheme for the manual annotation of attributions, as well as opinions, emotions, sentiments, speculations, evaluations and other private states in language. It discusses the scheme with respect to the Pie in the Sky Check List of Desirable Semantic Information for Annotation. We believe that the scheme is a good foundation for adding private state annotations to other layers of semantic meaning. 1  "
W05-0309 " The Proposition Bank (PropBank) project is aimed at creating a corpus of text annotated with information about semantic propositions. The second phase of the project, PropBank II adds additional levels of semantic annotation which include eventuality variables, co-reference, coarse-grained sense tags, and discourse connectives. This paper presents the results of the parallel PropBank II project, which adds these richer layers of semantic annotation to the first 100K of the Chinese Treebank and its English translation. Our preliminary analysis supports the hypothesis that this additional annotation reconciles many of the surface differences between the two languages. 1 Introductio  "
W05-0310 " This paper describes a semantically rich, human-aided machine annotation system created within the Ontological Semantics (OntoSem) environment using the DEKADE toolset. In contrast to mainstream annotation efforts, this method of annotation provides more information at a lower cost and, for the most part, shifts the maintenance of consistency to the system itself. In addition, each tagging effort not only produces knowledge resources for that corpus, but also leads to improvements in the knowledge environment that will better support subsequent tagging efforts. 1 Intro  "
W05-0311 " We report the results of a study of the reliability of anaphoric annotation which (i) involved a substantial number of naive subjects, (ii) used Krippendorffs  instead of K to measure agreement, as recently proposed by Passonneau, and (iii) allowed annotators to mark anaphoric expressions as ambiguous. 1 IN  "
W05-1501 " In this paper, we introduce a new parser, called SXLFG, based on the LexicalFunctional Grammars formalism (LFG). We describe the underlying context-free parser and how functional structures are efficiently computed on top of the CFG shared forest thanks to computation sharing, lazy evaluation, and compact data representation. We then present various error recovery techniques we implemented in order to build a robust parser. Finally, we offer concrete results when SXLFG is used with an existing grammar for French. We show that our parser is both efficient and robust, although the grammar is very ambiguous. 1 Introdu  "
W05-1502 " We describe four different parsing algorithms for Linear Context-Free Rewriting Systems (Vijay-Shanker et al., 1987). The algorithms are described as deduction systems, and possible optimizations are discussed. The only parsing algorithms presented for linear contextfree rewriting systems (LCFRS; Vijay-Shanker et al., 1987) and the equivalent formalism multiple context-free grammar (MCFG; Seki et al., 1991) are extensions of the CKY algorithm (Younger, 1967), more designed for their theoretical interest, and not for practical purposes. The reason for this could be that there are not many implementations of these grammar formalisms. However, since a very important subclass of the Grammatical Framework (Ranta, 2004) is equivalent to LCFRS/MCFG (Ljunglof, 2004a; Ljunglof, 2004b), there is a need for practical parsing algorithms. In this paper we describe four different parsing algorithms for Linear Context-Free Rewriting Systems. The algorithms are described as deduction systems, and possible optimizations are discussed. 1 Introdu  "
W05-1503 " Parsing in type logical grammars amounts to theorem proving in a substructural logic. This paper takes the proof net presentation of Lambeks associative calculus as a case study. It introduces switch graphs for online maintenance of the Danos-Regnier acyclicity condition on proof nets. Early detection of Danos-Regnier acyclicity violations supports early failure in shift-reduce parsers. Normalized switch graphs represent the combinatorial potential of a set of analyses derived from lexical and structural ambiguities. Packing these subanalyses and memoizing the results leads directly to a dynamic programming algorithm for Lambek grammars. 1 Intro  "
W05-1504 " In lexicalized phrase-structure or dependency parses, a words modifiers tend to fall near it in the string. We show that a crude way to use dependency length as a parsing feature can substantially improve parsing speed and accuracy in English and Chinese, with more mixed results on German. We then show similar improvements by imposing hard bounds on dependency length and (additionally) modeling the resulting sequence of parse fragments. This simple vine grammar formalism has only finite-state power, but a context-free parameterization with some extra parameters for stringing fragments together. We exhibit a linear-time chart parsing algorithm with a low grammar constant. 1 I  "
W05-1505 " We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-theart constituency-based parsers. The continuity constraint of these constituencybased parsers makes it impossible for them to posit non-projective dependency trees. Analysis of the types of dependency errors made by these parsers on a Czech corpus show that the correct governor is likely to be found within a local neighborhood of the governor proposed by the parser. Our model, based on a MaxEnt classifier, improves overall dependency accuracy by .7% (a 4.5% reduction in error) with over 50% accuracy for non-projective structures. "
W05-1506 " We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop efficient algorithms for k-best trees in the framework of hypergraph parsing. To demonstrate the efficiency, scalability and accuracy of these algorithms, we present experiments on Bikels implementation of Collins lexicalized PCFG model, and on Chiangs CFG-based decoder for hierarchical phrase-based translation. We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications. 1 Int  "
W05-1507 " We adapt the hook trick for speeding up bilexical parsing to the decoding problem for machine translation models that are based on combining a synchronous context free grammar as the translation model with an n-gram language model. This dynamic programming technique yields lower complexity algorithms than have previously been described for an important class of translation models. 1 I  "
W05-1508 " We introduce a method for transferring annotation from a syntactically annotated corpus in a source language to a target language. Our approach assumes only that an (unannotated) text corpus exists for the target language, and does not require that the parameters of the mapping between the two languages are known. We outline a general probabilistic approach based on Data Augmentation , discuss the algorithmic challenges, and present a novel algorithm for sampling from a posterior distribution over trees. 1 Intro  "
W05-1509 " In this paper, we explore two extensions to an existing statistical parsing model to produce richer parse trees, annotated with function labels. We achieve significant improvements in parsing by modelling directly the specific nature of function labels, as both expressions of the lexical semantics properties of a constituent and as syntactic elements whose distribution is subject to structural locality constraints. We also reach state-of-the-art accuracy on function labelling. Our results suggest that current statistical parsing methods are sufficiently robust to produce accurate shallow functional or semantic annotation, if appropriately biased. 1 Introductio  "
W05-1510 " We describe probabilistic models for a chart generator based on HPSG. Within the research field of parsing with lexicalized grammars such as HPSG, recent developments have achieved efficient estimation of probabilistic models and high-speed parsing guided by probabilistic models. The focus of this paper is to show that two essential techniques  model estimation on packed parse forests and beam search during parsing  are successfully exported to the task of natural language generation. Additionally, we report empirical evaluation of the performance of several disambiguation models and how the performance changes according to the feature set used in the models and the size of training data. 1 Introduct  "
W05-1511 "Abstract We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next , we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The contributions of the large constituent inhibition and global thresholding were not significant, while the quick check and chunk parser greatly contributed to total parsing performance. The precision, recall and average parsing time for the Penn treebank (Section 23) were 87.85%, 86.85%, and 360 ms, respectively. "
W05-1512 "Although state-of-the-art parsers for natural language are lexicalized, it was recently shown that an accurate unlexicalized parser for the Penn tree-bank can be simply read off a manually refined treebank. While lexicalized parsers often suffer from sparse data, manual mark-up is costly and largely based on individual linguistic intuition. Thus, across domains, languages, and tree-bank annotations, a fundamental question arises: Is it possible to automatically induce an accurate parser from a tree-bank without resorting to full lexicalization? In this paper, we show how to induce head-driven probabilistic parsers with latent heads from a tree-bank. Our automatically trained parser has a performance of 85.7% (LP/LR F 1 ), which is already better than that of early lexicalized ones. "
W05-1513 " We present a classifier-based parser that produces constituent trees in linear time. The parser uses a basic bottom-up shiftreduce algorithm, but employs a classifier to determine parser actions instead of a grammar. This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing. We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers. We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively. 1 Int  "
W05-1514 " Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use. In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple slidingwindow method and maximum entropy classifiers for phrase recognition in each level of chunking. Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/sentence). We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers, and show that the search method can further improve the parsing accuracy. 1 Introduct  "
W05-1515 " Ordinary classification techniques can drive a conceptually simple constituent parser that achieves near state-of-the-art accuracy on standard test sets. Here we present such a parser, which avoids some of the limitations of other discriminative parsers. In particular, it does not place any restrictions upon which types of features are allowed. We also present several innovations for faster training of discriminative parsers: we show how training can be parallelized, how examples can be generated prior to training without a working parser, and how independently trained sub-classifiers that have never done any parsing can be effectively combined into a working parser. Finally, we propose a new figure-of-merit for bestfirst parsing with confidence-rated inferences. Our implementation is freely available at: http://cs.nyu.edu/ turian/ software/parser/ "
W05-1516 " We present a strictly lexical parsing model where all the parameters are based on the words. This model does not rely on part-of-speech tags or grammatical categories. It maximizes the conditional probability of the parse tree given the sentence. This is in contrast with most previous models that compute the joint probability of the parse tree and the sentence. Although the maximization of joint and conditional probabilities are theoretically equivalent, the conditional model allows us to use distributional word similarity to generalize the observed frequency counts in the training corpus. Our experiments with the Chinese Treebank show that the accuracy of the conditional model is 13.6% higher than the joint model and that the strictly lexicalized conditional model outperforms the corresponding unlexicalized model based on part-of-speech tags. 1 Intro  "
W05-1517 " We present a novel approach for applying the Inside-Outside Algorithm to a packed parse forest produced by a unificationbased parser. The approach allows a node in the forest to be assigned multiple inside and outside probabilities, enabling a set of weighted GRs to be computed directly from the forest. The approach improves on previous work which either loses efficiency by unpacking the parse forest before extracting weighted GRs, or places extra constraints on which nodes can be packed, leading to less compact forests. Our experiments demonstrate substantial increases in parser accuracy and throughput for weighted GR output. 1 Intro  "
W05-1518 " This paper explores the possibilities of improving parsing results by combining outputs of several parsers. To some extent, we are porting the ideas of Henderson and Brill (1999) to the world of dependency structures. We differ from them in exploring context features more deeply. All our experiments were conducted on Czech but the method is language-independent. We were able to significantly improve over the best parsing result for the given setting, known so far. Moreover, our experiments show that even parsers far below the state of the art can contribute to the total improvement. 1 Introdu  "
W05-1519 " This paper describes our effort on the task of edited region identification for parsing disfluent sentences in the Switchboard corpus. We focus our attention on exploring feature spaces and selecting good features and start with analyzing the distributions of the edited regions and their components in the targeted corpus. We explore new feature spaces of a partof-speech (POS) hierarchy and relaxed for rough copy in the experiments. These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001], and 20.44% percent relative error reduction in F-score over the latest best result where punctuation is excluded from the training and testing data [Johnson and Charniak 2004]. 1  "
W05-1520 "<NoAbstract>"
W05-1521 "<NoAbstract>"
W05-1522 " This document shows how the factorized syntactic descriptions provided by MetaGrammars coupled with factorization operators may be used to derive compact large coverage tree adjoining grammars. 1 I  "
W05-1523 "<NoAbstract>"
W05-1524 "<NoAbstract>"
W05-1525 "<NoAbstract>"
W05-1526 " We describe a method for augmenting unification-based deep parsing with statistical methods. We extend and adapt the Bikel parser, which uses head-driven lexical statistics, to dialogue. We show that our augmented parser produces significantly fewer constituents than the baseline system and achieves comparable bracketing accuracy, even yielding slight improvements for longer sentences. 1 Introdu  "
W05-1527 " We describe SUPPLE, a freely-available, open source natural language parsing system, implemented in Prolog, and designed for practical use in language engineering (LE) applications. SUPPLE can be run as a stand-alone application, or as a component within the GATE General Architecture for Text Engineering. SUPPLE is distributed with an example grammar that has been developed over a number of years across several LE projects. This paper describes the key characteristics of the parser and the distributed grammar. 1 Introdu  "
W05-1528 " We describe a history-based generative parsing model which uses a k-nearest neighbour (k-NN) technique to estimate the models parameters. Taking the output of a base n-best parser we use our model to re-estimate the log probability of each parse tree in the n-best list for sentences from the Penn Wall Street Journal treebank. By further decomposing the local probability distributions of the base model, enriching the set of conditioning features used to estimate the models parameters, and using k-NN as opposed to the Witten-Bell estimation of the base model, we achieve an f-score of 89.2%, representing a 4% relative decrease in f-score error over the 1-best output of the base parser.  Penn Wall Street Journal treebank. By further decomposing the local probability distributions of the base model, enriching the set of conditioning features used to estimate the models parameters, and using k-NN as opposed to the Witten-Bell estimation of the base model, we achieve an f-score of 89.2%, representing a 4% relative decrease in f-score error over the 1-best output of the base parser. "
W05-1601 "Abstract Statistical NLG has largely meant n-gram modelling which has the considerable advantages of lending robustness to NLG systems, and of making automatic adaptation to new domains from raw corpora possible. On the downside, n-gram models are expensive to use as selection mechanisms and have a built-in bias towards shorter realisations. This paper looks at treebank-training of generators, an alternative method for building statistical models for NLG from raw corpora, and two different ways of using treebank-trained models during generation. Results show that the treebank-trained generators achieve improvements similar to a 2-gram generator over a baseline of random selection. However, the treebank-trained generators achieve this at a much lower cost than the 2-gram generator, and without its strong preference for shorter realisations. "
W05-1602 " We present an authoring system for logical forms encoded as conceptual graphs (CG). The system belongs to the family of WYSIWYM (What You See Is What You Mean) text generation systems: logical forms are entered interactively and the corresponding linguistic realization of the expressions is generated in several languages. The system maintains a model of the discourse context corresponding to the authored documents. The system helps users author documents formulated in the CG format. In a first stage, a domainspecific ontology is acquired by learning from example texts in the domain. The ontology acquisition module builds a typed hierarchy of concepts and relations derived from the WordNet and Verbnet. The user can then edit a specific document, by entering utterances in sequence, and maintaining a representation of the context. While the user enters data, the system performs the standard steps of text generation on the basis of the authored logical forms: reference planning, aggregation, lexical choice and syntactic realization  in several languages (we have implemented English and Hebrew and are exploring an implementation using the Bliss graphical language). The feedback in natural language is produced in real-time for every single modification performed by the author. We perform a cost-benefit analysis of the application of NLG techniques in the context of authoring cooking recipes in English and Hebrew. By combining existing large-scale knowledge resources (WordNet, Verbnet, the SURGE and HUGG realization grammars) and techniques from modern integrated software development environment (such as the Eclipse IDE), we obtain an efficient tool for the generation of logical forms, in domains where content is not available in the form of databases.  Research supported by the Israel Ministry of Science Knowledge Center for Hebrew Computational Linguistics and by the Frankel Fund "
W05-1603 " Since its first implementation in 1995, the shallow NLG system TG/2 has been used as a component in many NLG applications that range from very shallow template systems to in-depth realization engines. TG/2 has continuously been refined, the Java brother implementation XtraGen has become available, and the grammar development environment eGram today allows for designing grammars on a more abstract level. Besides a better understanding of the usability of shallow systems like TG/2 has emerged. Time has come to summarize the developments and look forward to new borders. 1 Intro  "
W05-1604 " This paper describes Acorn, a sentence planner and surface realizer for dialogue systems. Improvements to previous stochastic word-forest based approaches are described, countering recent criticism of this class of algorithms for their slow speed. An evaluation of the approach with semantic input shows runtimes of a fraction of a second and presents results that suggest it is also portable across domains. 1 Introduction This paper describes Acorn, a real-time sentence planner and surface realizer for dialogue systems that is independent of a specific domain. Acorn is based on a two-phased grammar and stochastic approach, such as the HALogen system [Langkilde-Geary, 2002], but offers several improvements to make it more realistic for dialogue use. The first is to offer an algorithm for trickle-down features that passes head/foot features through the grammar as the initial word forest is created, allowing the grammar to broadly represent phenomena such as wh-movement. The second is to more tightly link the grammar to a lexicon and represent syntactic properties such as number, person, and tense to constrain the over-generation process. Lastly, efficiency improvements are described which further decrease the runtime of the system, allowing Acorn to be used in a real-time dialogue context. It is named Acorn, based on the word forests that are created and searched. The task of Natural Language Generation is frequently split into three somewhat disjoint steps: document planning, microplanning (reference and sentence planning) and surface realization. Document planning is a more reduced task in dialogue, mainly involving content determination since there is no need for a document. Since the system follows a notion of discourse, content determination is typically performed by some reasoner external to generation, such as a Task Manager. This paper addresses the sentence planning and surface realization steps, assuming that content determination and referential generation has already occurred and is represented in a high-level semantics. This stochastic approach involves two phases; the first uses a grammar to over-generate the possible realizations of an input form into a word forest, and the second uses a language model to choose the preferred path through the forest. This approach is attractive to dialogue systems because it offers flexibility and adaptivity that cannot be achieved through most symbolic systems. By over-generating possible utterances, the (sometimes dynamic) language models can decide which is more natural in the current context. Other advantages include domain independence and an under-specified input. The main disadvantages most often cited include a very slow runtime and the inability to capture complex linguistic constraints, such as wh-movement. The latter is a side effect of the word-forest creation algorithm and a solution to broaden the coverage of language is presented in this paper. The issue of runtime is critical to dialogue. Slow runtime is a two-fold problem: the word-forest that is generated is extremely large and often not linguistically constrained, and second, the algorithm has not been efficiently implemented. These issues must be addressed before stochastic approaches can be suited for dialogue. Langkilde [Langkilde, 2000] provides an evaluation of coverage of HALogen and shows runtimes around 28 seconds for sentences with average lengths of 22 words. Callaway [Callaway, 2003] later commented on the runtime that HALogen is anywhere from 6.5 to 16 times slower than the symbolic realizer FUF/SURGE (which may also be too slow for dialogue). This paper shows that more work can be done in stochastic generation to reduce the runtime by constraining the grammar and making simple algorithm improvements. Runtimes of only a fraction of one second are presented. The next section provides a brief background on stochastic generation, followed by a description of Acorn in sect  "
W05-1605 " Natural language has a high paraphrastic power yet not all paraphrases are appropriate for all contexts. In this paper, we present a TAG based surface realiser which supports both the generation and the selection of paraphrases. To deal with the combinatorial explosion typical of such an NP-complete task, we introduce a number of new optimisations in a tabular, bottom-up surface realisation algorithm. We then show that one of these optimisations supports paraphrase selection. 1 Int  "
W05-1606 " Algorithms for generating referring expressions typically assume that an object in a scenary can be identified through a set of commonly agreed properties. This is a strong assumption, since in reality properties of objects may be perceived differently among people, due to a number of factors including vagueness, knowledge discrepancies, and limited perception capabilities. Taking these discrepancies into account, we reinterpret concepts of algorithms generating referring expressions in view of uncertainties about the appearance of objects. Our model includes two complementary measures of likelihood in object identification, and adapted property selection and termination criteria. The approach is relevant for situations with potential perception problems and for scenarios with knowledge discrepancies between conversants. 1 Int  "
W05-1607 " This paper presents a framework for generating locative expressions. The framework addresses the issue of combinatorial explosion inherent in the construction of relational context models by: (a) contextually defining the set of objects in the context that may function as a landmark, and (b) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations. 1  "
W05-1608 " This paper describes an approach for the generation of multimodal deixis to be uttered by an anthropomorphic agent in virtual reality. The proposed algorithm integrates pointing and definite description. Doing so, the context-dependent discriminatory power of the gesture determines the contentselection for the verbal constituent. The concept of a pointing cone is used to model the region singled out by a pointing gesture and to distinguish two referential functions called object-pointing and region-pointing. 1 Introdu  "
W05-1609 " The paper presents an approach to utterance planning, which can dynamically use context information about the environment in which a dialogue is situated. The approach is functional in nature, using systemic networks to specify its planning grammar. The planner takes a description of a communicative goal as input, and produces one or more logical forms that can express that goal in a contextually appropriate way. Both the goal and the resulting logical forms are expressed in a single formalism as ontologically rich, relational structures. To realize the logical forms, OpenCCG is used. The paper focuses primarily on the implementation, but also discusses how the planning grammar can be based on the grammar used in OpenCCG, and trained on (parseable) data. 1 Int  "
W05-1610 "<NoAbstract>"
W05-1611 " We present an NLG system that uses Integer Linear Programming to integrate different decisions involved in the generation process. Our approach provides an alternative to pipeline-based sequential processing which has become prevalent in todays NLG applications. 1  "
W05-1612 " Sentence fusion is a text-to-text (revision-like) generation task which takes related sentences as input and merges these into a single output sentence. In this paper we describe our ongoing work on developing a sentence fusion module for Dutch. We propose a generalized version of alignment which not only indicates which words and phrases should be aligned but also labels these in terms of a small set of primitive semantic relations, indicating how words and phrases from the two input sentences relate to each other. It is shown that human labelers can perform this task with a high agreement (Fscore of .95). We then describe and evaluate our adaptation of an existing automatic alignment algorithm, and use the resulting alignments, plus the semantic labels, in a generalized fusion and generation algorithm. A small-scale evaluation study reveals that most of the resulting sentences are adequate to good. 1 Introduction  "
W05-1613 " It is hard to come up with a general formalisation of the problem of content determination in natural language generation because of the degree of domaindependence that is involved. This paper presents a novel way of looking at a class of content determination problems in terms of a non-standard kind of inference, which we call natural language directed inference. This is illustrated through examples from a system under development to present parts of ontologies in natural language. Natural language directed inference represents an interesting challenge to research in automated reasoning and natural language processing. 1 Introduct  "
W05-1614 " Computer pun-generators have so far relied on arbitrary semantic content, not linked to the immediate context. The mechanisms used, although tractable, may be of limited applicability. Integrating puns into normal text may involve complex search. 1  "
W05-1615 "<NoAbstract>"
W05-1616 "<NoAbstract>"
W05-1617 " We present three ways in which a natural language generator that produces textual descriptions of objects from symbolic information can exploit OWL ontologies, using M-PIROs multilingual generation system as a concrete example. 1  "
W05-1618 "<NoAbstract>"
W05-1619 " Recent empirical experiments on surface realizers have shown that grammars for generation can be effectively evaluated using large corpora. Evaluation metrics are usually reported as single averages across all possible types of errors and syntactic forms. But the causes of these errors are diverse, and the extent to which the accuracy of generation over individual syntactic phenomena is unknown. This article explores the types of errors, both computational and linguistic, inherent in the evaluation of a surface realizer when using large corpora. We analyze data from an earlier wide coverage experiment on the FUF/SURGE surface realizer with the Penn TreeBank in order to empirically classify the sources of errors and describe their frequency and distribution. This both provides a baseline for future evaluations and allows designers of NLG applications needing off-the-shelf surface realizers to choose on a quantitative basis. "
W05-1620 " The work presented here is intended as an evolutionary task-specific module for referring expression generation and aggregation to be enclosed in a generic flexible architecture. Appearances of concepts are considered as genes, each one encoding the type of reference used. Three genetic operators are used: classic crossover and mutation, plus a specific operator dealing with aggregation. Fitness functions are defined to achieve elementary coherence and stylistic validity. Experiments are described and discussed. 1 Introdu  "
W05-1621 "Abstract This paper addresses two previously unresolved issues in the automatic evaluation of Text Structuring (TS) in Natural Language Generation (NLG). First, we describe how to verify the generality of an existing collection of sentence orderings defined by one domain expert using data provided by additional experts. Second, a general evaluation methodology is outlined which investigates the previously unaddressed possibility that there may exist many optimal solutions for TS in the employed domain. This methodology is implemented in a set of experiments which identify the most promising candidate for TS among several metrics of coherence previously suggested in the literature. 1 "
W05-1622 "<NoAbstract>"
W05-1623 "Abstract Reversibility is a key to efficient and maintainable NLG systems. In this paper, we present a formal definition of reversible NLG systems and develop a classification of existing natural language dialog systems in this framework. "
W05-1624 " We describe a Wizard-of-Oz experiment setup for the collection of multimodal interaction data for a Music Player application. This setup was developed and used to collect experimental data as part of a project aimed at building a flexible multimodal dialogue system which provides an interface to an MP3 player, combining speech and screen input and output. Besides the usual goal of WOZ data collection to get realistic examples of the behavior and expectations of the users, an equally important goal for us was to observe natural behavior of multiple wizards in order to guide our system development. The wizards responses were therefore not constrained by a script. One of the challenges we had to address was to allow the wizards to produce varied screen output a in real time. Our setup includes a preliminary screen output planning module, which prepares several versions of possible screen output. The wizards were free to speak, and/or to select a screen output. 1 Introduct  "
W05-1625 " In this paper, we propose an approach for content determination and surface generation of answers in a question-answering system on the web. The content determination is based on a coherence rate which takes into account coherence with other potential answers. Answer generation is made through the use of classical techniques and templates and is based on a certainty degree. 1 Int  "
W05-1626 "Abstract Productions systems, traditionally mainly used for developing expert systems, can also be employed for implementing chart generators. Focusing on bottom-up chart generation, we describe how the notions of chart algorithms relate to the knowledge base and Rete network of production systems. We draw on experience gained in two research projects on natural language generation (NLG), one involving surface realization, the other involving both a content determination task (referring expression generation) and surface realization. The projects centered around the idea of overgeneration, i.e. of generating large numbers of output candidates which served as input to a ranking component. The purpose of this paper is to extend the range of implementation options available to the NLG practitioner by detailing the specific advantages and disadvantages of using production systems for NLG. "
W05-1627 "Abstract We discuss work-in-progress on a hybrid approach to the generation of spatial descriptions, using the maps of the Map Task dialogue corpus as domain models. We treat spatial descriptions as referring expressions that distinguish particular points on the maps from all other points (potential distractors). Our approach is based on rule-based overgeneration of spatial descriptions combined with ranking which currently is based on explicit goodness criteria but will ultimately be corpus-based. Ranking for content determination tasks such as referring expression generation raises a number of deep and vexing questions about the role of corpora in NLG, the kind of knowledge they can provide and how it is used. "
W06-0301 " This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet. We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles. For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet. Our experimental results show that our system performs significantly better than the baseline. 1 Intro  "
W06-0302 ", USA {ves,cardie}@cs.cornell.edu Abstract We target the problem of linking source mentions that belong to the same entity (source coreference resolution), which is needed for creating opinion summaries. In this paper we describe how source coreference resolution can be transformed into standard noun phrase coreference resolution, apply a state-of-the-art coreference resolution approach to the transformed data, and evaluate on an available corpus of manually annotated opinions. "
W06-0303 " On the World Wide Web, the volume of subjective information, such as opinions and reviews, has been increasing rapidly. The trends and rules latent in a large set of subjective descriptions can potentially be useful for decision-making purposes. In this paper, we propose a method for summarizing subjective descriptions, specifically opinions in Japanese. We visualize the pro and con arguments for a target topic, such as Should Japan introduce the summertime system? Users can summarize the arguments about the topic in order to choose a more reasonable standpoint for decision making. We evaluate our system, called OpinionReader, experimentally. 1 Intro  "
W06-0304 " Recent advances in text analysis have led to finer-grained semantic analysis, including automatic sentiment analysis the task of measuring documents, or chunks of text, based on emotive categories, such as positive or negative. However, considerably less progress has been made on efficient ways of exploring these measurements. This paper discusses approaches for visualizing the affective content of documents and describes an interactive capability for exploring emotion in a large document collection. 1 Intro  "
W06-0305 " An emerging task in text understanding and generation is to categorize information as fact or opinion and to further attribute it to the appropriate source. Corpus annotation schemes aim to encode such distinctions for NLP applications concerned with such tasks, such as information extraction, question answering, summarization, and generation. We describe an annotation scheme for marking the attribution of abstract objects such as propositions, facts and eventualities associated with discourse relations and their arguments annotated in the Penn Discourse TreeBank. The scheme aims to capture the source and degrees of factuality of the abstract objects. Key aspects of the scheme are annotation of the text spans signalling the attribution, and annotation of features recording the source, type, scopal polarity, and determinacy of attribution. "
W06-0306 " This paper presents a method for searching the web for sentences expressing opinions. To retrieve an appropriate number of opinions that users may want to read, declaratively subjective clues are used to judge whether a sentence expresses an opinion. We collected declaratively subjective clues in opinionexpressing sentences from Japanese web pages retrieved with opinion search queries. These clues were expanded with the semantic categories of the words in the sentences and were used as feature parameters in a Support Vector Machine to classify the sentences. Our experimental results using retrieved web pages on various topics showed that the opinion expressing sentences identified by the proposed method are congruent with sentences judged by humans to express opinions. 1 Introductio  "
W06-0307 " We report progress on adding affectdetection to a program for virtual dramatic improvisation, monitored by a human director. We have developed an affect-detection module to control an automated virtual actor and to contribute to the automation of directorial functions. The work also involves basic research into how affect is conveyed through metaphor. The project contributes to the application of sentiment and subjectivity analysis to the creation of emotionally believable synthetic agents for interactive narrative environments. 1 Intro  "
W06-1202 "<NoAbstract>"
W06-1203 " Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have noncompositional meanings. We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional. We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality. 1 Introduct  "
W06-1204 " It is well known that multi-word expressions are problematic in natural language processing. In previous literature, it has been suggested that information about their degree of compositionality can be helpful in various applications but it has not been proven empirically. In this paper, we propose a framework in which information about the multi-word expressions can be used in the word-alignment task. We have shown that even simple features like point-wise mutual information are useful for word-alignment task in English-Hindi parallel corpora. The alignment error rate which we achieve (AER = 0.5040) is significantly better (about 10% decrease in AER) than the alignment error rates of the state-of-art models (Och and Ney, 2003) (Best AER = 0.5518) on the English-Hindi dataset. 1 Introdu  "
W06-1205 "<NoAbstract>"
W06-1206 "However large a hand-crafted widecoverage grammar is, there are always going to be words and constructions that are not included in it and are going to cause parse failure. Due to their heterogeneous and flexible nature, Multiword Expressions (MWEs) provide an endless source of parse failures. As the number of such expressions in a speakers lexicon is equiparable to the number of single word units (Jackendoff, 1997), one major challenge for robust natural language processing systems is to be able to deal with MWEs. In this paper we propose to semi-automatically detect MWE candidates in texts using some error mining techniques and validating them using a combination of the World Wide Web as a corpus and some statistical measures. For the remaining candidates possible lexicosyntactic types are predicted, and they are subsequently added to the grammar as new lexical entries. This approach provides a significant increase in the coverage of these expressions. "
W06-1207 " Previous computational work on learning the semantic properties of verb-particle constructions (VPCs) has focused on their compositionality, and has left unaddressed the issue of which meaning of the component words is being used in a given VPC. We develop a feature space for use in classification of the sense contributed by the particle in a VPC, and test this on VPCs using the particle up. The features that capture linguistic properties of VPCs that are relevant to the semantics of the particle outperform linguistically uninformed word co-occurrence features in our experiments on unseen test VPCs. 1 Intro  "
W06-1301 " In this paper we deal with learning and forgetting of speech commands in speech dialogue systems. We discuss two mathematical models for learning and four models for forgetting. Furthermore, we describe the experiments used to determine the learning and forgetting curve in our environment. Our findings are compared to the theoretical models and based on this we deduce which models best describe learning and forgetting in our automotive environment. The resulting models are used to develop an adaptive help system for a speech dialogue system. The system provides only relevant context specific information. 1 Introduct  "
W06-1302 " We developed a multi-domain spoken dialogue system that can handle user requests across multiple domains. Such systems need to satisfy two requirements: extensibility and robustness against speech recognition errors. Extensibility is required to allow for the modification and addition of domains independent of other domains. Robustness against speech recognition errors is required because such errors are inevitable in speech recognition. However, the systems should still behave appropriately, even when their inputs are erroneous. Our system was constructed on an extensible architecture and is equipped with a robust and extensible domain selection method. Domain selection was based on three choices: (I) the previous domain, (II) the domain in which the speech recognition result can be accepted with the highest recognition score, and (III) other domains. With the third choice we newly introduced, our system can prevent dialogues from continuously being stuck in an erroneous domain. Our experimental results, obtained with 10 subjects, showed that our method reduced the domain selection errors by 18.3%, compared to a conventional method. "
W06-1303 " In this paper, we describe methods for building and evaluation of limited domain question-answering characters. Several classification techniques are tested, including text classification using support vector machines, language-model based retrieval, and cross-language information retrieval techniques, with the latter having the highest success rate. We also evaluated the effect of speech recognition errors on performance with users, finding that retrieval is robust until recognition reaches over 50% WER. 1 Introdu  "
W06-1304 " We explore the relationship between question answering and constraint relaxation in spoken dialog systems. We develop dialogue strategies for selecting and presenting information succinctly. In particular, we describe methods for dealing with the results of database queries in informationseeking dialogs. Our goal is to structure the dialogue in such a way that the user is neither overwhelmed with information nor left uncertain as to how to refine the query further. We present evaluation results obtained from a user study involving 20 subjects in a restaurant selection task. 1 Introduct  "
W06-1305 " Deciding what is the content of an utterance in dialogue is a potentially tricky business: should it be an entity computed using (solely/primarily) grammatical information or is it determined by recognition of participant intention using domain level inference? The decisions one makes on this score play a crucial role in any model of the interaction involved in grounding an utterance. Integrating the clarificatory potential of an utterance into the grounding process transforms the issue of content recognition into a more concrete issue: grammatically determined content has markedly distinct clarificatory potential from content determined using domain level inference. This leads to a new challenge: how to integrate the two types of content in such a way that both enables their distinct clarificatory potential to be maintained and allows content determined by domain level inference to feature in grounding. My talk will address this challenge. 36  "
W06-1306 ", The Netherlands {s.keizer,harry.bunt}@uvt.nl Abstract In this paper we present an approach to dialogue management that supports the generation of multifunctional utterances. It is based on the multidimensional dialogue act taxonomy and associated context model as developed in Dynamic Interpretation Theory (DIT). The multidimensional organisation of the taxonomy reflects that there are various aspects that dialogue participants have to deal with simultaneously during a dialogue. Besides performing some underlying task, a participant also has to pay attention to various aspects of the communication process itself, including social conventions. Therefore, a multi-agent approach is proposed, in which for each of the dimensions in the taxonomy a specialised dialogue act agent is designed, dedicated to the generation of dialogue acts from that particular dimension. These dialogue act agents operate in parallel on the information state of the system. For a simplified version of the taxonomy, a dialogue manager has been implemented and integrated into an interactive QA system. "
W06-1307 " This paper investigates the problems facing modelling agents beliefs in Discourse Representation Theory (DRT) and presents a viable solution in the form of a dialogue-based DRT representation of beliefs. Integrating modelling dialogue interaction into DRT allows modelling agents beliefs, intentions and mutual beliefs. Furthermore, it is one of the aims of the paper to account for the important notion of agents varying degrees of belief in different contexts. 1 1 Int  "
W06-1308 " This paper presents an extension to the Reference Domain Theory (Salmon-Alt, 2001) in order to solve plural references. While this theory doesnt take plural reference into account in its original form, this paper shows how several entities can be grouped together by building a new domain and how they can be accessed later on. We introduce the notion of super-domain, representing the access structure to all the plural referents of a given type.  "
W06-1309 " Integration of new utterances into context is a central task in any model for rational (human-machine) dialogues in natural language. In this paper, a pragmatics-first approach to specifying the meaning of utterances in terms of plans is presented. A rational dialogue is driven by the reaction of dialogue participants on how they find their expectations on changes in the environment satisfied by their observations of the outcome of performed actions. We present a computational model for this view on dialogues and illustrate it with examples from a real-world application. 1 A Pragmatics-First View on Dialogues Rational dialogues that are based on GRICEs maxims of conversation serve for jointly executing a task in the domain of discourse (called the application domain) by following a plan that could solve the task assigned to the participants of the dialogue. Therefore, the interpretation of new contributions and their integration into a dialogue is controlled by global factors (e.g. the assumption that all dialogue participants behave in a cooperative manner and work effectively towards the completion of a joint task) as well as by local factors (e.g. how does the new contribution serve in completing the current shared plan?). Ony if these factors are represented in an effective and efficient formal language, dialogue systems can be implemented. Examples of such models and their implementation are the informationstate-update approach (an implemented system is described in (Larsson, 2002)), or  more linguistically oriented  approaches like the adjacency-pair models or intentional models such as GROSZ and SIDNERs (see (Grosz and Sidner, 1986)). Even if it has been noted often that discourse structure and task structure are not isomorphic, only a few contributions to dialogue research focus on the question of how both structures interfere (see Sect. 2). In this paper, we emphasize that it is important to distinguish between the dialogue situation and the application situation: The former is modified whenever speech acts are performed, whereas the latter changes according to the effects of each action being executed. In this section, we will use a MAPTASK dialogue to show what the notions dialogue situation and application situation intend to mean. After presenting related work in Sect. 2, we present our approach first informally and then formally by explaining which AI algorithms we apply in order to turn the informal model into a computationally tractable one. "
W06-1310 " This paper investigates semantic and pragmatic presupposition in Discourse Representation Theory (DRT) and enhances the pragmatic perspective of presupposition in DRT. In doing so, it draws attention to the need to account for agent presupposition (i.e. both speaker and hearer presupposition) when dealing with pragmatic presupposition. Furthermore, this paper links this pragmatic conception of presupposition with the semantic one (sentence presupposition) through using information checks which agents are hypothesized to employ when making and receiving utterances. 1 1 Introductio  "
W06-1311 " This paper presents an evaluation of indirect anaphor resolution which considers as lexical resource the semantic tagging provided by the PALAVRAS parser. We describe the semantic tagging process and a corpus experiment. 1 Int  "
W06-1312 " We study the interplay of the discourse structure of a scientific argument with formal citations. One subproblem of this is to classify academic citations in scientific articles according to their rhetorical function, e.g., as a rival approach, as a part of the solution, or as a flawed approach that justifies the current research. Here, we introduce our annotation scheme with 12 categories, and present an agreement study. 1 Scientifi  "
W06-1313 "Abstract We present a dialogue manager for Call for Fire training dialogues. We describe the training environment, the domain, the features of its novel information statebased dialogue manager, the system it is a part of, and preliminary evaluation results. "
W06-1314 "<NoAbstract>"
W06-1315 " A problem in dialogue research is that of finding and managing expectations. Adjacency pair theory has widespread acceptance, but traditional classification features (in particular, previous-tag type features) do not exploit this information optimally. We suggest a method of dialogue segmentation that verifies adjacency pairs and allows us to use dialogue-level information within the entire segment and not just the previous utterance. We also use the  2 test for statistical significance as noise reduction to refine a list of pairs. Together, these methods can be used to extend expectation beyond the traditional classification features . "
W06-1316 " In this paper, we explain a rapid development method of multimodal dialogue sys-tem using MIML (Multimodal Interaction Markup Language), which defines dialogue patterns between human and various types of interactive agents. The feature of this language is three-layered description of agent-based interactive systems which separates task level description, interaction description and device dependent realization. MIML has advantages in high-level interaction description, modality extensibility and compatibility with standardized technologies. 1 Introduct  "
W06-1317 " In this paper we consider the problem of identifying and classifying discourse coherence relations. We report initial results over the recently released Discourse GraphBank (Wolf and Gibson, 2005). Our approach considers, and determines the contributions of, a variety of syntactic and lexico-semantic features. We achieve 81% accuracy on the task of discourse relation type classification and 70% accuracy on relation identification. 1 I  "
W06-1318 " We present a first analysis of interannotator agreement for the DIT ++ tagset of dialogue acts, a comprehensive, layered, multidimensional set of 86 tags. Within a dimension or a layer, subsets of tags are often hierarchically organised. We argue that especially for such highly structured annotation schemes the well-known kappa statistic is not an adequate measure of inter-annotator agreement. Instead, we propose a statistic that takes the structural properties of the tagset into account, and we discuss the application of this statistic in an annotation experiment. The experiment shows promising agreement scores for most dimensions in the tagset and provides useful insights into the usability of the annotation scheme, but also indicates that several additional factors influence annotator agreement. We finally suggest that the proposed approach for measuring agreement per dimension can be a good basis for measuring annotator agreement over the dimensions of a multidimensional annotation scheme. 1 Introdu  "
W06-1319 " We present a probabilistic approach for the interpretation of arguments that casts the selection of an interpretation as a model selection task. In selecting the best model, our formalism balances conflicting factors: model complexity against data fit, and structure complexity against belief reasonableness. We first describe our basic formalism, which considers interpretations comprising inferential relations, and then show how our formalism is extended to suppositions that account for the beliefs in an argument, and justifications that account for the inferences in an interpretation. Our evaluations with users show that the interpretations produced by our system are acceptable, and that there is strong support for the postulated suppositions and justifications. 1 Introduct  "
W06-1320 " We consider here the task of linear thematic segmentation of text documents, by using features based on word distributions in the text. For this task, a typical and often implicit assumption in previous studies is that a document has just one topic and therefore many algorithms have been tested and have shown encouraging results on artificial data sets, generated by putting together parts of different documents. We show that evaluation on synthetic data is potentially misleading and fails to give an accurate evaluation of the performance on real data. Moreover, we provide a critical review of existing evaluation metrics in the literature and we propose an improved evaluation metric. 1 Intro  "
W06-1322 " Dialog systems for mobile robots operating in the real world should enable mixedinitiative dialog style, handle multi-modal information involved in the communication and be relatively independent of the domain knowledge. Most dialog systems developed for mobile robots today, however, are often system-oriented and have limited capabilities. We present an agentbased dialog model that are specially designed for human-robot interaction and provide evidence for its efficiency with our implemented system. 1 Introduct  "
W06-1401 "Extended Abstract As the language generation community explores the possibility of an evaluation program for language generation, it behooves us to examine our experience in evaluation of other systems that produce text as output. Large scale evaluation of summarization systems and of question answering systems has been carried out for several years now. Summarization and question answering systems produce text output given text as input, while language generation produces text from a semantic representation. Given that the output has the same properties, we can learn from the mistakes and the understandings gained in earlier evaluations. In this invited talk, I will discuss what we have learned in the large scale summarization evaluations carried out in the Document Understanding Conferences (DUC) from 2001 to present, and in the large scale question answering evaluations carried out in TREC (e.g., the definition pilot) as well as the new large scale evaluations being carried out in the DARPA GALE (Global Autonomous Language Environment) program. DUC was developed and run by NIST and provides a forum for regular evaluation of summarization systems. NIST oversees the gathering of data, including both input documents and gold standard summaries, some of which is done by NIST and some of which is done by LDC. Each year, some 30 to 50 document sets were gathered as test data and somewhere between two to nine summaries were written for each of the input sets. NIST has carried out both manual and automatic evaluation by comparing system output against the gold standard summaries written by humans. The results are made public at the annual conference. In the most recent years, the number of participants has grown to 25 or 30 sites from all over the world. TREC is also run by NIST and provides an annual opportunity for evaluating the output of question-answering (QA) systems. Of the various QA evaluations, the one that is probably most illuminating for language generation is the definition pilot. In this evaluation, systems generated long answers (e.g., paragraph length or lists of facts) in response to a request for a definition. In contrast to DUC, no model answers were developed. Instead, system output was pooled and human judges determined which facts within the output were necessary (termed vital nuggets) and which were helpful, but not absolutely necessary (termed OK nuggets). Systems could then be scored on their recall of nuggets and precision of their response. DARPA GALE is a new program funded by DARPA that is running its own evaluation, carried out by BAE Systems, an independent contractor. Evaluation more closely resembles that done in TREC, but the systems scores will be compared against the scores of human distillers who carry out the same task. Thus, final numbers will report percent of human performance. In the DARPA GALE evaluation, which is a future event at the time of this writing, in addition to measuring properties such as precision and recall, BAE will also measure systems ability to find all occurrences of the same fact in the input (redundancy). One consideration for an evaluation program is the feel of the program. Does the evaluation program motivate researchers or does it cause headaches? I liken Columbias experience in DUC and currently in GALE to that of Max in Where the Wild Things Are by Maurice Sendak. We began with punishment (i.e., if you dont do well, your funding will be in jeopardy), encounter monsters along the way (seemingly arbitrary methods for 3 measuring output quality), finally tame the monsters and sail back peacefully across time. DUC has reached the peaceful stage, but GALE has not. The TREC definition pilot had less of a threat of punishment. Evaluation in all of these programs began at the request of the funders, with the goal of comparing how well different funded systems perform. Improvement over the years is also measured in order to determine if funding is well spent. This kind of goal creates anxiety in participants and makes it most important to get the details of the evaluation right; errors in how evaluation is carried out can have great consequences. Coming to agreement on the metrics used, the methodology for measuring output and the tasks on which performance is measured can be difficult; the environment does not feel friendly. Even if evaluation within the language generation community was not initiated with the same goals, I think it is reasonable to expect a certain amount of disagreement as the program gets off the ground. However, over time, researchers come to agreement on some portion of the task and these features become accepted. At this point in time, it is possible to see the benefits of the program. Certainly, within DUC, we are at this stage. DUC has generated large amounts of data, including both input document sets and multiple models of good output for each input set, which has spurred studies both on evaluation and summarization. Halteren and Teufel, for example, provide a method for annotation of content units and study consensus across summarizers (van Halteren and Teufel, 2003; Teufel and van Halteren, 2004b). Nenkova studies significant differences across DUC04 systems (Nenkova, 2005) as well as the properties of human and system summaries (Nenkova, 2006). We can credit DUC with the emergence of automatic methods for evaluation such as ROUGE (Lin and Hovy, 2003; Lin, 2004) which allow quick measurement of systems during development and enable evaluation of larger amounts of data. We have seen the development of manual methods for evaluation developed both within DUC (Harman and Over, 2004) and without. The Pyramid method (Nenkova and Passonneau, 2004) provides a annotation method and metric that addresses the issues of reliability and stability of scoring. Thus, research on evaluation of summarization has become a field in its own right resulting in greater understanding of the effect of different metrics and methodologies. From DUC and TREC, we have learned important characteristics of a large-scale evaluation, of which the top three might be:  Output can be measured by comparison against a human model, but we know that this comparison will only be valid if multiple models are used. There are multiple good summaries of the same input and if system output is compared against just one, the results will be biased.  If the task is appealing to a wide audience, the evaluation will spur research and motivate researchers to join in. We have seen this with growth of participation in DUC. One benefit of summarization and QA is that the task is domain-independent and thus, no one site has an advantage over others through experience with a particular domain.  Given the different ways in which evaluation can be carried out and the fact that different researchers may be biased towards methods which favor their own approach, it is important the evaluation be overseen by a neutral party which is not deeply involved in research on the task itself. On the other hand, some knowledge is necessary if the evaluation is to be well-designed. While my talk will focus on large scale evaluation programs that feature quantitative evaluation through comparison with a gold standard, there has been work on task-based evaluation of summarization (McKeown et al, 2005). Task-based evaluation is more intensive and to date, has not been done on a large scale across sites, but shows potential for indicating the usefulness of summarization systems. In this brief abstract, Ive suggested some of the topics that will be covered in my talk, which will tour the land of the wild things for evaluation, illuminating monsters and highlighting events that will allow more peaceful sailing. Evaluation can be a nightmare, but over time and particularly if carried out away from the influence of funding pressures, it can nurture a community of researchers with common goals. 4 Acknowledgments This material is based upon work supported in part by the ARDA AQUAINT program (Contract No. MDA908-02-C-0008 and Contract No. NBCHC040040) and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023 and Contract No. N66001-00-1-8919. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the DARPA or ARDA. "
W06-1402 " We describe a generation-oriented workbench for the Performance Grammar (PG) formalism, highlighting the treatment of certain word order and movement constraints in Dutch and German. PG enables a simple and uniform treatment of a heterogeneous collection of linear order phenomena in the domain of verb constructions (variably known as Cross-serial Dependencies, Verb Raising, Clause Union, Extraposition, Third Construction, Particle Hopping, etc.). The central data structures enabling this feature are clausal topologies: one-dimensional arrays associated with clauses, whose cells (slots) provide landing sites for the constituents of the clause. Movement operations are enabled by unification of lateral slots of topologies at adjacent levels of the clause hierarchy. The PGW generator assists the grammar developer in testing whether the implemented syntactic knowledge allows all and only the well-formed permutations of constituents. "
W06-1403 "mwhite/ Abstract This paper presents a novel algorithm for efficiently generating paraphrases from disjunctive logical forms. The algorithm is couched in the framework of Combinatory Categorial Grammar (CCG) and has been implemented as an extension to the OpenCCG surface realizer. The algorithm makes use of packed representations similar to those initially proposed by Shemtov (1997), generalizing the approach in a more straightforward way than in the algorithm ultimately adopted therein. 1 Introduction In recent years, the generate-and-select paradigm of natural language generation has attracted increasing attention, particularly for the task of surface realization. In this paradigm, symbolic methods are used to generate a space of possible phrasings, and statistical methods are used to select one or more outputs from this space. To specify the desired paraphrase space, one may either provide an input logical form that underspecifies certain realization choices, or include explicit disjunctions in the input LF (or both). Our experience suggests that disjunctive LFs are an important capability, especially as one seeks to make grammars reusable across applications, and to employ domain-specific, sentence-level paraphrases (Barzilay and Lee, 2003). Prominent examples of surface realizers in the generate-and-select paradigm include Nitrogen/Halogen (Langkilde, 2000; Langkilde-Geary, 2002) and Fergus (Bangalore and Rambow, 2000). More recently, generate-and-select realizers in the chart realization tradition (Kay, 1996) have appeared, including the OpenCCG (White, 2004) and LinGO (Carroll and Oepen, 2005) realizers. Chart realizers make it possible to use the same reversible grammar for both parsing and realization, and employ well-defined methods of semantic composition to construct semantic representations that can properly represent the scope of logical operators. In the chart realization tradition, previous work has not generally supported disjunctive logical forms, with (Shemtov, 1997) as the only published exception (to the authors knowledge). Arguably, part of the reason that disjunctive LFs have not yet been embraced more broadly by those working on chart realization is that Shemtovs solution, while ingenious, is dauntingly complex. Looking beyond chart realizers, both Nitrogen/Halogen and Fergus support some forms of disjunctive input; however, in comparison to Shemtovs inputs, theirs are less expressive, in that they do not allow disjunctions across different levels of the input structure. As an alternative to Shemtovs method, this paper presents a chart realization algorithm for generating paraphrases from disjunctive logical forms that is more straightforward to implement, together with an initial case study of the algorithms efficiency. As discussed in Section 5, the algorithm makes use of packed representations similar to those initially proposed by Shemtov, generalizing the approach in a way that avoids the problems that led Shemtov to reject his preliminary method. The algorithm is couched in the framework of Steedmans (2000) Combinatory Categorial Grammar (CCG) and has been implemented as an extension to the OpenCCG surface realizer. Though the algorithm is well suited to CCG, it is expected to be applicable to other constraint-based grammatical frameworks as well. 12 be pres, dcl e  based_on the, sg design d p  collection the, sg c  Funny_Day f v Villeroy_and_Boch (a) Semantic dependency graph for The design (is|s) based on the Funny Day collection by Villeroy and Boch. be pres, dcl e  based_on the, sg design d p  series sg c  Funny_Day f v Villeroy_and_Boch (b) Semantic dependency graph for The design (is|s) based on Villeroy and Bochs Funny Day series. be pres, dcl e  based_on the, sg design d p  collection|series ( the)?, sg c  Funny_Day f v Villeroy_and_Boch (c) Disjunctive semantic dependency graph covering (a)(b), i.e. The design (is|s) based on (the Funny Day (collection|series) by Villeroy and Boch | Villeroy and Bochs Funny Day (collection|series)). Figure 1: Example semantic dependency graphs from the COMIC dialogue system. @e(be  TENSEpres  MOODdcl  ARG(d  design  DETthe  NUMsg)  PROP(p  based on  ARTIFACTd  SOURCE(c  collection  DETthe  NUMsg  HASPROP(f  Funny Day)  CREATOR(v  V&B)))) (a) . . . @e(be  TENSEpres  MOODdcl  ARG(d  design  DETthe  NUMsg)  PROP(p  based on  ARTIFACTd  SOURCE(c  NUMsg  (DETthe)?  (collection  series)  HASPROP(f  Funny Day)  (CREATOR v  GENOWNER v ))))  @v(Villeroy and Boch) (c) Figure 2: HLDS for examples in Figure 1. 2 Disjunctive Logical Forms As an illustration of disjunctive logical forms, consider the semantic dependency graphs in Figure 1, which are taken from the COMIC 1 multimodal dialogue system. 2 Graphs such as these constitute the input to the OpenCCG realizer. Each node has a lexical predication (e.g. design) and a set of semantic features (e.g. NUMsg); nodes are connected via dependency relations (e.g. ARTIFACT). Given the lexical categories in the COMIC grammar, the graphs in Figure 1(a) and (b) fully specify their respective realizations, with the exception of the choice of the full or contracted form of the copula. To generalize over these alternatives, the disjunctive graph in (c) may be employed. This graph allows a free choice between the domain synonyms collection and series, as indicated by the vertical bar between their respective predications. The graph also allows a free choice between the CREATOR and GENOWNER relationslexicalized via by and the possessive, respectivelyconnecting the head c (collection or series) with the dependent v (for 1 http://www.hcrc.ed.ac.uk/comic/ 2 To simplify the exposition, the features specifying information structure and deictic gestures have been omitted, as have the semantic sorts of the discourse referents. 13 @e(see  ARG0(m  man)  ARG1(g  girl)  @o(on  ARG1(h  hill))  @w(with  ARG1(t  telescope))  ((MODo  @ h (MODw))  (@g(MODo)  (@g(MODw)  @ h (MODw)))  (MODw  (MODo  @g(MODo))))) Figure 3: Disjunctive LF for 5-way ambiguity in A man saw a girl on the hill with a telescope. Villeroy and Boch); this choice is indicated by an arc between the two dependency relations. Finally, the determiner feature (DETthe) on c is indicated as optional, via the question mark. It is worth pausing at this point to observe that in designing the COMIC grammar, the differences between (a) and (b) could perhaps have been collapsed. However, such a move would make it more difficult to reuse the grammar in other applicationsand indeed, the core of the grammar is shared with the FLIGHTS system (Moore et al., 2004)as it would presuppose that these paraphrases should always available in the same contexts. An example of a sentence-level paraphrase, whose context of applicability is more clearly limited, appears in (1): (1) (This design | This one | This) (is|s) (classic | in the classic style) | Here we have a (classic design | design in the classic style). This example shows some of the phrasings that may be used in COMIC to describe the style of a design that has not been discussed previously. The example includes a top-level disjunction between the use of a deictic NP this design | this one | this (with an accompanying pointing gesture) followed by the copula, or the use of the phrase here we have to introduce the design. While these alternatives can function as paraphrases in this context, it is difficult to see how one might specify them in a single underspecified (and applicationneutral) logical form. Graphs such as those in Figure 1 are represented internally using Hybrid Logic Dependency Semantics (HLDS), as in Figure 2. HLDS is a dependency-based approach to representing linguistic meaning developed by Baldridge and Kruijff (2002). In HLDS, hybrid logic (Blackburn, 2000) terms 3 are used to describe dependency 3 Hybrid logic extends modal logic with nominals, a new sort of basic formula that explicitly names states/nodes. Like propositions, nominals are first-class citizens of the object graphs. These graphs have been suggested as representations for discourse structure, and have their own underlying semantics (White, 2006). In HLDS, as can be seen in Figure 2(a), each semantic head is associated with a nominal that identifies its discourse referent, and heads are connected to their dependents via dependency relations, which are modeled as modal relations. Modal relations are also used to represent semantic features. In (c), two new operators are introduced to represent periphrastic alternatives and optional parts of the meaning, namely  and ()?, for exclusive-or and optionality, respectively. To indicate that a nominal represents a reference to a node that is considered a shared part of multiple alternatives, the nominal is annotated with a box, as exemplified by v . As will be discussed in Section 3.1, this notion of shared references is needed during the logical form flattening stage of the algorithm in order to determine which elementary predications are part of each alternative. As mentioned earlier, disjunctive LFs may contain alternations that are not at the same level. To illustrate, Figure 3 shows the representation (minus semantic features) for the 5-way ambiguity in A man saw a girl on the hill with a telescope (Shemtov, 1997, p. 45); in the figure, the nominal o (for on) can be a dependent of e (for see) or g (for girl), for example. As Shemtov explains, such packed representations can be useful in machine translation for generating ambiguitypreserving target language sentences. In a straight generation context, disjunctions that span levels enable one to compactly represent alternatives that differ in their head-dependent assumptions; for instance, to express contrast, one might employ the coordinate conjunction but as the sentence head, or the subordinate conjunction although as a dependent of the main clause head. "
W06-1404 " We describe an implemented generator for a spoken dialogue system that follows the overgeneration and ranking approach. We find that overgeneration based on bottom-up chart generation is wellsuited to a) model phenomena such as alignment and variation in dialogue, and b) address robustness issues in the face of imperfect generation input. We report evaluation results of a first user study involving 20 subjects. 1 Introdu  "
W06-1405 " It would be useful to enable dialogue agents to project, through linguistic means, their individuality or personality. Equally, each member of a pair of agents ought to adjust its language (to a greater or lesser extent) to match that of its interlocutor. We describe CRAG, which generates dialogues between pairs of agents, who are linguistically distinguishable, but able to align. CRAG-2 makes use of OPENCCG and an over-generation and ranking approach, guided by a set of language models covering both personality and alignment. We illustrate with examples of output, and briefly note results from user studies with the earlier CRAG-1, indicating how CRAG-2 will be further evaluated. Related work is discussed, along with current limitations and future directions. 1 Introductio  "
W06-1406 "<NoAbstract>"
W06-1407 " This paper describes adjective-to-verb paraphrasing in Japanese. In this paraphrasing, generated verbs require additional suffixes according to their difference in meaning. To determine proper suffixes for a given adjective-verb pair, we have examined the verbal features involved in the theory of Lexical Conceptual Structure. 1 Int  "
W06-1408 " Algorithms that generate expressions to identify a referent are mostly tailored towards objects which are in some sense conceived as holistic entities, describing them in terms of their properties and relations to other objects. This approach may prove not fully adequate when referring to components of structured objects, specifically for abstract objects in formal domains, where scope and relative positions are essential features. In this paper, we adapt the standard Dale and Reiter algorithm to specifics of such references as observed in a corpus about mathematical proofs. Extensions incorporated include an incremental specialization of property values for metonymic references, local and global positions reflecting group formations and implicature-based scope preferences to justify unique identification of the intended referent. The approach is primarily relevant for domains where abstract formal objects are prominent, but some of its features are also useful to extend the expressive repertoire of reference generation algorithms in other domains. 1 Int  "
W06-1409 " It is often desirable that referring expressions be chosen in such a way that their referents are easy to identify. In this paper, we investigate to what extent identification becomes easier by the addition of logically redundant properties.We focus on hierarchically structured domains, whose content is not fully known to the reader when the referring expression is uttered. Intro  "
W06-1410 "Abstract The natural language generation literature provides many algorithms for the generation of referring expressions. In this paper, we explore the question of whether these algorithms actually produce the kinds of expressions that people produce. We compare the output of three existing algorithms against a data set consisting of human-generated referring expressions, and identify a number of significant differences between what people do and what these algorithms do. On the basis of these observations, we suggest some ways forward that attempt to address these differences. "
W06-1411 " Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects in order to distinguish the target object from others. However, such an approach does not work well when there is no distinctive attribute among objects. To overcome this limitation, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introductio  "
W06-1412 " We report on a study examining the generation of noun phrases within a spoken dialog agent for a navigation domain. The task is to provide real-time instructions that direct the user to move between a series of destinations within a large interior space. A subtask within sentence planning is determining what form to choose for noun phrases. This choice is driven by both the discourse history and spatial context features derived from the directionfollowers position, e.g. his view angle, distance from the target referent and the number of similar items in view. The algorithm was developed as a decision tree and its output was evaluated by a group of human judges who rated 62.6% of the expressions generated by the system to be as good as or better than the language originally produced by human dialog partners. "
W06-1413 "U.K. {ikhan,gritchie,kvdeemte}@csd.abdn.ac.uk Abstract Existing algorithms for the Generation of Referring Expressions (GRE) aim at generating descriptions that allow a hearer to identify its intended referent uniquely; the length of the expression is also considered, usually as a secondary issue. We explore the possibility of making the trade-off between these two factors more explicit, via a general cost function which scores these two aspects separately. We sketch some more complex phenomena which might be amenable to this treatment. "
W06-1414 " This paper presents a method of querying databases by means of a natural languagelike interface which offers the advantage of minimal configuration necessary for porting the system. The method allows us to first automatically infer the set of possible queries that can apply to a given database, automatically generate a lexicon and grammar rules for expressing these queries, and then provide users with an interface that allows them to pose these queries in natural language without the well-known limitations of most natural language interfaces to databases. The way the queries are inferred and constructed means that semantic translation is performed with perfect reliability. 1 I  "
W06-1415 " In this paper, we present a questionanswering system on the Web which aims at generating intelligent answers to numerical questions. These answers are generated in a cooperative way: besides a direct answer, comments are generated to explain to the user the variation of numerical data extracted from the Web. We present the content determination and realisation tasks. We also present some elements of evaluation with respect to end-users. 1 Intro  "
W06-1416 "B, UK {L.A.Ha, R.Mitkov}@wlv.ac.uk Abstract We report the results of a pilot study on generating Multiple-Choice Test Items from medical text and discuss the main tasks involved in this process and how our system was evaluated by domain experts. "
W06-1417 " This paper presents the design of a discourse generator that plans the content and organization of lay-oriented genetic counseling documents containing arguments, and an experiment to evaluate the arguments. Due to the separation of domain, argument, and genre-specific concerns and the methodology used for acquiring a domain model, this approach should be applicable to argument generation in other domains. "
W06-1418 "<NoAbstract>"
W06-1419 " In this position paper, we argue that a common task and corpus are not the only ways to evaluate Natural Language Generation (NLG) systems. It might be, in fact, too narrow a view on evaluation and thus not be the best way to evaluate these systems. The aim of a common task and corpus is to allow for a comparative evaluation of systems, looking at the systems performances. It is thus a systemoriented view of evaluation. We argue here that, if we are to take a system oriented view of evaluation, the community might be better served by enlarging the view of evaluation, defining common dimensions and metrics to evaluate systems and approaches. We also argue that end-user (or usability) evaluations form another important aspect of a systems evaluation and should not be forgotten. 1 Introdu  "
W06-1420 "een {kvdeemte,ivdsluis,agatt}@csd.abdn.ac.uk Abstract This paper discusses the construction of a corpus for the evaluation of algorithms that generate referring expressions. It is argued that such an evaluation task requires a semantically transparent corpus, and controlled experiments are the best way to create such a resource. We address a number of issues that have arisen in an ongoing evaluation study, among which is the problem of judging the output of GRE algorithms against a human gold standard. "
W06-1421 "<NoAbstract>"
W06-3101 " Evaluation of machine translation output is an important but difficult task. Over the last years, a variety of automatic evaluation measures have been studied, some of them like Word Error Rate (WER), Position Independent Word Error Rate (PER) and BLEU and NIST scores have become widely used tools for comparing different systems as well as for evaluating improvements within one system. However, these measures do not give any details about the nature of translation errors. Therefore some analysis of the generated output is needed in order to identify the main problems and to focus the research efforts. On the other hand, human evaluation is a time consuming and expensive task. In this paper, we investigate methods for using of morpho-syntactic information for automatic evaluation: standard error measures WER and PER are calculated on distinct word classes and forms in order to get a better idea about the nature of translation errors and possibilities for improvements. 1 Introdu  "
W06-3102 " This paper presents some very preliminary results for and problems in developing a statistical machine translation system from English to Turkish. Starting with a baseline word model trained from about 20K aligned sentences, we explore various ways of exploiting morphological structure to improve upon the baseline system. As Turkish is a language with complex agglutinative word structures, we experiment with morphologically segmented and disambiguated versions of the parallel texts in order to also uncover relations between morphemes and function words in one language with morphemes and functions words in the other, in addition to relations between open class content words. Morphological segmentation on the Turkish side also conflates the statistics from allomorphs so that sparseness can be alleviated to a certain extent. We find that this approach coupled with a simple grouping of most frequent morphemes and function words on both sides improve the BLEU score from the baseline of 0.0752 to 0.0913 with the small training data. We close with a discussion on why one should not expect distortion parameters to model word-local morpheme ordering and that a new approach to handling complex morphotactics is needed. "
W06-3103 " The Arabic language has far richer systems of inflection and derivation than English which has very little morphology. This morphology difference causes a large gap between the vocabulary sizes in any given parallel training corpus. Segmentation of inflected Arabic words is a way to smooth its highly morphological nature. In this paper, we describe some statistically and linguistically motivated methods for Arabic word segmentation. Then, we show the efficiency of proposed methods on the Arabic-English BTEC and NIST tasks. 1 Intro  "
W06-3104 ", USA {dasmith,eisner}@jhu.edu Abstract Many syntactic models in machine translation are channels that transform one tree into another, or synchronous grammars that generate trees in parallel. We present a new model of the translation process: quasi-synchronous grammar (QG). Given a source-language parse tree T 1 , a QG defines a monolingual grammar that generates translations of T 1 . The trees T 2 allowed by this monolingual grammar are inspired by pieces of substructure in T 1 and aligned to T 1 at those points. We describe experiments learning quasi-synchronous context-free grammars from bitext. As with other monolingual language models, we evaluate the crossentropy of QGs on unseen text and show that a better fit to bilingual data is achieved by allowing greater syntactic divergence. When evaluated on a word alignment task, QG matches standard baselines. "
W06-3105 "CA 94705 {denero, dgillick, jyzhang, klein}@eecs.berkeley.edu Abstract We investigate why weights from generative models underperform heuristic estimates in phrasebased machine translation. We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics. The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM. In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score. "
W06-3106 "<NoAbstract>"
W06-3107 " In statistical machine translation, an alignment defines a mapping between the words in the source and in the target sentence. Alignments are used, on the one hand, to train the statistical models and, on the other, during the decoding process to link the words in the source sentence to the words in the partial hypotheses generated. In both cases, the quality of the alignments is crucial for the success of the translation process. In this paper, we propose an algorithm based on an Estimation of Distribution Algorithm for computing alignments between two sentences in a parallel corpus. This algorithm has been tested on different tasks involving different pair of languages. In the different experiments presented here for the two word-alignment shared tasks proposed in the HLT-NAACL 2003 and in the ACL 2005, the EDAbased algorithm outperforms the best participant systems. "
W06-3108 " We present discriminative reordering models for phrase-based statistical machine translation. The models are trained using the maximum entropy principle. We use several types of features: based on words, based on word classes, based on the local context. We evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus. Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system. 1 I  "
W06-3109 " In this paper we propose a generalization of the Stack-based decoding paradigm for Statistical Machine Translation. The well known single and multi-stack decoding algorithms defined in the literature have been integrated within a new formalism which also defines a new family of stackbased decoders. These decoders allows a tradeoff to be made between the advantages of using only one or multiple stacks. The key point of the new formalism consists in parameterizeing the number of stacks to be used during the decoding process, and providing an efficient method to decide in which stack each partial hypothesis generated is to be insertedduring the search process. Experimental results are also reported for a search algorithm for phrase-based statistical translation models. "
W06-3110 " Word posterior probabilities are a common approach for confidence estimation in automatic speech recognition and machine translation. We will generalize this idea and introduce n-gram posterior probabilities and show how these can be used to improve translation quality. Additionally, we will introduce a sentence length model based on posterior probabilities. We will show significant improvements on the Chinese-English NIST task. The absolute improvements of the BLEU score is between 1.1% and 1.6%. 1 Introdu  "
W06-3111 " In statistical machine translation, large numbers of parallel sentences are required to train the model parameters. However, plenty of the bilingual language resources available on web are aligned only at the document level. To exploit this data, we have to extract the bilingual sentences from these documents. The common method is to break the documents into segments using predefined anchor words, then these segments are aligned. This approach is not error free, incorrect alignments may decrease the translation quality. We present an alternative approach to extract the parallel sentences by partitioning a bilingual document into two pairs. This process is performed recursively until all the sub-pairs are short enough. In experiments on the Chinese-English FBIS data, our method was capable of producing translation results comparable to those of a state-of-the-art sentence aligner. Using a combination of the two approaches leads to better translation performance. 1 Int  "
W06-3112 " In this paper we present a novel method for deriving paraphrases during automatic MT evaluation using only the source and reference texts, which are necessary for the evaluation, and word and phrase alignment software. Using target language paraphrases produced through word and phrase alignment a number of alternative reference sentences are constructed automatically for each candidate translation. The method produces lexical and lowlevel syntactic paraphrases that are relevant to the domain in hand, does not use external knowledge resources, and can be combined with a variety of automatic MT evaluation system. "
W06-3113 " State of the art in statistical machine translation is currently represented by phrasebased models, which typically incorporate a large number of probabilities of phrase-pairs and word n-grams. In this work, we investigate data compression methods for efficiently encoding n-gram and phrase-pair probabilities, that are usually encoded in 32-bit floating point numbers. We measured the impact of compression on translation quality through a phrase-based decoder trained on two distinct tasks: the translation of European Parliament speeches from Spanish to English, and the translation of news agencies from Chinese to English. We show that with a very simple quantization scheme all probabilities can be encoded in just 4 bits with a relative loss in BLEU score on the two tasks by 1.0% and 1.6%, respectively. 1 Introduction  "
W06-3114 "We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back. Evaluation was done automatically using the BLEU score and manually on fluency and adequacy. For the 2006 NAACL/HLT Workshop on Machine Translation, we organized a shared task to evaluate machine translation performance. 14 teams from 11 institutions participated, ranging from commercial companies, industrial research labs to individual graduate students. The motivation for such a competition is to establish baseline performance numbers for defined training scenarios and test sets. We assembled various forms of data and resources: a baseline MT system, language models, prepared training and test sets, resulting in actual machine translation output from several state-of-the-art systems and manual evaluations. All this is available at the workshop website 1 . The shared task is a follow-up to the one we organized in the previous year, at a similar venue (Koehn and Monz, 2005). As then, we concentrated on the translation of European languages and the use of the Europarl corpus for training. Again, most systems that participated could be categorized as statistical phrase-based systems. While there is now a number of competitions  DARPA/NIST (Li, 2005), IWSLT (Eck and Hori, 2005), TC-Star  this one focuses on text translation between various European languages. This years shared task changed in some aspects from last years:  We carried out a manual evaluation in addition to the automatic scoring. Manual evaluation 1 http://www.statmt.org/wmt06/  http://www.statmt.org/wmt06/ was done by the participants. This revealed interesting clues about the properties of automatic and manual scoring.  We evaluated translation from English, in addition to into English. English was again paired with German, French, and Spanish. We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.  We included an out-of-domain test set. This allows us to compare machine translation performance in-domain and out-of-domain. "
W06-3115 " We present two translation systems experimented for the shared-task of Workshop on Statistical Machine Translation, a phrase-based model and a hierarchical phrase-based model. The former uses a phrasal unit for translation, whereas the latter is conceptualized as a synchronousCFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrasebased model performed very comparable to the phrase-based model. We also report a phrase/rule extraction technique differentiating tokenization of corpora. 1 Introduct  "
W06-3116 " We present here the translation system we used in this years WMT shared task. The main objective of our participation was to test RAMSES, an open source phrasebased decoder. For that purpose, we used the baseline system made available by the organizers of the shared task 1 to build the necessary models. We then carried out a pair-to-pair comparison of RAMSES with PHARAOH on the six different translation directions that we were asked to perform. We present this comparison in this paper. 1  "
W06-3117 " An important problem that is related to phrase-based statistical translation models is the obtaining of word phrases from an aligned bilingual training corpus. In this work, we propose obtaining word phrases by means of a Stochastic Inversion Translation Grammar. Experiments on the shared task proposed in this workshop with the Europarl corpus have been carried out and good results have been obtained. 1 Intro  "
W06-3118 "Abstract Improvements to Portage and its participation in the shared task of NAACL 2006 Workshop on Statistical Machine Translation are described. Promising ideas in phrase table smoothing and global distortion using feature-rich models are discussed as well as numerous improvements in the software base. "
W06-3119 " We present translation results on the shared task Exploiting Parallel Texts for Statistical Machine Translation generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories. We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence. Considering phrases that correspond to syntactic categories in the parse trees we develop techniques to augment (declare a syntactically motivated category for a phrase pair) and generalize (form mixed terminal and nonterminal phrases) the phrase table into a synchronous bilingual grammar. We present results on the French-to-English task for this workshop, representing significant improvements over the workshops baseline system. Our translation system is available open-source under the GNU General Public License. "
W06-3120 " This paper reports translation results for the Exploiting Parallel Texts for Statistical Machine Translation (HLT-NAACL Workshop on Parallel Texts 2006). We have studied different techniques to improve the standard Phrase-Based translation system. Mainly we introduce two reordering approaches and add morphological information. 1 Introdu  "
W06-3121 " This paper describes the open-source Phrase-Based Statistical Machine Translation Decoder Phramer. The paper also presents the UTD (HLTRI) system build for the WMT06 shared task. Our goal was to improve the translation quality by enhancing the translation table and by preprocessing the source language text 1 Intro  "
W06-3122 " Complex Language Models cannot be easily integrated in the first pass decoding of a Statistical Machine Translation system  the decoder queries the LM a very large number of times; the search process in the decoding builds the hypotheses incrementally and cannot make use of LMs that analyze the whole sentence. We present in this paper the Language Computers system for WMT06 that employs LMpowered reranking on hypotheses generated by phrase-based SMT systems 1 Intr  "
W06-3123 "Abstract The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). The models usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level. We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training. Constraining the joint model improves performance, showing results that are very close to stateof-the-art phrase-based models. It also allows it to scale up to larger corpora and therefore be more widely applicable. "
W06-3124 "<NoAbstract>"
W06-3125 " This work presents translation results for the three data sets made available in the shared task Exploiting Parallel Texts for Statistical Machine Translation of the HLT-NAACL 2006 Workshop on Statistical Machine Translation. All results presented were generated by using the Ngram-based statistical machine translation system which has been enhanced from the last years evaluation with a tagged target language model (using Part-Of-Speech tags). For both Spanish-English translation directions and the English-to-French translation task, the baseline system allows for linguistically motivated sourceside reorderings. 1 Introduct  "
W06-3301 " Most current definitional question answering systems apply one-size-fits-all lexicosyntactic patterns to identify definitions. By analyzing a large set of online definitions, this study shows that the semantic types of definienda constrain both lexical semantics and lexicosyntactic patterns of the definientia. For example, heart has the semantic type [Body Part, Organ, or Organ Component] and its definition (e.g., heart locates between the lungs) incorporates semantic-typedependent lexicosyntactic patterns (e.g., TERM locates ...) and terms (e.g., lung has the same semantic type [Body Part, Organ, or Organ Component]). In contrast, AIDS has a different semantic type [Disease or Syndrome]; its definition (e.g., An infectious disease caused by human immunodeficiency virus) consists of different lexicosyntactic patterns (e.g., ...causes by...) and terms (e.g., infectious disease has the semantic type [Disease or Syndrome]). The semantic types are defined in the widely used biomedical knowledge resource, the Unified Medical Language System (UMLS). "
W06-3302 " This paper describes a natural language query engine that enables users to search for entities, relationships, and events that are extracted from biological literature. The query interpretation is guided by a domain ontology, which provides a mapping between linguistic structures and domain conceptual relations. We focus on the usability of the natural language interface to users who are used to keywordbased information retrieval. Preliminary evaluation of our approach using the GENIA corpus and ontology shows promising results. 1 Intro  "
W06-3303 "Term Generalization and Synonym Resolution for Biological Abstracts: Using the Gene Ontology for Subcellular Localization Prediction  "
W06-3304 "<NoAbstract>"
W06-3305 " We introduce a new approach to named entity classification which we term a Priority Model. We also describe the construction of a semantic database called SemCat consisting of a large number of semantically categorized names relevant to biomedicine. We used SemCat as training data to investigate name classification techniques. We generated a statistical language model and probabilistic contextfree grammars for gene and protein name classification, and compared the results with the new model. For all three methods, we used a variable order Markov model to predict the nature of strings not represented in the training data. The Priority Model achieves an F-measure of 0.958-0.960, consistently higher than the statistical language model and probabilistic context-free grammar. 1 Introduction  "
W06-3306 " The identification of genes in biomedical text typically consists of two stages: identifying gene mentions and normalization of gene names. We have created an automated process that takes the output of named entity recognition (NER) systems designed to identify genes and normalizes them to standard referents. The system identifies human gene synonyms from online databases to generate an extensive synonym lexicon. The lexicon is then compared to a list of candidate gene mentions using various string transformations that can be applied and chained in a flexible order, followed by exact string matching or approximate string matching. Using a gold standard of MEDLINE abstracts manually tagged and normalized for mentions of human genes, a combined tagging and normalization system achieved 0.669 F-measure (0.718 precision and 0.626 recall) at the mention level, and 0.901 F-measure (0.957 precision and 0.857 recall) at the document level for documents used for tagger training. "
W06-3307 " The task of mining relations from collections of documents is usually approached in two different ways. One type of systems do relation extraction from individual sentences, followed by an aggregation of the results over the entire collection. Other systems follow an entirely different approach, in which co-occurrence counts are used to determine whether the mentioning together of two entities is due to more than simple chance. We show that increased extraction performance can be obtained by combining the two approaches into an integrated relation extraction model. 1 Introduction  "
W06-3308 " In this paper, we construct a biomedical semantic role labeling (SRL) system that can be used to facilitate relation extraction. First, we construct a proposition bank on top of the popular biomedical GENIA treebank following the PropBank annotation scheme. We only annotate the predicate-argument structures (PASs) of thirty frequently used biomedical predicates and their corresponding arguments. Second, we use our proposition bank to train a biomedical SRL system, which uses a maximum entropy (ME) model. Thirdly, we automatically generate argument-type templates which can be used to improve classification of biomedical argument types. Our experimental results show that a newswire SRL system that achieves an F-score of 86.29% in the newswire domain can maintain an F-score of 64.64% when ported to the biomedical domain. By using our annotated biomedical corpus, we can increase that F-score by 22.9%. Adding automatically generated template features further increases overall F-score by 0.47% and adjunct arguments (AM) Fscore by 1.57%, respectively. 1 Intro  "
W06-3309 "Generative Content Models for Structural Analysis of Medical Abstracts  Jimmy Lin 1,2 , Damianos Karakos 3 , Dina Demner-Fushman 2 , and Sanjeev Khudanpur  ur 3 1 College of Information St dies 3 Center for Langu ag and 2 Institute for Advanced Computer Studies Speech Processing University of Maryland Johns Hopkins University College Park, MD 20742, USA Baltimore, MD 21218, USA jimmylin@umd.edu , demner@cs.umd.edu (damianos, khudanpur)@jhu.edu Abstract The ability to accurately model the content structure of text is important for many natural language processing applications. This paper describes experiments with generative models for analyzing the discourse structure of medical abstracts, which generally follow the pattern of introduction, methods, results, and conclusions. We demonstrate that Hidden Markov Models are capable of accurately capturing the structure of such texts, and can achieve classification accuracy comparable to that of discriminative techniques. In addition, generative approaches provide advantages that may make them preferable to discriminative techniques such as Support Vector Machines under certain conditions. Our work makes two contributions: at the application level, we report good performance on an interesting task in an important domain; more generally, our results contribute to an ongoing discussion regarding the tradeoffs between generative and discriminative techniques. "
W06-3310 " A picture is worth a thousand words. Biomedical researchers tend to incorporate a significant number of images (i.e., figures or tables) in their publications to report experimental results, to present research models, and to display examples of biomedical objects. Unfortunately, this wealth of information remains virtually inaccessible without automatic systems to organize these images. We explored supervised machine-learning systems using Support Vector Machines to automatically classify images into six representative categories based on text, image, and the fusion of both. Our experiments show a significant improvement in the average Fscore of the fusion classifier (73.66%) as compared with classifiers just based on image (50.74%) or text features (68.54%). 1 Intro  "
W06-3312 " We present a small set of attachment heuristics for postnominal PPs occurring in full-text articles related to enzymes. A detailed analysis of the results suggests their utility for extraction of relations expressed by nominalizations (often with several attached PPs). The system achieves 82% accuracy on a manually annotated test corpus of over 3000 PPs from varied biomedical texts. 1 Int  "
W06-3314 "<NoAbstract>"
W06-3315 "<NoAbstract>"
W06-3316 "Abstract Resolving anaphora is an important step in the identification of named entities such as genes and proteins in biomedical scientific articles. The goal of this work is to resolve associative and coreferential anaphoric expressions making use of the rich domain resources (such as databases and ontologies) available for the biomedical area, instead of annotated training data. The results are comparable to extant state-of-the-art supervised methods in the same domain. The system is integrated into an interactive tool designed to assist FlyBase curators by aiding the identification of the salient entities in a given paper as a first step in the aggregation of information about them. "
W06-3317 "s Ben Goertzel  Hugo Pinto Ari Helj kka Applied Research Lab for National and Homeland Secu rity Novamente LLC Novamente LLC Virginia Tech 1405 Bernerd Place 1405 Bernerd Place Arlington VA 22216 Rockville MD 20851 Rockville MD 20851 ben@goertzel.org  hugo@vettalabs.com  heljakka@iki.fi  Izabela Freire Goertzel  Mike Ross Cassio Pennachin Novamente LLC SAIC Novamente LLC 1405 Bernerd Place 5971 Kingstowne Village Parkway 1405 Bernerd Place Rockville MD 20851 Kingstowne, VA 22315 Rockville MD 20851 izabela@goertzel.org  miross@objectsciences.com  cassio@vettalabs.com We describe BioLiterate, a prototype software system which infers relationships involving relationships between genes, proteins and malignancies from research abstracts, and has initially been tested in the domain of the molecular genetics of oncology. The architecture uses a natural language processing module to extract entities, dependencies and simple semantic relationships from texts, and then feeds these features into a probabilistic reasoning module which combines the semantic relationships extracted by the NLP module to form new semantic relationships. One application of this system is the discovery of relationships that are not contained in any individual abstract but are implicit in the combined knowledge contained in two or more abstracts. "
W06-3318 " Nested Named Entities (nested NEs), one containing another, are commonly seen in biomedical text, e.g., accounting for 16.7% of all named entities in GENIA corpus. While many works have been done in recognizing non-nested NEs, nested NEs have been largely neglected. In this work, we treat the task as a binary classification problem and solve it using Support Vector Machines. For each token in nested NEs, we use two schemes to set its class label: labeling as the outmost entity or the inner entity. Our preliminary results show that while the outmost labeling tends to work better in recognizing the outmost entities, the inner labeling recognizes the inner NEs better. This result should be useful for recognition of nested NEs.  "
W06-3319 " We propose a novel approach to the identification of biomedical terms in research publications using the Perceptron HMM algorithm. Each important term is identified and classified into a biomedical concept class. Our proposed system achieves a 68.6% F-measure based on 2,000 training Medline abstracts and 404 unseen testing Medline abstracts. The system achieves performance that is close to the state-of-the-art using only a small feature set. The Perceptron HMM algorithm provides an easy way to incorporate many potentially interdependent features. 1 Introduct  "
W06-3320 " We describe a pilot project in semiautomatically refactoring a biomedical corpus. The total time expended was just over three person-weeks, suggesting that this is a cost-efficient process. The refactored corpus is available for download at http://bionlp.sourceforge.net  http://bionlp.sourceforge.net . 1 I  "
W06-3321 "<NoAbstract>"
W06-3322 "<NoAbstract>"
W06-3323 "Named entity recognition of gene names, protein names, cell-lines, and other biologically relevant concepts has received significant attention by the research community. In this work, we considered named entity recognition of experimental techniques in biomedical articles. In our system to mine gene and disease associations, each association is categorized by the techniques used to derive the association. Categories are used to weight or remove associations, such as removing associations derived from microarray experiments. We report on a pilot study to identify experimental techniques. Three main activities are discussed: manual annotation, lexicon-based tagging, and document classification. Analysis of manual annotation suggests several interesting linguistic characteristics arise. Two lexicon-based tagging approaches demonstrate little agreement, suggesting sophisticated tagging algorithms may be necessary. Document classification using abstracts and titles is compared with full-text classification. In most cases, abstracts and titles show comparable performance to full-text. Corpus We built a corpus around our interest in gene associations with breast cancer to leverage the domain expertise of the authors. The corpus consisted of 247 sampled from 2571 papers associating breast cancer with a human gene in EntrezGene. Manual Annotation Manual annotation was primarily performed by a graduate student in bioinformatics and a computer science Ph.D. with a research emphasis in bioinformatics. Annotators were instructed to highlight direct mentions of experimental techniques. During the study, we noted low interannotator agreement and stopped the manual process after annotating 102 of 247 documents. Results were analyzed for linguistic characteristics. Experimental technique mentions appear with varying frequency in 6 typical document sections: Title, Abstract, Introduction, Materials and Methods, Results and Discussion. In some sections, such as Introduction, mentions are often for references and not the current document. Techniques such as transfection and immunoblotting, demonstrated diverse morphology. Other characteristics included use of synonyms and abbreviations, conjunctive phrases, and endophora. Tagging Tagging is commonly used for named entity recognition. In our context, associations are categorized by generating a list of all tagged techniques tagged in a document. Two taggers were tested on 247 documents to investigate the efficacy of two lexicons  MeSH and UMLS  containing experimental techniques. One approach used regular expressions for terms and permuted terms in the Investigative Techniques MeSH subhierarchy. The other used a natural language approach based on MetaMap Transfer (Aronson, 2001), mapping text to UMLS entries with the Laboratory Procedure semantic type (ID: T059). Low inter-annotator agreement between taggers was exhibited with a maximum  of 0.220. Both taggers exhibited limitations  failing to properly tag phrases such as Northern and Western analyses  and neither one is clearly superior to the other. 122 Full-Text Abstract Technique 1,000 All(59,628) 1,000 All(4,395) Electrophoresis (144) 72.4/81.2/76.5 70.3/77.4/73.7 68.9/79.8/74.0 69.2/75.3/72.1 Western Blot Analysis (132) 71.3/83.6/77.0 71.4/77.0/74.1 67.5/79.8/73.1 68.2/76.2/72.0 Gene Transfer Technique (137) 76.3/92.1/83.4 74.6/88.6/81.0 77.0/89.6/82.8 76.2/88.3/81.8 Pedigree(10) 52.0/91.3/66.2 81.2/72.5/76.6 42.9/77.7/55.3 59.9/58.3/59.1 Sequence Alignment (24) 53.0/66.6/59.1 96.6/17.9/30.1 61.2/59.5/60.3 67.0/36.5/47.2 Statistics (107) 70.7/57.7/63.5 70.3/60.8/65.2 73.6/58.6/65.2 71.5/63.5/67.2 Table 1: Precision/Recall/F1-scores for classifiers with different vocabulary sizes. Document Classification Document classification was also used to obtain a list of utilized experimental techniques. Each article is assigned to one or more classes corresponding to techniques used to generate results. Two distinct questions were investigated. First, how well does classification perform if only the abstract and title of the article are available? Second, how does vocabulary size affect the classification? Multinomial Na  ve Bayes models were implemented in Rainbow (McCallum, 1996; McCallum and Nigam, 1998) for 24 MeSH experimental techniques. Document frequency in each class ranged from 10 (Pedigree) to 144 (Electrophoresis). Vocabularies consist of top information gain ranked words. Classifiers were evaluated by precision, recall, and F1-scores averaged over 100 runs. The corpus was split into 2/3 training and 1/3 testing, randomly chosen for each run. Selected results are shown in Table 1. Full-text classifiers performed better than abstract based classifiers with a few exceptions: Sequence Alignment and Gene Transfer Techniques. The performance of abstract and full-text classifiers is comparable: F1 scores often differ by less than 5 points. Smaller vocabularies tend to improve the recall and overall F1 scores, while larger ones improved precision. Classifiers for low frequency (< 25) techniques generally performed poorly. One class, Pedigree, performed surprisingly well, with a maximum F1 of 76.6. Considering that Na  ve Bayes models are often baseline models and the small size of the corpus, classification performance is good. Related and Future Work For comprehensive reviews of current work in biomedical literature mining, refer to (Cohen and Hersh, 2005) and (Krallinger et al., 2005). As future work, we will continue manual annotation, validate the informative capacity of sections with experiments similar to Sinclair and Webber (Sinclair and Webber, 2004), and investigate improvements in tagging and classification. "
W06-3324 "<NoAbstract>"
W06-3325 " In modern biology, digitization of biosystematics publications is an important task. Extraction of taxonomic names from such documents is one of its major issues. This is because these names identify the various genera and species. This article reports on our experiences with learning techniques for this particular task. We say why established Named-Entity Recognition techniques are somewhat difficult to use in our context. One reason is that we have only very little training data available. Our experiments show that a combining approach that relies on regular expressions, heuristics, and word-level language recognition achieves very high precision and recall and allows to cope with those difficulties. 1 Introdu  "
W06-3326 "<NoAbstract>"
W06-3327 "<NoAbstract>"
W06-3401 " This paper investigates the usefulness of prosodic features in classifying rhetorical relations between utterances in meeting recordings. Five rhetorical relations of contrast, elaboration, summary, question and cause are explored. Three training methods supervised, unsupervised, and combined are compared, and classification is carried out using support vector machines. The results of this pilot study are encouraging but mixed, with pairwise classification achieving an average of 68% accuracy in discerning between relation pairs using only prosodic features, but multi-class classification performing only slightly better than chance. 1 Introductio  "
W06-3402 " In a context where information retrieval is extended to spoken documents including conversations, it will be important to provide users with the ability to seek informational content, rather than socially motivated small talk that appears in many conversational sources. In this paper we present a preliminary study aimed at automatically identifying irrelevance in the domain of telephone conversations. We apply a standard machine learning algorithm to build a classifier that detects offtopic sections with better-than-chance accuracy and that begins to provide insight into the relative importance of features for identifying utterances as on topic or not. 1 Introduct  "
W06-3403 " This paper examines language similarity in messages over time in an online community of adolescents from around the world using three computational measures: Spearmans Correlation Coefficient, Zipping and Latent Semantic Analysis. Results suggest that the participants language diverges over a six-week period, and that divergence is not mediated by demographic variables such as leadership status or gender. This divergence may represent the introduction of more unique words over time, and is influenced by a continual change in subtopics over time, as well as community-wide historical events that introduce new vocabulary at later time periods. Our results highlight both the possibilities and shortcomings of using document similarity measures to assess convergence in language use. 1 Intro  "
W06-3404 " Our goal is to automatically detect the functional roles that meeting participants play, as well as the expertise they bring to meetings. To perform this task, we build decision tree classifiers that use a combination of simple speech features (speech lengths and spoken keywords) extracted from the participants speech in meetings. We show that this algorithm results in a role detection accuracy of 83% on unseen test data, where the random baseline is 33.3%. We also introduce a simple aggregation mechanism that combines evidence of the participants expertise from multiple meetings. We show that this aggregation mechanism improves the role detection accuracy from 66.7% (when aggregating over a single meeting) to 83% (when aggregating over 5 meetings). 1 Introduct  "
W06-3405 "A 94305 {mpurver,ehlen,niekrasz}@stanford.edu Abstract We investigated automatic action item detection from transcripts of multi-party meetings. Unlike previous work (Gruenstein et al., 2005), we use a new hierarchical annotation scheme based on the roles utterances play in the action item assignment process, and propose an approach to automatic detection that promises improved classification accuracy while enabling the extraction of useful information for summarization and reporting. "
W06-3406 " In email conversational analysis, it is often useful to trace the the intents behind each message exchange. In this paper, we consider classification of email messages as to whether or not they contain certain intents or email-acts, such as propose a meeting or commit to a task. We demonstrate that exploiting the contextual information in the messages can noticeably improve email-act classification. More specifically, we describe a combination of n-gram sequence features with careful message preprocessing that is highly effective for this task. Compared to a previous study (Cohen et al., 2004), this representation reduces the classification error rates by 26.4% on average. Finally, we introduce Ciranda: a new open source toolkit for email speech act prediction. 1 Introduction  "
W06-3407 " We introduce a novel topic segmentation approach that combines evidence of topic shifts from lexical cohesion with linguistic evidence such as syntactically distinct features of segment initial and final contributions. Our evaluation shows that this hybrid approach outperforms state-of-the-art algorithms even when applied to loosely structured, spontaneous dialogue. Further analysis reveals that using dialogue exchanges versus dialogue contributions improves topic segmentation quality. 1 Introdu  "
W06-3408 " We present a system for analyzing conversational data. The system includes state-ofthe-art natural language processing components that have been modified to accommodate the unique nature of conversational data. In addition, we leverage the added richness of conversational data by analyzing various aspects of the participants and their relationships to each other. Our tool provides users with the ability to easily identify topics or persons of interest, including who talked to whom, when, entities that were discussed, etc. Using this tool, one can also isolate more complex networks of information: individuals who may have discussed the same topics but never talked to each other. The tool includes a UI that plots information over time, and a semantic graph that highlights relationships of interest. 1 Introduction  "
W06-3409 " This paper presents a pragmatic approach to Discourse Representation Theory (DRT) in an attempt to address the pragmatic limitations of DRT (Werth 1999; Simons 2003). To achieve a more pragmatic DRT model, this paper extends standard DRT framework to incorporate more pragmatic elements such as representing agents cognitive states and the complex process through which agents recognize utterances employing the linguistic content in forming mental representations of other agents cognitive states. The paper gives focus to the usually ignored link in DRT literature between speaker beliefs and the linguistic content, and between the linguistic content and hearers beliefs. 1 Introduct  "
W06-3601 " A syntax-directed translator first parses the source-language input into a parsetree, and then recursively converts the tree into a string in the target-language. We model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring. Initial experimental results on English-to-Chinese translation are presented. "
W06-3602 " This paper presents a series of efficient dynamic-programming (DP) based algorithms for phrase-based decoding and alignment computation in statistical machine translation (SMT). The DP-based decoding algorithms are analyzed in terms of shortest path-finding algorithms, where the similarity to DP-based decoding algorithms in speech recognition is demonstrated. The paper contains the following original contributions: 1) the DP-based decoding algorithm in (Tillmann and Ney, 2003) is extended in a formal way to handle phrases and a novel pruning strategy with increased translation speed is presented 2) a novel alignment algorithm is presented that computes a phrase alignment efficiently in the case that it is consistent with an underlying word alignment. Under certain restrictions, both algorithms handle MT-related problems efficiently that are generally NP complete (Knight, 1999). 1 Introduct  "
W06-3603 " This paper presents a discriminative parser that does not use a generative model in any way, yet whose accuracy still surpasses a generative baseline. The parser performs feature selection incrementally during training, as opposed to a priori, which enables it to work well with minimal linguistic cleverness. The main challenge in building this parser was fitting the training data into memory. We introduce gradient sampling, which increased training speed 100-fold. Our implementation is freely available at http://nlp.cs.nyu.edu/parser/ . 1 Introdu  "
W06-3604 " We present a classification-based word prediction model based on IGTREE, a decision-tree induction algorithm with favorable scaling abilities and a functional equivalence to n-gram models with backoff smoothing. Through a first series of experiments, in which we train on Reuters newswire text and test either on the same type of data or on general or fictional text, we demonstrate that the system exhibits log-linear increases in prediction accuracy with increasing numbers of training examples. Trained on 30 million words of newswire text, prediction accuracies range between 12.6% on fictional text and 42.2% on newswire text. In a second series of experiments we compare all-words prediction with confusable prediction, i.e., the same task, but specialized to predicting among limited sets of words. Confusable prediction yields high accuracies on nine example confusable sets in all genres of text. The confusable approach outperforms the all-words-prediction approach, but with more data the difference decreases. 1 Introductio  "
W06-3605 " We consider the problem of identifying among many candidates a single best solution which jointly maximizes several domain-specific target functions. Assuming that the candidate solutions can be generated incrementally, we model the error in prediction due to the incompleteness of partial solutions as a normally distributed random variable. Using this model, we derive a probabilistic search algorithm that aims at finding the best solution without the necessity to complete and rank all candidate solutions. We do not assume a Viterbi-type decoding, allowing a wider range of target functions. We evaluate the proposed algorithm on the problem of best parse identification, combining simple heuristic with more complex machine-learning based target functions. We show that the search algorithm is capable of identifying candidates with a very high score without completing a significant proportion of the candidate solutions. "
W06-3606 "etts Amherst, MA 01003 {culotta, mccallum}@cs.umass.edu Abstract Markov logic is a highly expressive language recently introduced to specify the connectivity of a Markov network using first-order logic. While Markov logic is capable of constructing arbitrary first-order formulae over the data, the complexity of these formulae is often limited in practice because of the size and connectivity of the resulting network. In this paper, we present approximate inference and estimation methods that incrementally instantiate portions of the network as needed to enable firstorder existential and universal quantifiers in Markov logic networks. When applied to the problem of identity uncertainty, this approach results in a conditional probabilistic model that can reason about objects, combining the expressivity of recently introduced BLOG models with the predictive power of conditional training. We validate our algorithms on the tasks of citation matching and author disambiguation. "
W06-3701 " We describe a highly interactive system for bidirectional, broad-coverage spoken language communication in the healthcare area. The paper briefly reviews the system's interactive foundations, and then goes on to discuss in greater depth issues of practical usability. We present our Translation Shortcuts facility, which minimizes the need for interactive verification of sentences after they have been vetted once, considerably speeds throughput while maintaining accuracy, and allows use by minimally literate patients for whom any mode of text entry might be difficult. We also discuss facilities for multimodal input, in which handwriting, touch screen, and keyboard interfaces are offered as alternatives to speech input when appropriate. In order to deal with issues related to sheer physical awkwardness, we briefly mention facilities for hands-free or eyes-free operation of the system. Finally, we point toward several directions for future improvement of the system. 1 Introductio  "
W06-3702 " We present a task-level evaluation of the French to English version of MedSLT, a medium-vocabulary unidirectional controlled language medical speech translation system designed for doctor-patient diagnosis interviews. Our main goal was to establish task performance levels of novice users and compare them to expert users. Tests were carried out on eight medical students with no previous exposure to the system, with each student using the system for a total of three sessions. By the end of the third session, all the students were able to use the system confidently, with an average task completion time of about 4 minutes. 1 Introdu  "
W06-3703 " S-MINDS is a speech translation engine, which allows an English speaker to communicate with a non-English speaker easily within a question-and-answer, interview-style format. It can handle limited dialogs such as medical triage or hospital admissions. We have built and tested an English-Korean system for doing medical triage with a translation accuracy of 79.8% (for English) and 78.3% (for Korean) for all non-rejected utterances. We will give an overview of the system building process and the quantitative and qualitatively system performance. 1 Introdu  "
W06-3704 "<NoAbstract>"
W06-3705 " This position paper looks critically at a number of aspects of current research into spoken language translation (SLT) in the medical domain. We first discuss the user profile for medical SLT, criticizing designs which assume that the doctor will necessarily need or want to control the technology. If patients are to be users on an equal standing, more attention must be paid to usability issues. We focus briefly on the issue of feedback in SLT systems, pointing out the difficulties of relying on text-based paraphrases. We consider the delicate issue of evaluating medical SLT systems, noting that some of the standard and much-used evaluation techniques for all aspects of the SLT chain might not be suitable for use with real users, even if they are role-playing. Finally, we discuss the idea that the pathway to healthcare involves much more than a face-to-face interview with a medical professional, and that different technologies including but not restricted to SLT will be appropriate along this pathway. 1 I  "
W06-3706 " We describe a highly interactive system for bidirectional, broad-coverage spoken language communication in the healthcare area. The paper briefly reviews the system's interactive foundations, and then goes on to discuss in greater depth our Translation Shortcuts facility, which minimizes the need for interactive verification of sentences after they have been vetted. This facility also considerably speeds throughput while maintaining accuracy, and allows use by minimally literate patients for whom any mode of text entry might be difficult. 1 Introdu  "
W06-3707 " MedSLT is a unidirectional medical speech translation system intended for use in doctor-patient diagnosis dialogues, which provides coverage of several different language pairs and subdomains. Vocabulary ranges from about 350 to 1000 surface words, depending on the language and subdomain. We will demo both the system itself and the development environment, which uses a combination of rule-based and data-driven methods to construct efficient recognisers, generators and transfer rule sets from small corpora. 1 Ove  "
W06-3708 " Sehdas 2-way speech translation system, S-MINDS, interprets between provider and patient in routine medical interactions with very high accuracy. Optimizing the system for new tasks or languages requires very little data. New developments include a hybrid translation approach that allows participants to say complex or out-ofdomain utterances, the expansion of hands-free functionality, and the ability to deliver the most urgent expressions instantaneously. 1  "
W06-3709 "<NoAbstract>"
W06-3710 " In  this  paper,  we  are  proposing  a  multi  lingual prototype that can effectively col  lect, record and document medical data in  a domain specific  environment.  The aim  of this  project  is  to  develop an  electronic  support  system  that  can  be  used  to  assist  asthma management in an emergency de  partment.  "
W07-0601 " Unsupervised Data-Oriented Parsing models (U-DOP) represent a class of structure bootstrapping models that have achieved some of the best unsupervised parsing results in the literature. While U-DOP was originally proposed as an engineering approach to language learning (Bod 2005, 2006a), it turns out that the model has a number of properties that may also be of linguistic and cognitive interest. In this paper we will focus on the original U-DOP model proposed in Bod (2005) which computes the most probable tree from among the shortest derivations of sentences. We will show that this U-DOP model can learn both rule-based and exemplar-based aspects of language, ranging from agreement and movement phenomena to discontiguous contructions, provided that productive units of arbitrary size are allowed. We argue that our results suggest a rapprochement between nativism and empiricism.  "
W07-0602 " We apply machine learning techniques to study language transfer, a major topic in the theory of Second Language Acquisition (SLA). Using an SVM for the problem of native language classification, we show that a careful analysis of the effects of various features can lead to scientific insights. In particular, we demonstrate that character bigrams alone allow classification levels of about 66% for a 5-class task, even when content and function word differences are accounted for. This may show that native language has a strong effect on the word choice of people writing in a second language. 1 Intro  "
W07-0603 " This paper discusses a new, open-source software program, called Phon, that is designed for the transcription, coding, and analysis of phonological corpora. Phon provides support for multimedia data linkage, segmentation, multiple-blind transcription, transcription validation, syllabification, alignment of target and actual forms, and data analysis. All of these functions are available through a user-friendly graphical interface. Phon, available on most computer platforms, supports data exchange among researchers with the TalkBank XML document format and the Unicode character set.. This program provides the basis for the elaboration of PhonBank, a database project that seeks to broaden the scope of CHILDES into phonological development and disorders. 1 Introduct  "
W07-0604 "Abstract Corpora of child language are essential for psycholinguistic research. Linguistic annotation of the corpora provides researchers with better means for exploring the development of grammatical constructions and their usage. We describe an ongoing project that aims to annotate the English section of the CHILDES database with grammatical relations in the form of labeled dependency structures. To date, we have produced a corpus of over 65,000 words with manually curated gold-standard grammatical relation annotations. Using this corpus, we have developed a highly accurate data-driven parser for English CHILDES data. The parser and the manually annotated data are freely available for research purposes. "
W07-0605 " Empirical data regarding the syntactic complexity of childrens speech is important for theories of language acquisition. Currently much of this data is absent in the annotated versions of the CHILDES database. In this perliminary study, we show that a state-ofthe-art subcategorization acquisition system of Preiss et al. (2007) can be used to extract largescale subcategorization (frequency) information from the (i) child and (ii) child-directed speech within the CHILDES database without any domain-specific tuning. We demonstrate that the acquired information is sufficiently accurate to confirm and extend previously reported research findings. We also report qualitative results which can be used to further improve parsing and lexical acquisition technology for child language data in the future. "
W07-0606 " We present a cognitive model of inducing verb selectional preferences from individual verb usages. The selectional preferences for each verb argument are represented as a probability distribution over the set of semantic properties that the argument can possessa semantic profile. The semantic profiles yield verb-specific conceptualizations of the arguments associated with a syntactic position. The proposed model can learn appropriate verb profiles from a small set of noisy training data, and can use them in simulating human plausibility judgments and analyzing implicit object alternation. 1 Int  "
W07-0607 "Abstract We introduce Incremental Semantic Analysis, a fully incremental word space model, and we test it on longitudinal child-directed speech data. On this task, ISA outperforms the related Random Indexing algorithm, as well as a SVD-based technique. In addition, the model has interesting properties that might also be characteristic of the semantic space of children. "
W07-0608 " Naming requires recognition. Recognition requires the ability to categorize objects and events. Infants under six months of age are capable of making fine-grained discriminations of object boundaries and three-dimensional space. At 8 to 10 months, a childs object categories are sufficiently stable and flexible to be used as the foundation for labeling and referencing actions. What mechanisms in the brain underlie the unfolding of these capacities? In this article, we describe a neural network model which attempts to simulate, in a biologically plausible way, the process by which infants learn how to recognize objects and words through exposure to visual stimuli and vocal sounds. 1 I  "
W07-0609 " A testing procedure is proposed to reevaluate the syntactic burst in children over age two. The experimentation is based on the childrens capacities in perception, memory, association and cognition, and does not presuppose any specific innate grammatical capaciti he CHILDES acquisition of complex across the board gra does not appear to be necessary to explain childrens behavior before age three or more. At that age, much more complex and structured input data will be available to children, thereby increasing their learning capacities and reducing the limitations on knowledge they may acquire. 2  "
W07-0610 " Semantic networks have been used successfully to explain access to the mental lexicon. Topological analyses of these networks have focused on acquisition and generation. We extend this work to look at models that distinguish semantic relations. We find the scale-free properties of association networks are not found in synonymy-homonymy networks, and that this is consistent with studies of childhood acquisition of these relationships. We further find that distributional models of language acquisition display similar topological properties to these networks. 1 Introduct  "
W07-0611 " We compare three recent proposals adding a topology to OT: McCarthys Persistent OT, Smolenskys ICS and B  r  os SA-OT. To test their learnability, constraint rankings are learnt from SA-OTs output. The errors in the output, being more than mere noise, follow from the topology. Thus, the learner has to reconstructs her competence having access only to the teachers performance. 1 I  "
W07-0701 " Today's statistical machine translation systems generalize poorly to new domains. Even small shifts can cause precipitous drops in translation quality. Phrasal systems rely heavily, for both reordering and contextual translation, on long phrases that simply fail to match outof-domain text. Hierarchical systems attempt to generalize these phrases but their learned rules are subject to severe constraints. Syntactic systems can learn lexicalized and unlexicalized rules, but the joint modeling of lexical choice and reordering can narrow the applicability of learned rules. The treelet approach models reordering separately from lexical choice, using a discriminatively trained order model, which allows treelets to apply broadly, and has shown better generalization to new domains, but suffers a factorially large search space. We introduce a new reordering model based on dependency order templates, and show that it outperforms both phrasal and treelet systems on in-domain and out-of-domain text, while limiting the search space. 1  "
W07-0702 "School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW, UK Abstract Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level. The challenge is incorporating this information into the translation process. Factored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings. "
W07-0703 " We provide an in-depth analysis of the integration of an Arabic-to-English transliteration system into a general-purpose phrase-based statistical machine translation system. We study the integration from different aspects and evaluate the improvement that can be attributed to the integration using the BLEU metric. Our experiments show that a transliteration module can help significantly in the situation where the test data is rich with previously unseen named entities. We obtain 70% and 53% of the theoretical maximum improvement we could achieve, as measured by an oracle on development and test sets respectively for OOV words (out of vocabulary source words not appearing in the phrase table). 1 Introduct  "
W07-0704 "Abstract We investigate different representational granularities for sub-lexical representation in statistical machine translation work from English to Turkish. We find that (i) representing both Turkish and English at the morpheme-level but with some selective morpheme-grouping on the Turkish side of the training data, (ii) augmenting the training data with sentences comprising only the content words of the original training data to bias root word alignment, (iii) reranking the n-best morpheme-sequence outputs of the decoder with a word-based language model, and (iv) using model iteration all provide a non-trivial improvement over a fully word-based baseline. Despite our very limited training data, we improve from 20.22 BLEU points for our simplest model to 25.08 BLEU points for an improvement of 4.86 points or 24% relative. "
W07-0705 "Current statistical machine translation systems handle the translation process as the transformation of a string of symbols into another string of symbols. Normally the symbols dealt with are the words in different languages, sometimes with some additional information included, like morphological data. In this work we try to push the approach to the limit, working not on the level of words, but treating both the source and target sentences as a string of letters. We try to find out if a nearly unmodified state-of-the-art translation system is able to cope with the problem and whether it is capable to further generalize translation rules, for example at the level of word suffixes and translation of unseen words. Experiments are carried out for the translation of Catalan to Spanish. "
W07-0706 "<NoAbstract>"
W07-0707 " Evaluation and error analysis of machine translation output are important but difficult tasks. In this work, we propose a novel method for obtaining more details about actual translation errors in the generated output by introducing the decomposition of Word Error Rate (WER) and Position independent word Error Rate (PER) over different Partof-Speech (POS) classes. Furthermore, we investigate two possible aspects of the use of these decompositions for automatic error analysis: estimation of inflectional errors and distribution of missing words over POS classes. The obtained results are shown to correspond to the results of a human error analysis. The results obtained on the European Parliament Plenary Session corpus in Spanish and English give a better overview of the nature of translation errors as well as ideas of where to put efforts for possible improvements of the translation system. 1 Introdu  "
W07-0708 "<NoAbstract>"
W07-0709 " We propose a novel syntax-based model for statistical machine translation in which meta-structure (MS) and meta-structure sequence (SMS) of a parse tree are defined. In this framework, a parse tree is decomposed into SMS to deal with the structure divergence and the alignment can be reconstructed at different levels of recombination of MS (RM). RM pairs extracted can perform the mapping between the substructures across languages. As a result, we have got not only the translation for the target language, but an SMS of its parse tree at the same time. Experiments with BLEU metric show that the model significantly outperforms Pharaoh, a state-art-theart phrase-based system. 1 Introductio  "
W07-0710 "Modern statistical machine translation systems may be seen as using two components: feature extraction, that summarizes information about the translation, and a log-linear framework to combine features. In this paper, we propose to relax the linearity constraints on the combination, and hence relaxing constraints of monotonicity and independence of feature functions. We expand features into a non-parametric, non-linear, and high-dimensional space. We extend empirical Bayes reward training of model parameters to meta parameters of feature generation. In effect, this allows us to trade away some human expert feature design for data. Preliminary results on a standard task show an encouraging improvement. "
W07-0711 " In this paper, we present a Bayesian Learning based method to train word dependent transition models for HMM based word alignment. We present word alignment results on the Canadian Hansards corpus as compared to the conventional HMM and IBM model 4. We show that this method gives consistent and significant alignment error rate (AER) reduction. We also conducted machine translation (MT) experiments on the Europarl corpus. MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment. 1 Introdu  "
W07-0712 " Statistical machine translation, as well as other areas of human language processing, have recently pushed toward the use of large scale n-gram language models. This paper presents efficient algorithmic and architectural solutions which have been tested within the Moses decoder, an open source toolkit for statistical machine translation. Experiments are reported with a high performing baseline, trained on the Chinese-English NIST 2006 Evaluation task and running on a standard Linux 64-bit PC architecture. Comparative tests show that our representation halves the memory required by SRI LM Toolkit, at the cost of 44% slower translation speed. However, as it can take advantage of memory mapping on disk, the proposed implementation seems to scale-up much better to very large language models: decoding with a 289-million 5-gram language model runs in 2.1Gb of RAM. 1 Introdu  "
W07-0713 "Abstract We introduce a novel evaluation scheme for the human evaluation of different machine translation systems. Our method is based on direct comparison of two sentences at a time by human judges. These binary judgments are then used to decide between all possible rankings of the systems. The advantages of this new method are the lower dependency on extensive evaluation guidelines, and a tighter focus on a typical evaluation task, namely the ranking of systems. Furthermore we argue that machine translation evaluations should be regarded as statistical processes, both for human and automatic evaluation. We show how confidence ranges for state-of-the-art evaluation measures such as WER and TER can be computed accurately and efficiently without having to resort to Monte Carlo estimates. We give an example of our new evaluation scheme, as well as a comparison with classical automatic and human evaluation on data from a recent international evaluation campaign. "
W07-0714 "Karolina Owczarzak Josef van Genabith Andy Way National Centre for Language Technology School of Computing, Dublin City University Dublin 9, Ireland {owczarzak,josef,away}@computing.dcu.ie Abstract We present a method for evaluating the quality of Machine Translation (MT) output, using labelled dependencies produced by a Lexical-Functional Grammar (LFG) parser. Our dependencybased method, in contrast to most popular string-based evaluation metrics, does not unfairly penalize perfectly valid syntactic variations in the translation, and the addition of WordNet provides a way to accommodate lexical variation. In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores. "
W07-0715 " Attempts to estimate phrase translation probablities for statistical machine translation using iteratively-trained models have repeatedly failed to produce translations as good as those obtained by estimating phrase translation probablities from surface statistics of bilingual word alignments as described by Koehn, et al. (2003). We propose a new iteratively-trained phrase translation model that produces translations of quality equal to or better than those produced by Koehn, et al.s model. Moreover, with the new model, translation quality degrades much more slowly as pruning is tightend to reduce translation time. 1 Introduction  "
W07-0716 ", 20742 {nmadnani,nfa,resnik,bonnie}@umiacs.umd.edu Abstract Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without a significant decrease in translation quality. "
W07-0717 " We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components. We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to. The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system. 1 I  "
W07-0718 "University of Edinburgh j schroeder ed ac uk Abstract This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured timing and intraand inter-annotator agreement for three types of subjective evaluation. We measured the correlation of automatic evaluation metrics with human judgments. This meta-evaluation reveals surprising facts about the most commonly used methodologies. "
W07-0719 " In this work we revise the application of discriminative learning to the problem of phrase selection in Statistical Machine Translation. Inspired by common techniques used in Word Sense Disambiguation , we train classifiers based on local context to predict possible phrase translations. Our work extends that of Vickrey et al. (2005) in two main aspects. First, we move from word translation to phrase translation. Second, we move from the blank-filling task to the full translation task. We report results on a set of highly frequent source phrases, obtaining a significant improvement, specially with respect to adequacy, according to a rigorous process of manual evaluation. 1 I  "
W07-0720 " This paper describes the 2007 Ngram-based statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Polit` ecnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the previous years system, being highlighted and empirically compared. Mainly, these include a novel word ordering strategy based on: (1) statistically monotonizing the training source corpus and (2) a novel reordering approach based on weighted reordering graphs. In addition, this system introduces a target language model based on statistical classes, a feature for out-of-domain units and an improved optimization procedure. The paper provides details of this system participation in the ACL 2007 SECOND WORKSHOP ON STATISTICAL MACHINE TRANSLATION. Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks. 1 Introduction  "
W07-0721 " One main challenge of statistical machine translation (SMT) is dealing with word order. The main idea of the statistical machine reordering (SMR) approach is to use the powerful techniques of SMT systems to generate a weighted reordering graph for SMT systems. This technique supplies reordering constraints to an SMT system, using statistical criteria. In this paper, we experiment with different graph pruning which guarantees the translation quality improvement due to reordering at a very low increase of computational cost. The SMR approach is capable of generalizing reorderings, which have been learned during training, by using word classes instead of words themselves. We experiment with statistical and morphological classes in order to choose those which capture the most probable reorderings. Satisfactory results are reported in the WMT07 Es/En task. Our system outperforms in terms of BLEU the WMT07 Official baseline system. 1 Introduct  "
W07-0722 " Mixture modelling is a standard technique for density estimation, but its use in statistical machine translation (SMT) has just started to be explored. One of the main advantages of this technique is its capability to learn specific probability distributions that better fit subsets of the training dataset. This feature is even more important in SMT given the difficulties to translate polysemic terms whose semantic depends on the context in which that term appears. In this paper, we describe a mixture extension of the HMM alignment model and the derivation of Viterbi alignments to feed a state-of-the-art phrase-based system. Experiments carried out on the Europarl and News Commentary corpora show the potential interest and limitations of mixture modelling. 1 Introdu  "
W07-0723 " We present results and experiences from our experiments with phrase-based statistical machine translation using Moses. The paper is based on the idea of using an offthe-shelf parser to supply linguistic information to a factored translation model and compare the results of GermanEnglish translation to the shared task baseline system based on word form. We report partial results for this model and results for two simplified setups. Our best setup takes advantage of the parsers lemmatization and decompounding. A qualitative analysis of compound translation shows that decompounding improves translation quality. 1 Introduct  "
W07-0724 "Abstract We present the PORTAGE statistical machine translation system which participated in the shared task of the ACL 2007 Second Workshop on Statistical Machine Translation. The focus of this description is on improvements which were incorporated into the system over the last year. These include adapted language models, phrase table pruning, an IBM1-based decoder feature, and rescoring with posterior probabilities. "
W07-0725 " This paper describes the development of a statistical machine translation system based on the Moses decoder for the 2007 WMT shared tasks. Several different translation strategies were explored. We also use a statistical language model that is based on a continuous representation of the words in the vocabulary. By these means we expect to take better advantage of the limited amount of training data. Finally, we have investigated the usefulness of a second reference translation of the development data. 1 Introduction This paper describes the development of a statistical machine translation system based on the Moses decoder (Koehn et al., 2007) for the 2007 WMT shared tasks. Due to time constraints, we only considered the translation between French and English. A system with a similar architecture was successfully applied to the translation between Spanish and English in the framework of the 2007 TC-STAR evaluation. 1 For the 2007 WMT shared task a recipe is provided to build a b  "
W07-0726 " We describe an architecture that allows to combine statistical machine translation (SMT) with rule-based machine translation (RBMT) in a multi-engine setup. We use a variant of standard SMT technology to align translations from one or more RBMT systems with the source text. We incorporate phrases extracted from these alignments into the phrase table of the SMT system and use the open-source decoder Moses to find good combinations of phrases from SMT training data with the phrases derived from RBMT. First experiments based on this hybrid architecture achieve promising results. 1 I  "
W07-0727 " In this paper we describe the Interactive Systems Laboratories (ISL) phrase-based machine translation system used in the shared task Machine Translation for European Languages of the ACL 2007 Workshop on Statistical Machine Translation. We present results for a system combination of the ISL syntax-augmented MT system and the ISL phrase-based system by combining and rescoring the n-best lists of the two systems. We also investigate the combination of two of our phrase-based systems translating from different source languages, namely Spanish and German, into their common target language, English. 1 Int  "
W07-0728 " This article describes a machine translation system based on an automatic post-editing strategy: initially translate the input text into the target-language using a rule-based MT system, then automatically post-edit the output using a statistical phrase-based system. An implementation of this approach based on the SYSTRAN and PORTAGE MT systems was used in the shared task of the Second Workshop on Statistical Machine Translation. Experimental results on the test data of the previous campaign are presented. 1 Intro  "
W07-0729 "Abstract This paper presents a new paradigm for translation from inflectionally rich languages that was used in the University of Maryland statistical machine translation system for the WMT07 Shared Task. The system is based on a hierarchical phrase-based decoder that has been augmented to translate ambiguous input given in the form of a confusion network (CN), a weighted finite state representation of a set of strings. By treating morphologically derived forms of the input sequence as possible, albeit more costly paths that the decoder may select, we find that significant gains (10% BLEU relative) can be attained when translating from Czech, a language with considerable inflectional complexity, into English. "
W07-0730 "Abstract For the WMT 2007 shared task, the UC Berkeley team employed three techniques of interest. First, we used monolingual syntactic paraphrases to provide syntactic variety to the source training set sentences. Second, we trained two language models: a small in-domain model and a large out-ofdomain model. Finally, we made use of results from prior research that shows that cognate pairs can improve word alignments. We contributed runs translating English to Spanish, French, and German using various combinations of these techniques. "
W07-0731 "lsruhe {ashishv,zollmann,paulik,vogel+}@cs.cmu.edu Abstract We describe the CMU-UKA Syntax Augmented Machine Translation system SAMT used for the shared task Machine Translation for European Languages at the ACL 2007 Workshop on Statistical Machine Translation. Following an overview of syntax augmented machine translation, we describe parameters for components in our open-source SAMT toolkit that were used to generate translation results for the Spanish to English in-domain track of the shared task and discuss relative performance against our phrase-based submission. "
W07-0732 " This article describes the combination of a SYSTRAN system with a statistical postediting (SPE) system. We document qualitative analysis on two experiments performed in the shared task of the ACL 2007 Workshop on Statistical Machine Translation. Comparative results and more integrated hybrid techniques are discussed. 1 I  "
W07-0733 " The special challenge of the WMT 2007 shared task was domain adaptation. We took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain (here: news commentary), when most of the training data is from a different domain (here: European Parliament speeches). This paper also gives a description of the submission of the University of Edinburgh to the shared task. 1 Our framework: the Moses MT system The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop. It is now used at several academic institutions as the basic infrastructure for statistical machine translation research. The Moses system is an implementation of the phrase-based machine translation approach (Koehn et al., 2003). In this approach, an input sentence is first split into text chunks (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table. Phrases may be reordered, but typically a reordering limit (in our experiments a maximum movement over 6 words) is used. See Figure 1 for an illustration. Phrase translation probabilities, reordering probabilities and language model probabilities are combined to give each possible sentence translation a score. The best-scoring translation is searched for by the decoding algorithm and outputted by the system as the best translation. The different system components h i (phrase translation probabilities, language Figure 1: Phrase-based statistical machine translation model: Input is split into text chunks (phrases) which are mapped using a large phrase translation table. Phrases are mapped one-to-one, and may be reordered. model, etc.) are combined in a log-linear model to obtain the score for the translation e for an input sentence f: score(e, f) = exp i  i h i (e, f) (1) The weights of the components  i are set by a discriminative training method on held-out development data (Och, 2003). The basic components used in our experiments are: (a) two phrase translation probabilities (both p(e|f ) and p(f |e)), (b) two word translation probabilities (both p(e|f ) and p(f |e)), (c) phrase count, (d) output word count, (e) language model, (f) distance-based reordering model, and (g) lexicalized reordering model. For a more detailed description of this model, please refer to (Koehn et al., 2005). 2 Domain adaption Since training data for statistical machine translation is typically collected opportunistically from wherever it is available, the application domain for a machine translation system may be very different from the domain of the systems training data. For the WMT 2007 shared task, the challenge was to use a large amount of out-of-domain training data 224 (about 40 million words) combined with a much smaller amount of in-domain training data (about 1 million words) to optimize translation performance on that particular domain. We carried out these experiments on FrenchEnglish. 2.1 Only out-of-domain traini  "
W07-0734 "ity Pittsburgh, PA, 15213, USA {alavie,abhayaa}@cs.cmu.edu Abstract Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric. It is one of several automatic metrics used in this years shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English. "
W07-0735 " This paper describes experiments with English-to-Czech phrase-based machine translation. Additional annotation of input and output tokens (multiple factors) is used to explicitly model morphology. We vary the translation scenario (the setup of multiple factors) and the amount of information in the morphological tags. Experimental results demonstrate significant improvement of translation quality in terms of BLEU. 1  "
W07-0736 "Abstract The paper proposes formulating MT evaluation as a ranking problem, as is often done in the practice of assessment by human. Under the ranking scenario, the study also investigates the relative utility of several features. The results show greater correlation with human assessment at the sentence level, even when using an n-gram match score as a baseline feature. The feature contributing the most to the rank order correlation between automatic ranking and human assessment was the dependency structure relation rather than BLEU score and reference language model feature. "
W07-0737 " This paper studies the impact that difficult-totranslate source-language phrases might have on the machine translation process. We formulate the notion of difficulty as a measurable quantity; we show that a classifier can be trained to predict whether a phrase might be difficult to translate; and we develop a framework that makes use of the classifier and external resources (such as human translators) to improve the overall translation quality. Through experimental work, we verify that by isolating difficult-to-translate phrases and processing them as special cases, their negative impact on the translation of the rest of the sentences can be reduced. 1 Introdu  "
W07-1001 "anding OGI School of Science & Engineering Oregon Health & Science University Beaverton, Oregon, 97006 USA {roark,meg.mitchell,hollingk}@cslu.ogi.edu Abstract We consider the diagnostic utility of various syntactic complexity measures when extracted from spoken language samples of healthy and cognitively impaired subjects. We examine measures calculated from manually built parse trees, as well as the same measures calculated from automatic parses. We show statistically significant differences between clinical subject groups for a number of syntactic complexity measures, and these differences are preserved with automatic parsing. Different measures show different patterns for our data set, indicating that using multiple, complementary measures is important for such an application. "
W07-1002 " This paper demonstrates a method for determining the syntactic structure of medical terms. We use a model-fitting method based on the Log Likelihood Ratio to classify three-word medical terms as right or left-branching. We validate this method by computing the agreement between the classification produced by the method and manually annotated classifications. The results show an agreement of 75% 83%. This method may be used effectively to enable a wide range of applications that depend on the semantic interpretation of medical terms including automatic mapping of terms to standardized vocabularies and induction of terminologies from unstructured medical text. 1 Introduct  "
W07-1003 " This paper investigates the roles of named entities (NEs) in annotated biomedical text classification. In the annotation schema of BioCaster, a text mining system for public health protection, important concepts that reflect information about infectious diseases were conceptually analyzed with a formal ontological methodology. Concepts were classified as Types, while others were identified as being Roles. Types are specified as NE classes and Roles are integrated into NEs as attributes. We focus on the Roles of NEs by extracting and using them in different ways as features in the classifier. Experimental results show that: 1) Roles for each NE greatly helped improve performance of the system, 2) combining information about NE classes with their Roles contribute significantly to the improvement of performance. We discuss in detail the effect of each Role on the accuracy of text classification. 1 Introduct  "
W07-1004 " Several incompatible syntactic annotation schemes are currently used by parsers and corpora in biomedical information extraction. The recently introduced Stanford dependency scheme has been suggested to be a suitable unifying syntax formalism. In this paper, we present a step towards such unification by creating a conversion from the Link Grammar to the Stanford scheme. Further, we create a version of the BioInfer corpus with syntactic annotation in this scheme. We present an application-oriented evaluation of the transformation and assess the suitability of the scheme and our conversion to the unification of the syntactic annotations of BioInfer and the GENIA Treebank. We find that a highly reliable conversion is both feasible to create and practical, increasing the applicability of both the parser and the corpus to information extraction. 1 Introductio  "
W07-1005 " We propose an unsupervised method to automatically extract domain-specific prefixes and suffixes from biological corpora based on the use of PATRICIA tree. The method is evaluated by integrating the extracted affixes into an existing learning-based biological term annotation system. The system based on our method achieves comparable experimental results to the original system in locating biological terms and exact term matching annotation. However, our method improves the system efficiency by significantly reducing the feature set size. Additionally, the method achieves a better performance with a small training data set. Since the affix extraction process is unsupervised, it is assumed that the method can be generalized to extract domain-specific affixes from other domains, thus assisting in domain-specific concept recognition. 1 Introduct  "
W07-1006 "<NoAbstract>"
W07-1007 " We present a corpus-driven method for building a lexicon of semantically equivalent pairs of technical and lay medical terms. Using a parallel corpus of abstracts of clinical studies and corresponding news stories written for a lay audience, we identify terms which are good semantic equivalents of technical terms for a lay audience. Our method relies on measures of association. Results show that, despite the small size of our corpus, a promising number of pairs are identified. 1 Int  "
W07-1008 " We describe the annotation of chemical named entities in scientific text. A set of annotation guidelines defines 5 types of named entities, and provides instructions for the resolution of special cases. A corpus of fulltext chemistry papers was annotated, with an inter-annotator agreement   score of 93%. An investigation of named entity recognition using LingPipe suggests that   scores of 63% are possible without customisation, and scores of 74% are possible with the addition of custom tokenisation and the use of dictionaries. 1 I  "
W07-1009 " Although recent named entity (NE) annotation efforts involve the markup of nested entities, there has been limited focus on recognising such nested structures. This paper introduces and compares three techniques for modelling and recognising nested entities by means of a conventional sequence tagger. The methods are tested and evaluated on two biomedical data sets that contain entity nesting. All methods yield an improvement over the baseline tagger that is only trained on flat annotation. 1 Intro  "
W07-1010 " This paper presents the results of a pilot usability study of a novel approach to search user interfaces for bioscience journal articles. The main idea is to support search over figure captions explicitly, and show the corresponding figures directly within the search results. Participants in a pilot study expressed surprise at the idea, noting that they had never thought of search in this way. They also reported strong positive reactions to the idea: 7 out of 8 said they would use a search system with this kind of feature, suggesting that this is a promising idea for journal article search. 1 Introduct  "
W07-1011 "<NoAbstract>"
W07-1012 " The vast number of published medical documents is considered a vital source for relationship discovery. This paper presents a statistical unsupervised system, called BioNoculars, for extracting protein-protein interactions from biomedical text. BioNoculars uses graph-based mutual reinforcement to make use of redundancy in data to construct extraction patterns in a domain independent fashion. The system was tested using MEDLINE abstract for which the protein-protein interactions that they contain are listed in the database of interacting proteins and proteinprotein interactions (DIPPPI). The system reports an F-Measure of 0.55 on test MEDLINE abstracts. 1 Introduction  "
W07-1013 " This paper reports on a shared task involving the assignment of ICD-9-CM codes to radiology reports. Two features distinguished this task from previous shared tasks in the biomedical domain. One is that it resulted in the first freely distributable corpus of fully anonymized clinical text. This resource is permanently available and will (we hope) facilitate future research. The other key feature of the task is that it required categorization with respect to a large and commercially significant set of labels. The number of participants was larger than in any previous biomedical challenge task. We describe the data production process and the evaluation measures, and give a preliminary analysis of the results. Many systems performed at levels approaching the inter-coder agreement, suggesting that human-like performance on this task is within the reach of currently available technologies. "
W07-1014 " This paper describes the application of an ensemble of indexing and classification systems, which have been shown to be successful in information retrieval and classification of medical literature, to a new task of assigning ICD-9-CM codes to the clinical history and impression sections of radiology reports. The basic methods used are: a modification of the NLM Medical Text Indexer system, SVM, k-NN and a simple pattern-matching method. The basic methods are combined using a variant of stacking. Evaluated in the context of a Medical NLP Challenge, fusion produced an Fscore of 0.85 on the Challenge test set, which is considerably above the mean Challenge F-score of 0.77 for 44 participating groups. 1 Introduction  "
W07-1015 " This paper describes a system capable of semi-automatically filling an XML template from free texts in the clinical domain (practice guidelines). The XML template includes semantic information not explicitly encoded in the text (pairs of conditions and actions/recommendations). Therefore, there is a need to compute the exact scope of conditions over text sequences expressing the required actions. We present a system developed for this task. We show that it yields good performance when applied to the analysis of French practice guidelines. 1 Introdu  "
W07-1016 "s and the Semantic Classification of Sentences Grace Y. Chung and Enrico C iera Centre for Health Informatics University of New South Wales Sydney NSW 2052 Aust ralia {graceyc, e.coiera}@unsw.edu.au Abstract This paper describes experiments in classifying sentences of medical abstracts into a number of semantic classes given by section headings in structured abstracts. Using conditional random fields, we obtain F -scores ranging from 0.72 to 0.97. By using a small set of sentences that appear under the PARTICPANTS heading, we demonstrate that it is possible to recognize sentences that describe population characteristics of a study. We present a detailed study of the structure of abstracts of randomized clinical trials, and examine how sentences labeled under PARTICIPANTS could be used to summarize the population group. 1 Intro  "
W07-1017 "Abstract Code assignment is important for handling large amounts of electronic medical data in the modern hospital. However, only expert annotators with extensive training can assign codes. We present a system for the assignment of ICD-9-CM clinical codes to free text radiology reports. Our system assigns a code configuration, predicting one or more codes for each document. We combine three coding systems into a single learning system for higher accuracy. We compare our system on a real world medical dataset with both human annotators and other automated systems, achieving nearly the maximum score on the Computational Medicine Centers challenge. "
W07-1018 " We propose a methodology using underspecified semantic interpretation to process comparative constructions in MEDLINE citations, concentrating on two structures that are prevalent in the research literature reporting on clinical trials for drug therapies. The method exploits an existing semantic processor, SemRep, which constructs predications based on the Unified Medical Language System. Results of a preliminary evaluation were recall of 70%, precision of 96%, and F-score of 81%. We discuss the generalization of the methodology to other entities such as therapeutic and diagnostic procedures. The available structures in computable format are potentially useful for interpreting outcome statements in MEDLINE citations.  "
W07-1019 " There has been much recent interest in the extraction of PPIs (protein-protein interactions) from biomedical texts, but in order to assist with curation efforts, the PPIs must be enriched with further information of biological interest. This paper describes the implementation of a system to extract and enrich PPIs, developed and tested using an annotated corpus of biomedical texts, and employing both machine-learning and rulebased techniques. "
W07-1020 "Abstract Many approaches for named entity recognition rely on dictionaries gathered from curated databases (such as Entrez Gene for gene names.) Strategies for matching entries in a dictionary against arbitrary text use either inexact string matching that allows for known deviations, dictionaries enriched according to some observed rules, or a combination of both. Such refined dictionaries cover potential structural, lexical, orthographical, or morphological variations. In this paper, we present an approach to automatically analyze dictionaries to discover how names are composed and which variations typically occur. This knowledge can be constructed by looking at single entries (names and synonyms for one gene), and then be transferred to entries that show similar patterns in one or more synonyms. For instance, knowledge about words that are frequently missing in (or added to) a name (antigen, protein, human) could automatically be extracted from dictionaries. This paper should be seen as a vision paper, though we implemented most of the ideas presented and show results for the task of gene name recognition. The automatically extracted name composition rules can easily be included in existing approaches, and provide valuable insights into the biomedical sub-language. "
W07-1021 " This paper describes a preliminary analysis of issues involved in the production of reports aimed at patients from Electronic Patient Records. We present a system prototype and discuss the problems encountered. 1 Int  "
W07-1022 " The names of named entities very often occur as constituents of larger noun phrases which denote different types of entity. Understanding the structure of the embedding phrase can be an enormously beneficial first step to enhancing whatever processing is intended to follow the named entity recognition in the first place. In this paper, we examine the integration of general purpose linguistic processors together with domain specific named entity recognition in order to carry out the task of baseNP detection. We report a best F-score of 87.17% on this task. We also report an inter-annotator agreement score of 98.8 Kappa on the task of baseNP annotation of a new data set. 1 Intro  "
W07-1023 "Abstract At present, most biomedical Information Retrieval and Extraction tools process abstracts rather than full-text articles. The increasing availability of full text will allow more knowledge to be extracted with greater reliability. To investigate the challenges of full-text processing, we manually annotated a corpus of cited articles from a Molecular Interaction Map (Kohn, 1999). Our analysis demonstrates the necessity of full-text processing; identifies the article sections where interactions are most commonly stated; and quantifies both the amount of external knowledge required and the proportion of interactions requiring multiple or deeper inference steps. Further, it identifies a range of NLP tools required, including: identifying synonyms, and resolving coreference and negated expressions. This is important guidance for researchers engineering biomedical text processing systems. 1 Introduction It is no longer feasible for biologists to keep abreast of the vast quantity of biomedical literature. Even keyword-based Information Retrieval (IR) over abstracts retrieves too many articles to be individually inspected. There is considerable interest in NLP systems that overcome this information bottleneck. Most bioNLP systems have been applied to abstracts only, due to their availability (Hirschman et al., 2002). Unfortunately, the information in abstracts is dense but limited. Full-text articles have the advantage of providing more information and repeating facts in different contexts, increasing the likelihood of an imperfect system identifying them. Full text contains explicit structure, e.g. sections and captions, which can be exploited to improve Information Extraction (IE) (Regev et al., 2002). Previous work has investigated the importance of extracting information from specific sections, e.g. Schuemie et al. (2004), but there has been little analysis of when the entire document is needed for accurate knowledge extraction. For instance, extracting a fact from the Results may require a synonym to be resolved that is only mentioned in the Introduction. External domain knowledge may also be required. We investigated these issues by manually annotating full-text passages that describe the functional relationships between bio-entities summarised in a Molecular Interaction Map (MIM). Our corpus tracks the process Kohn (1999) followed in summarising interactions for the mammalian cell MIM, by identifying information required to infer facts, which we call dependencies. We replicate the process of manual curation and demonstrate the necessity of full-text processing for fact extraction. In the same annotation process we have identified NLP problems in these passages which must be solved to identify the facts correctly including: synonym and hyponym substitution, coreference resolution, negation handling, and the incorporation of knowledge from within the full text and the domain. This allows us to report on the relative importance of anaphora resolution and other tasks to the problem of biomedical fact extraction. As well as serving as a dataset for future tool development, our corpus is an excellent case study providing valuable guidance to developers of biomedical text mining and retrieval systems. 171 Figure 1: Map A of the Molecular Interaction Map compiled by Kohn (1999) 2 Biomedical NLP Full-text articles are becoming increasingly available to NLP researchers, who have begun investigating how specific sections and structures can be mined in various information extraction tasks. Regev et al. (2002) developed the first bioIR system specifically focusing on limited text sections. Their performance in the KDD Cup Challenge, primarily using Figure legends, showed the importance of considering document structure. Yu et al. (2002) showed that the Introduction defines the majority of synonyms, while Schuemie et al. (2004) and Shah et al. (2003) showed that the Results and Methods are the most and least informative, respectively. In contrast, Sinclair and Webber (2004) found the Methods useful in assigning Gene Ontology codes to articles. These section specific results highlight the information loss resulting from restricting searches to individual sections, as sections often provide unique information. Furthermore, facts appearing in different contexts across various sections, will be lost. This redundancy has been used for passage validation and ranking (Clarke et al., 2001). There are limited training resources for biomedical full-text systems. The majority of corpora consist of abstracts annotated for bio-entity recognition and Relationship Extraction, such as the GENIA (Kim et al., 2003) and the BioCreAtIvE corpora. However, due to the lack of full-text corpora, many current systems only process abstracts (Ohta et al., 2006). Few biomedical corpora exist for other tasks, such as coreference resolution (Casta  no et al., 2004; Vlachos et al., 2006), and these are very small. In this paper, we estimate the importance of these tasks in bioNLP systems, which will help determine which tasks system developers should focus effort on first. Despite limited full-text training corpora, competitions such as the Genomics track of TREC, require systems to retrieve and rank passages from full text that are relevant to question style queries. "
W07-1024 "<NoAbstract>"
W07-1025 " The paper presents two rule-based information extraction (IE) from two types of patients documentation in Polish. For both document types, values of sets of attributes were assigned using specially designed grammars. 1 Met  "
W07-1026 " The shift from paper to electronic documents has caused the curation of information sources in large electronic databases to become more generalized. In the biomedical domain, continuing efforts aim at refining indexing tools to assist with the update and maintenance of databases such as MEDLINE  . In this paper, we evaluate two statistical methods of producing MeSH  indexing recommendations for the genetics literature, including recommendations involving subheadings, which is a novel application for the methods. We show that a generic representation of the documents yields both better precision and recall. We also find that a domainspecific representation of the documents can contribute to enhancing recall. 1 Intro  "
W07-1027 " This paper proposes a machine learning approach to the task of assigning the international standard on classification of diseases ICD-9-CM codes to clinical records. By treating the task as a text categorisation problem, a classification system was built which explores a variety of features including negation, different strategies of measuring gloss overlaps between the content of clinical records and ICD-9-CM code descriptions together with expansion of the glosses from the ICD-9-CM hierarchy. The best classifier achieved an overall F 1 value of 88.2 on a data set of 978 free text clinical records, and was better than the performance of two out of three human annotators. 1 Introductio  "
W07-1028 "Quantitative Data on Referring Expressions in Biomedical Abstracts  "
W07-1029 "<NoAbstract>"
W07-1030 "<NoAbstract>"
W07-1031 " This paper is concerned with the evaluation of biomedical named entity recognition systems. We compare two such systems, one based on a Hidden Markov Model and one based on Conditional Random Fields and syntactic parsing. In our experiments we used automatically generated data as well as manually annotated material, including a new dataset which consists of biomedical full papers. Through our evaluation, we assess the strengths and weaknesses of the systems tested, as well as the datasets themselves in terms of the challenges they present to the systems. 1 Int  "
W07-1032 " Morphological analysis as applied to English has generally involved the study of rules for inflections and derivations. Recent work has attempted to derive such rules from automatic analysis of corpora. Here we study similar issues, but in the context of the biological literature. We introduce a new approach which allows us to assign probabilities of the semantic relatedness of pairs of tokens that occur in text in consequence of their relatedness as character strings. Our analysis is based on over 84 million sentences that compose the MEDLINE database and over 2.3 million token types that occur in MEDLINE and enables us to identify over 36 million token type pairs which have assigned probabilities of semantic relatedness of at least 0.7 based on their similarity as strings. 1 Int  "
W07-1101 "Abstract Natural languages contain many multi-word sequences that do not display the variety of syntactic processes we would expect given their phrase type, and consequently must be included in the lexicon as multiword units. This paper describes a method for identifying such items in corpora, focussing on English verb-noun combinations. In an evaluation using a set of dictionary-published MWEs we show that our method achieves greater accuracy than existing MWE extraction methods based on lexical association. "
W07-1102 " We identify several classes of multiword expressions that each require a different encoding in a (computational) lexicon, as well as a different treatment within a computational system. We examine linguistic properties pertaining to the degree of semantic idiosyncrasy of these classes of expressions. Accordingly, we propose statistical measures to quantify each property, and use the measures to automatically distinguish the classes. 1 Motiv  "
W07-1103 "Abstract This paper describes the design and implementation of a lexicon of Dutch multiword expressions (MWEs). No exhaustive research on a standard lexical representation of MWEs has been done for Dutch before. The approach taken is innovative, since it is based on the Equivalence Class Method. Furthermore, the selection of the lexical entries and their properties is corpus-based. The design of the lexicon and the standard representation will be tested in Dutch NLP systems. The purpose of the current paper is to give an overview of the decisions made in order to come to a standard lexical representation and to discuss the description fields this representation comprises. "
W07-1104 " This paper describes a fully unsupervised and automated method for large-scale extraction of multiword expressions (MWEs) from large corpora. The method aims at capturing the non-compositionality of MWEs; the intuition is that a noun within a MWE cannot easily be replaced by a semantically similar noun. To implement this intuition, a noun clustering is automatically extracted (using distributional similarity measures), which gives us clusters of semantically related nouns. Next, a number of statistical measures  based on selectional preferences  is developed that formalize the intuition of non-compositionality. Our approach has been tested on Dutch, and automatically evaluated using Dutch lexical resources. "
W07-1105 " This paper presents an electronic dictionary of Spanish adverbial frozen expressions. It focuses on their formal description in view of natural language processing and presents an experiment on the automatic application of this data to real texts using finite-state techniques. The paper makes an assessment of the advantages and limitations of this method for the identification of these multiword units in texts. 1 I  "
W07-1106 " Much work on idioms has focused on type identification, i.e., determining whether a sequence of words can form an idiomatic expression. Since an idiom type often has a literal interpretation as well, token classification of potential idioms in context is critical for NLP. We explore the use of informative prior knowledge about the overall syntactic behaviour of a potentially-idiomatic expression (type-based knowledge) to determine whether an instance of the expression is used idiomatically or literally (tokenbased knowledge). We develop unsupervised methods for the task, and show that their performance is comparable to that of state-of-the-art supervised techniques. "
W07-1107 " In this paper we investigate the role of the placement of pauses in automatically extracted multi-word expression (MWE) candidates from a learner corpus. The aim is to explore whether the analysis of pauses might be useful in the validation of these candidates as MWEs. The study is based on the assumption advanced in the area of psycholinguistics that MWEs are stored holistically in the mental lexicon and are therefore produced without pauses in naturally occurring discourse. Automatic MWE extraction methods are unable to capture the criterion of holistic storage and instead rely on statistics and raw frequency in the identification of MWE candidates. In this study we explore the possibility of a combination of the two approaches. We report on a study in which we analyse the placement of pauses in various instances of two very frequent automatically extracted MWE candidates from a learner corpus, i.e. the n-grams I dont know and I think I. Intuitively, they are judged differently in terms of holistic storage. Our study explores whether pause analysis can be used as an objective empirical criterion to support this intuition. A corpus of interview data of language learners of English forms the basis of this study. "
W07-1108 "Abstract Contextual information extracted from corpora is frequently used to model semantic similarity. We discuss distinct classes of context types and compare their effectiveness for compound noun interpretation. Contexts corresponding to word-word similarity perform better than contexts corresponding to relation similarity, even when relational co-occurrences are extracted from a much larger corpus. Combining wordsimilarity and relation-similarity kernels further improves SVM classification performance. "
W07-1109 " This paper proposes an approach of processing Japanese compound functional expressions by identifying them and analyzing their dependency relations through a machine learning technique. First, we formalize the task of identifying Japanese compound functional expressions in a text as a machine learning based chunking problem. Next, against the results of identifying compound functional expressions, we apply the method of dependency analysis based on the cascaded chunking model. The results of experimental evaluation show that, the dependency analysis model achieves improvements when applied after identifying compound functional expressions, compared with the case where it is applied without identifying compound functional expressions. "
W07-1501 "Abstract In this paper we describe the Graph Annotation Format (GrAF) and show how it is used represent not only independent linguistic annotations, but also sets of merged annotations as a single graph. To demonstrate this, we have automatically transduced several different annotations of the Wall Street Journal corpus into GrAF and show how the annotations can then be merged, analyzed, and visualized using standard graph algorithms and tools. We also discuss how, as a standard graph representation, it allows for the application of well-established graph traversal and analysis algorithms to produce information about interactions and commonalities among merged annotations. GrAF is an extension of the Linguistic Annotation Framework (LAF) (Ide and Romary, 2004, 2006) developed within ISO TC37 SC4 and as such, implements state-of-the-art best practice guidelines for representing linguistic annotations. "
W07-1502 " With ever-increasing demands on the diversity of annotations of language data, the need arises to reduce the amount of efforts involved in generating such value-added language resources. We introduce here the Jena ANnotation Environment (JANE), a platform that supports the complete annotation lifecycle and allows for focused annotation based on active learning. The focus we provide yields significant savings in annotation efforts by presenting only informative items to the annotator. We report on our experience with this approach through simulated and real-world annotations in the domain of immunogenetics for NE annotations. 1 Introdu  "
W07-1503 "<NoAbstract>"
W07-1504 "Abstract We present an annotated corpus of conversational facial displays designed to be used for generation. The corpus is based on a recording of a single speaker reading scripted output in the domain of the target generation system. The data in the corpus consists of the syntactic derivation tree of each sentence annotated with the full syntactic and pragmatic context, as well as the eye and eyebrow displays and rigid head motion used by the the speaker. The behaviours of the speaker show several contextual patterns, many of which agree with previous findings on conversational facial displays. The corpus data has been used in several studies exploring different strategies for selecting facial displays for a synthetic talking head. "
W07-1505 " We introduce an annotation type system for a data-driven NLP core system. The specifications cover formal document structure and document meta information, as well as the linguistic levels of morphology, syntax and semantics. The type system is embedded in the framework of the Unstructured Information Management Architecture (UIMA). 1 I  "
W07-1506 "Abstract This paper introduces a new, reversible method for converting syntactic structures with discontinuous constituents into traditional syntax trees. The method is applied to the Tiger Corpus of German and results for PCFG parsing requiring such contextfree trees are provided. A labeled dependency evaluation shows that the new conversion method leads to better results by preserving local relationships and introducing fewer inconsistencies into the training data. "
W07-1507 "Abstract This paper describes an annotation system for S  ami language corpora, which consists of structured, running texts. The annotation of the texts is fully automatic, starting from the original documents in different formats. The texts are first extracted from the original documents preserving the original structural markup. The markup is enhanced by a document-specific XSLT script which contains document-specific formatting instructions. The overall maintenance is achieved by system-wide XSLT scripts. "
W07-1508 " In this paper, we argue that clustering WordNet senses into more coarse-grained groupings results in higher inter-annotator agreement and increased system performance. Clustering of verb senses involves examining syntactic and semantic features of verbs and arguments on a caseby-case basis rather than applying a strict methodology. Determining appropriate criteria for clustering is based primarily on the needs of annotators. 1  "
W07-1509 " We investigate a way to partially automate corpus annotation for named entity recognition, by requiring only binary decisions from an annotator. Our approach is based on a linear sequence model trained using a k-best MIRA learning algorithm. We ask an annotator to decide whether each mention produced by a high recall tagger is a true mention or a false positive. We conclude that our approach can reduce the effort of extending a seed training corpus by up to 58%. 1 Introdu  "
W07-1510 "Abstract This paper presents a multimodal corpus of comparable pack messages and the concordancer that has been built to query it. The design of the corpus and its annotation is introduced. This is followed by a description of the concordancers interface, implementation and concordance display. Finally, some ideas for future work are outlined. "
W07-1511 " This paper presents the design and construction of an annotated Chinese collocation bank as the resource to support systematic research on Chinese collocations. With the help of computational tools, the bi-gram and n-gram collocations corresponding to 3,643 headwords are manually identified. Furthermore, annotations for bi-gram collocations include dependency relation, chunking relation and classification of collocation types. Currently, the collocation bank annotated 23,581 bigram collocations and 2,752 n-gram collocations extracted from a 5-million-word corpus. Through statistical analysis on the collocation bank, some characteristics of Chinese bigram collocations are examined which is essential to collocation research, especially for Chinese. 1 Introduct  "
W07-1512 "Abstract The linguistic quality of a parallel treebank depends crucially on the parallelism between the source and target language annotations. We propose a linguistic notion of translation units and a quantitative measure of parallelism for parallel dependency treebanks, and demonstrate how the proposed translation units and parallelism measure can be used to compute transfer rules, spot annotation errors, and compare different annotation schemes with respect to each other. The proposal is evaluated on the 100,000 word Copenhagen Danish-English Dependency Treebank. "
W07-1513 "herlands {Paola.Monachesi, Gerwert.Stevens, Jantine.Trapman}@let.uu.nl Abstract We present an approach to automatic semantic role labeling (SRL) carried out in the context of the Dutch Language Corpus Initiative (D-Coi) project. Adapting earlier research which has mainly focused on English to the Dutch situation poses an interesting challenge especially because there is no semantically annotated Dutch corpus available that can be used as training data. Our automatic SRL approach consists of three steps: bootstrapping from a syntactically annotated corpus by means of a rulebased tagger developed for this purpose, manual correction on the basis of the PropBank guidelines which have been adapted to Dutch and training a machine learning system on the manually corrected data. "
W07-1514 " This paper describes a tool for aligning and searching parallel treebanks. Such treebanks are a new type of parallel corpora that come with syntactic annotation on both languages plus sub-sentential alignment. Our tool allows the visualization of tree pairs and the comfortable annotation of word and phrase alignments. It also allows monolingual and bilingual searches including the specification of alignment constraints. We show that the TIGER-Search query language can easily be combined with such alignment constraints to obtain a powerful cross-lingual query language. 1 Intro  "
W07-1515 "gdom {j.l.read,drh21,j.a.carroll}@sussex.ac.uk Abstract The Appraisal framework is a theory of the language of evaluation, developed within the tradition of systemic functional linguistics. The framework describes a taxonomy of the types of language used to convey evaluation and position oneself with respect to the evaluations of other people. Accurate automatic recognition of these types of language can inform an analysis of document sentiment. This paper describes the preparation of test data for algorithms for automatic Appraisal analysis. The difficulty of the task is assessed by way of an inter-annotator agreement study, based on measures analogous to those used in the MUC-7 evaluation. "
W07-1516 " In the construction of a part-of-speech annotated corpus, we are constrained by a fixed budget. A fully annotated corpus is required, but we can afford to label only a subset. We train a Maximum Entropy Markov Model tagger from a labeled subset and automatically tag the remainder. This paper addresses the question of where to focus our manual tagging efforts in order to deliver an annotation of highest quality. In this context, we find that active learning is always helpful. We focus on Query by Uncertainty (QBU) and Query by Committee (QBC) and report on experiments with several baselines and new variations of QBC and QBU, inspired by weaknesses particular to their use in this application. Experiments on English prose and poetry test these approaches and evaluate their robustness. The results allow us to make recommendations for both types of text and raise questions that will lead to further inquiry. 1 Introduction  "
W07-1517 " We present MAIS, a UIMA-based environment for combining information from various annotated resources. Each resource contains one mode of linguistic annotation and remains independent from the other resources. Interactions between annotations are defined based on use cases. 1 Int  "
W07-1518 " XARA is a rule-based PropBank labeler for Alpino XML files, written in Java. I used XARA in my research on semantic role labeling in a Dutch corpus to bootstrap a dependency treebank with semantic roles. Rules in XARA are based on XPath expressions, which makes it a versatile tool that is applicable to other treebanks as well. In addition to automatic role annotation, XARA is able to extract training instances (sets of features) from an XML based treebank. Such an instance base can be used to train machine learning algorithms for automatic semantic role labeling (SRL). In my semantic role labeling research, I used the Tilburg Memory Learner (TiMBL) for this purpose. 1 Intro  "
W07-1519 " In this paper, we present a treebank annotation tool developed for processing Turkish sentences. The tool consists of three different annotation stages; morphological analysis, morphological disambiguation and syntax analysis. Each of these stages are integrated with existing analyzers in order to guide human annotators. Our semiautomatic treebank annotation tool is currently used both for creating new data sets and correcting the existing Turkish treebank. 1 Intro  "
W07-1520 " We present two web-based, interactive tools for creating and visualizing sub-sentential alignments of parallel text. Yawat is a tool to support distributed, manual wordand phrase-alignment of parallel text through an intuitive, web-based interface. Kwipc is an interface for displaying words or bilingual word pairs in parallel, word-aligned context. A key element of the tools presented here is the interactive visualization: alignment information is shown only for one pair of aligned words or phrases at a time. This allows users to explore the alignment space interactively without being overwhelmed by the amount of information available. 1  "
W07-1521 " This paper presents the building procedure of a Chinese sense annotated corpus. A set of software tools is designed to help human annotator to accelerate the annotation speed and keep the consistency. The software tools include 1) a tagger for word segmentation and POS tagging, 2) an annotating interface responsible for the sense describing in the lexicon and sense annotating in the corpus, 3) a checker for consistency keeping, 4) a transformer responsible for the transforming from text file to XML format, and 5) a counter for sense frequency distribution calculating. 1 Introduct  "
W07-1522 "192, Japan {ryu-i,mamoru-k,inui,matsu}@is.naist.jp Abstract In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese written text. There have been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations as are done in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for refining their specifications. For this reason, we discuss issues in annotating these two types of relations, and propose a new specification for each. In accordance with the specification, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus 1 , which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). "
W07-1523 "Abstract Annotating large text corpora is a timeconsuming effort. Although single-user annotation tools are available, web-based annotation applications allow for distributed annotation and file access from different locations. In this paper we present the webbased annotation application Serengeti for annotating anaphoric relations which will be extended for the annotation of lexical chains. "
W07-1524 "Department of Linguistics. University of Potsdam. {dipper|goetze}@ling.uni-potsdam.de  Center for Mind/Brain Sciences. University of Trento. massimo.poesio@unitn.it  Department of Information and Communication Technology. University of Trento. {christian.raymond|riccardi}@dit.unitn.it  Institute of Computer Science. Polish Academy of Science. jwisniewska@poczta.uw.edu.pl Abstract The LUNA corpus is a multi-lingual, multidomain spoken dialogue corpus currently under development that will be used to develop a robust natural spoken language understanding toolkit for multilingual dialogue services. The LUNA corpus will be annotated at multiple levels to include annotations of syntactic, semantic, and discourse information; specialized annotation tools will be used for the annotation at each of these levels. In order to synchronize these multiple layers of annotation, the PAULA standoff exchange format will be used. In this paper, we present the corpus and its PAULA-based architecture. 1 1 Introduction XML standoff markup (Thompson and McKelvie, 1997; Dybkjr et al., 1998) is emerging as the cleanest way to organize multi-level annotations of corpora. In many of the current annotation efforts based on standoff a single multi-purpose tool such as the NITE XML Toolkit (Carletta et al., 2003) or WordFreak (Morton and LaCivita, 2003) is used to anno1 The members of the LUNA project consortium are: Piedmont Consortium for Information Systems (IT), University of Trento (IT), Loquendo SpA (IT), RWTH-Aachen (DE), University of Avignon (FR), France Telecom R&D Division S.A. (FR), Polish-Japanese Institute of Information Technology (PL) and the Institute for Computer Science of the Polish Academy of Sciences (PL), http://www  http://www.ist-luna.eu . This research was performed in the LUNA project funded by the EC, DG Infso, Unit E1 and in the Collaborative Research Center 632 Information Structure, funded by the German Science Foundation, http://www.sfb632.uni-potsdam.de . tate as well as maintain all annotation levels (cf. the SAMMIE annotation effort (Kruijff-Korbayov  a et al., 2006b)). However, it is often the case that specialized tools are developed to facilitate the annotation of particular levels: examples include tools for segmentation and transcription of the speech signal like PRAAT (Boersma and Weenink, 2005) and TRANSCRIBER (Barras et al., 1998), the SALSA tools for FrameNetstyle annotation (Burchardt et al., 2006), and MMAX (M  uller and Strube, 2003) for coreference annotation. Even in these cases, however, it may still be useful, or even necessary, to be able to visualize more than one level at once, or to knit together 2 multiple levels to create a file that can be used to train a model for a particular type of annotation. The Linguistic Annotation Framework by (Ide et al., 2003) was proposed as a unifying markup format to be used to synchronize heterogeneous markup formats for such purposes. In this paper, we discuss how the PAULA representation format, a standoff format inspired by the Linguistic Annotation Framework, is being used to synchronize multiple levels of annotation in the LUNA corpus, a corpus of spoken dialogues in multiple languages and multiple domains that is being created to support the development of robust spoken language understanding models for multilingual dialogue services. The corpus is richly annotated with linguistic information that is considered relevant for research on dialogue, including chunks, named entities, argument structure, coreference, and dialogue acts. We chose to adopt specialized tools for each level: e.g., 2 In the sense of the knit tool of the LT-XML suite. 148 transcription using TRANSCRIBER, coreference using MMAX, attributes using SEMANTIZER, etc. To synchronize the annotation and allow cross-layer operations, the annotations are mapped to a common representation format, PAULA. The structure of the paper is as follows. In Section 2, we present the LUNA project and the LUNA corpus with its main annotation levels. In Section 3, we introduce the PAULA exchange format, focusing on the representation of time alignment and dialogue phenomena. Finally we show how PAULA is used in the LUNA corpus and discuss alternative formats. 2 The LUNA project The aim of the LUNA project is to advance the state of the art in understanding conversational speech in Spoken Dialogue Systems (Gupta et al., 2005), (Bimbot et al., 2006). Three aspects of Spoken Language Understanding (SLU) are of particular concern in LUNA: generation of semantic concept tags, semantic composition into conceptual structures and context sensitive validation using information provided by the dialogue manager. In order to train and evaluate SLU models, we will create an annotated corpus of spoken dialogues in multiple domains and multiple languages: French, Italian, and Polish. 2.1 The LUNA corpus The LUNA corpus is currently being collected, with a target to collect 8100 human-machine dialogues and 1000 human-human dialogues in Polish, Italian and French. The dialogues are collected in the following application domains: stock exchange, hotel reservation and tourism inquiries, customer support service/help-desk and public transportation. 2.2 Multilevel annotation Semantic interpretation involves a number of subtasks, ranging from identifying the meaning of individual words to understanding which objects are being referred to up to recovering the relation between different semantic objects in the utterance and discourse level to, finally, understanding the communicative force of an utterance. In some annotation effortse.g., in the annotation of the French MEDIA Corpus (Bonneau-Maynard and Rosset, 2003) information about the meaning of semantic chunks, contextual information about coreference, and information about dialogue acts are all kept in a single file. This approach however suffers from a number of problems, including the fact that errors introduced during the annotation at one level may make other levels of annotation unusable as well, and that it is not possible for two annotators to work on different types of annotation for the same file at the same time. Most current annotation efforts, therefore, tend to adopt the multilevel approach pioneered during the development of the MAPTASK corpus and then developed as part of work on the EU-funded MATE project (McKelvie et al., 2001), in which each aspect of interpretation is annotated in a separate level, independently maintained. This approach is being followed, for instance, in the ONTONOTES project (Hovy et al., 2006) and the SAMMIE project (Kruijff-Korbayova et al., 2006a). For the annotation of the LUNA corpus, we decided to follow the multilevel approach as well. That allows us to achieve more granularity in the annotation of each of the levels and to investigate more easily dependencies between features that belong to different levels. Furthermore, we can use different specialized off-the-shelf annotation tools, splitting up the annotation task and thus facilitating consistent annotation. 2.3 Annotation levels The LUNA corpus will contain different types of information. The first levels are necessary to prepare the corpus for subsequent semantic annotation, and include segmentation of the corpus in dialogue turns, transcription of the speech signal, and syntactic preprocessing with POS-tagging and shallow parsing. The next level consists of the annotation of domain information using attribute-value pairs. This annotation will be performed on all dialogues in the corpus. The other levels of the annotation scheme are not mandatory, but at least a part of the dialogues will be annotated in order to investigate contextual aspects of the semantic interpretation. These levels include the predicate structure, the relations between referring expressions, and the annotation of dialogue acts. 149 2.3.1 Segmentation and transcription of the speech signal Before transcription and annotation can begin, it is necessary to segment the speech signal into dialogue turns and annotate them with speaker identity and mark where speaker overlap occurs. The goal of this segmentation is to be able to perform a transcription and annotation of the dialogue turns with or without dialogue context. While dialogue context is preferable for semantic annotation, it slows down the annotation process. The tool we will use for the segmentation and transcription of the speech signal is the open source tool TRANSCRIBER 3 (Barras et al., 1998). The next step is the transcription of the speech signal, using conventions for the orthographic transcription and for the annotation of non-linguistic acoustic events. 2.3.2 Part Of Speech Tagging and Chunking The transcribed material will be annotated with POS-tags, morphosyntactic information like agreement features, and segmented based on syntactic constituency. For the POS-tags and morphosyntactic features, we will follow the recommendations made in EAGLES (EAGLES, 1996), which allows us to have a unified representation format for the corpus, independently of the tools used for each language. 2.3.3 Domain Attribute Annotation At this level, semantic segments will be annotated following an approach used for the annotation for the French MEDIA dialogue corpus (BonneauMaynard and Rosset, 2003). We specify the domain knowledge in domain ontologies. These are used to build domain-specific dictionaries. Each dictionary contains:  Concepts corresponding to classes of the ontology and attributes of the annotation.  Values corresponding to the individuals of the domain.  Constraints on the admissible values for each concept. 3 http://trans.sourceforge.net  http://trans.sourceforge.net The concept dictionaries are used to annotate semantic segments with attribute-value pairs. The semantic segments are produced by concatenation of the chunks produced by the shallow parser. A semantic segment is a unit that corresponds unambiguously to a concept of the dictionary. (1) buongiorno lei [pu` o iscriversi] concept1 [agli esami] concept2 [oppure] concept3 [ottenere delle informazioni] concept4 come la posso aiutare 4    2.3.4 Predicate structure The annotation of predicate structure facilitates the interpretation of the relation between entities and events occurring in the dialogue. There are different approaches to annotate predicate structure. Some of them are based upon syntactic structure, with PropBank (Kingsbury and Palmer, 2003) being one of the most relevant, building the annotation upon the syntactic representation of the TreeBank corpus (Marcus et al., 1993). An alternative to syntax-driven approaches is the annotation using semantic roles as in FrameNet (Baker et al., 1998). For the annotation of predicate structure in the LUNA corpus, we decided to use a FrameNet-like approach, rather than a syntax-based approach: 1. Annotation of dialogue interaction has to deal with disfluencies, non-complete sentences, ungrammaticality, etc., which complicates the use of deep syntactic representations. 2. If we start from a syntactic representation, we have to follow a long way to achieve the semantic interpretation. Syntactic constituents must be mapped to -roles, and then to semantic roles. FrameNet offers the possibility of annotating using directly semantic criteria. 4 Good morning, you can register for the exam or obtain information. How can I help you? 150 For each domain, we define a set of frames. These frames are defined based on the domain ontology, with the named entities providing the frame elements. For all the frames we introduce the negation as a default frame element. For the annotation, first of all we annotate the entities with a frame and a frame element. Then if the target is overtly realized we make a pointer from the frame elements to the target. The next step is putting the frame elements and the target (if overtly realized) in a set. (2) buongiorno [lei] f e1 [pu` o iscriversi] f e2 [agli esami] f e3 oppure [ottenere delle informazioni] f e4 come la posso aiutare set1 = {id1, id2, id3} frame: inscription frame-elements:{student, examen, date} set2 = {id4} frame = info-request frame-elements:{student, addressee, topic}    2.3.5 Coreference / Anaphoric relations To annotate anaphoric relations we will use an annotation scheme close to the one used in the ARRAU project (Artstein and Poesio, 2006). This scheme has been extensively tested with dialogue corpora and includes instructions for annotating a variety of anaphoric relations, including bridging relations. A further reason is the robustness of the scheme that doesnt require one single interpretation in the annotation. The first step is the annotation of the information status of the markables with the tags given and new. If the markables are annotated with given, the annotator will select the most recent occurrence of the object and add a pointer to it. If the markable is annotated with new, we distinguish between markables that are related to a previously mentioned object (associative reference) or dont have such a relation. If there are alternative interpretations, which of a list of candidates can be the antecedent, the annotator can annotate the markable as ambiguous and add a pointer to each of the possible antecedents. (3) Wizard: buongiorno [lei] cr1 [pu` o iscriversi] cr2 [agli esami] cr3 oppure ottenere [delle informazioni] cr4 come la posso aiutare    Caller: [iscrizione] cr5 [esami] cr6 5  2.3.6 Dialogue acts In order to associate the intentions of the speaker with the propositional content of the utterances, the segmentation of the dialogue turns in utterances is based on the annotation of predicate structure. Each set of frame elements will correspond to an utterance. Each utterance will be annotated using a multidimensional annotation scheme partially based on the DAMSL scheme (Allen and Core, 1997) and on the proposals of ICSI-MRDA (Dhillon et al., 2004). We have selected nine dialogue acts from the DAMSL scheme as initial tagset, that can be extended for the different application domains. Each utterance will be annotated with as many tags as applicable. (4) Wizard: [buongiorno] utt1 [lei pu` o iscriversi agli esami] utt2 oppure [ottenere delle 5 Register for the exam. 151 informzaioni] utt3 [come la posso aiutare] utt4    Caller: [iscrizione esami] utt5 3 PAULA a Linguistic Standoff Exchange Format PAULA stands for Potsdamer Austauschformat f  ur linguistische Annotation (Potsdam Interchange Format for Linguistic Annotation) and has been developed for the representation of data annotated at multiple layers. The application scenario is sketched in Fig 1: researchers use multiple, specialized offthe-shelf annotation tools, such as EXMARALDA or MMAX, to enrich data with linguistic information. The tools store the data in tool-specific formats and, hence, it is not straightforward to combine information from different sources and, e.g., to search for correlations across multiple annotation layers. This is where PAULA comes in: PAULA maps the tool-specific formats to a common format and serves as an interchange format between these tools. 6 Moreover, the annotations from the different sources are merged into one single representation. PAULA makes this data available for further applications, such as searching the data by means of the tool ANNIS 7 , or to feed statistical applications like WEKA 8 . PAULA is an XML-based standoff format for linguistic annotations, inspired by the dump format 6 Currently, we provide PAULA import filters for the following tools and formats: Exmaralda, MMAX, RST Tool/URML, annotate/TIGER XML. Export from PAULA to the tool formats is at present supported for the original source format only. We plan to support the export of selected annotations to other tools. This is, however, not a trivial task since it may involve loss of information. 7 ANNIS: http://www.sfb632.uni-potsdam.de/ annis 8 WEKA: http://www.cs.waikato.ac.nz/ml/ weka Figure 1: PAULA annotation scenario of the Linguistic Annotation Framework (Ide et al., 2003). 9 With PAULA, not only is the primary data separated from its annotations, but individual annotation layers (such as parts of speech and dialogue acts) are separated from each other as well. The standoff approach allows us to mark overlapping segments in a straightforward way: by distributing annotations over different files (XML as such does not easily account for overlapping segments, since its object model is a hierarchical, tree-like structure). Moreover, new annotation layers can be added easily. PAULA assumes that a representation of the primary data is stored in a file that optionally specifies a header with meta information, followed by a tag , which contains a representation of the primary data. In Fig. 2, the first box displays the transcription, with all contributions from the first speaker coming first, and the contributions from the other speaker(s) following (put in italics in the Figure). The basic type of annotation are markables, encoded by the XML element . Markables specify anchors, i.e., locations or ranges that can be annotated by linguistic information. The locations and ranges are positions or spans in the source text or timeline, which are referenced by means of XLinks and XPointer expressions. For instance, the Token markables in Fig. 2 define spans that co 9 The term standoff describes the situation where primary data (e.g., the transcription) and annotations of this data are stored in separate files (Thompson and McKelvie, 1997). 152 \n \r\r\r \n\r \r\n ! \"\"#$$#%#%&''\" (!!!! \r \r\n ! \"\"#$$#%#)''\" (!!\n!! * \n\r \r\r\r \r\n\r \r\n ! \"\"#$$#%#+,'' %&++\" (!!\r\n!! \r\r \r\n ! \"\"#$$#)./#%/'' +\" ))./. (!!\n 0\n \n \r*\r \r\r\n 1*\n\" \r\r 1*\n\" \r\r 1*\" \r\r 1*\" 23 \r*\r 4*\r\r\n \n\r 1*\r\r \r\r\" \n 1*\r\" \"4* 4* 4\n \n \r\r \r\n 1*\n \r!\" (!!!! \r\n\" 1*\n \r! \r! \" (!!!! \r\n# 1*\r$\r\" (!!!! 560 \n \r\r \r\n 1*%&\" (!!!! \r\n\" 1*'(\r\" (!!\r\n\n!! \r\n# 1*%\" (!!\n\r!! \r\r )78 \n\r\r 8\" )* * \n*9311 \n\n* ;3 \n* \n\n \n\r\r \")* 3 elements, which point to elements by referencing their IDs. Token markables are annotated by Morph and POS features. The name of the annotated feature is specified by the attribute type of the element; the value of the feature is given by the attribute value of the elements. For instance, the token with ID tok15 is annotated with morph=\"1.comm.sing\" and pos=\"PR\". Similarly, the Turn markables are specified for the speakers uttering the turns (Speaker features), and details of the dialogue acts (Action) are given. The file with the dialogue act annotations specify multiple features within one tag , rather than distributing the features over several files, as we do in the case of morphology and POS annotations. This way, we explicitely encode the fact that the individual annotations (action=\"inscription obtain-info\" and objectDB=\"examen\") jointly form one complex annotation. PAULA markables can also refer to points or areas within pictures or videos (by referring to coordinates) or point to other markables (Fig. 2 does not illustrate these options). Moreover, for the encoding of hierarchical structures like graphs, PAULA provides (structure) elements (see Fig. 3 below for an example). The PAULA standoff format is a generic format that does not necessarily prescribe in detail how to represent annotations. Often there is more than one way to represent the data in PAULA standoff format. In the next section, we present the way we intend to represent dialogue data, which involve possibly overlapping contributions by several speakers, and often include time-alignment information. 4 Representing LUNA Dialogue Annotations in PAULA In this section, we illustrate the use of PAULA for the LUNA corpus with a more elaborated example, fo153 cusing on the representation of frame annotation. In Fig. 3, the top elements represent the dialogue turns and the semantic units underlying the frame annotations, which are defined on the base of the dialogue turns. FrameUnit markables define the scope or extension of the frames, and roughly correspond to a sentence or turn. FrameP markables specify the frame participants, i.e., all elements that receive a semantic role within some frame. The annotations at the bottom contain information about individual frames. The frames are encoded as elements, constituting complex objects that group semantic units to form frames instances. In Fig. 3, the frame with ID frame 1 consists of the frame unit, the lexical unit and the frame participants. The FrameAnno box encodes the name of the frame: inscription. The frames can be defined by external Framesets, such as FrameNet (Baker et al., 1998), which in our example is stored in an external XML-resource called frameSet.xml. \n\r\r \r\r\n\n\r \r\r\r\r\r\n\r\r\r\r\r\r\r \r\r\r\r\r\r \r\r\r\r\r\r\n \r\r\r\r\r \r\r\r\r\r\r\r\r\r\r\r\r \r\r\r\r\r\n \r\r\r\r\r \r\r\r\r\r\r\r\r\r\r\r \r\r\r\r\r\r\r\r\r\r\r \r \r\r\r\r\r \r\r\r\r\r\r\r\r\r\r\r! \r\r\r\r\r\r\r\r\r\r\r \r \r\r\n\n\r\r\r\r \" #\r\r$ \r \r\r\r\r%&' ' \r#\r\r \r\r\r\r #%( \r\r\r\r\r\n\n\n\r\r \r#\r\r \r\r\r\r #%( ) \r\r\r\r\r\r\r \r * #\r\r \r\r\r\r%& \r\r#\r\r #%( \r\r\r\r\r+ , -, \r\r\r\r\r\r\r\r\r\r\r\r\r&+./\r/..0122\r \r\r\r\r\r*30) \r\r\r\r\r *0 \r\r\r\n\n\n\r\r \r#\r\r #%( \r\r\r\r+ , -, \r\r\r\r\r\r\r\r\r\r&+./\r/.!45.522\r \r\r\r\r*0 \r\r\r\r *!!454 \r\r\r\r\r\n\n\n 6 #\r \r\r\r\r\r\r\n\r\r\r \r\r\r\r\r\r%&'' \r\r\r#\r\r #%( \r\r\r\r\r\r\r+ , -, \r\r\r\r\r\r\r\r\r\r\r\r\r&+./\r/..!22\r \r\r\r\r\r\r\r\r\r\r \r\r\r#\r\r #%( \r\r\r\r\r\r\r+ , -, \r\r\r\r\r\r\r\r\r\r\r\r\r&+./\r/.5.422\r \r\r\r\r\r\r\r\r\r\r\r \r\r\r 7 + 8 \r\r\r%& \r\r\r+\n\n$ \r\r\r8\r \r\r\r\r\r\r\r #%( \r\r\r\r\r\r\r\n\r \r\r\r\r\r\r\r9:\r\r\r \r\r8 \r\r8\r \r\r\r\r\r\r\r #%( \r\r\r\r\r\r\r\n\n\r \r\r\r\r\r\r\r9: \r\r8 \r\r ; 7 8 < $ < * Figure 3: Frame annotation in PAULA 5 Alternative Formats For richly annotated dialogue corpora, alternative representation formats have been proposed. Two of the most prominent ones are the NITE-XML 10 10 NITE: http://http://www.ltg.ed.ac.uk/ NITE and the ELAN 11 format. Similar to PAULA, NITEXML focuses on richly annotated corpus data. It comes with a rich data model and employs a rich meta specification, which determinesbased upon the individual corpus characteristics the concrete linearization of the respective XML representation. Furthermore, it is accompanied by a JAVA API and a query tool, forming a valuable toolkit for corpus engineers who can adapt available resources to their specific needs. The ELAN format is used by a family of tools developed primarily for language documentation, of which the most advanced one is ELAN, a robust, ready-to-use tool for multi-level annotation of video. Its underlying data model is the Abstract Corpus Model (ACM) (Brugman and Russel, 2004). PAULA aims at an application scenario different from both of these formats. First, it builds upon the usage of specialized off-the-shelf annotation tools for the variety of annotation tasks. Both the NITEXML and ELAN approaches require additional effort and skills from the user, to add the required functionality, which PAULA aims to avoid. Second, PAULA takes care of merging the annotations from different sources, which is not in focus of ELAN or NITE. 6 Discussion and Future Directions We presented the LUNA dialogue corpus and its representation format, the standoff exchange format PAULA. In contrast to other formats, PAULA focuses on an application scenario in which different annotations come in their own specific format and are to be merged into one corpus representation. This includes, for instance, the use of specialized off-theshelf annotation tools for specific annotation tasks, as well as distributed and incremental annotation. The creation of the LUNA dialogue corpus is a prototypical example for this scenario. However, the usefulness of a format also depends on its interoperability and the available tools. With its import filters, PAULA already serves the needs of linguists of different linguistic communities, while more export functionality is still to be integrated. With the export to WEKA, a first step in this direction is done. Furthermore, ANNIS a web-based tool for visualizing and searching complex multi-level 11 ELAN: http://www.lat-mpi.eu/tools/elan 154 annotations is available and will be developed further. In our next steps, we will focus on a deliberate extension of the PAULA format for further and more complex dialogue annotations, which will enable the use of PAULA as an exchange format also in this domain. References J. Allen and M. Core. 1997. Draft of DAMSL: Dialog Act Markup in Several Layers.  R. Artstein and M. Poesio, 2006. ARRAU Annotation Manual (TRAINS dialogues). Univerity of Essex, U.K.  C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of COLING-ACL. Association for Computational Linguistics.  C. Barras, W. Geoffrois, Z. Wu, and M. Libermann. 1998. Transcriber: a free tool for segmenting, labeling and transcribing speech. In Proceedings of the First International Conference on Language Ressources and Evaluation (LREC).  F. Bimbot, M. Faundez-Zanuy, and R. deMori, editors. 2006. Special Issue on Spoken Language Understanding, volume 48 of Speech Communication. Elsevier.   P. Boersma and D. Weenink. 2005. Praat: doing phonetics by computer (Version 4.3.14). http://www.praat.org.   http://www.praat.org . H. Bonneau-Maynard and S. Rosset. 2003. A semantic representation for spoken dialogues. In Proceedings of Eurospeech, Geneva.  H. Brugman and A. Russel. 2004. Annotating multimedia/multi-modal resources with ELAN. In Proceedings of the Fourth International Conference on Language Resources and Evaluation, pages 2065 2068, Paris: ELRA. A. Burchardt, K. Erk, A. Frank, A. Kowalski, and S. Pado. 2006. SALTO  A Versatile Multi-Level Annotation Tool. In Proceedings of LREC 2006. J. Carletta, S. Evert, U. Heid, J. Kilgour, J. Robertson, and H. Voormann. 2003. The NITE XML Toolkit: flexible annotation for multi-modal language data. Behavior Research Methods, Instruments, and Computers  special issue on Measuring Behavior,, 35(3). R. Dhillon, S. Bhagat, H. Carvez, and E. Shriberg. 2004. Meeting Recorder Project: Dialog Act Labeling Guide. Technical report, TR-04-002 ICSI.  L. Dybkjr, N.O. Bernsen, H. Dybkjr, D. McKelvie, and A. Mengel. 1998. The MATE markup framework. MATE Deliverable D1.2.  EAGLES. 1996. Recommendations for the Morphosyntactic Annotation of Corpora. EAGLES Document EAG-TCWG-MAC/R. N. Gupta, G. Tur adn D. Hakkani-Tur, S. Bangalore, G. Riccardi, and M. Rahim. 2005. The AT&T Spoken Language Understanding System. IEEE Transactions on Speech and Audio, PP(99). E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2006. Ontonotes: the 90% solution. In Proc. HLT-NAACL.  N. Ide, L. Romary, and E. de la Clergerie. 2003. International standard for a linguistic annotation framework. In Proceedings of HLT-NAACL03 Workshop on The Software Engineeri ng and Architecture of Language Technology.  P. Kingsbury and M. Palmer. 2003. PropBank: the Next Level of TreeBank. In Proceedings of the Second Workshop on Treebanks and Linguistic Theories (TLT).  I. Kruijff-Korbayova, C. Gerstenberger, V. Rieser, and J. Schehl. 2006a. The SAMMIE multimodal dialogue corpus meets the NITE XML toolkit. In Proc. LREC, Genoa.  I. Kruijff-Korbayov  a, V. Rieser, J. Schehl, and T. Becker. 2006b. The Sammie Multimodal Dialogue Corpus Meets the Nite XML Toolkit. In Proceedings of the Fifth Workshop on multi-dimensional Markup in Natural Language Processing, EACL2006. EACL.  M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English. Coputational Linguistics, (19).  D. McKelvie, A. Isard, A. Mengel, M. B. Moeller, M. Grosse, and M. Klein. 2001. The MATE workbench an annotation tool for XML corpora. Speech Communication, 33(1-2):97112.  T. Morton and J. LaCivita. 2003. WordFreak: an open tool for linguistic annotation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Demonstrations.  Ch. M  uller and M. Strube. 2003. Multi-Level Annotation in MMAX. In Proceedings of the 4th SIGdial Workshop on Discourse and Dialogue.  "
W07-1525 "<NoAbstract>"
W07-1526 " Whilst the degree to which a treebank subscribes to a specific linguistic theory limits the usefulness of the resource, the availability of more formats for the same resource plays a crucial role both in NLP and linguistics. Conversion tools and multi-format treebanks are useful for investigating portability of NLP systems and validity of annotation. Unfortunately, conversion is a quite complex task since it involves grammatical rules and linguistic knowledge to be incorporated into the converter program. The paper focusses on a methodology for treebank conversion which consists in splitting the process in steps corresponding to the kinds of information that have to be converted, i.e. morphological, structural or relational syntactic. The advantage is the generation of a set of parallel treebanks featuring progressively differentiated formats. An application to the case of an Italian dependency-based treebank in a Penn like format is described. "
W07-1527 " This paper presents observations on our experience with an annotation scheme that was used in the training of a state-of-the-art noun phrase semantic interpretation system. The system relies on cross-linguistic evidence from a set of five Romance languages: Spanish, Italian, French, Portuguese, and Romanian. Given a training set of English noun phrases in context along with their translations in the five Romance languages, our algorithm automatically learns a classification function that is later on applied to unseen test instances for semantic interpretation. As training and test data we used two text collections of different genre: Europarl and CLUVI. The training data was annotated with contextual features based on two stateof-the-art classification tag sets. 1 Introduction  "
W07-1528 "stin {alexispalmer,katrin.erk}@mail.utexas.edu Abstract We propose a new XML format for representing interlinearized glossed text (IGT), particularly in the context of the documentation and description of endangered languages. The proposed representation, which we call IGT-XML, builds on previous models but provides a more loosely coupled and flexible representation of different annotation layers. Designed to accommodate both selective manual reannotation of individual layers and semi-automatic extension of annotation, IGT-XML is a first step toward partial automation of the production of IGT. "
W07-1529 " We seek to identify a limited amount of representative corpora, suitable for annotation by the computational linguistics annotation community. Our hope is that a wide variety of annotation will be undertaken on the same corpora, which would facilitate: (1) the comparison of annotation schemes; (2) the merging of information represented by various annotation schemes; (3) the emergence of NLP systems that use information in multiple annotation schemes; and (4) the adoption of various types of best practice in corpus annotation. Such best practices would include: (a) clearer demarcation of phenomena being annotated; (b) the use of particular test corpora to determine whether a particular annotation task can feasibly achieve good agreement scores; (c) The use of underlying models for representing annotation content that facilitate merging, comparison, and analysis; and (d) To the extent possible, the use of common annotation categories or a mapping among categories for the same phenomenon used by different annotation groups. This study will focus on the problem of identifying such corpora as well as the suitability of two candidate corpora: the Open portion of the American National Corpus (Ide and Macleod, 2001; Ide and Suderman, 2004) and the Controversial portions of the WikipediaXML corpus (Denoyer and Gallinari, 2006). "
W07-2201 "Abstract A method is described to incorporate bilexical preferences between phrase heads, such as selection restrictions, in a MaximumEntropy parser for Dutch. The bilexical preferences are modelled as association rates which are determined on the basis of a very large parsed corpus (about 500M words). We show that the incorporation of such selftrained preferences improves parsing accuracy significantly. "
W07-2202 " This paper describes an effective approach to adapting an HPSG parser trained on the Penn Treebank to a biomedical domain. In this approach, we train probabilities of lexical entry assignments to words in a target domain and then incorporate them into the original parser. Experimental results show that this method can obtain higher parsing accuracy than previous work on domain adaptation for parsing the same data. Moreover, the results show that the combination of the proposed method and the existing method achieves parsing accuracy that is as high as that of an HPSG parser retrained from scratch, but with much lower training cost. We also evaluated our method in the Brown corpus to show the portability of our approach in another domain. 1 Introdu  "
W07-2203 "Abstract We compare the accuracy of a statistical parse ranking model trained from a fully-annotated portion of the Susanne treebank with one trained from unlabeled partially-bracketed sentences derived from this treebank and from the Penn Treebank. We demonstrate that confidence-based semi-supervised techniques similar to self-training outperform expectation maximization when both are constrained by partial bracketing. Both methods based on partially-bracketed training data outperform the fully supervised technique, and both can, in principle, be applied to any statistical parser whose output is consistent with such partial-bracketing. We also explore tuning the model to a different domain and the effect of in-domain data in the semi-supervised training processes. "
W07-2204 " We introduce a set of 1,000 gold standard parse trees for the British National Corpus (BNC) and perform a series of self-training experiments with Charniak and Johnsons reranking parser and BNC sentences. We show that retraining this parser with a combination of one million BNC parse trees (produced by the same parser) and the original WSJ training data yields improvements of 0.4% on WSJ Section 23 and 1.7% on the new BNC gold standard set. 1 I  "
W07-2205 " As the organizers of the ACL 2007 Deep Linguistic Processing workshop (Baldwin et al., 2007), we were asked to discuss our perspectives on the role of current trends in deep linguistic processing for parsing technology. We are particularly interested in the ways in which efficient, broad coverage parsing systems for linguistically expressive grammars can be built and integrated into applications which require richer syntactic structures than shallow approaches can provide. This often requires hybrid technologies which use shallow or statistical methods for preor post-processing, to extend coverage, or to disambiguate the output. 1 Introduct  "
W07-2206 "Abstract The C&C CCG parser is a highly efficient linguistically motivated parser. The efficiency is achieved using a tightly-integrated supertagger, which assigns CCG lexical categories to words in a sentence. The integration allows the parser to request more categories if it cannot find a spanning analysis. We present several enhancements to the CKY chart parsing algorithm used by the parser. The first proposal is chart repair, which allows the chart to be efficiently updated by adding lexical categories individually, and we evaluate several strategies for adding these categories. The second proposal is to add constraints to the chart which require certain spans to be constituents. Finally, we propose partial beam search to further reduce the search space. Overall, the parsing speed is improved by over 35% with negligible loss of accuracy or coverage. "
W07-2207 " We extend a recently proposed algorithm for n-best unpacking of parse forests to deal efficiently with (a) Maximum Entropy (ME) parse selection models containing important classes of non-local features, and (b) forests produced by unification grammars containing significant proportions of globally inconsistent analyses. The new algorithm empirically exhibits a linear relationship between processing time and the number of analyses unpacked at all degrees of ME feature nonlocality; in addition, compared with agendadriven best-first parsing and exhaustive parsing with post-hoc parse selection it leads to improved parsing speed, coverage, and accuracy.  1 BackgroundMo  "
W07-2208 " This paper describes a log-linear model with an n-gram reference distribution for accurate probabilistic HPSG parsing. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and POS n-gram as defined in the CCG/HPSG/CDG supertagging. Recently, supertagging becomes well known to drastically improve the parsing accuracy and speed, but supertagging techniques were heuristically introduced, and hence the probabilistic models for parse trees were not well defined. We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG. This is the first model which properly incorporates the supertagging probabilities into parse trees probabilistic model. 1 Introduct  "
W07-2209 " Writing aids such as spelling and grammar checkers are often based on texts by adult writers and are not sufficiently targeted to support children in their writing process. This paper reports on the development of a writing tool based on a corpus of Swedish text written by children and on the parsing methods developed to handle text containing errors. The system uses finite state techniques for finding grammar errors without actually specifying the error. The broadness of the grammar and the lexical ambiguity in words, necessary for parsing text containing errors, also yields ambiguous and/or alternative phrase annotations. We block some of the (erroneous) alternative parses by the order in which phrase segments are selected, which causes bleeding of some rules and more correct parsing results are achieved. The technique shows good coverage results for agreement and verb selection phenomena. 1 Int  "
W07-2210 " We try to improve the classifier-based deterministic dependency parsing in two ways: by introducing a better search method based on a non-deterministic nbest algorithm and by devising a series of linguistically richer models. It is experimentally shown on a ConLL 2007 shared task that this results in a system with higher performance while still keeping it simple enough for an efficient implementation. 1 Intro  "
W07-2211 " Despite the popularity of stochastic parsers, symbolic parsing still has some advantages, but is not practical without an effective mechanism for selecting among alternative analyses. This paper describes the symbolic preference system of a hybrid parser that combines a shallow parser with an overlay parser that builds on the chunks. The hybrid currently equals or exceeds most stochastic parsers in speed and is approaching them in accuracy. The preference system is novel in using a simple, three-valued scoring method (-1, 0, or +1) for assigning preferences to constituents viewed in the context of their containing constituents. The approach addresses problems associated with earlier preference systems, and has considerably facilitated development. It is ultimately based on viewing preference scoring as an engineering mechanism, and only indirectly related to cognitive principles or corpus-based frequencies. 1 Introduction Despite the popularity of stochastic parsers, symbolic parsing still has some advantages, but is not practical without an effective mechanism for selecting among alternative analyses. Without it, accept/fail grammar rules must either be overly strong or admit very large numbers of parses. . Symbolic parsers have recently been augmented by stochastic post-processors for output disambiguation, which reduces their independence from corpora. Both the LFG XLE parser (Kaplan et.al. 2004), and the HPSG LinGO ERG parser (Toutanova et al. 2005) have such additions. This paper examines significant aspects of a purely symbolic alternative: the preference and pruning system of the RH (Retro-Hybrid) parser (Newman, 2007). The parser combines a preexisting, efficient shallow parser with an overlay parser that builds on the emitted chunks. The overlay parser is \"retro\" in that the grammar is related to ATNs (Augmented Transition Networks) originated by Woods (1970). RH delivers single \"best\" parses providing syntactic categories, syntactic functions, head features, and other information (Figure 1). The parenthesized numbers following the category labels in the figure are preference scores, and are explained further on. While the parses are not quite as detailed as those obtained using \"deep\" grammars, the missing information, mostly relating to long distance dependencies, can be added at far less cost in a post-parse phase that operates only on a single best parse. Methods for doing so, for stochastic parser output, are described by Johnson (2002) and Cahill et al (2004). The hybrid parser exceeds most stochastic parsers in speed, and approaches them in accuracy, even based on limited manual \"training\" on a particular idiom, so the preference system is a successful one (see Section 6), and continues to improve. The RH preference system builds on earlier methods. The major difference is a far simpler  "
W07-2212 "<NoAbstract>"
W07-2213 "<NoAbstract>"
W07-2214 "Abstract This paper identifies two orthogonal dimensions of context sensitivity, the first being context sensitivity in concurrency and the second being structural context sensitivity. We present an example from natural language which seems to require both types of context sensitivity, and introduce partially ordered multisets (pomsets) mcfgs as a formalism which succintly expresses both. Introduction Researchers in computer science and formal language theory have separately investigated context sensitivity of languages, addressing disjoint dimensions of context sensitivity. Researchers in parallel computing have explored the addition of concurrency and free word order to context free languages, i.e. a concurrency context sensitivity (Gischer, 1981; Warmuth and Haussler, 1984; Pratt, 1985; Pratt, 1986; Lodaya and Weil, 2000). Computational linguistis have explored adding crossing dependency and discontinuous constituency, i.e. a structural context sensitivity (Seki et al., 1991; Vijay-Shanker et al., 1987; Stabler, 1996). Research considering the combination of two dimensions of expressing context sensitivity have been sparse, e.g. (Becker et al., 1991), with research dedicated to this topic virtually nonexistent. Natural languages are not well expressed by either form of context sensitivity alone. For example, in Table 1, sentences 1-8 are valid, but 9, 10 are invalid constructions of Norwegian. In addition to the crossing dependency between the determiner and adverb phrase, this example can be described by either Derfor ga Jens Kari kyllingen tydeligvis ikke lenger kald Therefore gave Jens Kari the chicken evidently not longer cold Derfor ga Jens Kari tydeligvis kyllingen ikke lenger kald Derfor ga Jens tydeligvis Kari kyllingen ikke lenger kald Derfor ga Jens tydeligvis Kari ikke kyllingen lenger kald Derfor ga Jens tydeligvis Kari ikke lenger kyllingen kald Derfor ga Jens tydeligvis ikke lenger Kari kyllingen kald Derfor ga tydeligvis Jens ikke lenger Kari kyllingen kald Derfor ga tydeligvis ikke Jens lenger Kari kyllingen kald * Derfor ga Jens ikke tydeligvis Kari lenger kyllingen kald * Derfor ga Jens ikke tydeligvis kyllingen lenger Kari kald Table 1: Bobaljiks paradox/shape conservation example Bobaljiks paradox (Bobaljik, 1999), which asserts that relative ordering of clausal constituents are not unambiguously determined by the phrase structure, or shape conservation (M  uller, 2000), i.e. that linear precedence is preserved despite movement operations. In other words, the two structurally context sensitive components (due to the crossing dependency between them) can be shuffled arbitrarily, leading to concurrent context sensitivity. This paper proposes pomset mcfgs as a formalism for perspicuously expressing both types of context sensitivity. 1 The rest of the paper is organized as follows. Section 1 introduces pomsets, pomset operations, and pomset properties. Section 2 provides a definition of pomset mcfgs by extending the standard definition of mcfgs, defined over tuples of strings, to tuples of pomsets. Section 3 discusses pomset mcfg parsing. 1 Other pomset based formalisms (Lecomte and Retore, 1995; Basten, 1997; Nederhof et al., 2003) have been limited to the use of pomsets in context free grammars only. 106 1 Pomsets In this section, we define pomsets as a model for describing concurrency. A labelled partial order (LPO) is a 4 tuple (V, , , ) where V is a set of vertices,  is the alphabet, is the partial order on the vertices, and  is the labelling function :V . A pomset is a LPO up to isomorphism. The concatenation of pomsets p and q is defined as ;(p,q) = (V p V q , p   q , p  q V p V q , p   q ). The concurrency of pomsets p and q is defined as (p,q) = (V p V q , p   q , p  q , p   q ). Pomset isolation () is observed only in the context of concurrency. The concurrence of an isolated pomset with another pomset is defined as (p,q) = ({v p }V q ,p    q , q ,{(p  ,v p )} q ), where p is the set of linearizations for p, and p  is a function which returns an element of p. Let i be a pomset concurrency operator restricted to an arity of i. Because concurrency is both associative and commutative, without isolation, m n = n m = m+n , defeating any arity restrictions. Isolation allows us to restrict the arity of the concurrency operator, guaranteeing that in all linearizations of the pomset, the linearizations of the isolated subpomsets are contiguous. 2 A mildly concurrent operator  n , i.e. an nconcurrent operator, is a composite operator whose concurrency is isolated and restricted to an arity of n, such that it operates on at most n items concurrently. "
W07-2215 " In functional and logic programming, parsers can be built as modular executable specifications of grammars, using parser combinators and definite clause grammars respectively. These techniques are based on top-down backtracking search. Commonly used implementations are inefficient for ambiguous languages, cannot accommodate left-recursive grammars, and require exponential space to represent parse trees for highly ambiguous input. Memoization is known to improve efficiency, and work by other researchers has had some success in accommodating left recursion. This paper combines aspects of previous approaches and presents a method by which parsers can be built as modular and efficient executable specifications of ambiguous grammars containing unconstrained left recursion. 1  "
W07-2216 "Abstract In this paper we investigate several nonprojective parsing algorithms for dependency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the edge-factored model. We also investigate algorithms for non-projective parsing that account for nonlocal information, and present several hardness results. This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model. 1 Introduction Dependency representations of natural language are a simple yet flexible mechanism for encoding words and their syntactic dependencies through directed graphs. These representations have been thoroughly studied in descriptive linguistics (Tesni` ere, 1959; Hudson, 1984; Sgall et al., 1986; Me   l cuk, 1988) and have been applied in numerous language processing tasks. Figure 1 gives an example dependency graph for the sentence Mr. Tomash will remain as a director emeritus, which has been extracted from the Penn Treebank (Marcus et al., 1993). Each edge in this graph represents a single syntactic dependency directed from a word to its modifier. In this representation all edges are labeled with the specific syntactic function of the dependency, e.g., SBJ for subject and NMOD for modifier of a noun. To simplify computation and some important definitions, an artificial token is inserted into the sentence as the left most word and will always represent the root of the dependency graph. We assume all dependency graphs are directed trees originating out of a single node, which is a common constraint (Nivre, 2005). The dependency graph in Figure 1 is an example of a nested or projective graph. Under the assumption that the root of the graph is the left most word of the sentence, a projective graph is one where the edges can be drawn in the plane above the sentence with no two edges crossing. Conversely, a non-projective dependency graph does not satisfy this property. Figure 2 gives an example of a nonprojective graph for a sentence that has also been extracted from the Penn Treebank. Non-projectivity arises due to long distance dependencies or in languages with flexible word order. For many languages, a significant portion of sentences require a non-projective dependency analysis (Buchholz et al., 2006). Thus, the ability to learn and infer nonprojective dependency graphs is an important problem in multilingual language processing. Syntactic dependency parsing has seen a number of new learning and inference algorithms which have raised state-of-the-art parsing accuracies for many languages. In this work we focus on datadriven models of dependency parsing. These models are not driven by any underlying grammar, but instead learn to predict dependency graphs based on a set of parameters learned solely from a labeled corpus. The advantage of these models is that they negate the need for the development of grammars when adapting the model to new languages. One interesting class of data-driven models are 121 Figure 1: A projective dependency graph.  Figure 2: Non-projective dependency graph. those that assume each dependency decision is independent modulo the global structural constraint that dependency graphs must be trees. Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a). Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al., 2005b). The primary problem in treating each dependency as independent is that it is not a realistic assumption. Non-local information, such as arity (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies (Klein and Manning, 2002; McDonald and Pereira, 2006). However, in the data-driven parsing setting this can be partially adverted by incorporating rich feature representations over the input (McDonald et al., 2005a). The goal of this work is to further our current understanding of the computational nature of nonprojective parsing algorithms for both learning and inference within the data-driven setting. We start by investigating and extending the edge-factored model of McDonald et al. (2005b). In particular, we appeal to the Matrix Tree Theorem for multi-digraphs to design polynomial-time algorithms for calculating both the partition function and edge expectations over all possible dependency graphs for a given sentence. To motivate these algorithms, we show that they can be used in many important learning and inference problems including min-risk decoding, training globally normalized log-linear models, syntactic language modeling, and unsupervised learning via the EM algorithm  none of which have previously been known to have exact non-projective implementations. We then switch focus to models that account for non-local information, in particular arity and neighbouring parse decisions. For systems that model arity constraints we give a reduction from the Hamiltonian graph problem suggesting that the parsing problem is intractable in this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. "
W07-2217 " This paper investigates new design options for the feature space of a dependency parser. We focus on one of the simplest and most efficient architectures, based on a deterministic shift-reduce algorithm, trained with the perceptron. By adopting second-order feature maps, the primal form of the perceptron produces models with comparable accuracy to more complex architectures, with no need for approximations. Further gains in accuracy are obtained by designing features for parsing extracted from semantic annotations generated by a tagger. We provide experimental evaluations on the Penn Treebank. 1 Intro  "
W07-2218 " We propose a generative dependency parsing model which uses binary latent variables to induce conditioning features. To define this model we use a recently proposed class of Bayesian Networks for structured prediction, Incremental Sigmoid Belief Networks. We demonstrate that the proposed model achieves state-of-the-art results on three different languages. We also demonstrate that the features induced by the ISBNs latent variables are crucial to this success, and show that the proposed model is particularly good on long dependencies. 1 Int  "
W07-2219 "Current parameters of accurate unlexicalized parsers based on Probabilistic ContextFree Grammars (PCFGs) form a twodimensional grid in which rewrite events are conditioned on both horizontal (headoutward) and vertical (parental) histories. In Semitic languages, where arguments may move around rather freely and phrasestructures are often shallow, there are additional morphological factors that govern the generation process. Here we propose that agreement features percolated up the parse-tree form a third dimension of parametrization that is orthogonal to the previous two. This dimension differs from mere state-splits as it applies to a whole set of categories rather than to individual ones and encodes linguistically motivated co-occurrences between them. This paper presents extensive experiments with extensions of unlexicalized PCFGs for parsing Modern Hebrew in which tuning the parameters in three dimensions gradually leads to improved performance. Our best result introduces a new, stronger, lower bound on the performance of treebank grammars for parsing Modern Hebrew, and is on a par with current results for parsing Modern Standard Arabic obtained by a fully lexicalized parser trained on a much larger treebank. "
W07-2301 " The paper discusses quality of service evaluation which emphasises the users experience in the evaluation of system functionality and efficiency. For NLG systems, an important quality feature is communicatively adequate language generation, which affects the users perception of the system and consequently, evaluation results. The paper drafts an evaluation task that aims at measuring quality of service, taking the systems communicative competence into account. 1  "
W07-2302 " Generation of Referring Expressions is a thriving subfield of Natural Language Generation which has traditionally focused on the task of selecting a set of attributes that unambiguously identify a given referent. In this paper, we address the complementary problem of generating repeated, potentially different referential expressions that refer to the same entity in the context of a piece of discourse longer than a sentence. We describe a corpus of short encyclopaedic texts we have compiled and annotated for reference to the main subject of the text, and report results for our experiments in which we set human subjects and automatic methods the task of selecting a referential expression from a wide range of choices in a full-text context. We find that our human subjects agree on choice of expression to a considerable degree, with three identical expressions selected in 50% of cases. We tested automatic selection strategies based on most frequent choice heuristics, involving different combinations of information about syntactic MSR type and domain type. We find that more information generally produces better results, achieving a best overall test set accuracy of 53.9% when both syntactic MSR type and domain type are known. "
W07-2303 "ny {cahillae|forst|rohrer}@ims.uni-stuttgart.de Abstract We present a log-linear model that is used for ranking the string realisations produced for given corpus f-structures by a reversible broadcoverage LFG for German and compare its results with the ones achieved by the application of a language model (LM). Like other authors that have developed log-linear models for realisation ranking, we use a hybrid model that uses linguistically motivated learning features and a LM (whose score is simply integrated into the log-linear model as an additional feature) for the task of realisation ranking. We carry out a large evaluation of the model, training on over 8,600 structures and testing on 323. We observe that the contribution that the structural features make to the quality of the output is slightly greater in the case of a free word order language like German than it is in the case of English. The exact match metric improves from 27% to 37% when going from the LM-based realisation ranking to the hybrid model, BLEU score improves from 0.7306 to 0.7939. "
W07-2304 " In this paper we present a view of natural language generation in which the control structure of the generator is clearly separated from the content decisions made during generation, allowing us to explore and compare different control strategies in a systematic way. Our approach factors control into two components, a generation tree which maps out the relationships between different decisions, and an algorithm for traversing such a tree which determines which choices are actually made. We illustrate the approach with examples of stylistic control and automatic text revision using both generative and empirical techniques. We argue that this approach provides a useful basis for the theoretical study of control in generation, and a framework for implementing generators with a range of control strategies. We also suggest that this approach can be developed into tool for analysing and adapting control aspects of other advanced wide-coverage generation systems. "
W07-2305 "Abstract We investigate two methods for enhancing variation in the output of a stochastic surface realiser: choosing from among the highest-scoring realisation candidates instead of taking the single highestscoring result (-best sampling), and penalising the words from earlier sentences in a discourse when generating later ones (anti-repetition scoring). In a human evaluation study, subjects were asked to compare texts generated with and without the variation enhancements. Strikingly, subjects judged the texts generated using these two methods to be better written and less repetitive than the texts generated with optimal n-gram scoring; at the same time, no significant difference in understandability was found between the two versions. In analysing the two methods, we show that the simpler -best sampling method is considerably more prone to introducing dispreferred variants into the output, indicating that best results can be obtained using antirepetition scoring with strict or no -best sampling. "
W07-2306 " We present a method for quickly spotting overgeneration suspects (i.e., likely cause of overgeneration) in hand-coded grammars. The method is applied to a medium size Tree Adjoining Grammar (TAG) for French and is shown to help reduce the number of outputs by 70% almost all of it being overgeneration. 1 I  "
W07-2307 " Despite being the focus of intensive research, evaluation of algorithms that generate referring expressions is still in its infancy. We describe a corpusbased evaluation methodology, applied to a number of classic algorithms in this area. The methodology focuses on balance and semantic transparency to enable comparison of human and algorithmic output. Although the Incremental Algorithm emerges as the best match, we found that its dependency on manually-set parameters makes its performance difficult to predict. 1 Introduction The current state of the art in the Generation of Referring Expressions (GRE) is dominated by versions of the Incremental Algorithm (IA) of Dale and Reiter (1995). Focusing on the generation of first-mention definite descriptions, Dale and Reiter compared the IA to a number of its predecessors, including a Full Brevity (FB) algorithm, which generates descriptions of minimal length, and a Greedy algorithm (GR), which approximates Full Brevity (Dale, 1989). In doing so, the authors focused on Content Determination (CD, which is the purely semantic part of GRE), and on a descriptions ability to identify a referent for a hearer. Under this problem definition, GRE algorithms take as input a Knowledge Base (KB), which lists domain entities and their properties (often represented as attribute-value pairs), together with a set of intended referents, R. The output of CD is a distinguishing description of R, that is, a logical form which distinguishes this set from its distractors. Dale and Reiter argued that the IA was a superior model, and predicted that it would be the better match to human referential behaviour. 1 This was due in part to the way the IA searches for a distinguishing description by performing gradient descent along a predetermined list of domain attributes, called the preference order, whose ranking reflects general or domain-specific preferences (see 4.1). The Incremental Algorithm has served as a starting point for later models (Horacek, 1997; Kelleher and Kruijff, 2006), and has also served as a yardstick against which to compare other approaches (Gardent, 2002; Jordan and Walker, 2005). Despite its influence, few empirical evaluations have focused on the IA. Evaluation is even more desirable given the dependency of the algorithm on a preference order, which can radically change its behaviour, so that in a domain with n attributes, there are in principle n! different algorithms. This paper is concerned with applying a corpusbased methodology to evaluate content determination for GRE, comparing the three classic algorithms that formed the basis for Dale and Reiters (1995) contribution, adapted to also deal with pluralities and gradable properties. "
W07-2308 "  Politeness  is  an  integral  part  of  human  language  variation,  e.g.  consider  the  difference  in  the  pragmatic  effect  of  realizing the same communicative goal with  either Get me a glass of water mate! or I  wonder if I could possibly have some water  please?  This  paper  presents  POLLy  (Politeness  for  Language  Learning),  a  system  which  combines  a  natural  language  generator  with  an    AI  Planner  to  model  Brown  and  Levinsons  theory  of  politeness  (B&L)  in  collaborative  taskoriented  dialogue, with the ultimate goal of providing  a  fun  and  stimulating  environment  for  learning  English  as  a  second  language.  An  evaluation  of  politeness  perceptions  of  POLLys  output shows that: (1) perceptions  are  generally  consistent  with  B&Ls  predictions  for  choice  of  form  and  for  discourse  situation,  i.e.  utterances  to  strangers  need to  be  much  more polite than  those  to  friends;  (2)  our  indirect  strategies  which should be the politest forms, are seen  as  the  rudest;  and  (3)  English  and  Indian  native  speakers  of  English  have  different  perceptions of the level of politeness needed  to mitigate particular face threat s. "
W07-2309 " The potential of sentence generators as engines in Intelligent Computer-Assisted Language Learning and teaching (ICALL) software has hardly been explored. We sketch the prototype of COMPASS, a system that supports integrated writing and grammar curricula for 10 to 14 year old elementary or secondary schoolers. The system enables firstor second-language teachers to design controlled writing exercises, in particular of the sentence combining variety. The system includes facilities for error diagnosis and on-line feedback. Syntactic structures built by students or system can be displayed as easily understood phrase-structure or dependency trees, adapted to the students level of grammatical knowledge. The heart of the system is a specially designed generator capable of lexically guided sentence generation, of generating syntactic paraphrases, and displaying syntactic structures visually. 1 Introduction:  "
W07-2310 "<NoAbstract>"
W07-2311 " This paper describes a model of the choice of modal verbs and modal particles. The choice mechanism does not require a modality-specific input as, e.g., a modal logical formula. Instead semantic (modal force) and pragmatic constraints (speech act marking) are applied to the available information on the whole and constrain the set of modal candidates to those that are appropriate in the respective contexts. The choice model is realized in the CAN system that generates recommendations about courses of study.  "
W07-2312 "Abstract The issue of sentence ordering is an important one for natural language tasks such as multi-document summarization, yet there has not been a quantitative exploration of the range of acceptable sentence orderings for short texts. We present results of a sentence reordering experiment with three experimental conditions. Our findings indicate a very high degree of variability in the orderings that the eighteen subjects produce. In addition, the variability of reorderings is significantly greater when the initial ordering seen by subjects is different from the original summary. We conclude that evaluation of sentence ordering should use multiple reference orderings. Our evaluation presents several metrics that might prove useful in assessing against multiple references. We conclude with a deeper set of questions: (a) what sorts of independent assessments of quality of the different reference orderings could be made and (b) whether a large enough test set would obviate the need for such independent means of quality assessment. "
W07-2313 " In this paper we introduce a method for generating interactive documents which exploits the visual features of hypertext to represent discourse structure. We explore the consistent and principled use of graphics and animation to support navigation and comprehension of non-linear text, where textual discourse markers do not always work effectively. 1 Int  "
W07-2314 "Abstract Existing generation systems use verbs almost exclusively to describe actions/events or to ascribe properties. In doing so, they achieve a direct concrete style of the kind often recommended in style manuals. However in many genres, including academic writing, it is common to find verbs expressing abstract relationships, with events etc. pushed down into nominalisations. This paper illustrates two important classes of abstract verb, one expressing discourse relations, the other expressing participant roles, and discusses some theoretical and practical reasons for studying such verbs and including them in generation systems. "
W07-2316 " This paper introduces our domain independent approach to free generation from single RDF triples without using any domain dependent knowledge. Our approach is developed based on our argument that RDF representations carry rich linguistic information, which can be used to achieve readable domain independent generation. In order to examine to what extent our argument is realistic, we carry out an evaluation experiment, which is the first evaluation of this kind of domain independent generation in the field. Introduction In the Semantic Web, both instance data and ontological data 1 are represented as graphs based on the Resource Description Framework (RDF) (W3C 2004). In order to facilitate non-technician users to access the knowledge and information coded in RDF, we are eventually aiming at developing a domain independent approach to presenting RDF graphs in natural language, which can greatly reduce the cost of applying NLG techniques to various RDF domains (e.g., medical RDF data and chemical RDF data). In this paper we introduce our domain independent approach to generating phrases or sentences from single RDF triples 2 without using any domain knowledge but only generic linguistic knowledge sources. This contrasts with almost all existing work generating natural language from 1 Ontological languages are developed based on the RDF syntax, so ontological data are still RDF graphs. 2 Generation from RDF triples in our case means only presenting the information in the triples, rather than explaining the information. ontologies, which assumes the existence of domain-dependent lexicons. This work is a key part of our final system because generation from single RDF triples, which are the atomic units of RDF graphs, is the foundation and also the first step for any further generation from larger RDF graphs. In order to examine to what extent the linguistic structures can be used to achieve domain independent generation from single RDF triples, we have built a generation system, Triple-Text (TT), and here we compare TTs generation with human experts generation in an evaluation experiment.  1 Ontological languages are developed based on the RDF syntax, so ontological data are still RDF graphs. 2 Generation from RDF triples in our case means only presenting the information in the triples, rather than explaining the information. ontologies, which assumes the existence of domain-dependent lexicons. This work is a key part of our final system because generation from single RDF triples, which are the atomic units of RDF graphs, is the foundation and also the first step for any further generation from larger RDF graphs. In order to examine to what extent the linguistic structures can be used to achieve domain independent generation from single RDF triples, we have built a generation system, Triple-Text (TT), and here we compare TTs generation with human experts generation in an evaluation experiment. "
W07-2317 "Abstract We present the Narrator, an NLG component used for the generation of narratives in a digital storytelling system. We describe how the Narrator works and show some examples of generated stories. "
W07-2318 "ia {jviethen|rdale}@ics.mq.edu.au Abstract Almost all existing referring expression generation algorithms aim to find one best referring expression for a given intended referent. However, human-produced data demonstrates that, for any given entity, many perfectly acceptable referring expressions exist. At the same time, it is not the case that all logically possible descriptions are acceptable; so, if we remove the requirement to produce only one best solution, how do we avoid generating undesirable descriptions? Our aim in this paper is to sketch a framework that allows us to capture constraints on referring expression generation, so that the set of logically possible descriptions can be reduced to just those that are acceptable. "
W07-2319 "University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW, United Kingdom {ccallawa,jmoore}@inf.ed.ac.uk Abstract We present an empirical approach to adaptively selecting a tutoring systems remediation strategy based on an annotated corpus of human-human tutorial dialogues. We are interested in the remediation selection problem, that of generating the best remediation strategy given a diagnosis for an incorrect answer and the current problem solving context. By comparing the use of individual remediation strategies to their success in varying contexts, we can empirically extract and implement tutoring rules for the content planner of an intelligent tutoring system. We describe a methodology for analyzing a tutoring corpus and using the resulting data to inform a content planning model. "
W07-2320 "1QD. Abstract This paper discusses an implemented dialogue system which generates the meanings of utterances by taking into account: the surface mood of the users last utterance; the meanings of all the users utterances from the current discourse; the systems expert knowledge; and the systems beliefs about the current situation arising from the discourse (including its beliefs about the user and her beliefs, and its beliefs about what is common knowledge). The system formulates the content of its responses by employing an epistemic theorem prover to do deep reasoning. During the reasoning process, it remembers the proof tree it constructs, and from this derives the meaning of an explanatory response. "
W07-2321 " This paper reports on work in progress on extending the entity-based approach on measuring coherence (Barzilay & Lapata, 2005; Lapata & Barzilay, 2005) from coreference to semantic relatedness. We use a corpus of manually annotated German newspaper text (T  uBa-D/Z) and aim at improving the performance by grouping related entities with the WikiRelate! API (Strube & Ponzetto, 2006). 1 I  "
W07-2322 "Patission 76, GR-104 34 Athens, Greece Abstract We introduce Naturalowl, an open-source multilingual natural language generator that produces descriptions of instances and classes, starting from a linguistically annotated ontology. The generator is heavily based on ideas from ilex and m-piro, but it is in many ways simpler and it provides full support for owl dl ontologies with rdf linguistic annotations. Naturalowl is written in Java, and it is supported by m-piros authoring tool, as well as an alternative plug-in for the Prot  eg  e ontology editor. "
W07-2323 " This paper discusses the generation of cryptic crossword clues: a task that involves generating texts that have both a surface reading, based on a natural language interpretation of the words, and a hidden meaning in which the strings that form the text can be interpreted as a puzzle. The process of clue generation realizes a representation of the hidden, puzzle meaning of the clue through the aggregation of chunks of text. As these chunks are combined, syntactic and semantic selectional constraints are explored, and through this language understanding task a meaningful surface reading is recovered. This hybrid language generation/language understanding process transforms a representation of the clue as a word puzzle into a representation of some meaningful assertion in the domain of the real world, mediated through the generated multi-layered text; a text which has two separate readings. 1 Introduct  "
W07-2324 "Abstract We describe a new application for NLG technology: the generation of indicative, abstractive summaries of multi-party meetings. Based on the freely available AMI corpus of 100 hours of recorded meetings, we are developing a summarizer that uses the rich annotations in the AMI corpus. "
W07-2325 "<NoAbstract>"
W07-2326 "Abstract An existing taxonomy of Dutch cue phrases, designed for use in story generation, was validated by analysing cue phrase usage in a corpus of classical fairy tales. The analysis led to some adaptations of the original taxonomy. "
W07-2327 " Geo-referenced data which are often communicated via maps are inaccessible to the visually impaired population. We summarise existing approaches to improving accessibility of geo-referenced data and present the Atlas.txt project which aims to produce textual summaries of such data which can be read out via a screenreader. We outline issues involved in generating descriptions of geo-referenced data and present initial work on content determination based on knowledge acquisition from both parallel corpus analysis and input from visually impaired people. In our corpus analysis we build an ontology containing abstract representations of expert-written sentences which we associate with macros containing sequences of data analysis methods. This helps us to identify which data analysis methods need to be applied to generate text from data. 1 Introdu  "
W08-0101 " This paper describes a novel algorithm to dynamically set endpointing thresholds based on a rich set of dialogue features to detect the end of user utterances in a dialogue system. By analyzing the relationship between silences in users speech to a spoken dialogue system and a wide range of automatically extracted features from discourse, semantics, prosody, timing and speaker characteristics, we found that all features correlate with pause duration and with whether a silence indicates the end of the turn, with semantics and timing being the most informative. Based on these features, the proposed method reduces latency by up to 24% over a fixed threshold baseline. Offline evaluation results were confirmed by implementing the proposed algorithm in the Lets Go system. 1 Introdu  "
W08-0102 " Spoken and multimodal dialogue systems typically make use of confidence scores to choose among (or reject) a speech recognizers Nbest hypotheses for a particular utterance. We argue that it is beneficial to instead choose among a list of candidate system responses. We propose a novel method in which a confidence score for each response is derived from a classifier trained on acoustic and lexical features emitted by the recognizer, as well as features culled from the generation of the candidate response itself. Our responsebased method yields statistically significant improvements in F-measure over a baseline in which hypotheses are chosen based on recognition confidence scores only. 1 Introduct  "
W08-0104 "Abstract In this paper we define agreement in terms of shared public commitments, and implicit agreement is conditioned on the semantics of the relational speech acts (e.g., Narration, Explanation) that each agent performs. We provide a consistent interpretation of disputes, and updating a logical form with the current utterance always involves extending it and not revising it, even if the current utterance denies earlier content. "
W08-0105 " We explore the role of redundancy, both in anticipation of and in response to listener confusion, in task-oriented dialogue. We find that direction-givers provide redundant utterances in response to both verbal and non-verbal signals of listener confusion. We also examine the effects of prior acquaintance and visibility upon redundancy. As expected, givers use more redundant utterances overall, and more redundant utterances in response to listener questions, when communicating with strangers. We discuss our findings in relation to theories of redundancy, the balance of speaker and listener effort, and potential applications. 1 Int  "
W08-0106 " A key problem for models of dialogue is to explain how semantic co-ordination in dialogue is achieved and sustained. This paper presents findings from a series of Maze Task experiments which are not readily explained by the primary co-ordination mechanisms of existing models. It demonstrates that alignment in dialogue is not simply an outcome of successful interaction, but a communicative resource exploited by interlocutors in converging on a semantic model. We argue this suggests mechanisms of co-ordination in dialogue which are of relevance for a general account of how semantic co-ordination is achieved. 1 Introdu  "
W08-0107 " We introduce the Degrees of Grounding model, which defines the extent to which material being discussed in a dialogue has been grounded. This model has been developed and evaluated by a corpus analysis, and includes a set of types of evidence of understanding, a set of degrees of groundedness, a set of grounding criteria, and methods for identifying each of these. We describe how this model can be used for dialogue management. 1 I  "
W08-0108 "<NoAbstract>"
W08-0109 "Italy {varges|riccardi|silviaq}@disi.unitn.it Abstract We present the ADAMACH data centric dialog system, that allows to perform onand offline mining of dialog context, speech recognition results and other system-generated representations, both within and across dialogs. The architecture implements a fat pipeline for speech and language processing. We detail how the approach integrates domain knowledge and evolving empirical data, based on a user study in the University Helpdesk domain. "
W08-0110 " Humans produce speech incrementally and on-line as the dialogue progresses using information from several different sources in parallel. A dialogue system that generates output in a stepwise manner and not in preplanned syntactically correct sentences needs to signal how new dialogue contributions relate to previous discourse. This paper describes a data collection which is the foundation for an effort towards more humanlike language generation in DEAL, a spoken dialogue system developed at KTH. Two annotators labelled cue phrases in the corpus with high inter-annotator agreement (kappa coefficient 0.82). 1 Introduct  "
W08-0111 "Abstract We look at the average frequency of contrastive connectives in the SPaRKy Restaurant Corpus with respect to realization ratings by human judges. We implement a discriminative n-gram ranker to model these ratings and analyze the resulting n-gram weights to determine if our ranker learns this distribution. Surprisingly, our ranker learns to avoid contrastive connectives. We look at possible explanations for this distribution, and recommend improvements to both the generator and ranker of the sentence plans/realizations. "
W08-0112 " Significant research efforts have been devoted to speech summarization, including automatic approaches and evaluation metrics. However, a fundamental problem about what summaries are for the speech data and whether humans agree with each other remains unclear. This paper performs an analysis of human annotated extractive summaries using the ICSI meeting corpus with an aim to examine their consistency and the factors impacting human agreement. In addition to using Kappa statistics and ROUGE scores, we also proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem. 1 Introdu  "
W08-0113 " We present a method for resolving definite exophoric reference to visually shared objects that is based on a) an automatically learned, simple mapping of words to visual features (visual word semantics), b) an automatically learned, semantically-motivated utterance segmentation (visual grammar), and c) a procedure that, given an utterance, uses b) to combine a) to yield a resolution. We evaluated the method both on a pre-recorded corpus and in an online setting, where it performed with 81% (chance: 14%) and 66% accuracy, respectively. This is comparable to results reported in related work on simpler settings. 1 The Tas  "
W08-0114 "Honda Research Institute Japan Co., Ltd. 8-1 Honcho, Wako, Saitama 351-0188, Japan {nakano, funakoshi, yuji.hasegawa, tsujino}@jp.honda-ri.com Abstract This paper presents a novel framework for building symbol-level control modules of animated agents and robots having a spoken dialogue interface. It features distributed modules called experts each of which is specialized to perform certain kinds of tasks. A common interface that all experts must support is specified, and any kind of expert can be incorporated if it has the interface. Several modules running in parallel coordinate the experts by accessing them through the interface, so that the whole system can achieve flexible control, such as interruption handling and parallel task execution. "
W08-0115 " In this paper DiaGen is presented, a tool that provides support in generating code for embedded dialogue applications. By aid of it, the dialogue development process is speeded up considerably. At the same time it is guaranteed that only well-formed and well-defined constructs are used. Having had its roots in the EU-funded project GEMINI, fundamental changes were necessary to adopt it to the requirements of the application environment. Additionally within this paper the basics of embedded speech dialogue systems are covered. 1 Intro  "
W08-0116 " This paper presents a coding protocol that allows naive users to annotate dialogue transcripts for anaphora and ellipsis. Cohen's kappa statistic demonstrates that the protocol is sufficiently robust in terms of reliability. It is proposed that quantitative ellipsis data may be used as an index of mutual-engagement. Current and potential uses of ellipsis coding are described. 1.  "
W08-0117 "Abstract An attempt was made to statistically estimate proposals which survived the discussion to be incorporated in the final agreement in an instance of a Japanese design conversation. Low level speech and vision features of hearer behaviors corresponding to aiduti, noddings and gaze were found to be a positive predictor of survival. The result suggests that non-linguistic hearer responses work as implicit proposal filters in consensus building, and could provide promising candidate features for the purpose of recognition and summarization of meeting events. "
W08-0118 "Microsoft Corporation 1 Microsoft Way, Redmond, WA 98052, USA {panguyen,gzweig}@microsoft.com Abstract Voice-Rate is an experimental dialog system through which a user can call to get product information. In this paper, we describe an optimal dialog management algorithm for Voice-Rate. Our algorithm uses a POMDP framework, which is probabilistic and captures uncertainty in speech recognition and user knowledge. We propose a novel method to learn a user knowledge model from a review database. Simulation results show that the POMDP system performs significantly better than a deterministic baseline system in terms of both dialog failure rate and dialog interaction time. To the best of our knowledge, our work is the first to show that a POMDP can be successfully used for disambiguation in a complex voice search domain like Voice-Rate. "
W08-0119 " This paper investigates the claim that a dialogue manager modelled as a Partially Observable Markov Decision Process (POMDP) can achieve improved robustness to noise compared to conventional state-based dialogue managers. Using the Hidden Information State (HIS) POMDP dialogue manager as an exemplar, and an MDP-based dialogue manager as a baseline, evaluation results are presented for both simulated and real dialogues in a Tourist Information Domain. The results on the simulated data show that the inherent ability to model uncertainty, allows the POMDP model to exploit alternative hypotheses from the speech understanding system. The results obtained from a user trial show that the HIS system with a trained policy performed significantly better than the MDP baseline. "
W08-0120 " This paper proposes a probabilistic framework for spoken dialog management using dialog examples. To overcome the complexity problems of the classic partially observable Markov decision processes (POMDPs) based dialog manager, we use a frame-based belief state representation that reduces the complexity of belief update. We also used dialog examples to maintain a reasonable number of system actions to reduce the complexity of the optimizing policy. We developed weather information and car navigation dialog system that employed a frame-based probabilistic framework. This framework enables people to develop a spoken dialog system using a probabilistic approach without complexity problem of POMDP. 1 Introductio  "
W08-0121 " When people engage in conversation, they adapt the way they speak to the speaking style of their conversational partner in a variety of ways. For example, they may adopt a certain way of describing something based upon the way their conversational partner describes it, or adapt their pitch range or speaking rate to a conversational partners. They may even align their turn-taking style or use of cue phrases to match their partners. These types of entrainment have been shown to correlate with various measures of task success and dialogue naturalness. While there is considerable evidence for lexical entrainment from laboratory experiments, much less is known about other types of acoustic-prosodic and discourse-level entrainment and little work has been done to examine entrainments in multiple modalities for the same dialogue. We will discuss work on entrainment in multiple dimensions in the Columbia Games Corpus. Our goal is to understand how the different varieties of entrainment correlate with one another and to determine which types of entrainment will be both useful and feasible for Spoken Dialogue Systems. 128  "
W08-0122 " This work proposes opinion frames as a representation of discourse-level associations that arise from related opinion targets and which are common in task-oriented meeting dialogs. We define the opinion frames and explain their interpretation. Additionally we present an annotation scheme that realizes the opinion frames and via human annotation studies, we show that these can be reliably identified. 1  "
W08-0123 " Argumentation is an emerging topic in the field of human computer dialogue. In this paper we describe a novel approach to dialogue management that has been developed to achieve persuasion using a textual argumentation dialogue system. The paper introduces a layered management architecture that mixes task-oriented dialogue techniques with chatbot techniques to achieve better persuasiveness in the dialogue. 1 Intro  "
W08-0124 " An important task in automatic conversation understanding is the inference of social structure governing participant behavior. We explore the dependence between several social dimensions, including assigned role, gender, and seniority, and a set of low-level features descriptive of talkspurt deployment in a multiparticipant context. Experiments conducted on two large, publicly available meeting corpora suggest that our features are quite useful in predicting these dimensions, excepting gender. The classification experiments we present exhibit a relative error rate reduction of 37% to 67% compared to choosing the majority class. 1 Introduction An important task in automatic conversation understanding is the inference of social structure governing participant behavior; in many conversations, the maintenance or expression of that structure is an implicit goal, and may be more important than the propositional content of what is said. There are many social dimensions along which participants may differ (Berger, Rosenholtz and Zelditch, 1980). Research in social psychology has shown that such differences among participants entail systematic differences in observed turn-taking and floor-control patterns (e.g. (Bales, 1950), (Tannen, 1996), (Carletta, Garrod and Fraser-Krauss, 1998)), and that participant types are not independent of the types and sizes of conversations in which they appear. In the present work, we consider the dimensions of assigned role, gender, and seniority level. We explore the predictability of these dimensions from a set of low-level speech activity features, namely the probabilities of initiating and continuing talkspurts in specific multiparticipant contexts, estimated from entire conversations. For our purposes, talkspurts (Norwine and Murphy, 1938) are contiguous intervals of speech, with internal pauses no longer than 0.3 seconds. Features derived from talkspurts are not only easier to compute than higher-level lexical, prosodic, or dialogue act features, they are also applicable to scenarios in which only privacy-sensitive data (Wyatt et al, 2007) is available. At the current time, relatively little is known about the predictive power of talkspurt timing in the context of large multi-party corpora. As stated, our primary goal is to quantify the dependence between specific types of speech activity features and specific social dimensions; however, doing so offers several additional benefits. Most importantly, the existence of significant dependence would suggest that multiparticipant speech activity detectors (Laskowski, F  ugen and Schultz, 2007) relying on models conditioned on such attributes may outperform those relying on general models. Furthermore, conversational dialogue systems deployed in multi-party scenarios may be perceived as more human-like, by humans, if their talkspurt deployment strategies are tailored to the personalities they are designed to embody. Computational work which is most similar to that presented here includes the inference of static dominance (Rienks and Heylen, 2005) and influence (Rienks et al., 2006) rankings. In that work, the authors employed several speech activity features differing from ours in temporal scale and normaliza148 tion. Notably, their features are not probabilities which are directly employable in a speech activity detection system. In addition, several higher-level features were included, such as topic changes, participant roles, and rates of phenomena such as turns and interruptions, and these were shown to yield the most robust performance. Our aim is also similar to that in (Vinciarelli, 2007) on radio shows, where the proposed approach relies on the relatively fixed temporal structure of production broadcasts, a property which is absent in spontaneous conversation.  "
W08-0125 " We describe a process for automatically detecting decision-making sub-dialogues in transcripts of multi-party, human-human meetings. Extending our previous work on action item identification, we propose a structured approach that takes into account the different roles utterances play in the decisionmaking process. We show that this structured approach outperforms the accuracy achieved by existing decision detection systems based on flat annotations, while enabling the extraction of more fine-grained information that can be used for summarization and reporting. 1 Introduction  "
W08-0126 " We propose to use user simulation for testing during the development of a sophisticated dialog system. While the limited behaviors of the state-of-the-art user simulation may not cover important aspects in the dialog system testing, our proposed approach extends the functionality of the simulation so that it can be used at least for the early stage testing before the system reaches stable performance for evaluation involving human users. The proposed approach includes a set of evaluation measures that can be computed automatically from the interaction logs between the user simulator and the dialog system. We first validate these measures on human user dialogs using user satisfaction scores. We also build a regression model to estimate the user satisfaction scores using these evaluation measures. Then, we apply the evaluation measures on a simulated dialog corpus trained from the real user corpus. We show that the user satisfaction scores estimated from the simulated corpus are not statistically different from the real users satisfaction scores. 1 Introduct  "
W08-0127 "iji way, Marina del Rey, CA, 90292 {gandhe,traum}@ict.usc.edu Abstract Evaluating a dialogue system is seen as a major challenge within the dialogue research community. Due to the very nature of the task, most of the evaluation methods need a substantial amount of human involvement. Following the tradition in machine translation, summarization and discourse coherence modeling, we introduce the the idea of evaluation understudy for dialogue coherence models. Following (Lapata, 2006), we use the information ordering task as a testbed for evaluating dialogue coherence models. This paper reports findings about the reliability of the information ordering task as applied to dialogues. We find that simple n-gram co-occurrence statistics similar in spirit to BLEU (Papineni et al., 2001) correlate very well with human judgments for dialogue coherence. "
W08-0128 "Abstract Improvements in the quality, usability and acceptability of spoken dialog systems can be facilitated by better evaluation methods. To support early and efficient evaluation of dialog systems and their components, this paper presents a tripartite framework describing the evaluation problem. One part models the behavior of user and system during the interaction, the second one the perception and judgment processes taking place inside the user, and the third part models what matters to system designers and service providers. The paper reviews available approaches for some of the model parts, and indicates how anticipated improvements may serve not only developers and users but also researchers working on advanced dialog functions and features. "
W08-0129 " A dialogue system can present itself and/or address the user as an active agent by means of linguistic constructions in personal style, or suppress agentivity by using impersonal style. We compare system evaluation judgments and input style alignment of users interacting with an in-car dialogue system generating output in personal vs. impersonal style. Although our results are consistent with earlier findings obtained with simulated systems, the effects are weaker. 1  "
W08-0301 "Abstract The treatment of spurious words of source language is an important problem but often ignored in the discussion on phrase-based SMT. This paper explains why it is important and why it is not a trivial problem, and proposes three models to handle spurious source words. Experiments show that any source word deletion model can improve a phrase-based system by at least 1.6 BLEU points and the most sophisticated model improves by nearly 2 BLEU points. This paper also explores the impact of training data size and training data domain/genre on source word deletion. "
W08-0302 "13, USA {kgimpel,nasmith}@cs.cmu.edu Abstract We explore the augmentation of statistical machine translation models with features of the context of each phrase to be translated. This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al., 2007). The context features we consider use surrounding words and part-of-speech tags, local syntactic structure, and other properties of the source language sentence to help predict each phrases translation. Our approach requires very little computation beyond the standard phrase extraction algorithm and scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chineseto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. "
W08-0303 "Abstract In this paper a new discriminative word alignment method is presented. This approach models directly the alignment matrix by a conditional random field (CRF) and so no restrictions to the alignments have to be made. Furthermore, it is easy to add features and so all available information can be used. Since the structure of the CRFs can get complex, the inference can only be done approximately and the standard algorithms had to be adapted. In addition, different methods to train the model have been developed. Using this approach the alignment quality could be improved by up to 23 percent for 3 different language pairs compared to a combination of both IBM4alignments. Furthermore the word alignment was used to generate new phrase tables. These could improve the translation quality significantly. "
W08-0304 "Abstract Minimum error rate training (MERT) is a widely used learning procedure for statistical machine translation models. We contrast three search strategies for MERT: Powells method, the variant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powells method and coordinate descent. "
W08-0305 " We present an extensive experimental study of a Statistical Machine Translation system, Moses (Koehn et al., 2007), from the point of view of its learning capabilities. Very accurate learning curves are obtained, by using high-performance computing, and extrapolations are provided of the projected performance of the system under different conditions. We provide a discussion of learning curves, and we suggest that: 1) the representation power of the system is not currently a limitation to its performance, 2) the inference of its models from finite sets of i.i.d. data is responsible for current performance limitations, 3) it is unlikely that increasing dataset sizes will result in significant improvements (at least in traditional i.i.d. setting), 4) it is unlikely that novel statistical estimation methods will result in significant improvements. The current performance wall is mostly a consequence of Zipfs law, and this should be taken into account when designing a statistical machine translation system. A few possible research directions are discussed as a result of this investigation, most notably the integration of linguistic rules into the model inference phase, and the development of active learning procedures. 1 Introduction and Back  "
W08-0306 " Word alignments that violate syntactic correspondences interfere with the extraction of string-to-tree transducer rules for syntaxbased machine translation. We present an algorithm for identifying and deleting incorrect word alignment links, using features of the extracted rules. We obtain gains in both alignment quality and translation quality in Chinese-English and Arabic-English translation experiments relative to a GIZA++ union baseline. 1 Intro  "
W08-0307 " We describe two methods to improve SMT accuracy using shallow syntax information. First, we use chunks to refine the set of word alignments typically used as a starting point in SMT systems. Second, we extend an N -grambased SMT system with chunk tags to better account for long-distance reorderings. Experiments are reported on an Arabic-English task showing significant improvements. A human error analysis indicates that long-distance reorderings are captured effectively. 1 Int  "
W08-0308 "14627 Abstract We propose three enhancements to the treeto-string (TTS) transducer for machine translation: first-level expansion-based normalization for TTS templates, a syntactic alignment framework integrating the insertion of unaligned target words, and subtree-based ngram model addressing the tree decomposition probability. Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU4 metric. We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system. "
W08-0309 "University of Edinburgh j schroeder ed ac uk Abstract This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish. We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort. We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level. We validate our manual evaluation methodology by measuring intraand inter-annotator agreement, and collecting timing information. "
W08-0310 "Abstract This paper describes our statistical machine translation systems based on the Moses toolkit for the WMT08 shared task. We address the Europarl and News conditions for the following language pairs: English with French, German and Spanish. For Europarl, n-best rescoring is performed using an enhanced n-gram or a neuronal language model; for the News condition, language models incorporate extra training data. We also report unconvincing results of experiments with factored models. "
W08-0311 " In this article, we present MetaMorpho, a rule based machine translation system that was used to create MorphoLogics submission to the WMT08 shared Hungarian to English translation task. The architecture of MetaMorpho does not fit easily into traditional categories of rule based systems: the building blocks of its grammar are pairs of rules that describe source and target language structures in a parallel fashion and translated structures are created while parsing the input. 1  "
W08-0312 "ersity Pittsburgh, PA, 15213, USA {abhayaa,alavie}@cs.cmu.edu Abstract This paper describes our submissions to the machine translation evaluation shared task in ACL WMT-08. Our primary submission is the Meteor metric tuned for optimizing correlation with human rankings of translation hypotheses. We show significant improvement in correlation as compared to the earlier version of metric which was tuned to optimized correlation with traditional adequacy and fluency judgments. We also describe m-bleu and m-ter, enhanced versions of two other widely used metrics bleu and ter respectively, which extend the exact word matching used in these metrics with the flexible matching based on stemming and Wordnet in Meteor . "
W08-0313 " This paper describes an initial version of a general purpose French/English statistical machine translation system. The main features of this system are the open-source Moses decoder, the integration of a bilingual dictionary and a continuous space target language model. We analyze the performance of this system on the test data of the WMT08 evaluation. 1 I  "
W08-0314 " This paper present the University of Washingtons submission to the 2008 ACL SMT shared machine translation task. Two systems, for English-toSpanish and German-to-Spanish translation are described. Our main focus was on testing a novel boosting framework for N-best list reranking and on handling German morphology in the German-toSpanish system. While boosted N-best list reranking did not yield any improvements for this task, simplifying German morphology as part of the preprocessing step did result in significant gains. 1 Introductio  "
W08-0315 " This paper reports on the participation of the TALP Research Center of the UPC (Universitat Politecnica de Catalunya) to the ACL WMT 2008 evaluation campaign. This years system is the evolution of the one we employed for the 2007 campaign. Main updates and extensions involve linguistically motivated word reordering based on the reordering patterns technique. In addition, this system introduces a target language model, based on linguistic classes (Part-of-Speech), morphology reduction for an inflectional language (Spanish) and an improved optimization procedure. Results obtained over the development and test sets on Spanish to English (and the other way round) translations for both the traditional Europarl and a challenging News stories tasks are analyzed and commented. 1 I  "
W08-0316 " We describe the Cambridge University Engineering Department phrase-based statistical machine translation system for SpanishEnglish and French-English translation in the ACL 2008 Third Workshop on Statistical Machine Translation Shared Task. The CUED system follows a generative model of translation and is implemented by composition of component models realised as Weighted Finite State Transducers, without the use of a special-purpose decoder. Details of system tuning for both Europarl and News translation tasks are provided. 1 Introduct  "
W08-0317 " We describe the LIU systems for GermanEnglish and English-German translation submitted to the Shared Task of the Third Workshop of Statistical Machine Translation. The main features of the systems, as compared with the baseline, is the use of morphological preand post-processing, and a sequence model for German using morphologically rich parts-of-speech. It is shown that these additions lead to improved translations. 1 Introduct  "
W08-0318 "Abstract The Edinburgh submissions to the shared task of the Third Workshop on Statistical Machine Translation (WMT-2008) incorporate recent advances to the open source Moses system. We made a special effort on the German English and EnglishGerman language pairs, leading to substantial improvements. "
W08-0319 " This paper describes our two contributions to WMT08 shared task: factored phrase-based model using Moses and a probabilistic treetransfer model at a deep syntactic layer. 1  "
W08-0320 "Abstract We describe the experiments of the UC Berkeley team on improving English-Spanish machine translation of news text, as part of the WMT08 Shared Translation Task. We experiment with domain adaptation, combining a small in-domain news bi-text and a large out-of-domain one from the Europarl corpus, building two separate phrase translation models and two separate language models. We further add a third phrase translation model trained on a version of the news bi-text augmented with monolingual sentencelevel syntactic paraphrases on the sourcelanguage side, and we combine all models in a log-linear model using minimum error rate training. Finally, we experiment with different tokenization and recasing rules, achieving 35.09% Bleu score on the WMT07 news test data when translating from English to Spanish, which is a sizable improvement over the highest Bleu score achieved on that dataset at WMT07: 33.10% (in fact, by our system). On the WMT08 English to Spanish news translation, we achieve 21.92%, which makes our team the second best on Bleu score. "
W08-0321 "iversity Pittsburgh, PA 15213, USA {nbach, qing, vogel+}@cs.cmu.edu Abstract This paper describes the statistical machine translation systems submitted to the ACL-WMT 2008 shared translation task. Systems were submitted for two translation directions: EnglishSpanish and SpanishEnglish. Using sentence pair confidence scores estimated with source and target language models, improvements are observed on the NewsCommentary test sets. Genre-dependent sentence pair confidence score and integration of sentence pair confidence score into phrase table are also investigated. "
W08-0322 " The novel kernel regression model for SMT only demonstrated encouraging results on small-scale toy data sets in previous works due to the complexities of kernel methods. It is the first time results based on the real-world data from the shared translation task will be reported at ACL 2008 Workshop on Statistical Machine Translation. This paper presents the key modules of our system, including the kernel ridge regression model, retrieval-based sparse approximation, the decoding algorithm, as well as language modeling issues under this framework. 1  "
W08-0323 " Our participation in the shared translation task at WMT-08 focusses on news translation from English to French. Our main goal is to contrast a baseline version of the phrase-based MATRAX system, with a version that incorporates syntactic coupling features in order to discriminate translations produced by the baseline system. We report results comparing different feature combinations. 1 I  "
W08-0324 " We apply the Stat-XFER statistical transfer machine translation framework to the task of translating from French and German into English. We introduce statistical methods within our framework that allow for the principled extraction of syntax-based transfer rules from parallel corpora given word alignments and constituency parses. Performance is evaluated on test sets from the 2007 WMT shared task. 1  "
W08-0325 "Republic {zabokrtsky,ptacek,pajas}@ufal.mff.cuni.cz Abstract We present a new EnglishCzech machine translation system combining linguistically motivated layers of language description (as defined in the Prague Dependency Treebank annotation scenario) with statistical NLP approaches. 1 Introduction We describe a new MT system (called TectoMT) based on the conventional analysis-transfersynthesis architecture. We use the layers of language description defined in the Prague Dependency Treebank 2.0 (PDT for short, (Haji c and others, 2006)), namely (1) word layer  raw text, no linguistic annotation, (2) morphological layer  sequence of tagged and lemmatized tokens, (3) analytical layer  each sentence represented as a surface-syntactic dependency tree, and (4) tectogrammatical layer  each sentence represented as a deep-syntactic dependency tree in which only autosemantic words do have nodes of their own; prefixes w-, m-, a-, or twill be used for denoting these layers. 1 We use Praguian tectogrammatics (introduced in (Sgall, 1967)) as the transfer layer because we believe that, first, it largely abstracts from language-specific (inflection, agglutination, functional words. . . ) means of expressing non-lexical  The research reported in this paper is financially supported by grants GAAV  CR 1ET101120503 and MSM0021620838. 1 In addition, we use also p-layer (phrase structures) as an a-layer alternative, the only reason for which is that we do not have a working a-layer parser for English at this moment. meanings, second, it allows for a natural transfer factorization, and third, local tree contexts in t-trees carry more information (esp. for lexical choice) than local linear contexts in the original sentences. In order to facilitate separating the transfer of lexicalization from the transfer of syntactization, we introduce the concept of formeme. Each t-nodes has a formeme attribute capturing which morphosyntactic form has been (in the case of analysis) or will be (synthesis) used for the t-node in the surface sentence shape. Here are some examples of formemes we use for English: n:subj (semantic noun (sn) in subject position), n:for+X (sn with preposition for), n:X+ago (sn with postposition ago), n:poss (possessive form of sn), v:because+fin (semantic verb (sv) as a subordinating finite clause introduced by because), v:without+ger (sv as a gerund after without), adj:attr (semantic adjective (sa) in attributive position), adj:compl (sa in complement position). The presented system intensively uses the PDT technology (data formats, software tools). Special attention is paid to modularity: the translation is implemented (in Perl) as a long sequence of processing modules (called blocks) with relatively tiny, welldefined tasks, so that each module is independently testable, improvable, or substitutable. TectoMT allows to easily combine blocks based on different approaches, from blocks using complex probabilistic solutions (e.g., B2, B6, B35, see the next section), through blocks applying simpler Machine Learning techniques (e.g., B69) or empirically based heuristics (e.g., B7, B25, B36, B71), to blocks implementing crisp linguistic rules (e.g., B48-B51, B59). There are also blocks for trivial technical tasks (e.g., B33, B72). 167 English m-layer She she PRP has have VBZ never never RB laughed laugh VBN in in IN her her PRP$ new new JJ boss boss NN 's 's POS office office NN . . . NPB She has English p-layer S ADVP never VP laughed in VP her new PP NPB boss 's NPB office . English a-layer She has never laughed in her new boss 's office . English t-layer #PersPron n:subj never adv: laugh v:fin #PersPron n:poss new adj:attr boss n:poss office n:in+X Czech t-layer #PersPron n:1 nikdy adv: smat_se v:fin urad n:v+6 #PersPron adj:attr novy adj:attr sef n:2 Czech a-layer Nikdy D........1A... se P7 nesmala VpFS...3..NA.. v R uradu N.IS6.....A... sveho P8MS2......... noveho AAMS2....1A... sefa N.MS2.....A... . Z Figure 1: MT pyramid as implemented in TectoMT. All the representations are rooted with artificial nodes, serving only as labels. Virtually, the pyramid is bottomed with the input sentence on the source side (She has never laughed in her new bosss office.) and its automatic translation on the target side (Nikdy se nesm  ala v   u radu sv  eho nov  eho  s  efa.). "
W08-0326 " In this paper, we give a description of the machine translation system developed at DCU that was used for our participation in the evaluation campaign of the Third Workshop on Statistical Machine Translation at ACL 2008. We describe the modular design of our datadriven MT system with particular focus on the components used in this participation. We also describe some of the significant modules which were unused in this task. We participated in the EuroParl task for the following translation directions: Spanish English and FrenchEnglish, in which we employed our hybrid EBMT-SMT architecture to translate. We also participated in the Czech English News and News Commentary tasks which represented a previously untested language pair for our system. We report results on the provided development and test sets. 1 Introdu  "
W08-0327 " This paper describes SYSTRAN submissions for the shared task of the third Workshop on Statistical Machine Translation at ACL. Our main contribution consists in a French-English statistical model trained without the use of any human-translated parallel corpus. In substitution, we translated a monolingual corpus with SYSTRAN rule-based translation engine to produce the parallel corpus. The results are provided herein, along with a measure of error analysis. 1  "
W08-0328 "1 1: Saarland University, Saarbr  ucken, Germany 2: DFKI GmbH, Saarbr  ucken, Germany Abstract Based on an architecture that allows to combine statistical machine translation (SMT) with rule-based machine translation (RBMT) in a multi-engine setup, we present new results that show that this type of system combination can actually increase the lexical coverage of the resulting hybrid system, at least as far as this can be measured via BLEU score. "
W08-0329 " Confusion network decoding has been the most successful approach in combining outputs from multiple machine translation (MT) systems in the recent DARPA GALE and NIST Open MT evaluations. Due to the varying word order between outputs from different MT systems, the hypothesis alignment presents the biggest challenge in confusion network decoding. This paper describes an incremental alignment method to build confusion networks based on the translation edit rate (TER) algorithm. This new algorithm yields significant BLEU score improvements over other recent alignment methods on the GALE test sets and was used in BBNs submission to the WMT08 shared translation task. 1 Introduction Confusion network decoding has been applied in combining outputs from multiple machine translation systems. The earliest approach in (Bangalore et al., 2001) used edit distance based multiple string alignment (MSA) (Durbin et al., 1988) to build the confusion networks. The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007). The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment. The confusion networks are built around a skeleton hypothesis. The skeleton hypothesis defines the word order of the decoding output. Usually, the 1-best hypotheses from each system are considered as possible skeletons. Using the pair-wise hypothesis alignment, the confusion networks are built in two steps. First, all hypotheses are aligned against the skeleton independently. Second, the confusion networks are created from the union of these alignments. The incremental hypothesis alignment algorithm combines these two steps. All words from the previously aligned hypotheses are available, even if not p  "
W08-0330 " Previous studies have shown automatic evaluation metrics to be more reliable when compared against many human translations. However, multiple human references may not always be available. It is more common to have only a single human reference (extracted from parallel texts) or no reference at all. Our earlier work suggested that one way to address this problem is to train a metric to evaluate a sentence by comparing it against pseudo references, or imperfect references produced by off-the-shelf MT systems. In this paper, we further examine the approach both in terms of the training methodology and in terms of the role of the human and pseudo references. Our expanded experiments show that the approach generalizes well across multiple years and different source languages. 1 Introductio  "
W08-0331 "<NoAbstract>"
W08-0332 " This document describes the approach by the NLP Group at the Technical University of Catalonia (UPC-LSI), for the shared task on Automatic Evaluation of Machine Translation at the ACL 2008 Third SMT Workshop. 1 I  "
W08-0333 "Abstract In recent years, the quantity of parallel training data available for statistical machine translation has increased far more rapidly than the performance of individual computers, resulting in a potentially serious impediment to progress. Parallelization of the modelbuilding algorithms that process this data on computer clusters is fraught with challenges such as synchronization, data exchange, and fault tolerance. However, the MapReduce programming paradigm has recently emerged as one solution to these issues: a powerful functional abstraction hides system-level details from the researcher, allowing programs to be transparently distributed across potentially very large clusters of commodity hardware. We describe MapReduce implementations of two algorithms used to estimate the parameters for two word alignment models and one phrase-based translation model, all of which rely on maximum likelihood probability estimates. On a 20-machine cluster, experimental results show that our solutions exhibit good scaling characteristics compared to a hypothetical, optimally-parallelized version of current state-of-the-art single-core tools. "
W08-0334 " This paper presents a technique for classdependent decoding for statistical machine translation (SMT). The approach differs from previous methods of class-dependent translation in that the class-dependent forms of all models are integrated directly into the decoding process. We employ probabilistic mixture weights between models that can change dynamically on a segment-by-segment basis depending on the characteristics of the source segment. The effectiveness of this approach is demonstrated by evaluating its performance on travel conversation data. We used the approach to tackle the translation of questions and declarative sentences using classdependent models. To achieve this, our system integrated two sets of models specifically built to deal with sentences that fall into one of two classes of dialog sentence: questions and declarations, with a third set of models built to handle the general class. The technique was thoroughly evaluated on data from 17 language pairs using 6 machine translation evaluation metrics. We found the results were corpus-dependent, but in most cases our system was able to improve translation performance, and for some languages the improvements were substantial. "
W08-0335 "-0288, Japan {ruiqiang.zhang,keiji.yasuda,eiichiro.sumita}@atr.jp Abstract Chinese word segmentation (CWS) is a necessary step in Chinese-English statistical machine translation (SMT) and its performance has an impact on the results of SMT. However, there are many settings involved in creating a CWS system such as various specifications and CWS methods. This paper investigates the effect of these settings to SMT. We tested dictionarybased and CRF-based approaches and found there was no significant difference between the two in the qualty of the resulting translations. We also found the correlation between the CWS F-score and SMT BLEU score was very weak. This paper also proposes two methods of combining advantages of different specifications: a simple concatenation of training data and a feature interpolation approach in which the same types of features of translation models from various CWS schemes are linearly interpolated. We found these approaches were very effective in improving quality of translations. "
W08-0601 " In this paper, we propose a graph kernel based approach for the automated extraction of protein-protein interactions (PPI) from scientific literature. In contrast to earlier approaches to PPI extraction, the introduced alldependency-paths kernel has the capability to consider full, general dependency graphs. We evaluate the proposed method across five publicly available PPI corpora providing the most comprehensive evaluation done for a machine learning based PPI-extraction system. Our method is shown to achieve state-of-theart performance with respect to comparable evaluations, achieving 56.4 F-score and 84.8 AUC on the AImed corpus. Further, we identify several pitfalls that can make evaluations of PPI-extraction systems incomparable, or even invalid. These include incorrect crossvalidation strategies and problems related to comparing F-score results achieved on different evaluation resources. 1 Introduction  "
W08-0602 "ield S1 4DP {initial.surname}@dcs.shef.ac.uk Abstract The Clinical E-Science Framework (CLEF) project has built a system to extract clinically significant information from the textual component of medical records, for clinical research, evidence-based healthcare and genotype-meets-phenotype informatics. One part of this system is the identification of relationships between clinically important entities in the text. Typical approaches to relationship extraction in this domain have used full parses, domain-specific grammars, and large knowledge bases encoding domain knowledge. In other areas of biomedical NLP, statistical machine learning approaches are now routinely applied to relationship extraction. We report on the novel application of these statistical techniques to clinical relationships. We describe a supervised machine learning system, trained with a corpus of oncology narratives hand-annotated with clinically important relationships. Various shallow features are extracted from these texts, and used to train statistical classifiers. We compare the suitability of these features for clinical relationship extraction, how extraction varies between interand intra-sentential relationships, and examine the amount of training data needed to learn various relationships. "
W08-0603 "Abstract An adaptable relation extraction system for the biomedical domain is presented. The system makes use of a large set of contextual and shallow syntactic features, which can be automatically optimised for each relation type. The system is tested on three different relation types; protein-protein interactions, tissue expression relations and fragment to parent protein relations. "
W08-0604 "Abstract eGIFT (Extracting Gene Information From Text) is an intelligent system which is intended to aid scientists in surveying literature relevant to genes of interest. From a gene specific set of abstracts retrieved from PubMed, eGIFT determines the most important terms associated with the given gene. Annotators using eGIFT can quickly find articles describing gene functions and individuals scientists surveying the results of highthroughput experiments can quickly extract information important to their hits. "
W08-0605 " This paper presents an active learning-like framework for reducing the human effort for making named entity annotations in a corpus. In this framework, the annotation work is performed as an iterative and interactive process between the human annotator and a probabilistic named entity tagger. At each iteration, sentences that are most likely to contain named entities of the target category are selected by the probabilistic tagger and presented to the annotator. This iterative annotation process is repeated until the estimated coverage reaches the desired level. Unlike active learning approaches, our framework produces a named entity corpus that is free from the sampling bias introduced by the active strategy. We evaluated our framework by simulating the annotation process using two named entity corpora and show that our approach could drastically reduce the number of sentences to be annotated when applied to sparse named entities. "
W08-0606 " This article reports on a corpus annotation project that has produced a freely available resource for research on handling negation and uncertainty in biomedical texts (we call this corpus the BioScope corpus). The corpus consists of three parts, namely medical free texts, biological full papers and biological scientific abstracts. The dataset contains annotations at the token level for negative and speculative keywords and at the sentence level for their linguistic scope. The annotation process was carried out by two independent linguist annotators and a chief annotator  also responsible for setting up the annotation guidelines  who resolved cases where the annotators disagreed. We will report our statistics on corpus size, ambiguity levels and the consistency of annotations. 1 Int  "
W08-0607 "<NoAbstract>"
W08-0608 "Abstract Chemical named entities represent an important facet of biomedical text. We have developed a system to use character-based ngrams, Maximum Entropy Markov Models and rescoring to recognise chemical names and other such entities, and to make confidence estimates for the extracted entities. An adjustable threshold allows the system to be tuned to high precision or high recall. At a threshold set for balanced precision and recall, we were able to extract named entities at an F score of 80.7% from chemistry papers and 83.2% from PubMed abstracts. Furthermore, we were able to achieve 57.6% and 60.3% recall at 95% precision, and 58.9% and 49.1% precision at 90% recall. These results show that chemical named entities can be extracted with good performance, and that the properties of the extraction can be tuned to suit the demands of the task. "
W08-0609 " When term ambiguity and variability are very high, dictionary-based Named Entity Recognition (NER) is not an ideal solution even though large-scale terminological resources are available. Many researches on statistical NER have tried to cope with these problems. However, it is not straightforward how to exploit existing and additional Named Entity (NE) dictionaries in statistical NER. Presumably , addition of NEs to an NE dictionary leads to better performance. However, in reality, the retraining of NER models is required to achieve this. We have established a novel way to improve the NER performance by addition of NEs to an NE dictionary without retraining. We chose protein name recognition as a case study because it most suffers the problems related to heavy term variation and ambiguity. In our approach, first, known NEs are identified in parallel with Part-of-Speech (POS) tagging based on a general word dictionary and an NE dictionary. Then, statistical NER is trained on the tagger outputs with correct NE labels attached. We evaluated performance of our NER on the standard JNLPBA-2004 data set. The F-score on the test set has been improved from 73.14 to 73.78 after adding the protein names appearing in the training data to the POS tagger dictionary without any model retraining. The performance further increased to 78.72 after enriching the tagging dictionary with test set protein names. Our approach has demonstrated high performance in protein name recognition, which indicates how to make the most of known NEs in statistical NER. "
W08-0610 "School of Informatics, University of Edinburgh 2 Buccleuch Place, Edinburgh, EH8 9LW, UK {xwang,mmatsews}@inf.ed.ac.uk Abstract An important task in information extraction (IE) from biomedical articles is term identification (TI), which concerns linking entity mentions (e.g., terms denoting proteins) in text to unambiguous identifiers in standard databases (e.g., RefSeq). Previous work on TI has focused on species-specific documents. However, biomedical documents, especially full-length articles, often talk about entities across a number of species, in which case resolving species ambiguity becomes an indispensable part of TI. This paper describes our rule-based and machine-learning based approaches to species disambiguation and demonstrates that performance of TI can be improved by over 20% if the correct species are known. We also show that using the species predicted by the automatic species taggers can improve TI by a large margin. "
W08-0611 "Abstract Like text in other domains, biomedical documents contain a range of terms with more than one possible meaning. These ambiguities form a significant obstacle to the automatic processing of biomedical texts. Previous approaches to resolving this problem have made use of a variety of knowledge sources including linguistic information (from the context in which the ambiguous term is used) and domain-specific resources (such as UMLS). In this paper we compare a range of knowledge sources which have been previously used and introduce a novel one: MeSH terms. The best performance is obtained using linguistic features in combination with MeSH terms. Results from our system outperform published results for previously reported systems on a standard test set (the NLM-WSD corpus). "
W08-0612 "Abstract This paper describes the use and customization of Inductive Logic Programming (ILP) to infer indexing rules from MEDLINE citations. Preliminary results suggest this method may enhance the subheading attachment module of the Medical Text Indexer, a system for assisting MEDLINE indexers. "
W08-0613 " This paper presents a novel prediction approach for protein sub-cellular localization. We have incorporated text and sequence-based approaches. 1 Introduction Natural Language Processing (NLP) has tackled and solved a lot of prediction problems in Biology. One practical research issue is Protein Sub-Cellular Localization (PSL) Prediction. Many previous approaches have combined information from both texts and sequences by a machine learning (ML) technique (Shatkay et al., 2007). All of them have not used traditional NLP techniques such as parsing. Our aim is to develop a novel PSL prediction system using information from texts and sequences. At the same time, we demonstrated the effectiveness of the traditional NLP and the sequence-based features in the viewpoint of the text-based approach. 2 Methodolo  "
W08-0614 " The goal of the Penn Discourse Treebank (PDTB) project is to develop a large-scale corpus, annotated with coherence relations marked by discourse connectives. Currently, the primary application of the PDTB annotation has been to news articles. In this study, we tested whether the PDTB guidelines can be adapted to a different genre. We annotated discourse connectives and their arguments in one 4,937-token full-text biomedical article. Two linguist annotators showed an agreement of 85% after simple conventions were added. For the remaining 15% cases, we found that biomedical domain-specific knowledge is needed to capture the linguistic cues that can be used to resolve inter-annotator disagreement. We found that the two annotators were able to reach an agreement after discussion. Thus our experiments suggest that the PDTB annotation can be adapted to new domains by minimally adjusting the guidelines and by adding some further domain-specific linguistic cues. 1 Introdu  "
W08-0615 " We present a comparative study between two machine learning methods, Conditional Random Fields and Support Vector Machines for clinical named entity recognition. We explore their applicability to clinical domain. Evaluation against a set of gold standard named entities shows that CRFs outperform SVMs. The best F-score with CRFs is 0.86 and for the SVMs is 0.64 as compared to a baseline of 0.60. 1  "
W08-0616 "We hypothesize that machine-learning algorithms (MLA) can classify completer and simulated suicide notes as well as mental health professionals (MHP). Five MHPs classified 66 simulated or completer notes; MLAs were used for the same task. Results: MHPs were accurate 71% of the time; using the sequential minimization optimization algorithm (SMO) MLAs were accurate 78% of the time. There was no significant difference between the MLA and MPH classifiers. This is an important first step in developing an evidence based suicide predictor for emergency department use. "
W08-0617 "<NoAbstract>"
W08-0618 "Abstract This paper presents a system 1 for drug name identification and classification in biomedical texts. 1 Introduction Numerous studies have tackled gene and protein names recognition (Collier et al, 2002), (Tanabe and Wilbur, 2002). Nevertheless, drug names have not been widely addressed (Rindflesch et al., 2000). Automating the process of new drugs recognition and classification is a challenging task. With the rapidly changing vocabulary, new drugs are introduced while old ones are made obsolete. Though the terminological resources are frequently updated, they can not follow the accelerated pace of the changing terminology. Drug receives three distinct names: the chemical name, the generic (or nonproprietary) name, and the brand (or trademark) name. The U.S. Adopted Name (USAN) Council establishes specific nomenclature rules for naming generic drugs. These rules rely on the use of affixes that classify drugs according to their chemical structure, indication or mechanism of action. For example, analgesics substances can receive affixes such as -adol-, -butazone, -fenine, -eridine and fentanil. In the present work, we focus, particulary, on the implementation of a set of 531 affixes approved by 1 This work has been partially supported by the projects: FIT350300-2007-75 (Semantic Interoperability in Electronic Health Care) and TIN2007-67407-C03-01 (BRAVO: Advanced Multimodal and Multilingual Question Answering). the USAN Council and published in 2007 2 . The affixes allow a specific classification of drugs on pharmacological families, which ULMS Semantic NetWork is unable to provide. 2 The System The system consists of four main modules: a basic text processing module, WordNet look-up module, UMLS look-up module and the USAN rules module, as shown in Figure 1. A corpus of 90 medical abstracts was compiled for the experiment. For the basic processing of the abstracts, GATE 3 architecture is used. This text processing provides sentence segmentation, tokenization and POS tagging. Tokens which receive a noun or proper noun POS tag are extracted. The nouns found on WordNet are discarded and those which are not found in WordNet are looked up in the UMLS Metathesaurus. If a noun is found in UMLS, it is tagged with its corresponding semantic types as assigned by UMLS. A subset of these nouns is tagged as drug if their semantic types are Pharmacological Substance or Antibiotic. Finally, nouns which have not been found in UMLS are tagged as unknown. The list of nouns tagged as drug is passed to the rule module to detect their pharmacological families according to the affixes. In addition, the rule module processes the list of unknown nouns which are not found in UMLS to check the presence of affixes, and thereby, of possible drugs. "
W08-0619 " Clinicians write the reports in natural language which contains a large amount of informal medical term. Automating conversion of text into clinical terminologies allows reliable retrieval and analysis of the clinical notes. We have created an algorithm that maps medical expressions in clinical notes into a medical terminology. This algorithm indexes medical terms into an augmented lexicon. It performs lexical searches in text and finds the longest possible matches in the target terminology, SNOMED CT. The mapping system was run on a collection of 470,000 clinical notes from an Intensive Care Service (ICS). The evaluation on a small part of the corpus shows the precision is 70.4%. 1 Introductio  "
W08-0620 "<NoAbstract>"
W08-0621 "<NoAbstract>"
W08-0622 " This work proposes a case-based classifier to tackle the gene/protein mention problem in biomedical literature. The so called gene mention problem consists of the recognition of gene and protein entities in scientific texts. A classification process aiming at deciding if a term is a gene mention or not is carried out for each word in the text. It is based on the selection of the best or most similar case in a base of known and unknown cases. The approach was evaluated on several datasets for different organisms and results show the suitability of this approach for the gene mention problem. 1 Int  "
W08-0623 "<NoAbstract>"
W08-0624 "<NoAbstract>"
W08-0625 "<NoAbstract>"
W08-0626 "<NoAbstract>"
W08-0627 " While there are several corpora which claim to have annotations for protein references, the heterogeneity between the annotations is recognized as an obstacle to develop expensive resources in a synergistic way. Here we present a series of experimental results which show the differences of protein mention annotations made to two corpora, GENIA and AImed. 1 I  "
W08-1101 " We review psycholinguistic research on the use of intonation in dialogue, focusing on our own recent work. In experiments using complex real-world tasks and naive speakers and listeners, we show that speakers reliably specific prosodic cues to signal their intensions, and that listeners use these cues to recognize syntactic and pragmatic aspects of discourse meaning. 1 I  "
W08-1102 " It is an honor to have this chance to tie together themes from my recent research, and to sketch some challenges and opportunities for NLG in face-to-face conversational interaction. Communication reflects our general involvement in one anothers lives. Through the choices we manifest with one another, we share our thoughts and feelings, strengthen our relationships and further our joint projects. We rely not only on words to articulate our perspectives, but also on a heterogeneous array of accompanying efforts: embodied deixis, expressive movement, presentation of iconic imagery and instrumental action in the world. Words showcase the distinctive linguistic knowledge which human communication exploits. But peoples diverse choices in conversation in fact come together to reveal multifaceted, interrelated meanings, in which all our actions, verbal and nonverbal, fit the situation and further social purposes. In the best case, they let interlocutors understand not just each others words, but each other. As NLG researchers, I argue, we have good reason to work towards models of social cognition that embrace the breadth of conversation. Scientifically, it connects us to an emerging consensus in favor of a general human pragmatic competence, rooted in capacities for engagement, coordination, shared intentionality and extended relationships. Technically, it lets us position ourselves as part of an emerging revolution in integrative Artificial Intelligence, characterized by research challenges like humanrobot interaction and the design of virtual humans, and applications in assistive and educational technology and interactive entertainment. Researchers are already hard at work to place our accounts of embodied action in conversation in contact with pragmatic theories derived from text discourse and spoken dialogue. In my own experience, such work proves both illuminating and exciting. For example, it challenges us to support and refine theories of discourse coherence by accounting for the discourse relations and default inference that determine the joint interpretation of coverbal gesture and its accompanying speech (Lascarides and Stone, 2008). And it challenges us to show how speakers work across modalities to engage with, disambiguate, and (on acceptance) recapitulate each others communicative actions, to ground their meanings (Lascarides and Stone, In Preparation). The closer we look at conversation, the more we can fit all its behaviors into a unitary frameworkinviting us to implement behavioral control for embodied social agents through a pervasive analogy to NLG. We can already pursue such implementations easily. Computationally, motion is just sequence data, and we can manipulate it in parallel ways to the speech data we already use in spoken language generation (Stone et al., 2004). At a higher level, we can represent an embodied performance through a matrix of discrete actions selected and synchronized to an abstract time-line, as in our RUTH system (DeCarlo et al., 2004; Stone and Oh, 2008). This lets us use any NLG method that manipulates structured selections of discrete actions as an architecture for the production of embodied behavior. Templates, as in (Stone and DeCarlo, 2003; Stone et al., 2004), offer 5 a good illustration. Nevertheless, face-to-face dialogue does demand qualitatively new capabilities. In fact, peoples choices and meanings in interactive conversation are profoundly informed by their social settings. We are a long way from general models that could allow NLG systems to recognize and exploit these connections in the words and other behaviors they use. In my experience, even the simplest social practices, such as interlocutors cooperation on an ongoing practical task, require new models of linguistic meaning and discourse context. For example, systems must be creative to evoke the distinctions that matter for their ongoing task, and use meanings that are not programmed or learned but invented on the fly (DeVault and Stone, 2004). They must count on their interlocutors to recognize the background knowledge they presuppose by general inference from the logic of their behavior as a cooperative contribution to the task (Thomason et al., 2006). Such reasoning becomes particularly important in problematic cases, such as when systems must finetune the form and meaning of a clarification request so that the response is more likely to resolve a pending task ambiguity (DeVault and Stone, 2007). I expect many further exciting developments in our understanding of meaning and interpretation as we enrich the social intelligence of NLG. Modeling efforts will remain crucial to the exploration of these new capabilities. When we build and assemble models of actions and interpretations, we get systems that can plan their own behavior simply by exploiting what they know about communication. These systems give new evidence about the information and problem-solving thats involved. The challenge is that these models must describe semantics and pragmatics, as well as syntax and behavior. My own slow progress (Cassell et al., 2000; Stone et al., 2003; Koller and Stone, 2007) shows that theres still lots of hard work needed to develop suitable techniques. I keep going because of the methodological payoffs I see on the horizon. Modeling lets us take social intelligence seriously as a general implementation principle, and thus to aim for systems whose multimodal behavior matches the flexibility and coordination that distinguishes our own embodied meanings. More generally, modeling replaces programming with data fitting, and a good model of action and interpretation in particular would let an agents own experience in conversational interaction determine the repertoire of behaviors and meanings it uses to make itself understood. Acknowledgments To colleagues and coauthors, especially David DeVault and the organizers of INLG 2008, and to NSF IGERT 0549115, CCF 0541185 and HSD 0624191. "
W08-1103 " Information graphics, such as bar charts and line graphs, play an important role in multimodal documents. This paper presents a novel approach to producing a brief textual summary of a simple bar chart. It outlines our approach to augmenting the core message of the graphic to produce a brief summary. Our method simultaneously constructs both the discourse and sentence structures of the textual summary using a bottom-up approach. The result is then realized in natural language. An evaluation study validates our generation methodology. "
W08-1104 "Abstract Summarising georeferenced (can be identified according to its location) data in natural language is challenging because it requires linking events describing its nongeographic attributes to their underlying geography. This mapping is not straightforward as often the only explicit geographic information such data contains is latitude and longitude. In this paper we present an approach to generating textual summaries of georeferenced data based on spatial reference frames. This approach has been implemented in a data-to-text system we have deployed in the weather forecasting domain. "
W08-1105 " We present a novel unsupervised method for sentence compression which relies on a dependency tree representation and shortens sentences by removing subtrees. An automatic evaluation shows that our method obtains result comparable or superior to the state of the art. We demonstrate that the choice of the parser affects the performance of the system. We also apply the method to German and report the results of an evaluation with humans. 1 Intro  "
W08-1106 "ive Summarization of Evaluative Text: The Effect of Corpus Controversiality Giuseppe Carenini and Jackie Chi Kit Cheung 1 Department of Computer Science University of British Columbia Vancouver, B.C. V6T 1Z4, Canada {carenini,cckitpw}@cs.ubc.ca Abstract Extractive summarization is the strategy of concatenating extracts taken from a corpus into a summary, while abstractive summarization involves paraphrasing the corpus using novel sentences. We define a novel measure of corpus controversiality of opinions contained in evaluative text, and report the results of a user study comparing extractive and NLG-based abstractive summarization at different levels of controversiality. While the abstractive summarizer performs better overall, the results suggest that the margin by which abstraction outperforms extraction is greater when controversiality is high, providing a context in which the need for generationbased methods is especially great. 1 Introdu  "
W08-1107 "Abstract In this paper, we propose to reinterpret the problem of generating referring expressions (GRE) as the problem of computing a formula in a description logic that is only satisfied by the referent. This view offers a new unifying perspective under which existing GRE algorithms can be compared. We also show that by applying existing algorithms for computing simulation classes in description logic, we can obtain extremely efficient algorithms for relational referring expressions without any danger of running into infinite regress. "
W08-1108 "Abstract Referring expression generation has recently been the subject of the first Shared Task Challenge in NLG. In this paper, we analyse the systems that participated in the Challenge in terms of their algorithmic properties, comparing new techniques to classic ones, based on results from a new human task-performance experiment and from the intrinsic measures that were used in the Challenge. We also consider the relationship between different evaluation methods, showing that extrinsic taskperformance experiments and intrinsic evaluation methods yield results that are not significantly correlated. We argue that this highlights the importance of including extrinsic evaluation methods in comparative NLG evaluations. "
W08-1109 "Abstract There is a prevailing assumption in the literature on referring expression generation that relations are used in descriptions only as a last resort, typically on the basis that including the second entity in the relation introduces an additional cognitive load for either speaker or hearer. In this paper, we describe an experiemt that attempts to test this assumption; we determine that, even in simple scenes where the use of relations is not strictly required in order to identify an entity, relations are in fact often used. We draw some conclusions as to what this means for the development of algorithms for the generation of referring expressions. "
W08-1110 "Abstract This paper reports on attempts at Aberdeen 1 to measure the effects on readers emotions of positively and negatively slanted texts with the same basic message. The slanting methods could be implemented in an (NLG) system. We discuss a number of possible reasons why the studies were unable to show clear, statistically significant differences between the effects of the different texts. "
W08-1111 " We present a technique that opens up grammar-based generation to a wider range of practical applications by dramatically reducing the development costs and linguistic expertise that are required. Our method infers the grammatical resources needed for generation from a set of declarative examples that link surface expressions directly to the applications available semantic representations. The same examples further serve to optimize a run-time search strategy that generates the best output that can be found within an application-specific time frame. Our method offers substantially lower development costs than hand-crafted grammars for applicationspecific NLG, while maintaining high output quality and diversity. 1 Introduct  "
W08-1112 " We describe three PCFG-based models for Chinese sentence realisation from LexicalFunctional Grammar (LFG) f-structures. Both the lexicalised model and the history-based model improve on the accuracy of a simple wide-coverage PCFG model by adding lexical and contextual information to weaken inappropriate independence assumptions implicit in the PCFG models. In addition, we provide techniques for lexical smoothing and rule smoothing to increase the generation coverage. Trained on 15,663 automatically LFG fstructure annotated sentences of the Penn Chinese treebank and tested on 500 sentences randomly selected from the treebank test set, the lexicalised model achieves a BLEU score of 0.7265 at 100% coverage, while the historybased model achieves a BLEU score of 0.7245 also at 100% coverage. 1 Introduction  "
W08-1113 "Abstract When evaluating a generation system, if a corpus of target outputs is available, a common and simple strategy is to compare the system output against the corpus contents. However, cross-validation metrics that test whether the system makes exactly the same choices as the corpus on each item have recently been shown not to correlate well with human judgements of quality. An alternative evaluation strategy is to compute intrinsic, task-specific properties of the generated output; this requires more domain-specific metrics, but can often produce a better assessment of the output. In this paper, a range of metrics using both of these techniques are used to evaluate three methods for selecting the facial displays of an embodied conversational agent, and the predictions of the metrics are compared with human judgements of the same generated output. The corpus-reproduction metrics show no relationship with the human judgements, while the intrinsic metrics that capture the number and variety of facial displays show a significant correlation with the preferences of the human users. "
W08-1114 "Abstract To generate natural language feedback for an intelligent tutoring system, we developed a simple planning model with a distinguishing feature: its plan operators are derived automatically, on the basis of the association rules mined from our tutorial dialog corpus. Automatically mined rules are also used for realization. We evaluated 5 different versions of a system that tutors on an abstract sequence learning task. The version that uses our planning framework is significantly more effective than the other four versions. We compared this version to the human tutors we employed in our tutorial dialogs, with intriguing results. "
W08-1115 " In this paper we describe content determination issues involved in the Atlas.txt project, which aims to automatically describe georeferenced information such as census data as text for the visually-impaired (VI). Texts communicating geo-referenced census information contain census data abstractions and their corresponding geographic references. Because visually impaired users find interpreting geographic references hard, we hypothesized that an introduction message about the underlying geography should help the users to interpret the geographic references easily. We performed user studies to design and evaluate the introduction message. An initial evaluation study with several sighted users and one partially sighted user showed that an introduction message is certainly preferred by most participants. Many of them used an introduction message themselves when they described maps textually. But the study also showed that the introduction message made no difference when the participants were asked to draw maps using the information in the textual descriptions. "
W08-1116 " This paper explores how the main question addressed in Text Planning has evolved over the last twenty years. Earlier approaches to text planning asked the question: How do we write a good text?, and whatever answers were found were programmed directly into code. With the introduction of search-based text planning in recent years, the focus shifted to the evaluation function, and thus the question became: How do we tell if a text is good? This paper will explore these evolving questions, and subsequent refinements of them as the field matures. Int  "
W08-1117 " A dialogue system can present itself and/or address the user as an active agent by means of linguistic constructions in personal style, or suppress agentivity by using impersonal style. We describe how we generate and control personal and impersonal style variation in the output of SAMMIE, a multimodal in-car dialogue system for an MP3 player. We carried out an experiment to compare subjective evaluation judgments and input style alignment behavior of users interacting with versions of the system generating output in personal vs. impersonal style. Although our results are consistent with earlier findings obtained with simulated systems, the effects are weaker. 1 Introduct  "
W08-1118 "Abstract This paper describes an evaluation study of an ontology-driven WYSIWYM interface for metadata creation. Although the results are encouraging, they are not as positive as those of a similar tool developed for the medical domain. We believe this may be due, not to the WYSIWYM interface, but to the complexity of the underlying ontologies and the fact that subjects were unfamiliar with them. We discuss the ways in which ontology development might be influenced by issues stemming from using an NLG approach for user access to data, and the effect these factors have on general usability. "
W08-1119 "gh, UK m.a.van-der-meulen@ sms.ed.ac.uk Abstract The BABYTALK BT-45 system generates textual summaries of clinical data about babies in a neonatal intensive care unit. A recent taskbased evaluation of the system suggested that these summaries are useful, but not as effective as they could be. In this paper we present a qualitative analysis of problems that the evaluation highlighted in BT-45 texts. Many of these problems are due to the fact that BT45 does not generate good narrative texts; this is a topic which has not previously received much attention from the NLG research community, but seems to be quite important for creating good data-to-text systems. "
W08-1120 " Natural language generation technology is mature enough for implementing an NLG system in a commercial environment, but the circumstances differ significantly from building a research system. This paper describes the challenges and rewards of building a commercial NLG component for an electronic medical records system. While the resulting NLG system has been successfully completed, the path to that success could have been somewhat smoother knowing the issues in advance. 1 Intro  "
W08-1121 "ion in Referring Expression Generation and its Relation with the Construction of the Contrast Set Raquel Herv  as Facultad de Inform  atica Universidad Complutense de Madrid Madrid, Spai  Universidad Complutense de Madrid Madrid, Spai n raquelhb@fdi.ucm.es Pablo Gerv  as Facultad de Inform  atica Universidad Complutense de Madrid Madrid, Spain pgervas@sip.ucm.es Referring Expression Generation (REG) is the task that deals with references to entities appearing in a spoken or written discourse. If these referents are organized in terms of a taxonomy, there are two problems when establishing a reference that would distinguish an intended referent from its possible distractors. The first one is the choice of the set of possible distractrors or contrast set in the given situation. The second is to identify at what level of the taxonomy to phrase the reference so that it unambiguously picks out only the intended referent, leaving all possible distractors in different branches of the taxonomy. We discuss the use of ontologies to deal with the REG task, paying special attention to the choice of the the contrast set and to the use of the information of the ontology to select the most appropriate type to be used for the referent. 1 Introduction Referring Expression Generation (REG) is the task that deals with references of entities appearing in a discourse. In a context where possible referents are organized in terms of a taxonomy (or subsumption hierarchy) and may additionally be differentiated by their attributes, there are two possible ways of establishing a reference that will distinguish an intended referent from its possible distractors. One is to identify at what level of the taxonomy to phrase the reference so that it unambiguously picks out only the intended referent, leaving all possible distractors in different branches of the taxonomy. Another, applied once a particular level of reference has been chosen, is to resort to mentioning additional attributes of the intended referents that distinguish it from any remaining distractors that share the same branch of the taxonomy. While the second task has been addressed often in existing literature, the first one is often glossed over by requiring that the levels to be used for each element come specified in the input. However, if this task is to be considered as a specific problem to be solved computationally, it opens up an additional problem. If the elements in the  "
W08-1122 "nd {dhogan, jfoster, jwagner, josef}@computing.dcu.ie Abstract While the effect of domain variation on Penntreebank-trained probabilistic parsers has been investigated in previous work, we study its effect on a Penn-Treebank-trained probabilistic generator. We show that applying the generator to data from the British National Corpus results in a performance drop (from a BLEU score of 0.66 on the standard WSJ test set to a BLEU score of 0.54 on our BNC test set). We develop a generator retraining method where the domain-specific training data is automatically produced using state-of-the-art parser output. The retraining method recovers a substantial portion of the performance drop, resulting in a generator which achieves a BLEU score of 0.61 on our BNC test data. "
W08-1123 "Abstract We describe the creation of a new domain for the Methodius Natural Language Generation System, and an evaluation of Methodius parameterized comparison generation algorithm. The new domain was based around music and performers, and texts about the domain were generated using Methodius. Our evaluation showed that test subjects learned more from texts that contained comparisons than from those that did not. We also established that the comparison generation algorithm could generalize to the music domain. "
W08-1124 "Abstract This paper presents a reordering algorithm for generating multiple stories from different perspectives based on a single baseball game. We take a description of a game and a neutral summary, reorder the content of the neutral summary based on event features, and produce two summaries that the users rated as showing perspectives of each of the two teams. We describe the results from an initial user survey that revealed the power of reordering on the users perception of perspective. Then we describe our reordering algorithm which was derived from analyzing the corpus of local newspaper articles of teams involved in the games as well as a neutral corpus for the respective games. The resulting reordering algorithm is successful at turning a neutral article into two different summary articles that express the two teams perspectives. "
W08-1125 "<NoAbstract>"
W08-1126 "<NoAbstract>"
W08-1127 "<NoAbstract>"
W08-1128 " The GREC task of the Referring Expression Generation Challenge 2008 is to select appropriate references to the main subject in given texts. This means to select the correct type of the referring expressions such as name, pronoun, common, or elision (empty). We employ for the selection different learning techniques with the aim to find the most appropriate one for the task and the used attributes. As training data, we use the syntactic category of the searched referring expressions and additionally gathered data from the text itself. 1 Intro  "
W08-1129 " In this paper we describe our machine learning approach to the generation of referring expressions. As our algorithm we use memory-based learning. Our results show that in case of predicting the TYPE of the expression, having one general classifier gives the best results. On the contrary, when predicting the full set of properties of an expression, a combined set of specialized classifiers for each subdomain gives the best performance. 1 Intro  "
W08-1130 "Abstract Selection of natural-sounding referring expressions is useful in text generation and information summarization (Kan et al., 2001). We use discourse-level feature predicates in a maximum entropy classifier (Berger et al., 1996) with binary and n-class classification to select referring expressions from a list. We find that while mention-type n-class classification produces higher accuracy of type, binary classification of individual referring expressions helps to avoid use of awkward referring expressions. "
W08-1131 "UK {asb, eykk10}@brighton.ac.uk Abstract The TUNA Challenge was a set of three shared tasks at REG08, all of which used data from the TUNA Corpus. The three tasks covered attribute selection for referring expressions (TUNA-AS), realisation (TUNA-R) and end-toend referring expression generation (TUNAREG). 8 teams submitted a total of 33 systems to the three tasks, with an additional submission to the Open Track. The evaluation used a range of automatically computed measures. In addition, an evaluation experiment was carried out using the peer outputs for the TUNAREG task. This report describes each task and the evaluation methods used, and presents the evaluation results. 1 Introduction The TUNA Challenge 2008 built on the foundations laid in the ASGRE 2007 Challenge (Belz and Gatt, 2007), which consisted of a single shared task, based on a subset of the TUNA Corpus (Gatt et al., 2007). The TUNA Corpus is a collection of human-authored descriptions of a referent, paired with a representation of the domain in which that description was elicited. The 2008 Challenge expanded the scope of the previous edition in a variety of ways. This year, there were three shared tasks. TUNA-AS is the Attribute Selection task piloted in the 2007 ASGRE Challenge, which involves the selection of a set of attributes which are true of a target referent, and help to distinguish it from its distractors in a domain. TUNA-R is a realisation task, involving the mapping from attribute sets to linguistic descriptions. TUNA-REG is an end to end referring expression generation task, involving a mapping from an input domain to a linguistic description of a target referent. In addition, there was an Open Submission Track, where participants were invited to submit a report on any interesting research that involved the shared task data, and an Evaluation Track, for which submissions were invited on proposals for evaluation methods. This years TUNA Challenge also expanded considerably on the evaluation methods used in the various tasks. The measures can be divided into intrinsic, automatically computed methods, and extrinsic measures obtained through a task-oriented experiment involving human participants. The training and development data for the Challenge included the full dataset used in the ASGRE Challenge, that is, all of the 2007 training, development and test data. For the 2008 edition, two new test sets were constructed. Test Set 1 was used for TUNA-R, Test Set 2 was used for both TUNA-AS and TUNA-REG. "
W08-1132 " The algorithm IS-FP takes up the idea from the IS-FBN algorithm developed for the shared task 2007. Both algorithms learn the individual attribute selection style for each human that provided referring expressions to the corpus. The IS-FP algorithm was developed with two additional goals (1) to improve the indentification time that was poor for the FBN algorithm and (2) to push the dice score even higher. In order to generate a word string for the selected attributes, we build based on individual preferences a surface syntactic dependency tree as input. We derive the individual preferences from the training set. Finally, a graph transducer maps the input strucutre to a deep morphologic structure. 1 IS-  "
W08-1133 "AT&T Labs Research, Inc. 180 Park Avenue Florham Park, NJ 07932, USA {pino,stent,srini}@research.att.com Abstract In the first REG competition, researchers proposed several general-purpose algorithms for attribute selection for referring expression generation. However, most of this work did not take into account: a) stylistic differences between speakers; or b) trainable surface realization approaches that combine semantic and word order information. In this paper we describe and evaluate several end-to-end referring expression generation algorithms that take into consideration speaker style and use data-driven surface realization techniques. "
W08-1134 "<NoAbstract>"
W08-1135 " Both greedy and domain-oriented REG algorithms have significant strengths but tend to perform poorly according to humanlikeness criteria as measured by, e.g., Dice scores. In this work we describe an attempt to combine both perspectives into a single attribute selection strategy to be used as part of the Dale & Reiter Incremental algorithm in the REG Challenge 2008, and the results in both Furniture and People domains. 1 Int  "
W08-1136 "<NoAbstract>"
W08-1137 "Abstract This systems approach to the attribute selection task was to use a genetic programming algorithm to search for a solution to the task. The evolved programs for the furniture and people domain exhibit quite naive behavior, and the DICE and MASI scores on the training sets reflect the poor humanlikeness of the programs. "
W08-1138 "Abstract We describe a graph-based generation system that participated in the TUNA attribute selection and realisation task of the REG 2008 Challenge. Using a stochastic cost function (with certain properties for free), and trying attributes from cheapest to more expensive, the system achieves overall .76 DICE and .54 MASI scores for attribute selection on the development set. For realisation, it turns out that in some cases higher attribute selection accuracy leads to larger differences between system-generated and human descriptions. "
W08-1139 " This paper presents a Prefix Tree based model of Generation of Referring Expression (RE). Our algorithm PTBSGRE works in two phases. First, an encoded prefix tree is constructed describing the domain structure. Subsequently, RE is generated using that structure. We evaluated our system using Dice, MASI, Accuracy, Minimality and Uniqueness scoring method using standard TEVAl tool and the result is encouraging. 1 Intro  "
W08-1140 " This document describes the development of a surface realisation component for the Portuguese language that takes advantage of the data and evaluation tools provided by the REG-2008 team. At this initial stage, our work uses simple n-gram statistics to produce descriptions in the Furniture domain, with little or no linguistic variation. Preliminary results suggest that, unlike the generation of English descriptions, contextual information may be required to account for Portuguese word order. 1 Int  "
W08-1501 "Abstract The concept classifier has been used as a translation unit in speech-to-speech translation systems. However, the sparsity of the training data is the bottle neck of its effectiveness. Here, a new method based on using a statistical machine translation system has been introduced to mitigate the effects of data sparsity for training classifiers. Also, the effects of the background model which is necessary to compensate the above problem, is investigated. Experimental evaluation in the context of crosslingual doctor-patient interaction application show the superiority of the proposed method. "
W08-1502 "Abstract Grammatical Framework (GF) is a grammar formalism which supports interlinguabased translation, library-based grammar engineering, and compilation to speech recognition grammars. We show how these features can be used in the construction of portable high-precision domain-specific speech translators. "
W08-1503 " This paper proposes a novel integrated dialog simulation technique for evaluating spoken dialog systems. Many techniques for simulating users and errors have been proposed for use in improving and evaluating spoken dialog systems, but most of them are not easily applied to various dialog systems or domains because some are limited to specific domains or others require heuristic rules. In this paper, we propose a highly-portable technique for simulating user intention, utterance and Automatic Speech Recognition (ASR) channels. This technique can be used to rapidly build a dialog simulation system for evaluating spoken dialog systems. We propose a novel user intention modeling and generating method that uses a linear-chain conditional random field, a data-driven domain specific user utterance simulation method, and a novel ASR channel simulation method with adjustable error recognition rates. Experiments using these techniques were carried out to evaluate the performance and behavior of previously developed dialog systems designed for navigation dialogs, and it turned out that our approach is easy to set up and shows the similar tendencies of real users. "
W08-1504 " Voice over IP and the open source technologies are becoming popular choices for organizations. However, while accessing the VoiceXML gateways these systems fail to attract the global users economically. The objective of this paper is to demonstrate how an existing web application can be modified using VoiceXML to enable non-visual access from any phone. Moreover, we unleash a way for linking an existing PSTN-based phone line to a VoiceXML gateway even though the voice service provider (VSP) does not provide a local geographical number to global customers to access the application. In addition, we introduce an economical way for small sized businesses to overcome the high cost of setting up and using a commercial VoiceXML gateway. The method is based on Asterisk server. In order to elucidate the entire process, we present a sample Package Tracking System application, which is based on an existing website and provides the same functionality as the website does. We also present an online demonstration, which provides global access to commercial voice platforms (i.e. Voxeo, Tellme Studio, Bevocal and DemandVoice). This paper also discusses various scenarios in which spoken interaction can play a significant role.  "
W08-1505 " As spoken dialogue systems move beyond task oriented dialogues and become distributed in the pervasive computing environments, their growing complexity calls for more modular structures. When different aspects of a single system can be accessed with different interfaces, knowledge representation and separation of low level interaction modeling from high level reasoning on domain level becomes important. In this paper, a model utilizing a dialogue plan to communicate information from domain level planner to dialogue management and from there to a separate mobile interface is presented. The model enables each part of the system handle the same information from their own perspectives without containing overlapping logic. 1 Intro  "
W08-1506 " MedSLT is a grammar-based medical speech translation system intended for use in doctor-patient diagnosis dialogues, which provides coverage of several different subdomains and multiple language pairs. Vocabulary ranges from about 350 to 1000 surface words, depending on the language and subdomain. We will demo three different versions of the system: an anyto-any multilingual version involving the languages Japanese, English, French and Arabic, a bidirectional English  Spanish version, and a mobile version running on a hand-held PDA. We will also demo the Regulus development environment, focussing on features which support rapid prototyping of grammar-based speech translation systems. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license http://creativecommons.org/licenses/by-nc-sa/3.0/ ). Some rights reserved. 1 Introduction  "
W08-1507 "This paper discusses language understanding in the Maryland Virtual Patient environment. Language understanding is just one of many cognitive functions of the virtual patients in MVP, others including decision making about healthcare and lifestyle, and the experiencing and remembering of interoceptive events. "
W08-1508 " Spoken Language Translation systems have usually been produced for such specific domains as health care or military use. Ideally, such systems would be easily portable to other domains in which translation is mission critical, such as emergency response or law enforcement. However, porting has in practice proven difficult. This paper will comment on the sources of this difficulty and briefly present an approach to rapid inter-domain portability. Three aspects will be discussed: (1) large general-purpose lexicons for automatic speech recognition and machine translation, made reliable and usable through interactive facilities for monitoring and correcting errors; (2) easily modifiable facilities for instant translation of frequent phrases; and (3) quickly modifiable custom glossaries. As support for our approach, we apply our current SLT system, now optimized for the health care domain, to sample utterances from the military, emergency service, and law enforcement domains, with discussion of numerous specific sentences.  "
W08-1509 " We describe Ayudame, a system designed to recognize and translate Spanish emergency calls for better dispatching. We analyze the research challenges in adapting speech translation technology to 9-1-1 domain. We report our initial research in 9-1-1 translation system design, ASR experiments, and utterance classification for translation. 1 Int  "
W08-1510 " S-MINDS is a speech translation system, which allows an English speaker to communicate with a limited English proficiency speaker easily within a question-and-answer, interview-style format. It can handle dialogs in specific settings such as nurse-patient interaction, or medical triage. We have built and tested an English-Spanish system for enabling nurse-patient interaction in a number of domains in Kaiser Permanente achieving a total translation accuracy of 92.8% (for both English and Spanish). We will give an overview of the system as well as the quantitative and qualitatively system performance. 1 Introdu  "
W08-1901 " While large-scale corpora and various corpus query tools have long been recognized as essential language resources, the value of word association norms as language resources has been largely overlooked. This paper conducts some initial comparisons of the lexical relationships observed within Japanese collocation data extracted from a large corpus using the Japanese language version of the Sketch Engine (SkE) tool (Srdanovic et al., 2008) and the relationships found within Japanese word association sets taken from the large-scale Japanese Word Association Database (JWAD) under ongoing construction by Joyce (2005, 2007). The comparison results indicate that while some relationships are common to both linguistic resources, many lexical relationships are only observed in one resource. These findings suggest that both resources are necessary in order to more adequately cover the diverse range of lexical relationships. Finally, the paper reflects briefly on the implementation of association-based word-search strategies into electronic dictionaries proposed by Zock and Bilac (2004) and Zock (2006). 1 Introductio  "
W08-1902 "Abstract Words play a major role in language production, hence finding them is of vital importance, be it for speaking or writing. Words are stored in a dictionary, and the general belief holds, the bigger the better. Yet, to be truly useful the resource should contain not only many entries and a lot of information concerning each one of them, but also adequate means to reveal the stored information. Information access depends crucially on the organization of the data (words) and on the navigational tools. It also depends on the grouping, ranking and indexing of the data, a factor too often overlooked. We will present here some preliminary results, showing how an existing electronic dictionary could be enhanced to support language producers to find the word they are looking for. To this end we have started to build a corpus-based association matrix, composed of target words and access keys (meaning elements, related concepts/words), the two being connected at their intersection in terms of weight and type of link, information used subsequently for grouping, ranking and navigation. "
W08-1903 " This paper describes the functional design of an interface for an online scholarly dictionary of contemporary standard Dutch, the ANW. One of the main innovations of the ANW is a twofold meaning description: definitions are accompanied by semagrams. In this paper we focus on the strategies that are available for accessing information in the dictionary and the role semagrams play in the dictionary practice. 1 Intro  "
W08-1904 " ProPOSEL is a prosody and PoS English lexicon, purpose-built to integrate and leverage domain knowledge from several well-established lexical resources for machine learning and NLP applications. The lexicon of 104049 separate entries is in accessible text file format, is human and machine-readable, and is intended for open source distribution with the Natural Language ToolKit. It is therefore supported by Python software tools which transform ProPOSEL into a Python dictionary or associative array of linguistic concepts mapped to compound lookup keys. Users can also conduct searches on a subset of the lexicon and access entries by word class, phonetic transcription, syllable count and lexical stress pattern. ProPOSEL caters for a range of different cognitive aspects of the lexicon  . 1 Introd  "
W08-1905 " When consulting a dictionary, people can find the meaning of a word via the definition, which usually contains the relevant information to fulfil their requirement. Lexicographers produce dictionaries and their work consists in presenting information essential for grasping the meaning of words. However, when people need to find a word it is likely that they do not obtain the information they are looking for. There is a gap between dictionary definitions and the information being available in peoples mind. This paper attempts to present the conceptualisation people engage in, in order to arrive at a word from its meaning. The insights of an experiment conducted show us the differences between the knowledge available in peoples minds and in dictionary definitions. 1 Introdu  "
W08-1906 " This paper is a project report of the lexicographic Internet portal OWID, an Online Vocabulary Information System of German which is being built at the Institute of German Language in Mannheim (IDS). Overall, the contents of the portal and its technical approaches will be presented. The lexical database is structured in a granular way which allows to extend possible search options for lexicographers. Against the background of current research on using electronic dictionaries, the project OWID is also working on first ideas of useradapted access and user-adapted views of the lexicographic data. Due to the fact that the portal OWID comprises dictionaries which are available online it is possible to change the design and functions of the website easily (in comparison to printed dictionaries). Ideas of implementing user-adapted views of the lexicographic data will be demonstrated by using an example taken from one of the dictionaries of the portal, namely elexiko. "
W08-1907 " In this paper we propose a model for conceptual access to multilingual lexicon based on shared orthography. Our proposal relies crucially on two facts: That both Chinese and Japanese conventionally use Chinese orthography in their respective writing systems, and that the Chinese orthography is anchored on a system of radical parts which encodes basic concepts. Each orthographic unit, called hanzi and kanji respectively, contains a radical which indicates the broad semantic class of the meaning of that unit. Our study utilizes the homomorphism between the Chinese hanzi and Japanese kanji systems to ide 1 ntify bilingual word correspondences. We use bilingual dictionaries, including WordNet, to verify semantic relation between the crosslingual pairs. These bilingual pairs are then mapped to an ontology constructed based on relations to the relation between the meaning of each character and the  2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. basic concept of their radical parts. The conceptual structure of the radical ontology is proposed as a model for simultaneous conceptual access to both languages. A study based on words containing characters composed of the (mouth) radical is given to illustrate the proposal and the actual model. The fact that this model works for two typologically very different languages and that the model contains generative lexicon like coersive links suggests that this model has the conceptual robustness to be applied to other languages. 1 Motivation  "
W08-1908 " This paper aims to introduce a new parsing strategy for large dictionary (thesauri) parsing, called Dictionary Sense Segmentation & Dependency (DSSD), devoted to obtain the sense tree, i.e. the hierarchy of the defined meanings, for a dictionary entry. The real novelty of the proposed approach is that, contrary to dictionary standard parsing, DSSD looks for and succeeds to separate the two essential processes within a dictionary entry parsing: sense tree construction and sense definition parsing. The key tools to accomplish the task of (autonomous) sense tree building consist in defining the dictionary sense marker classes, establishing a tree-like hierarchy of these classes, and using a proper searching procedure of sense markers within the DSSD parsing algorithm. A similar but more general approach, using the same techniques and data structures for (Romanian) free text parsing is SCD (Segmentation-CohesionDependency) (Curteanu; 1988, 2006), which DSSD is inspired from. A DSSDbased parser is implemented in Java, building currently 91% correct sense trees from DTLR (Dictionarul Tezaur al  2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. Limbii Romane  Romanian Language Thesaurus) entries, with significant resources to improve and enlarge the DTLR lexical semantics analysis. "
W08-1909 " ETAP-3 is a system of machine translation consisting of various types of rules and dictionaries. Those dictionaries, being created especially for NLP system, provide for every lexeme not only data about its characteristics as a separate item, but also different types of information about its syntactic and semantic links to other lexemes. The paper shows how the information about certain types of semantic links between lexemes represented in the dictionaries can be used in a machine translation system. The paper deals with correspondences between lexicalfunctional constructions of different types in the Russian and the English languages. Lexical-functional construction is a word-combination consisting of an argument of a lexical function and a value of this lexical function for this argument. The paper describes the cases when a lexical functional construction in one of these languages corresponds to a lexicalfunctional construction in the other language, but lexical functions represented by these two constructions are different. The paper lists different types of correspondences and gives the reasons for their existence. It also shows how the information about these correspondences can be used to improve the work of the linguistic component of the machine translation system ETAP-3.  2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. 1 In  "
W08-1910 " In this paper we aim to detect some aspects of adjectival meanings. Concepts of adjectives are distributed by SOM (SelfOrganizing map) whose feature vectors are calculated by MI (Mutual Information). For the SOM obtained, we make tight clusters from map nodes, calculated by cosine. In addition, the number of tight clusters obtained by cosine was increased using map nodes and Japanese thesaurus. As a result, the number of extended clusters of concepts was 149 clusters. From the map, we found 8 adjectival clusters in super-ordinate level and some tendencies of similar and dissimilar clusters. 1 Introductio  "
W08-1911 " Rephrasing text spans is a common task when revising a text. However, traditional dictionaries often cannot provide direct assistance to writers in performing this task. In this article, we describe an approach to obtain a monolingual phrase lexicon using techniques used in Statistical Machine Translation. A part to be rephrased is first translated into a pivot language, and then translated back into the original language. Models for assessing fluency, meaning preservation and lexical divergence are used to rank possible rephrasings, and their relative weight can be tuned by the user so as to better address her needs. An evaluation shows that these models can be used successfully to select rephrasings that are likely to be useful to a writer. 1 Introduct  "
W08-1912 "e, CNRS {gaume,duvignau,prevot,desalle}@univ-tlse2.fr Abstract We compare a psycholinguistic approach of mental lexicon organization with a computational approach of implicit lexical organization as found in dictionaries. In this work, we associate dictionaries with small world graphs. This multidisciplinary approach aims at showing that implicit structure of dictionaries, mathematically identified, fits the way young children categorize. These dictionary graphs might therefore be considered as cognitive artifacts. This shows the importance of semantic proximity both in cognitive and computational organization of verbs lexicon. "
W08-1913 " Providing sets of semantically related words in the lexical entries of an electronic dictionary should help language learners quickly understand the meaning of the target words. Relational information might also improve memorisation, by allowing the generation of structured vocabulary study lists. However, an open issue is which semantic relations are cognitively most salient, and should therefore be used for dictionary construction. In this paper, we present a concept description elicitation experiment conducted with German and Italian speakers. The analysis of the experimental data suggests that there is a small set of concept-classdependent relation types that are stable across languages and robust enough to allow discrimination across broad concept domains. Our further research will focus on harvesting instantiations of these classes from corpora. 1 Intro  "
W09-0401 "University of Edinburgh j schroeder ed ac uk Abstract This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness. "
W09-0402 "Abstract We explored novel automatic evaluation measures for machine translation output oriented to the syntactic structure of the sentence: the BLEU score on the detailed Part-of-Speech (POS) tags as well as the precision, recall and F-measure obtained on POS n-grams. We also introduced Fmeasure based on both word and POS ngrams. Correlations between the new metrics and human judgments were calculated on the data of the first, second and third shared task of the Statistical Machine Translation Workshop. Machine translation outputs in four different European languages were taken into account: English, Spanish, French and German. The results show that the new measures correlate very well with the human judgements and that they are competitive with the widely used BLEU, METEOR and TER metrics. "
W09-0403 "Charles University Prague, Czech Republic Abstract This paper describes a simple evaluation metric for MT which attempts to overcome the well-known deficits of the standard BLEU metric from a slightly different angle. It employes Levenshteins edit distance for establishing alignment between the MT output and the reference translation in order to reflect the morphological properties of highly inflected languages. It also incorporates a very simple measure expressing the differences in the word order. The paper also includes evaluation on the data from the previous SMT workshop for several language pairs. "
W09-0404 "versity {pado,mgalley,jurafsky,manning}@stanford.edu Abstract We present two regression models for the prediction of pairwise preference judgments among MT hypotheses. Both models are based on feature sets that are motivated by textual entailment and incorporate lexical similarity as well as local syntactic features and specific semantic phenomena. One model predicts absolute scores; the other one direct pairwise judgments. We find that both models are competitive with regression models built over the scores of established MT evaluation metrics. Further data analysis clarifies the complementary behavior of the two feature sets. "
W09-0405 "1 , Christian Federmann 2 , Hans Uszkoreit 1,2 1: Universit  at des Saarlandes, Saarbr  ucken, Germany 2: Deutsches Forschungszentrum f  ur K  unstliche Intelligenz GmbH, Saarbr  ucken, Germany {yuchen,micha,yzhang,sabineh,sith}@coli.uni-saarland.de {eisele,cfedermann,uszkoreit}@dfki.de Abstract We present a simple method for generating translations with the Moses toolkit (Koehn et al., 2007) from existing hypotheses produced by other translation engines. As the structures underlying these translation engines are not known, an evaluationbased strategy is applied to select systems for combination. The experiments show promising improvements in terms of BLEU. "
W09-0406 "Abstract This paper describes the CMU entry for the system combination shared task at WMT09. Our combination method is hypothesis selection, which uses information from n-best lists from several MT systems. The sentence level features are independent from the MT systems involved. To compensate for various n-best list sizes in the workshop shared task including firstbest-only entries, we normalize one of our high-impact features for varying sub-list size. We combined restricted data track entries in French English, German English and Hungarian English using provided data only. "
W09-0407 "Germany Abstract RWTH participated in the System Combination task of the Fourth Workshop on Statistical Machine Translation (WMT 2009). Hypotheses from 9 GermanEnglish MT systems were combined into a consensus translation. This consensus translation scored 2.1% better in BLEU and 2.3% better in TER (abs.) than the best single system. In addition, cross-lingual output from 10 French, German, and SpanishEnglish systems was combined into a consensus translation, which gave an improvement of 2.0% in BLEU/3.5% in TER (abs.) over the best single system. "
W09-0408 " We describe a synthetic method for combining machine translations produced by different systems given the same input. One-best outputs are explicitly aligned to remove duplicate words. Hypotheses follow system outputs in sentence order, switching between systems mid-sentence to produce a combined output. Experiments with the WMT 2009 tuning data showed improvement of 2 BLEU and 1 METEOR point over the best HungarianEnglish system. Constrained to data provided by the contest, our system was submitted to the WMT 2009 shared system combination task. 1 Introdu  "
W09-0409 " This paper describes the incremental hypothesis alignment algorithm used in the BBN submissions to the WMT09 system combination task. The alignment algorithm used a sentence specific alignment order, flexible matching, and new shift heuristics. These refinements yield more compact confusion networks compared to using the pair-wise or incremental TER alignment algorithms. This should reduce the number of spurious insertions in the system combination output and the system combination weight tuning converges faster. System combination experiments on the WMT09 test sets from five source languages to English are presented. The best BLEU scores were achieved by combing the English outputs of three systems from all five source languages. 1 Intro  "
W09-0410 "Germany Abstract RWTH participated in the shared translation task of the Fourth Workshop of Statistical Machine Translation (WMT 2009) with the German-English, French-English and Spanish-English pair in each translation direction. The submissions were generated using a phrase-based and a hierarchical statistical machine translation systems with appropriate morpho-syntactic enhancements. POS-based reorderings of the source language for the phrase-based systems and splitting of German compounds for both systems were applied. For some tasks, a system combination was used to generate a final hypothesis. An additional English hypothesis was produced by combining all three final systems for translation into English. "
W09-0411 " We present a word substitution approach to combine the output of different machine translation systems. Using part of speech information, candidate words are determined among possible translation options, which in turn are estimated through a precomputed word alignment. Automatic substitution is guided by several decision factors, including part of speech, local context, and language model probabilities. The combination of these factors is defined after careful manual analysis of their respective impact. The approach is tested for the language pair GermanEnglish, however the general technique itself is language independent. 1 Introdu  "
W09-0412 " We describe the system developed by the team of the National University of Singapore for English to Spanish machine translation of News Commentary text for the WMT09 Shared Translation Task. Our approach is based on domain adaptation, combining a small in-domain News Commentary bi-text and a large out-of-domain one from the Europarl corpus, from which we built and combined two separate phrase tables. We further combined two language models (in-domain and out-of-domain), and we experimented with cognates, improved tokenization and recasing, achieving the highest lowercased NIST score of 6.963 and the second best lowercased Bleu score of 24.91% for training without using additional external data for English-toSpanish translation at the shared task. 1 Introductio  "
W09-0413 " In this paper we describe the statistical machine translation system of the Universit  at Karlsruhe developed for the translation task of the Fourth Workshop on Statistical Machine Translation. The state-ofthe-art phrase-based SMT system is augmented with alternative word reordering and alignment mechanisms as well as optional phrase table modifications. We participate in the constrained condition of German-English and English-German as well as in the constrained condition of French-English and English-French. 1 Introduction This paper describes the statistical MT system used for our participation in the WMT09 Shared Translation Task and the particular language-pairdependent variations of the system. We use standard alignment and training tools and a phrasebased SMT decoder for creating state-of-the-art MT systems for our contribution in the translation directions English-German, German-English, English-French and French-English. Depending on the language pair, the baseline system is augmented with part-of-speech (POS)based short-range and long-range word reordering models, discriminative word alignment (DWA) and several modifications of the phrase table. Experiments with different system variants were conducted including some of those additional system components. Significantly better translation results could be achieved compared to the baseline results. An overview of the system will follow in Section 2, which describes the baseline architecture, followed by descriptions of the additional system components. Translation results for the different languages and system variants are presented in Section 5. "
W09-0414 " This study presents the TALP-UPC submission to the EACL Fourth Worskhop on Statistical Machine Translation 2009 evaluation campaign. It outlines the architecture and configuration of the 2009 phrase-based statistical machine translation (SMT) system, putting emphasis on the major novelty of this year: combination of SMT systems implementing different word reordering algorithms. Traditionally, we have concentrated on the Spanish-to-English and English-toSpanish News Commentary translation tasks. 1 Introduction TALP-UPC (Center of Speech and Language Applications and Technology at the Universitat Politecnica de Catalunya) is a permanent participant of the ACL WMT shared translations tasks, traditionally concentrating on the Spanishto-English and vice versa language pairs. In this paper, we describe the 2009 systems architecture and design describing individual components and distinguishing features of our model. This years system stands aside from the previous years configurations which were performed following an N -gram-based (tuple-based) approach to SMT. By contrast to them, this year we investigate the translation models (TMs) interpolation for a state-of-the-art phrase-based translation system. Inspired by the work presented in (Schwenk and Esteve, 2008), we attack this challenge using the coefficients obtained for the corresponding monolingual language models (LMs) for TMs interpolation. On the second step, we have performed additional word reordering experiments, comparing the results obtained with a statistical method (R. Costa-jussa and R. Fonollosa, 2009) and syntax-based algorithm (Khalilov and R. Fonollosa, 2008). Further the outputs of the systems were combined selecting the translation with the Minimum Bayes Risk (MBR) algorithm (Kumar, 2004) that allowed significantly outperforming the baseline configuration. The remainder of this paper is organized as follows: Section 2 presents the TALP-UPC09 phrase-based system, along with the translation models interpolation procedure and other minor novelties of this year. Section 3 reports on the experimental setups and outlines the results of the participation in the EACL WMT 2009 evaluation campaign. Section 4 concludes the paper with discussions. "
W09-0415 " This paper describes the MulTra project, aiming at the development of an efficient multilingual translation technology based on an abstract and generic linguistic model as well as on object-oriented software design. In particular, we will address the issue of the rapid growth both of the transfer modules and of the bilingual databases. For the latter, we will show that a significant part of bilingual lexical databases can be derived automatically through transitivity, with corpus validation. 1 Introdu  "
W09-0416 "Centre for Next Generation Localisation Dublin City University Dublin 9, Ireland {jdu,yhe,spenkale,away}@computing.dcu.ie Abstract In this paper, we describe the machine translation system in the evaluation campaign of the Fourth Workshop on Statistical Machine Translation at EACL 2009. We describe the modular design of our multi-engine MT system with particular focus on the components used in this participation. We participated in the translation task for the following translation directions: FrenchEnglish and EnglishFrench, in which we employed our multi-engine architecture to translate. We also participated in the system combination task which was carried out by the MBR decoder and Confusion Network decoder. We report results on the provided development and test sets. "
W09-0417 "Abstract This paper describes our Statistical Machine Translation systems for the WMT09 (en:fr) shared task. For this evaluation, we have developed four systems, using two different MT Toolkits: our primary submission, in both directions, is based on Moses, boosted with contextual information on phrases, and is contrasted with a conventional Moses-based system. Additional contrasts are based on the Ncode toolkit, one of which uses (part of) the English/French GigaWord parallel corpus. "
W09-0418 " This paper describes the NICT statistical machine translation (SMT) system used for the WMT 2009 Shared Task (WMT09) evaluation. We participated in the Spanish-English translation task. The focus of this years participation was to investigate model adaptation and transliteration techniques in order to improve the translation quality of the baseline phrasebased SMT system. 1 Intro  "
W09-0419 "Kingdom Abstract We describe here the two Systran/University of Edinburgh submissions for WMT2009. They involve a statistical post-editing model with a particular handling of named entities (English to French and German to English) and the extraction of phrasal rules (English to French). "
W09-0420 " We describe two shared task systems and associated experiments. The German to English system used reordering rules applied to parses and morphological splitting and stemming. The English to German system used an additional translation step which recreated compound words and generated morphological inflection. 1 Int  "
W09-0421 "Sweden {marho,sarst,jodfo,lah}@ida.liu.se Abstract We describe the LIU systems for EnglishGerman and German-English translation in the WMT09 shared task. We focus on two methods to improve the word alignment: (i) by applying Giza++ in a second phase to a reordered training corpus, where reordering is based on the alignments from the first phase, and (ii) by adding lexical data obtained as highprecision alignments from a different word aligner. These methods were studied in the context of a system that uses compound processing, a morphological sequence model for German, and a partof-speech sequence model for English. Both methods gave some improvements to translation quality as measured by Bleu and Meteor scores, though not consistently. All systems used both out-ofdomain and in-domain data as the mixed corpus had better scores in the baseline configuration. "
W09-0422 " We describe two systems for English-toCzech machine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach. 1 I  "
W09-0423 " This paper describes the development of several machine translation systems for the 2009 WMT shared task evaluation. We only consider the translation between French and English. We describe a statistical system based on the Moses decoder and a statistical post-editing system using SYSTRANs rule-based system. We also investigated techniques to automatically extract additional bilingual texts from comparable corpora. 1 Intro  "
W09-0424 " We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task. 1 Introductio  "
W09-0425 "15213 USA {ghannema, vamshi, jhclark, aup, alavie}@cs.cmu.edu Abstract This paper presents the Carnegie Mellon University statistical transfer MT system submitted to the 2009 WMT shared task in French-to-English translation. We describe a syntax-based approach that incorporates both syntactic and non-syntactic phrase pairs in addition to a syntactic grammar. After reporting development test results, we conduct a preliminary analysis of the coverage and effectiveness of the systems components. "
W09-0426 " This paper describes the techniques we explored to improve the translation of news text in the German-English and Hungarian-English tracks of the WMT09 shared translation task. Beginning with a convention hierarchical phrase-based system, we found benefits for using word segmentation lattices as input, explicit generation of beginning and end of sentence markers, minimum Bayes risk decoding, and incorporation of a feature scoring the alignment of function words in the hypothesized translation. We also explored the use of monolingual paraphrases to improve coverage, as well as co-training to improve the quality of the segmentation lattices used, but these did not lead to improvements. 1 Introduct  "
W09-0427 "Abstract We describe the system used in our submission to the WMT-2009 French-English translation task. We use the Moses phrasebased Statistical Machine Translation system with two simple modications of the decoding input and word-alignment strategy based on morphology, and analyze their impact on translation quality. "
W09-0428 " In this article, we describe the machine translation systems we used to create MorphoLogics submissions to the WMT09 shared Hungarian to English and English to Hungarian shared translation tasks. We used our rule based MetaMorpho system to generate our primary submission. In addition, we created a hybrid system where the Moses decoder is used to rank translations or assemble partial translations created by MetaMorpho. Our third system was a purely statistical morpheme based system for the Hungarian to English task. 1 Intro  "
W09-0429 "Abstract Edinburgh University participated in the WMT 2009 shared task using the Moses phrase-based statistical machine translation decoder, building systems for all language pairs. The system configuration was identical for all language pairs (with a few additional components for the GermanEnglish language pairs). This paper describes the configuration of the systems, plus novel contributions to Moses including truecasing, more efficient decoding methods, and a framework to specify reordering constraints. "
W09-0430 " This paper presents our first attempt at constructing a Vietnamese-French statistical machine translation system. Since Vietnamese is an under-resourced language, we concentrate on building a large VietnameseFrench parallel corpus. A document alignment method based on publication date, special words and sentence alignment result is proposed. The paper also presents an application of the obtained parallel corpus to the construction of a Vietnamese-French statistical machine translation system, where the use of different units for Vietnamese (syllables, words, or their combinations) is discussed. "
W09-0431 "Abstract We present a comparison of two approaches for Arabic-Chinese machine translation using English as a pivot language: sentence pivoting and phrase-table pivoting. Our results show that using English as a pivot in either approach outperforms direct translation from Arabic to Chinese. Our best result is the phrase-pivot system which scores higher than direct translation by 1.1 BLEU points. An error analysis of our best system shows that we successfully handle many complex Arabic-Chinese syntactic variations. "
W09-0432 "18, Povo (TN), Italy {bertoldi, federico}@fbk.eu Abstract Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions. The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system. Previous work showed small performance gains by adapting from limited in-domain bilingual data. Here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language. We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language. Investigations were conducted on a stateof-the-art phrase-based system trained on the SpanishEnglish part of the UN corpus, and adapted on the corresponding Europarl data. Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline. By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set. "
W09-0433 " Chinese and Korean belong to different language families in terms of word-order and morphological typology. Chinese is an SVO and morphologically poor language while Korean is an SOV and morphologically rich one. In Chinese-to-Korean SMT systems, systematic differences between the verbal systems of the two languages make the generation of Korean verbal phrases difficult. To resolve the difficulties, we address two issues in this paper. The first issue is that the verb position is different from the viewpoint of word-order typology. The second is the difficulty of complex morphology generation of Korean verbs from the viewpoint of morphological typology. We propose a Chinese syntactic reordering that is better at generating Korean verbal phrases in Chinese-to-Korean SMT. Specifically, we consider reordering rules targeting Chinese verb phrases (VPs), preposition phrases (PPs), and modality-bearing words that are closely related to Korean verbal phrases. We verify our system with two corpora of different domains. Our proposed approach significantly improves the performance of our system over a baseline phrased-based SMT system. The relative improvements in the two corpora are +9.32% and +5.43%, respectively. "
W09-0434 "<NoAbstract>"
W09-0435 "Universit  at Karlsruhe Karlsruhe, Germany {jniehues,kolss}@ira.uka.de Abstract In this paper we describe a new approach to model long-range word reorderings in statistical machine translation (SMT). Until now, most SMT approaches are only able to model local reorderings. But even the word order of related languages like German and English can be very different. In recent years approaches that reorder the source sentence in a preprocessing step to better match target sentences according to POS(Part-of-Speech)-based rules have been applied successfully. We enhance this approach to model long-range reorderings by introducing discontinuous rules. We tested this new approach on a GermanEnglish translation task and could significantly improve the translation quality, by up to 0.8 BLEU points, compared to a system which already uses continuous POSbased rules to model short-range reorderings. "
W09-0436 "Abstract Linking constructions involving (DE) are ubiquitous in Chinese, and can be translated into English in many different ways. This is a major source of machine translation error, even when syntaxsensitive translation models are used. This paper explores how getting more information about the syntactic, semantic, and discourse context of uses of (DE) can facilitate producing an appropriate English translation strategy. We describe a finergrained classification of (DE) constructions in Chinese NPs, construct a corpus of annotated examples, and then train a log-linear classifier, which contains linguistically inspired features. We use the DE classifier to preprocess MT data by explicitly labeling (DE) constructions, as well as reordering phrases, and show that our approach provides significant BLEU point gains on MT02 (+1.24), MT03 (+0.88) and MT05 (+1.49) on a phrasedbased system. The improvement persists when a hierarchical reordering model is applied. "
W09-0437 " Translation systems are complex, and most metrics do little to pinpoint causes of error or isolate system differences. We use a simple technique to discover induction errors, which occur when good translations are absent from model search spaces. Our results show that a common pruning heuristic drastically increases induction error, and also strongly suggest that the search spaces of phrase-based and hierarchical phrase-based models are highly overlapping despite the well known structural differences. 1 Introdu  "
W09-0438 "Abstract In this paper we present a novel transliteration technique which is based on deep belief networks. Common approaches use finite state machines or other methods similar to conventional machine translation. Instead of using conventional NLP techniques, the approach presented here builds on deep belief networks, a technique which was shown to work well for other machine learning problems. We show that deep belief networks have certain properties which are very interesting for transliteration and possibly also for translation and that a combination with conventional techniques leads to an improvement over both components on an Arabic-English transliteration task. "
W09-0439 "Abstract The most commonly used method for training feature weights in statistical machine translation (SMT) systems is Ochs minimum error rate training (MERT) procedure. A well-known problem with Ochs procedure is that it tends to be sensitive to small changes in the system, particularly when the number of features is large. In this paper, we quantify the stability of Ochs procedure by supplying different random seeds to a core component of the procedure (Powells algorithm). We show that for systems with many features, there is extensive variation in outcomes, both on the development data and on the test data. We analyze the causes of this variation and propose modifications to the MERT procedure that improve stability while helping performance on test data. "
W09-0440 " Linguistic metrics based on syntactic and semantic information have proven very effective for Automatic MT Evaluation. However, no results have been presented so far on their performance when applied to heavily ill-formed low quality translations. In order to glean some light into this issue, in this work we present an empirical study on the behavior of a heterogeneous set of metrics based on linguistic analysis in the paradigmatic case of speech translation between non-related languages. Corroborating previous findings, we have verified that metrics based on deep linguistic analysis exhibit a very robust and stable behavior at the system level. However, these metrics suffer a significant decrease at the sentence level. This is in many cases attributable to a loss of recall, due to parsing errors or to a lack of parsing at all, which may be partially ameliorated by backing off to lexical similarity. 1 Introduction  "
W09-0601 " Augmentative and Alternative Communication (AAC) systems are communication aids for people who cannot speak because of motor or cognitive impairments. We are developing AAC systems where users select information they wish to communicate, and this is expressed using an NLG system. We believe this model will work well in contexts where AAC users wish to go beyond simply making requests or answering questions, and have more complex communicative goals such as story-telling and social interaction. 1  "
W09-0602 "Abstract Widespread use of Semantic Web technologies requires interfaces through which knowledge can be viewed and edited without deep understanding of Description Logic and formalisms like OWL and RDF. Several groups are pursuing approaches based on Controlled Natural Languages (CNLs), so that editing can be performed by typing in sentences which are automatically interpreted as statements in OWL. We suggest here a variant of this approach which relies entirely on Natural Language Generation (NLG), and propose requirements for a system that can reliably generate transparent realisations of statements in Description Logic.  "
W09-0603 " Data-to-text generation systems tend to be knowledge-based and manually built, which limits their reusability and makes them time and cost-intensive to create and maintain. Methods for automating (part of) the system building process exist, but do such methods risk a loss in output quality? In this paper, we investigate the cost/quality trade-off in generation system building. We compare four new data-to-text systems which were created by predominantly automatic techniques against six existing systems for the same domain which were created by predominantly manual techniques. We evaluate the ten systems using intrinsic automatic metrics and human quality ratings. We find that increasing the degree to which system building is automated does not necessarily result in a reduction in output quality. We find furthermore that standard automatic evaluation metrics underestimate the quality of handcrafted systems and over-estimate the quality of automatically created systems. "
W09-0604 "Abstract Data-driven approaches to sentence compression define the task as dropping any subset of words from the input sentence while retaining important information and grammaticality. We show that only 16% of the observed compressed sentences in the domain of subtitling can be accounted for in this way. We argue that part of this is due to evaluation issues and estimate that a deletion model is in fact compatible with approximately 55% of the observed data. We analyse the remaining problems and conclude that in those cases word order changes and paraphrasing are crucial, and argue for more elaborate sentence compression models which build on NLG work. "
W09-0605 "<NoAbstract>"
W09-0606 "Abstract Many studies in natural language processing are concerned with how to generate definite descriptions that evoke a discourse entity already introduced in the context. A solution to this problem has been initially proposed by Dale (1989) in terms of distinguishing descriptions and distinguishable entities. In this paper, we give a formal definition of the terms distinguishable entity in non trivial cases and we show that its properties lead us to the definition of a distance between entities. Then, we give a polynomial algorithm to compute distinguishing descriptions. "
W09-0607 "een, UK {r.turner,yaji.sripada,e.reiter}@abdn.ac.uk Abstract Georeferenced data sets are often large and complex. Natural Language Generation (NLG) systems are beginning to emerge that generate texts from such data. One of the challenges these systems face is the generation of geographic descriptions referring to the location of events or patterns in the data. Based on our studies in the domain of meteorology we present a two staged approach to generating geographic descriptions. The first stage involves using domain knowledge based on the task context to select a frame of reference, and the second involves using constraints imposed by the end user to select values within a frame of reference. Because geographic concepts are inherently vague our approach does not guarantee a distinguishing description. Our evaluation studies show that NLG systems, because they can analyse input data exhaustively, can produce more fine-grained geographic descriptions that are more useful to end users than those generated by human experts. "
W09-0608 " This paper introduces a class-based approach to ordering prenominal modifiers. Modifiers are grouped into broad classes based on where they tend to occur prenominally, and a framework is developed to order sets of modifiers based on their classes. This system is developed to generate several orderings for modifiers with more flexible positional constraints, and lends itself to bootstrapping for the classification of previously unseen modifiers. 1 Intro  "
W09-0609 "Abstract In this paper, we explore a corpus of human-produced referring expressions to see to what extent we can learn the referential behaviour the corpus represents. Despite a wide variation in the way subjects refer across a set of ten stimuli, we demonstrate that component elements of the referring expression generation process appear to generalise across participants to a significant degree. This leads us to propose an alternative way of thinking of referring expression generation, where each attribute in a description is provided by a separate heuristic. "
W09-0610 " This paper shows a model of automatic instruction giving for guiding human users in virtual 3D environments. A multilevel model for choosing what instruction to give in every state is presented, and so are the different modules that compose the whole generation system. How 3D information in the virtual world is used is explained, and the final order generation is detailed. This model has been implemented as a solution for the GIVE Challenge, an instruction generation challenge. 1 Intro  "
W09-0611 "Abstract We address the problem that different users have different lexical knowledge about problem domains, so that automated dialogue systems need to adapt their generation choices online to the users domain knowledge as it encounters them. We approach this problem using policy learning in Markov Decision Processes (MDP). In contrast to related work we propose a new statistical user model which incorporates the lexical knowledge of different users. We evaluate this user model by showing that it allows us to learn dialogue policies that automatically adapt their choice of referring expressions online to different users, and that these policies are significantly better than adaptive hand-coded policies for this problem. The learned policies are consistently between 2 and 8 turns shorter than a range of different hand-coded but adaptive baseline lexical alignment policies. "
W09-0612 "<NoAbstract>"
W09-0613 "E, UK {a.gatt,e.reiter}@abdn.ac.uk Abstract This paper describes SimpleNLG, a realisation engine for English which aims to provide simple and robust interfaces to generate syntactic structures and linearise them. The library is also flexible in allowing the use of mixed (canned and noncanned) representations. "
W09-0614 "Abstract We present a Wizard-of-Oz environment for data collection on Referring Expression Generation (REG) in a real situated spoken dialogue task. The collected data will be used to build user simulation models for reinforcement learning of referring expression generation strategies. "
W09-0615 "ted Kingdom {i.h.khan,k.vdeemter,g.ritchie,a.gatt,a.cleland}@abdn.ac.uk Abstract This paper discusses the evaluation of a Generation of Referring Expressions algorithm that takes structural ambiguity into account. We describe an ongoing study with human readers. 1 Introduction In recent years, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of humanlikeness of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algorithms that produce referring expressions that are of optimal benefit to a hearer. That is, we are interested in generating descriptions that are easy to read and understand. But the readability and intelligibility of a description can crucially depend on the way in which it is  This work is supported by a University of Aberdeen Sixth Century Studentship, and EPSRC grant EP/E011764/1. worded. This happens particularly when there is potential for misunderstanding, as can happen in the case of attachment and scope ambiguities. Suppose, for example, one wants to make it clear that all radical students and all radical teachers are in agreement with a certain idea. It might be risky to express this as the radical students and teachers are agreed, since the reader 1 might be inclined to interpret this as pertaining to all teachers rather than only the radical ones. For this reason, a GRE program might opt for the longer noun phrase the radical students and the radical teachers. But because this expression is lengthier, the choice involves a compromise between comprehensibiliity and brevity, a special case of a difficult trade-off that is typical of generation as well as interpretation of language (van Deemter, 2004). We previously reported the design of an algorithm (based on an earlier work on expressions referring to sets (Gatt, 2007)), which was derived from experiments in which readers were asked to express their preference between different descriptions and to respond to instructions which used a variety of phrasings (Khan et al., 2008). Here we discuss the issues that arise when such an algorithm is evaluated in terms of its benefits for readers. "
W09-0616 " This paper argues for a game-theoretic approach to content determination that uses text-type specific strategies in order to determine the optimal content for various user types. By means of content determination for the description of numerical data the benefits of a game-theoretic treatment of content determination are outlined. 1 Introdu  "
W09-0617 "Abstract This paper gives an overview of ongoing work on a system for the generation of NL descriptions of classes defined in OWL ontologies. We present a general structuring approach for such descriptions. Since OWL ontologies do not by default contain the information necessary for lexicalization, lexical information has to be added to the data via annotations. A rulebased mechanism for automatically deriving these annotations is presented. "
W09-0618 " In order to pursue research on generating referring expressions in a situated collaboration task, we set up a data-collection experiment based on the Tangram puzzle. For a pair of participants we recorded every utterance in synchronisation with the current state of the puzzle as well as all operations by the participants. Referring expressions were annotated with their referents in order to build a referring expression corpus in Japanese. We provide preliminary results on the analysis of the corpus from various standpoints, focussing on action-mentioning expressions. 1 Introduct  "
W09-0619 " In this paper we examine the effect of linguistic devices on recall and comprehension in information presentation using both recall and eye-tracking data. In addition, the results were validated via an experiment using Amazons Mechanical Turk micro-task environment. 1 I  "
W09-0620 " In a corpus study we found that authors vary both mathematical form and precision 1 when expressing numerical quantities. Indeed, within the same document, a quantity is often described vaguely in some places and more accurately in others. Vague descriptions tend to occur early in a document and to be expressed in simpler mathematical forms (e.g., fractions or ratios), whereas more accurate descriptions of the same proportions tend to occur later, often expressed in more complex forms (e.g., decimal percentages). Our results can be used in Natural Language Generation (1) to generate repeat descriptions within the same document, and (2) to generate descriptions of numerical quantities for different audiences according to mathematical ability. 1 Introduct  "
W09-0621 " For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines. 1 Intro  "
W09-0622 "and Ivana Kruijff-Korbayov  a Language Technology Lab, German Research Center for Artificial Intelligence (DFKI) Saarbr  ucken, Germany {zender, gj, ivana.kruijff}@dfki.de Abstract The background for this paper is the aim to build robotic assistants that can naturally interact with humans. One prerequisite for this is that the robot can correctly identify objects or places a user refers to, and produce comprehensible references itself. As robots typically act in environments that are larger than what is immediately perceivable, the problem arises how to identify the appropriate context, against which to resolve or produce a referring expression (RE). Existing algorithms for generating REs generally bypass this problem by assuming a given context. In this paper, we explicitly address this problem, proposing a method for context determination in large-scale space. We show how it can be applied both for resolving and producing REs. "
W09-0623 "Department of Computer Science Trinity College Dublin Dublin 2 Ireland Abstract The content selection component of a natural language generation system decides which information should be communicated in its output. We use information from reports on the game of cricket. We first describe a simple factoid-to-text alignment algorithm then treat content selection as a collective classification problem and demonstrate that simple grouping of statistics at various levels of granularity yields substantially improved results over a probabilistic baseline. We additionally show that holding back of specific types of input data, and linking database structures with commonality further increase performance. "
W09-0624 "<NoAbstract>"
W09-0625 "Abstract One major aim of research in affective natural language generation is to be able to use language intelligently to induce effects on the emotions of the reader/ hearer. Although varying the content of generated language (strategic choices) might be expected to change the effect on emotions, it is not obvious that varying the form of the language (tactical choices) can do this. Indeed, previous experiments have been unable to show emotional effects of tactical variations. Building on what has been discovered in previous experiments, we present a new experiment which does demonstrate such effects. This represents an important step towards the empirical evaluation of affective NLG systems. "
W09-0626 "Abstract This informal position paper brings together some recent developments in formal semantics and pragmatics to argue that the discipline of Game Theory is well placed to become the theoretical backbone of Natural Language Generation. To demonstrate some of the strengths and weaknesses of the Game-Theoretical approach, we focus on the utility of vague expressions. More specifically, we ask what light Game Theory can shed on the question when an NLG system should generate vague language. "
W09-0627 "<NoAbstract>"
W09-0628 " We describe the first installment of the Challenge on Generating Instructions in Virtual Environments (GIVE), a new shared task for the NLG community. We motivate the design of the challenge, describe how we carried it out, and discuss the results of the system evaluation. 1 Introduction This paper reports on the methodology and results of the First Challenge on Generating Instructions in Virtual Environments (GIVE-1), which we ran from March 2008 to February 2009. GIVE is a new shared task for the NLG community. It provides an end-to-end evaluation methodology for NLG systems that generate instructions which are meant to help a user solve a treasure-hunt task in a virtual 3D world. The most innovative aspect from an NLG evaluation perspective is that the NLG system and the user are connected over the Internet. This makes it possible to cheaply collect large amounts of evaluation data. Five NLG systems were evaluated in GIVE1 over a period of three months from November 2008 to February 2009. During this time, we collected 1143 games that were played by users from 48 countries. As far as we know, this makes GIVE-1 the largest evaluation effort in terms of experimental subjects ever. We have evaluated the five systems both on objective measures (success rate, completion time, etc.) and subjective measures which were collected by asking the users to fill in a questionnaire. GIVE-1 was intended as a pilot experiment in order to establish the validity of the evaluation methodology and understand the challenges involved in the instruction-giving task. We believe that we have achieved these purposes. At the same time, we provide evaluation results for the five NLG systems which will help their developers improve them for participation in a future challenge, GIVE-2. GIVE-2 will retain the successful aspects of GIVE-1, while refining the task to emphasize aspects that we found to be challenging. We invite the ENLG community to participate in designing GIVE-2. Plan of the paper. The paper is structured as follows. In Section 2, we will describe and motivate the GIVE Challenge. In Section 3, we will then describe the evaluation method and infrastructure for the challenge. Section 4 reports on the evaluation results. Finally, we conclude and discuss future work in Section 5. 2 The GIVE Challe  "
W09-0629 " The TUNA-REG09 Challenge was one of the shared-task evaluation competitions at Generation Challenges 2009. TUNAREG09 used data from the TUNA Corpus of paired representations of entities and human-authored referring expressions. The shared task was to create systems that generate referring expressions for entities given representations of sets of entities and their properties. Four teams submitted six systems to TUNAREG09. We evaluated the six systems and two sets of human-authored referring expressions using several automatic intrinsic measures, a human-assessed intrinsic evaluation and a human task performance experiment. This report describes the TUNAREG task and the evaluation methods used, and presents the evaluation results. "
W09-0630 "Abstract We describe a new realiser developed for the TUNA 2009 Challenge, and present its evaluation scores on the development set, showing a clear increase in performance compared to last years simple realiser. "
W09-0631 " A major outcome of the last Shared Tasks for Referring Expressions Generation was that each human prefers distinct properties, syntax and lexical units for building referring expressions. One of the reasons for this seems to be that entities might be identified faster since the conversation partner has already some knowledge about how his conversation partner builds referring expressions. Therefore, artificial referring expressions should provide such individual preferences as well so that they become human like. With this contribution to the shared task, we follow this idea again. For the development set, we got a very good DICE score of 0.88 for the furniture domain and of 0.79 for the people domain. 1 Introdu  "
W09-0632 "<NoAbstract>"
W09-0633 " We present a follow-up of our previous frequency-based greedy attribute selection strategy. The current version takes into account also the instructions given to the participants of TUNA trials regarding the use of location information, showing an overall improvement on string-edit distance values driven by the results on the Furniture domain. 1 Intro  "
W09-0901 "nburg {sl,cooper}@ling.gu.se Abstract This paper introduces a formal view of the semantics and pragmatics of corrective feedback in dialogues between adults and children. The goal of this research is to give a formal account of language coordination in dialogue, and semantic coordination in particular. Accounting for semantic coordination requires (1) a semantics, i.e. an architecture allowing for dynamic meanings and meaning updates as results of dialogue moves, and (2) a pragmatics, describing the dialogue moves involved in semantic coordination. We illustrate the general approach by applying it to some examples from the literature on corrective feedback, and provide a fairly detailed discussion of one example using TTR (Type Theory with Records) to formalize concepts. TTR provides an analysis of linguistic content which is structured in order to allow modification and similarity metrics, and a framework for describing dialogue moves and resulting updates to linguistic resources. "
W09-0902 "Abstract A   large   number   of   computational   language  learners have been proposed for modelling the  process  of child language acquisition.   Com paring them, however, can be difficult due to  the different  assumptions that they make, the  diverse test results presented, and the different  linguistic behaviours investigated.   This paper  introduces  a toolkit that allows different  lan guage  learners  to be trained,  tested and  ana lysed under standardised conditions.  The res ults can be easily compared with one another  and  with typical  child language  development  to highlight the relative advantages and disad vantages of learners. "
W09-0903 "Abstract In this paper we present the first step in a larger series of experiments for the induction of predicate/argument structures. The structures that we are inducing are very similar to the conceptual structures that are used in Frame Semantics (such as FrameNet). Those structures are called messages and they were previously used in the context of a multi-document summarization system of evolving events. The series of experiments that we are proposing are essentially composed from two stages. In the first stage we are trying to extract a representative vocabulary of words. This vocabulary is later used in the second stage, during which we apply to it various clustering approaches in order to identify the clusters of predicates and argumentsor frames and semantic roles, to use the jargon of Frame Semantics. This paper presents in detail and evaluates the first stage. "
W09-0904 "Abstract Indirect negative evidence is clearly an important way for learners to constrain overgeneralisation, and yet a good learning theoretic analysis has yet to be provided for this, whether in a PAC or a probabilistic identification in the limit framework. In this paper we suggest a theoretical analysis of indirect negative evidence that allows the presence of ungrammatical strings in the input and also accounts for the relationship between grammaticality/acceptability and probability. Given independently justified assumptions about lower bounds on the probabilities of grammatical strings, we establish that a limited number of membership queries of some strings can be probabilistically simulated. "
W09-0905 "Abstract Building on the use of local contexts, or frames, for human category acquisition, we explore the treatment of contexts as categories. This allows us to examine and evaluate the categorical properties that local unsupervised methods can distinguish and their relationship to corpus POS tags. From there, we use lexical information to combine contexts in a way which preserves the intended category, providing a platform for grammatical category induction. "
W09-1301 "Jin-Dong Kim  Junichi Tsujii   Department of Computer Science, University of Tokyo, Tokyo, Japan  School of Computer Science, University of Manchester, Manchester, UK  National Centre for Text Mining, University of Manchester, Manchester, UK {smp,okap,jdkim,tsujii}@is.s.u-tokyo.ac.jp Abstract We propose a static relation extraction task to complement biomedical information extraction approaches. We argue that static relations such as part-whole are implicitly involved in many common extraction settings, define a task setting making them explicit, and discuss their integration into previously proposed tasks and extraction methods. We further identify a specific static relation extraction task motivated by the BioNLP09 shared task on event extraction, introduce an annotated corpus for the task, and demonstrate the feasibility of the task by experiments showing that the defined relations can be reliably extracted. The task setting and corpus can serve to support several forms of domain information extraction. "
W09-1302 " Determining whether a condition is historical or recent is important for accurate results in biomedicine. In this paper, we investigate four types of information found in clinical text that might be used to make this distinction. We conducted a descriptive, exploratory study using annotation on clinical reports to determine whether this temporal information is useful for classifying conditions as historical or recent. Our initial results suggest that few of these feature values can be used to predict temporal classification. 1 I  "
W09-1303 " This paper introduces ONYX, a sentencelevel text analyzer that implements a number of innovative ideas in syntactic and semantic analysis. ONYX is being developed as part of a project that seeks to translate spoken dental examinations directly into chartable findings. ONYX integrates syntax and semantics to a high degree. It interprets sentences using a combination of probabilistic classifiers, graphical unification, and semantically annotated grammar rules. In this preliminary evaluation, ONYX shows inter-annotator agreement scores with humans of 86% for assigning semantic types to relevant words, 80% for inferring relevant concepts from words, and 76% for identifying relations between concepts. 1 Int  "
W09-1304 "Belgium {Roser.Morante,Walter.Daelemans}@ua.ac.be Abstract Identifying hedged information in biomedical literature is an important subtask in information extraction because it would be misleading to extract speculative information as factual information. In this paper we present a machine learning system that finds the scope of hedge cues in biomedical texts. The system is based on a similar system that finds the scope of negation cues. We show that the same scope finding approach can be applied to both negation and hedging. To investigate the robustness of the approach, the system is tested on the three subcorpora of the BioScope corpus that represent different text types. "
W09-1305 "2 1 Jena University Language & Information Engineering (JULIE) Lab Friedrich-Schiller-Universit  at Jena, Germany {udo.hahn|katrin.tomanek|ekaterina.buyko}@uni-jena.de 2 EMBL-EBI, Wellcome Trust Genome Campus, Hinxton, Cambridge, UK {kim|rebholz}@ebi.ac.uk Abstract We explore a rule system and a machine learning (ML) approach to automatically harvest information on gene regulation events (GREs) from biological documents in two different evaluation scenarios  one uses self-supplied corpora in a clean lab setting, while the other incorporates a standard reference database of curated GREs from REGULONDB, real-life data generated independently from our work. In the lab condition, we test how feasible the automatic extraction of GREs really is and achieve F-scores, under different, not directly comparable test conditions though, for the rule and the ML systems which amount to 34% and 44%, respectively. In the REGULONDB condition, we investigate how robust both methodologies are by comparing them with this routinely used database. Here, the best F-scores for the rule and the ML systems amount to 34% and 19%, respectively. "
W09-1306 "NICTA Victoria Research Laboratory  Dept of Computer Science and Software Engineering The University of Melbourne {wongwl,davidm,lcavedon}@csse.unimelb.edu.au Abstract We investigate the challenge of extracting information about genetic mutations from tables, an important source of information in scientific papers. We use various machine learning algorithms and feature sets, and evaluate performance in extracting fields associated with an existing handcreated database of mutations. We then show how classifying tabular information can be leveraged for the task of named entity detection for mutations. 1 Keywords Information extraction; tables; biomedical applications. 1 Introduction We are interested in applying information extraction and text mining techniques to aiding the construction of databases of biomedical information, in particular information about genetic mutations. Such databases are currently constructed by hand: a long, involved, time-consuming and human-intensive process. Each paper considered for inclusion in the database must be read, the interesting data identified and then entered by hand into a database. 2 However, the biomedical domain throws up many new and serious challenges to information extraction and text mining. Unusual terminology and underdeveloped standards for nomenclature present problems for tokenisation and add complexity to standard information extraction tasks, such as named entity recognition (NER). A lack of resources (at least 1 A short version of this paper was presented at the Australasian Document Computing Symposium, 2008. All copyrights from that event were retained by the authors. 2 Karamis et al (2008) illustrate how even simple tools can have an impact on improving the database-curation process. compared to other domains), such as collections of annotated full-text documents and relevance judgements for various tasks, are a bottleneck to developing and evaluating the core techniques required. In this paper, we report on work performed on extracting information from tables in biomedical research papers. Tables present a succinct and information-rich format for providing information, and are particularly important when reporting results in biological and medical research papers. For example, the Human Genome Variation Society (HGVS), in its general recommendations for mutation nomenclature, recommends making use of tabular listings when several changes are described in a manuscript. 3 A specific premise of our work is that the highly-structured nature of tabular information allows leverage of some techniques that are not so sensitive to the well-reported problems inherent in biomedical terminology, which complicate NER tasks in this domain. In particular, we describe initial techniques for extending NER performance through the analysis of tables: columns/rows are classified as containing items of the entities of interest, thereby allowing those entities to be recognized as of the target type. Since a significant amount of such entities may be found in tables in biomedical scientific papers, this can have positive impact on the performance of base NER techniques. NER tools specifically targeted at recognising mutations have been developed (e.g. (Horn et al., 2004; Baker and Witte, 2006; Caporaso et al., 2007; Lee et al., 2007)); however, they only detect a subclass of mutations, so-called single-point mutations, 3 http://www.hgvs.org/mutnomen/recs.html#general  http://www.hgvs.org/mutnomen/recs.html#general 46 i.e. those that affect a single base. MutationFinder (Caporaso et al., 2007) is the only publicly available tool, built with around 700 automatically-generated rules (both for different nomenclatures and natural language). However, most of the mutations that we find in our dataset are not point mutations or do not follow point-mutation nomenclature, limiting the usefulness of MutationFinder (and related tools) over our document collection. In the next section, we describe the setting of our task, the Mismatch Repair (MMR) Database, and outline the task of extraction from tables. In Section 3, we describe the preparation of our document collection, and in Section 4, we analyse the amount of mutation-related information that is in the associated tables. Section 5 describes the main task, which is classifying table rows and columns as containing mutations, and Section 6 leverages this technique to detect mutations of interest to the MMR Database. We discuss the results in Section 7. "
W09-1307 " Text mining for biomedicine requires a significant amount of domain knowledge. Much of this information is contained in biomedical ontologies. Developers of text mining applications often look for appropriate ontologies that can be integrated into their systems, rather than develop new ontologies from scratch. However, there is often a lack of documentation of the qualities of the ontologies. A number of methodologies for evaluating ontologies have been developed, but it is difficult for users by using these methods to select an ontology. In this paper, we propose a framework for selecting the most appropriate ontology for a particular text mining application. The framework comprises three components, each of which considers different aspects of requirements of text mining applications on ontologies. We also present an experiment based on the framework choosing an ontology for a gene normalization system. "
W09-1308 "94305, USA Abstract Dictionaries of biomedical concepts (e.g. diseases, medical treatments) are critical source of background knowledge for systems doing biomedical information retrieval, extraction, and automated discovery. However, the rapid pace of biomedical research and the lack of constraints on usage ensure that such dictionaries are incomplete. Focusing on medical treatment concepts (e.g. drugs, medical procedures and medical devices), we have developed an unsupervised, iterative pattern learning approach for constructing a comprehensive dictionary of medical treatment terms from randomized clinical trial (RCT) abstracts. We have investigated different methods of seeding, either with a seed pattern or seed instances (terms), and have compared different ranking methods for ranking extracted context patterns and instances. When used to identify treatment concepts from 100 randomly chosen, manually annotated RCT abstracts, our medical treatment dictionary shows better performance (precision:0.40, recall: 0.92 and F-measure: 0.54) over the most widely used manually created medical treatment terminology (precision: 0.41, recall: 0.52 and F-measure: 0.42). "
W09-1309 "Abstract Abbreviations are common in biomedical documents and many are ambiguous in the sense that they have several potential expansions. Identifying the correct expansion is necessary for language understanding and important for applications such as document retrieval. Identifying the correct expansion can be viewed as a Word Sense Disambiguation (WSD) problem. A WSD system that uses a variety of knowledge sources, including two types of information specific to the biomedical domain, is also described. This system was tested on a corpus of ambiguous abbreviations, created by automatically identifying the correct expansion in Medline abstracts, and found to identify the correct expansion with up to 99% accuracy. "
W09-1310 "Abstract In biomedical information extraction (IE), a central problem is the disambiguation of ambiguous names for domain specific entities, such as proteins, genes, etc. One important dimension of ambiguity is the organism to which the entities belong: in order to disambiguate an ambiguous entity name (e.g. a protein), it is often necessary to identify the specific organism to which it refers. In this paper we present an approach to the detection and disambiguation of the focus organism(s), i.e. the organism(s) which are the subject of the research described in scientific papers, which can then be used for the disambiguation of other entities. The results are evaluated against a gold standard derived from IntAct annotations. The evaluation suggests that the results may already be useful within a curation environment and are certainly a baseline for more complex approaches. "
W09-1311 "Abstract Computing the semantic similarity between terms relies on existence and usage of semantic resources. However, these resources, often composed of equivalent units, or synonyms, must be first analyzed and weighted in order to define within them the reliability zones where the semantic cohesiveness is stronger. We propose an original method for acquisition of elementary synonyms based on exploitation of structured terminologies, analysis of syntactic structure of complex (multi-unit) terms and their compositionality. The acquired synonyms are then profiled thanks to endogenous lexical and linguistic indicators (other types of relations, lexical inclusions, productivity), which are automatically inferred within the same terminologies. Additionally, synonymy relations are observed within graph, and its structure is analyzed. Particularly, we explore the usefulness of the graph theory notions such as connected component, clique, density, bridge, articulation vertex, and centrality of vertices. "
W09-1312 " In this paper we present an extractive system that automatically generates gene summaries from the biomedical literature. The proposed text summarization system selects and ranks sentences from multiple MEDLINE abstracts by exploiting gene-specific information and similarity relationships between sentences. We evaluate our system on a large dataset of 7,294 human genes and 187,628 MEDLINE abstracts using Recall-Oriented Understudy for Gisting Evaluation (ROUGE), a widely used automatic evaluation metric in the text summarization community. Two baseline methods are used for comparison. Experimental results show that our system significantly outperforms the other two methods with regard to all ROUGE metrics. A demo website of our system is freely accessible at http://60.195.250.72/onbires/summary.jsp . 1 Int  "
W09-1313 "<NoAbstract>"
W09-1314 " One of the most neglected areas of biomedical Text Mining (TM) is the development of systems based on carefully assessed user needs. We investigate the needs of an important task yet to be tackled by TM  Cancer Risk Assessment (CRA)  and take the first step towards the development of TM for the task: identifying and organizing the scientific evidence required for CRA in a taxonomy. The taxonomy is based on expert annotation of 1297 MEDLINE abstracts. We report promising results with inter-annotator agreement tests and automatic classification experiments, and a user test which demonstrates that the resources we have built are well-defined, accurate, and applicable to a real-world CRA scenario. We discuss extending and refining the taxonomy further via manual and machine learning approaches, and the subsequent steps required to develop TM for the needs of CRA. 1 Introductio  "
W09-1315 "Turkey Abstract We introduce a controlled natural language for biomedical queries, called BIOQUERYCNL, and present an algorithm to convert a biomedical query in this language into a program in answer set programming (ASP)a formal framework to automate reasoning about knowledge. BIOQUERYCNL allows users to express complex queries (possibly containing nested relative clauses and cardinality constraints) over biomedical ontologies; and such a transformation of BIOQUERYCNL queries into ASP programs is useful for automating reasoning about biomedical ontologies by means of ASP solvers. We precisely describe the grammar of BIOQUERYCNL, implement our transformation algorithm, and illustrate its applicability to biomedical queries by some examples. "
W09-1316 "Abstract Medical concepts in clinical reports can be found with a high degree of variability of expression. Normalizing medical concepts to standardized vocabularies is a common way of accounting for this variability. One of the challenges in medical concept normalization is the difficulty in comparing two concepts which are orthographically different in representation but are identical in meaning. In this work we describe a method to compare medical phrases by utilizing the information found in syntactic dependencies. We collected a large corpus of radiology reports from our university medical center. A shallow semantic parser was used to identify anatomical phrases. We performed a series of transformations to convert the anatomical phrase into a normalized syntactic dependency representation. The new representation provides an easy intuitive way of comparing the phrases for the purpose of concept normalization. "
W09-1317 " An important task in information retrieval is to identify sentences that contain important relationships between key concepts. In this work, we propose a novel approach to automatically extract sentence patterns that contain interactions involving concepts of molecular biology. A pattern is defined in this work as a sequence of specialized Part-of-Speech (POS) tags that capture the structure of key sentences in the scientific literature. Each candidate sentence for the classification task is encoded as a POS array and then aligned to a collection of pre-extracted patterns. The quality of the alignment is expressed as a pairwise alignment score. The most innovative component of this work is the use of a Genetic Algorithm (GA) to maximize the classification performance of the alignment scoring scheme. The system achieves an F-score of 0.834 in identifying sentences which describe interactions between biological entities. This performance is mostly affected by the quality of the preprocessing steps such as term identification and POS tagging. 1 Introductio  "
W09-1318 "<NoAbstract>"
W09-1319 " In the framework of contextual information retrieval in the biomedical domain, this paper reports on the automatic detection of disease concepts in two genres of biomedical text: sentences from the literature and PubMed user queries. A statistical model and a Natural Language Processing algorithm for disease recognition were applied on both corpora. While both methods show good performance (F=77% vs. F=76%) on the sentence corpus, results on the query corpus indicate that the statistical model is more robust (F=74% vs. F=70%).  Language Processing algorithm for disease recognition were applied on both corpora. While both methods show good performance (F=77% vs. F=76%) on the sentence corpus, results on the query corpus indicate that the statistical model is more robust (F=74% vs. F=70%). "
W09-1320 "<NoAbstract>"
W09-1321 "apan {uematsu,jdkim,tsujii}@is.s.u-tokyo.ac.jp Abstract This paper compares domain-oriented and linguistically-oriented semantics, based on the GENIA event corpus and FrameNet. While the domain-oriented semantic structures are direct targets of Text Mining (TM), their extraction from text is not straghtforward due to the diversity of linguistic expressions. The extraction of linguistically-oriented semactics is more straghtforward, and has been studied independentely of specific domains. In order to find a use of the domain-independent research achievements for TM, we aim at linking classes of the two types of semantics. The classes were connected by analyzing linguistically-oriented semantics of the expressions that mention one biological class. With the obtained relationship between the classes, we discuss a link between TM and linguistically-oriented semantics. "
W09-1322 " Question answering is different from information retrieval in that it attempts to answer questions by providing summaries from numerous retrieved documents rather than by simply providing a list of documents that requires users to do additional work. However, the quality of answers that question answering provides has not been investigated extensively, and the practical approach to presenting question answers still needs more study. In addition to factoid answering using phrases or entities, most question answering systems use a sentence-based approach for generating answers. However, many sentences are often only meaningful or understandable in their context, and a passage-based presentation can often provide richer, more coherent context. However, passage-based presentations may introduce additional noise that places greater burden on users. In this study, we performed a quantitative evaluation on the two kinds of presentation produced by our online clinical question answering system, AskHERMES http://www.AskHERMES.org ). The overall finding is that, although irrelevant context can hurt the quality of an answer, the passage-based approach is generally more effective in that it provides richer context and matching across sentences. 1 Introduction Questi  "
W09-1323 " Historically, suicide risk assessment has relied on question-and-answer type tools. These tools, built on psychometric advances, are widely used because of availability. Yet there is no known tool based on biologic and cognitive evidence. This absence often cause a vexing clinical problem for clinicians who question the value of the result as time passes. The purpose of this paper is to describe one experiment in a series of experiments to develop a tool that combines Biological Markers (B m ) with Thought Markers (T m ), and use machine learning to compute a real-time index for assessing the likelihood repeated suicide attempt in the next six-months. For this study we focus using unsupervised machine learning to distinguish between actual suicide notes and newsgroups. This is important because it gives us insight into how well these methods discriminate between real notes and general conversation. "
W09-1324 " With the rapidly growing use of electronic health records, the possibility of large-scale clinical information extraction has drawn much attention. It is not, however, easy to extract information because these reports are written in natural language. To address this problem, this paper presents a system that converts a medical text into a table structure. This systems core technologies are (1) medical event recognition modules and (2) a negative event identification module that judges whether an event actually occurred or not. Regarding the latter module, this paper also proposes an SVM-based classifier using syntactic information. Experimental results demonstrate empirically that syntactic information can contribute to the methods accuracy. 1 Introdu  "
W09-1401 " The paper presents the design and implementation of the BioNLP09 Shared Task, and reports the final results with analysis. The shared task consists of three sub-tasks, each of which addresses bio-molecular event extraction at a different level of specificity. The data was developed based on the GENIA event corpus. The shared task was run over 12 weeks, drawing initial interest from 42 teams. Of these teams, 24 submitted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges. 1 Intro  "
W09-1402 "Abstract We describe a system for extracting complex events among genes and proteins from biomedical literature, developed in context of the BioNLP09 Shared Task on Event Extraction. For each event, its text trigger, class, and arguments are extracted. In contrast to the prevailing approaches in the domain, events can be arguments of other events, resulting in a nested structure that better captures the underlying biological statements. We divide the task into independent steps which we approach as machine learning problems. We define a wide array of features and in particular make extensive use of dependency parse graphs. A rule-based post-processing step is used to refine the output in accordance with the restrictions of the extraction task. In the shared task evaluation, the system achieved an F-score of 51.95% on the primary task, the best performance among the participants. "
W09-1403 " We describe the approach to event extraction which the JULIELab Team from FSU Jena (Germany) pursued to solve Task 1 in the BioNLP09 Shared Task on Event Extraction. We incorporate manually curated dictionaries and machine learning methodologies to sort out associated event triggers and arguments on trimmed dependency graph structures. Trimming combines pruning irrelevant lexical material from a dependency graph and decorating particularly relevant lexical material from that graph with more abstract conceptual class information. Given that methodological framework, the JULIELab Team scored on 2nd rank among 24 competing teams, with 45.8% precision, 47.5% recall and 46.7% F1-score on all 3,182 events. 1 Introduct  "
W09-1404 "Abstract We describe a biological event detection method implemented for the BioNLP 2009 Shared Task 1. The method relies entirely on the chunk and syntactic dependency relations provided by a general NLP pipeline which was not adapted in any way for the purposes of the shared task. The method maps the syntactic relations to event structures while being guided by the probabilities of the syntactic features of events which were automatically learned from the training data. Our method achieved a recall of 26% and a precision of 44% in the official test run, under strict equality of events. "
W09-1405 "Abstract We describe our system for the BioNLP 2009 event detection task. It is designed to be as domain-independent and unsupervised as possible. Nevertheless, the precisions achieved for single theme event classes range from 75% to 92%, while maintaining reasonable recall. The overall F-scores achieved were 36.44% and 30.80% on the development and the test sets respectively. "
W09-1406 " In this paper we describe our entry to the BioNLP 2009 Shared Task regarding biomolecular event extraction. Our work can be described by three design decisions: (1) instead of building a pipeline using local classier technology, we design and learn a joint probabilistic model over events in a sentence; (2) instead of developing specic inference and learning algorithms for our joint model, we apply Markov Logic, a general purpose Statistical Relation Learning language, for this task; (3) we represent events as relational structures over the tokens of a sentence, as opposed to structures that explicitly mention abstract event entities. Our results are competitive: we achieve the 4th best scores for task 1 (in close range to the 3rd place) and the best results for task 2 with a 13 percent point margin. 1 Introdu  "
W09-1407 "Abstract We approached the problems of event detection, argument identification, and negation and speculation detection as one of concept recognition and analysis. Our methodology involved using the OpenDMAP semantic parser with manually-written rules. We achieved state-of-the-art precision for two of the three tasks, scoring the highest of 24 teams at precision of 71.81 on Task 1 and the highest of 6 teams at precision of 70.97 on Task 2. The OpenDMAP system and the rule set are available at bionlp.sourceforge.net. *These two authors contributed equally to the paper. 1 Introduction We approached the problem of biomedical event recognition as one of concept recognition and analysis. Concept analysis is the process of taking a textual input and building from it an abstract representation of the concepts that are reflected in it. Concept recognition can be equivalent to the named entity recognition task when it is limited to locating mentions of particular semantic types in text, or it can be more abstract when it is focused on recognizing predicative relationships, e.g. events and their participants. "
W09-1408 "Belgium {Roser.Morante,Walter.Daelemans,Vincent.VanAsch}@ua.ac.be Abstract In this paper we describe the memory-based machine learning system that we submitted to the BioNLP Shared Task on Event Extraction. We modeled the event extraction task using an approach that has been previously applied to other natural language processing tasks like semantic role labeling or negation scope finding. The results obtained by our system (30.58 F-score in Task 1 and 29.27 in Task 2) suggest that the approach and the system need further adaptation to the complexity involved in extracting biomedical events. "
W09-1409 " The BioNLP 09 Shared Task on Event Extraction presented an evaluation on the extraction of biological events related to genes/proteins from the literature. We propose a system that uses the case-based reasoning (CBR) machine learning approach for the extraction of the entities (events, sites and location). The mapping of the proteins in the texts to the previously extracted entities is carried out by some simple manually developed rules for each of the arguments under consideration (cause, theme, site or location). We have achieved an f-measure of 24.15 and 21.15 for Task 1 and 2, respectively. 1 Intro  "
W09-1410 "stralia {amack,davidm,tim}@csse.unimelb.edu.au Abstract This work describes a system for the tasks of identifying events in biomedical text and marking those that are speculative or negated. The architecture of the system relies on both Machine Learning (ML) approaches and hand-coded precision grammars. We submitted the output of our approach to the event extraction shared task at BioNLP 2009, where our methods suffered from low recall, although we were one of the few teams to provide answers for task 3. "
W09-1411 " We present an approach for extracting molecular events from literature based on a deep parser, using in a query language for parse trees. Detected events range from gene expression to protein localization, and cover a multitude of different entity types, including genes/proteins, binding sites, and locations. Furthermore, our approach is capable of recognizing negation and the speculative character of extracted statements. We first parse documents using Link Grammar (BioLG) and store the parse trees in a database. Events are extracted using a newly developed query language with traverses the BioLG linkages between trigger terms, arguments, and events. The concrete queries are learnt from an annotated corpus. On BioNLP Shared Task data, we achieve an overall F1-measure of 29.6%. 1 Introductio  "
W09-1412 ", Sofia 1784, Bulgaria 2 The Department of Computer Science, Regent Court 211 Portobello, Sheffield, S1 4DP. UK. Abstract We describe the system of the PIKB team for BioNLP09 Shared Task 1, which targets tunable domain-independent event extraction. Our approach is based on a three-stage classification: (1) trigger word tagging, (2) simple event extraction, and (3) complex event extraction. We use the MIRA framework for all three stages, which allows us to trade precision for increased recall by appropriately changing the loss function during training. We report results for three systems focusing on recall (R = 28.88%), precision (P = 65.58%), and F 1 -measure (F 1 = 33.57%), respectively. "
W09-1413 "Arizona {toufeeq, ranair1, chpatel, hdavulcu}@asu.edu Abstract In this paper, we present BioEve a fully automated event extraction system for bio-medical text. It first semantically classifies each sentence to the class type of the event mentioned in the sentence, and then using high coverage hand-crafted rules, it extracts the participants of that event. We participated in Task 1 of BioNLP 2009 Shared task, and the final evaluation results are described here. Our experimentation with different approaches to classify a sentence to bio-interaction classes are also shared. "
W09-1414 " This document describes the methods and results for our participation in the BioNLP09 Shared Task #1 on Event Extraction. It also contains some error analysis and a brief discussion of the results. Previous shared tasks in the BioNLP community have focused on extracting gene and protein names, and on finding (direct) protein-protein interactions (PPI). This years task was slightly different, since the protein names were already manually annotated in the text. The new challenge was to extract biological events involving these given gene and gene products. We modified a publicly available system (AkanePPI) to apply it to this new, but similar, protein interaction task. AkanePPI has previously achieved state-of-the-art performance on all existing public PPI corpora, and only small changes were needed to achieve competitive results on this event extraction task. Our official result was an F-score of 36.9%, which was ranked as number six among submissions from 24 different groups. We later balanced the recall/precision by including more predictions than just the most confident one in ambiguous cases, and this raised the F-score on the test-set to 42.6%. The new Akane program can be used freely for academic purposes. "
W09-1415 " In this paper, we propose a system for biomedical event extraction using multi-phase approach. It consists of event trigger detector, event type classifier, and relation recognizer and event compositor. The system firstly identifies triggers in a given sentence. Then, it classifies the triggers into one of nine predefined classes. Lastly, the system examines each trigger whether it has a relation with participant candidates, and composites events with the extracted relations. The official score of the proposed system recorded 61.65 precision, 9.40 recall and 16.31 f-score in approximate span matching. However, we found that the threshold tuning for the third phase had negative effect. Without the threshold tuning, the system showed 55.32 precision, 16.18 recall and 25.04 f-score. 1 Introductio  "
W09-1416 "Abstract We introduce a supervised approach for extracting bio-molecular events by using linguistic features that represent the contexts of the candidate event triggers and participants. We use Support Vector Machines as our learning algorithm and train separate models for event types that are described with a single theme participant, multiple theme participants, or a theme and a cause participant. We perform experiments with linear kernel and edit-distance based kernel and report our results on the BioNLP09 Shared Task test data set. "
W09-1417 " This paper reports on a system developed for the BioNLP'09 shared task on detection and characterisation of biomedical events. Event triggers and types were recognised using a conditional random field classifier and a set of rules, while event participants were identified using a rule-based system that relied on relative distances between candidate entities and the trigger in the associated parse tree. The results on previously unseen test data were encouraging: for non-regulatory events, the Fscore was almost 50% (with precision above 60%), with the overall F-score of around 30% (49% precision). The performance on more complex regulatory events was poor (F-measure of 7%). Among the 24 teams submitting the test results, our results were ranked 12 th for the overall F-score and 8 th for the F-score of non-regulation events. 1 Intro  "
W09-1418 "Halil Kilicoglu and Sabine Bergler Department of Computer Science and Software Engineering Concordia University 1455 de Maisonneuve Blvd. West Montr  eal, Canada {h kilico,bergler}@cse.concordia.ca Abstract We explore a rule-based methodology for the BioNLP09 Shared Task on Event Extraction, using dependency parsing as the underlying principle for extracting and characterizing events. We approach the speculation and negation detection task with the same principle. Evaluation results demonstrate the utility of this syntax-based approach and point out some shortcomings that need to be addressed in future work. "
W09-1419 "Abstract The BioNLP09 Shared Task on Event Extraction is a challenge which concerns the detection of bio-molecular events from text. In this paper, we present a detailed account of the challenges encountered during the construction of a machine learning framework for participation in this task. We have focused our work mainly around the filtering of false positives, creating a high-precision extraction method. We have tested techniques such as SVMs, feature selection and various filters for data preand post-processing, and report on the influence on performance for each of them. To detect negation and speculation in text, we describe a custom-made rule-based system which is simple in design, but effective in performance. "
W09-2901 " Multiword Expressions (MWEs) are one of the stumbling blocks for more precise Natural Language Processing (NLP) systems. Particularly, the lack of coverage of MWEs in resources can impact negatively on the performance of tasks and applications, and can lead to loss of information or communication errors. This is especially problematic in technical domains, where a significant portion of the vocabulary is composed of MWEs. This paper investigates the use of a statisticallydriven alignment-based approach to the identification of MWEs in technical corpora. We look at the use of several sources of data, including parallel corpora, using English and Portuguese data from a corpus of Pediatrics, and examining how a second language can provide relevant cues for this tasks. We report results obtained by a combination of statistical measures and linguistic information, and compare these to the reported in the literature. Such an approach to the (semi-)automatic identification of MWEs can considerably speed up lexicographic work, providing a more targeted list of MWE candidates. "
W09-2902 "Abstract We tackle two major issues in automatic keyphrase extraction using scientific articles: candidate selection and feature engineering. To develop an efficient candidate selection method, we analyze the nature and variation of keyphrases and then select candidates using regular expressions. Secondly, we re-examine the existing features broadly used for the supervised approach, exploring different ways to enhance their performance. While most other approaches are supervised, we also study the optimal features for unsupervised keyphrase extraction. Our research has shown that effective candidate selection leads to better performance as evaluation accounts for candidate coverage. Our work also attests that many of existing features are also usable in unsupervised extraction. "
W09-2903 "Abstract We address the problem of classifying multiword expression tokens in running text. We focus our study on Verb-Noun Constructions (VNC) that vary in their idiomaticity depending on context. VNC tokens are classified as either idiomatic or literal. We present a supervised learning approach to the problem. We experiment with different features. Our approach yields the best results to date on MWE classification combining different linguistically motivated features, the overall performance yields an F-measure of 84.58% corresponding to an Fmeasure of 89.96% for idiomaticity identification and classification and 62.03% for literal identification and classification. "
W09-2904 " Based on a study of verb translations in the Europarl corpus, we argue that a wide range of MWE patterns can be identified in translations that exhibit a correspondence between a single lexical item in the source language and a group of lexical items in the target language. We show that these correspondences can be reliably detected on dependency-parsed, word-aligned sentences. We propose an extraction method that combines word alignment with syntactic filters and is independent of the structural pattern of the translation. 1 I  "
W09-2905 " We review lexical Association Measures (AMs) that have been employed by past work in extracting multiword expressions. Our work contributes to the understanding of these AMs by categorizing them into two groups and suggesting the use of rank equivalence to group AMs with the same ranking performance. We also examine how existing AMs can be adapted to better rank English verb particle constructions and light verb constructions. Specifically, we suggest normalizing (Pointwise) Mutual Information and using marginal frequencies to construct penalization terms. We empirically validate the effectiveness of these modified AMs in detection tasks in English, performed on the Penn Treebank, which shows significant improvement over the original AMs.  "
W09-2906 " Complex predicate is a noun, a verb, an adjective or an adverb followed by a light verb that behaves as a single unit of verb. Complex predicates (CPs) are abundantly used in Hindi and other languages of Indo Aryan family. Detecting and interpreting CPs constitute an important and somewhat a difficult task. The linguistic and statistical methods have yielded limited success in mining this data. In this paper, we present a simple method for detecting CPs of all kinds using a Hindi-English parallel corpus. A CP is hypothesized by detecting absence of the conventional meaning of the light verb in the aligned English sentence. This simple strategy exploits the fact that CP is a multiword expression with a meaning that is distinct from the meaning of the light verb. Although there are several shortcomings in the methodology, this empirical method surprisingly yields mining of CPs with an average precision of 89% and a recall of 90%. 1 Introduction  "
W09-2907 " Multiword expressions (MWEs) have been proved useful for many natural language processing tasks. However, how to use them to improve performance of statistical machine translation (SMT) is not well studied. This paper presents a simple yet effective strategy to extract domain bilingual multiword expressions. In addition, we implement three methods to integrate bilingual MWEs to Moses, the state-ofthe-art phrase-based machine translation system. Experiments show that bilingual MWEs could improve translation performance significantly. 1 Introdu  "
W09-2908 "501, Japan {funayama, shibata, kuro}@nlp.kuee.kyoto-u.ac.jp Abstract This paper proposes Japanese bottom-up named entity recognition using a twostage machine learning method. Most work has formalized Named Entity Recognition as a sequential labeling problem, in which only local information is utilized for the label estimation, and thus a long named entity consisting of several morphemes tends to be wrongly recognized. Our proposed method regards a compound noun (chunk) as a labeling unit, and first estimates the labels of all the chunks in a phrasal unit (bunsetsu) using a machine learning method. Then, the best label assignment in the bunsetsu is determined from bottom up as the CKY parsing algorithm using a machine learning method. We conducted an experimental on CRL NE data, and achieved an F measure of 89.79, which is higher than previous work. "
W09-3001 " Emotion computing is very important for expressive information extraction. In this paper, we provide a robust and versatile emotion annotation scheme based on cognitive emotion theories, which not only can annotate both explicit and implicit emotion expressions, but also can encode different levels of emotion information for the given emotion content. In addition, motivated by a cognitive framework, an automatic emotion annotation system is developed, and large and comparatively high-quality emotion corpora are created for emotion computing, one in Chinese and the other in English. Such an annotation system can be easily adapted for different kinds of emotion applications and be extended to other languages. 1 Int  "
W09-3002 " Alternative paths to linguistic annotation, such as those utilizing games or exploiting the web users, are becoming popular in recent times owing to their very high benefit-to-cost ratios. In this paper, however, we report a case study on POS annotation for Bangla and Hindi, where we observe that reliable linguistic annotation requires not only expert annotators, but also a great deal of supervision. For our hierarchical POS annotation scheme, we find that close supervision and training is necessary at every level of the hierarchy, or equivalently, complexity of the tagset. Nevertheless, an intelligent annotation tool can significantly accelerate the annotation process and increase the inter-annotator agreement for both expert and non-expert annotators. These findings lead us to believe that reliable annotation requiring deep linguistic knowledge (e.g., POS, chunking, Treebank, semantic role labeling) requires expertise and supervision. The focus, therefore, should be on design and development of appropriate annotation tools equipped with machine learning based predictive modules that can significantly boost the productivity of the annotators. 1 "
W09-3003 "rsity {rehbein,josefr,csporled}@coli.uni-sb.de Abstract In this paper, we present the results of an experiment in which we assess the usefulness of partial semi-automatic annotation for frame labeling. While we found no conclusive evidence that it can speed up human annotation, automatic pre-annotation does increase its overall quality. "
W09-3004 "Abstract This paper explores interoperability for data represented using the Graph Annotation Framework (GrAF) (Ide and Suderman, 2007) and the data formats utilized by two general-purpose annotation systems: the General Architecture for Text Engineering (GATE) (Cunningham, 2002) and the Unstructured Information Management Architecture (UIMA). GrAF is intended to serve as a pivot to enable interoperability among different formats, and both GATE and UIMA are at least implicitly designed with an eye toward interoperability with other formats and tools. We describe the steps required to perform a round-trip rendering from GrAF to GATE and GrAF to UIMA CAS and back again, and outline the commonalities as well as the differences and gaps that came to light in the process. "
W09-3005 " Given the contemporary trend to modular NLP architectures and multiple annotation frameworks, the existence of concurrent tokenizations of the same text represents a pervasive problem in everydays NLP practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general. This paper describes a solution for integrating different tokenizations using a standoff XML format, and discusses the consequences for the handling of queries on annotated corpora. 1 M  "
W09-3006 ", Ankara, Turkey b Anadolu University, Eskis  ehir, Turkey Abstract In this paper we explain how we annotated subordinators in the Turkish Discourse Bank (TDB), an effort that started in 2007 and is still continuing. We introduce the project and describe some of the issues that were important in annotating three subordinators, namely kars  n, ra  gmen and halde, all of which encode the coherence relation Contrast-Concession. We also describe the annotation tool. "
W09-3007 "Abstract We present two modules for the recognition and annotation of temporal expressions and events in French texts according to the TimeML specification language. The Temporal Expression Tagger we have developed is based on a large coverage cascade of finite state transducers and our Event Tagger on a set of simple heuristics applied over local context in a chunked text. We present results of a preliminary evaluation and compare them with those obtained by a similar system. "
W09-3008 "} Abstract PlayCoref is a concept of an on-line language game designed to acquire a substantial amount of text data with the coreference annotation. We describe in detail various aspects of the game design and discuss features that affect the quality of the annotation. "
W09-3009 " In this paper, we report our work on automatic image annotation by combining several textual features drawn from the text surrounding the image. Evaluation of our system is performed on a dataset of images and texts collected from the web. We report our findings through comparative evaluation with two gold standard collections of manual annotations on the same dataset. 1  "
W09-3010 " Evaluating systems that correct errors in non-native writing is difficult because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number. 1 I  "
W09-3011 " We present the annotation architecture of the National Corpus of Polish and discuss problems identified in the TEI stand-off annotation system, which, in its current version, is still very much unfinished and untested, due to both technical reasons (lack of tools implementing the TEIdefined XPointer schemes) and certain problems concerning data representation. We concentrate on two features that a stand-off system should possess and that are conspicuously missing in the current TEI Guidelines. 1  "
W09-3012 "Vinodkumar Prabhakaran Weiwei Guo CS CS Columbia U. Columbia U. Abstract We present a preliminary pilot study of belief annotation and automatic tagging. Our objective is to explore semantic meaning beyond surface propositions. We aim to model peoples cognitive states, namely their beliefs as expressed through linguistic means. We model the strength of their beliefs and their (the human) degree of commitment to their utterance. We explore only the perspective of the author of a text. We classify predicates into one of three possibilities: committed belief, non committed belief, or not applicable. We proceed to manually annotate data to that end, then we build a supervised framework to test the feasibility of automatically predicting these belief states. Even though the data is relatively small, we show that automatic prediction of a belief class is a feasible task. Using syntactic features, we are able to obtain significant improvements over a simple baseline of 23% F-measure absolute points. The best performing automatic tagging condition is where we use POS tag, word type feature AlphaNumeric, and shallow syntactic chunk information CHUNK. Our best overall performance is 53.97% F-measure. "
W09-3013 "Charles University in Prague, Institute of Formal and Applied Linguistics Malostransk  e n  am. 25, 118 00 Prague 1, Czech Republic {lopatkova,kljueva,homola}@ufal.mff.cuni.cz Abstract The goal of the presented project is to assign a structure of clauses to Czech sentences from the "
W09-3014 " In this paper we show how to exploit typographical and textual features of raw text for creating a finegrain XML Schema Markup with special focus on capturing linguistic variation in dictionaries. We use declarative programming techniques and contextfree grammars implemented in PROLOG. 1 Intro  "
W09-3015 " Corpus annotation plays an important role in linguistic analysis and computational processing of both written and spoken language. Syntactic annotation of spoken texts becomes clearly a topic of considerable interest nowadays, driven by the desire to improve automatic speech recognition systems by incorporating syntax in the language models, or to build language understanding applications. Syntactic annotation of both written and spoken texts in the Czech Academic Corpus was created thirty years ago when no other (even annotated) corpus of spoken texts has existed. We will discuss how much relevant and inspiring this annotation is to the current frameworks of spoken text annotation. 1 Motiv  "
W09-3016 " NLP systems that deal with large collections of text require significant computational resources, both in terms of space and processing time. Moreover, these systems typically add new layers of linguistic information with references to another layer. The spreading of these layered annotations across different files makes them more difficult to process and access the data. As the amount of input increases, so does the difficulty to process it. One approach is to use distributed parallel computing for solving these larger problems and save time. We propose a framework that simplifies the integration of independently existing NLP tools to build language-independent NLP systems capable of creating layered annotations. Moreover, it allows the development of scalable NLP systems, that executes NLP tools in parallel, while offering an easy-to-use programming environment and a transparent handling of distributed computing problems. With this framework the execution time was decreased to 40 times less than the original one on a cluster with 80 cores. "
W09-3017 " The present paper outlines an ongoing project of annotation of the extended nominal coreference and the bridging anaphora in the Prague Dependency Treebank. We describe the annotation scheme with respect to the linguistic classification of coreferential and bridging relations and focus also on details of the annotation process from the technical point of view. We present methods of helping the annotators  by a pre-annotation and by several useful features implemented in the annotation tool. Our method of the inter-annotator agreement is focused on the improvement of the annotation guidelines; we present results of three subsequent measurements of the agreement. 1 Introd  "
W09-3018 "Jena University Language & Information Engineering (JULIE) Lab Friedrich-Schiller-Universit  at Jena, Germany {katrin.tomanek|udo.hahn}@uni-jena.de Abstract We report on the re-annotation of selected types of named entities from the MUC7 corpus where our focus lies on recording the time it takes to annotate these entities given two basic annotation units  sentences vs. complex noun phrases. Such information may be helpful to lay the empirical foundations for the development of cost measures for annotation processes based on the investment in time for decision-making per entity mention. "
W09-3019 " GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers. 1 1 Int  "
W09-3020 "Abstract This short paper describes the use of the linguistic annotation available in parallel PropBanks (Chinese and English) for the enhancement of automatically derived word alignments. Specifically, we suggest ways to refine and expand word alignments for verb-predicates by using predicate-argument structures. Evaluations demonstrate improved alignment accuracies that vary by corpus type. "
W09-3021 " WordNet and FrameNet are widely used lexical resources, but they are very different from each other and are often used in completely different ways in NLP. In a case study in which a short passage is annotated in both frameworks, we show how the synsets and definitions of WordNet and the syntagmatic information from FrameNet can complement each other, forming a more complete representation of the lexical semantic of a text than either could alone. Close comparisons between them also suggest ways in which they can be brought into alignment. 1 Backgroun  "
W09-3022 " In this short paper, we present annotations for tagging grammatical and stylistic errors, together with attributes about the nature of the correction which are then interpreted as arguments. A decision model is introduced in order for the author to be able to decide on the best correction to make. This introduces an operational semantics for tags and related attributes. 1 Aims  "
W09-3023 " In this paper we present a collaborative work between computer and social scientists resulting in the development of annotation software for conducting research and analysis in social semiotics in both multimodal and linguistic aspects. The paper describes the proposed software and discusses how this tool can contribute for development of social semiotic theory and practice. "
W09-3024 "ublic {novak,razimova}@ufal.mff.cuni.cz Abstract We present a new method for automated discovery of inconsistencies in a complex manually annotated corpora. The proposed technique is based on Apriori algorithm for mining association rules from datasets. By setting appropriate parameters to the algorithm, we were able to automatically infer highly reliable rules of annotation and subsequently we searched for records for which the inferred rules were violated. We show that the violations found by this simple technique are often caused by an annotation error. We present an evaluation of this technique on a hand-annotated corpus PDT 2.0, present the error analysis and show that in the first 100 detected nodes 20 of them contained an annotation error. "
W09-3025 "Abstract Today, the named entity recognition task is considered as fundamental, but it involves some specific difficulties in terms of annotation. Those issues led us to ask the fundamental question of what the annotators should annotate and, even more important, for which purpose. We thus identify the applications using named entity recognition and, according to the real needs of those applications, we propose to semantically define the elements to annotate. Finally, we put forward a number of methodological recommendations to ensure a coherent and reliable annotation scheme. "
W09-3026 " A user-friendly interface to search bilingual resources is of great help to NLP developers as well as pure-linguists. Using bilingual resources is difficult for linguists who are unfamiliar with computation, which hampers capabilities of bilingual resources. NLP developers sometimes need a kind of workbench to check their resources. The online interface this paper introduces can satisfy these needs. In order to implement the interface, this research deals with how to align Korean and Japanese phrases and interpolates them into the original bilingual corpus in an automatic way. 1 Introdu  "
W09-3027 " As part of the STATEMENT MAP project, we are constructing a Japanese corpus annotated with the semantic relations bridging facts and opinions that are necessary for online information credibility evaluation. In this paper, we identify the semantic relations essential to this task and discuss how to efficiently collect valid examples from Web documents by splitting complex sentences into fundamental units of meaning called statements and annotating relations at the statement level. We present a statement annotation scheme and examine its reliability by annotating around 1,500 pairs of statements. We are preparing the corpus for release this winter. 1 Introductio  "
W09-3028 "Abstract The DADA system is being developed to support collaborative access to and annotation of language resources over the web. DADA implements an abstract model of annotation suitable for storing many kinds of data from a wide range of language resources. This paper describes the process of ingesting data from a corpus of Australian Sign Language (Auslan) into the DADA system. We describe the format of the RDF data used by DADA and the issues raised in converting the ELAN annotations from the corpus. "
W09-3029 " We describe the Hindi Discourse Relation Bank project, aimed at developing a large corpus annotated with discourse relations. We adopt the lexically grounded approach of the Penn Discourse Treebank, and describe our classification of Hindi discourse connectives, our modifications to the sense classification of discourse relations, and some crosslinguistic comparisons based on some initial annotations carried out so far. 1 Int  "
W09-3030 " This paper is an attempt to show that an intermediary level of analysis is an effective way for carrying out various NLP tasks for linguistically similar languages. We describe a process for developing a simple parser for doing such tasks. This parser uses a grammar driven approach to annotate dependency relations (both inter and intra chunk) at an intermediary level. Ease in identifying a particular dependency relation dictates the degree of analysis reached by the parser. To establish efficiency of the simple parser we show the improvement in its results over previous grammar driven dependency parsing approaches for Indian languages like Hindi. We also propose the possibility of usefulness of the simple parser for Indian languages that are similar in nature. 1 Introductio  "
W09-3031 "Abstract In this paper, we present preliminary work on corpus-based anaphora resolution of discourse deixis in German. Our annotation guidelines provide linguistic tests for locating the antecedent, and for determining the semantic types of both the antecedent and the anaphor. The corpus consists of selected speaker turns from the Europarl corpus. "
W09-3032 " This paper presents an on-going effort which aims to annotate the Wall Street Journal sections of the Penn Treebank with the help of a hand-written large-scale and wide-coverage grammar of English. In doing so, we are not only focusing on the various stages of the semi-automated annotation process we have adopted, but we are also showing that rich linguistic annotations, which can apart from syntax also incorporate semantics, ensure that the treebank is guaranteed to be a truly sharable, re-usable and multi-functional linguistic resource  . 1 Intr  "
W09-3033 " We present in this paper a formal and computational scheme in the perspective of broad-coverage multimodal annotation. We propose in particular to introduce the notion of annotation hypergraphs in which primary and secondary data are represented by means of the same structure. This paper addresses the question of resources and corpora for natural human-human interaction, in other words broad-coverage annotation of natural data. In this kind of study, most of domains have do be taken into consideration: prosody, pragmatics, syntax, gestures, etc. All these different domains interact in order to build an understandable message. We need then large multimodal annotated corpora of real data, precisely annotated for all domains. Building this kind of resource is a relatively new, but very active research domain, illustrated by the number of workshops (cf. (Martin, 2008)), international initiatives, such as MUMIN (Allwood, 2005), annotation tools such as NITE NXT (Carletta, 2003), Anvil (Kipp, 2001), etc. 1 A characterization of primary data Different types of primary data constitute the basis of an annotation: speech signal, video input, word strings, images, etc. But other kinds of primary data also can be used, for example in the perspective of semantic annotations such as concepts, references, types, etc. Such data are considered to be atomic in the sense that they are not built on top of lower level data. When looking more closely at these kinds of data, several characteristics can be identified: Location: primary data is usually localized with respect to a timeline or a position: gestures can be localized into the video signal, phonemes into the speech one, words into the string or objects into a scene or a context. Two different kinds of localisation are used: temporal and spatial. In the first case, a data is situated by means of a time interval whereas spatial data are localised in terms of relative or absolute positions. Realization: primary data usually refer to concrete (or physical) objects: phonemes, gestures, referential elements into a scene, etc. However, other kinds of primary data can be abstract such as concepts, ideas, emotions, etc. Medium: The W3C recommendation EMMA (Extensible Multi-Modal Annotations) proposes to distinguish different medium: acoustic, tactile and visual. This classification is only relevant for data corresponding to concrete objects. Production: the study of information structure shows the necessity to take into account accessibility of the objects: some data are directly accessible from the signal or the discourse, they have an existence or have already been mentioned. In this case, they are said to be produced. For example, gestures, sounds, physical objects fall in this category. On the other hand, other kinds of data are deduced from the context, typically the abstract ones. They are considered as accessible. In the remaining of the paper, we propose the following definition: Primary data: atomic objects that cannot be decomposed. They represent possible constituent on top of which higher level objects can be built. Primary data does not require any interpretation to be identified, they are of direct access. This primary data typology is given in figure (1). It shows a repartition between concrete vs. abstract objects. Concrete objects are usually those taken into account in corpus annotation. As a consequence, annotation usually focuses on speech and gestures, which narrows down the set of data to those with a temporal localization. However, other kinds of data cannot be situated in the 174 Phonemes Words Gestures Discourse referents Synsets Physical objects Produced + + + +/+ Accessible +/+ Concrete + + + +/+ Abstract +/+ Temporal + + + +/Spatial +/+/+ Acoustic + +/Visual + +/+ Tactile +/+/+ Figure 1: Primary data description timeline (e.g. objects in the environment of the scene) nor spatially (e.g. abstract data). We need to propose a more general approach of data indexing that has to distinguish on the one hand between temporal and spatial localization and on the other hand between data that can be located and data that cannot. "
W09-3034 "Abstract Two major projects in the U.S. and Europe have joined in a collaboration to work toward achieving interoperability among language resources. In the U.S., a project entitled Sustainable Interoperability for Language Technology (SILT) has been funded by the National Science Foundation under the INTEROP program, and in Europe, FLaReNet Fostering Language Resources Network has been funded by the European Commission under the eContentPlus framework. This international collaborative effort involves members of the language processing community and others working in related areas to build consensus regarding the sharing of data and technologies for language resources and applications, to work towards interoperability of existing data, and, where possible, to promote standards for annotation and resource building. In addition to broad-based US and European participation, we are seeking the participation of colleagues in Asia. This presentation describing the projects and their goals will, we hope, serve to involve members of the community who may not have been aware of the effort before, in particular colleagues in Asia. "
W09-3035 " Treebank is an important resource for both research and application of natural language processing. For Vietnamese, we still lack such kind of corpora. This paper presents up-to-date results of a project for Vietnamese treebank construction. Since Vietnamese is an isolating language and has no word delimiter, there are many ambiguities in sentence analysis. We systematically applied a lot of linguistic techniques to handle such ambiguities. Annotators are supported by automaticlabeling tools and a tree-editor tool. Raw texts are extracted from Tuoi Tre (Youth), an online Vietnamese daily newspaper. The current annotation agreement is around 90 percent. 1 I  "
W09-3801 "Abstract We investigate several algorithms related to the parsing problem for weighted automata, under the assumption that the input is a string rather than a tree. This assumption is motivated by several natural language processing applications. We provide algorithms for the computation of parse-forests, best tree probability, inside probability (called partition function), and prefix probability. Our algorithms are obtained by extending to weighted tree automata the Bar-Hillel technique, as defined for context-free grammars. "
W09-3802 "cotland Abstract We show how parsing of trees can be formalized in terms of the intersection of two tree languages. The focus is on weighted regular tree grammars and weighted tree adjoining grammars. Potential applications are discussed, such as parameter estimation across formalisms. "
W09-3803 " We describe for dependency parsing an annotation adaptation strategy, which can automatically transfer the knowledge from a source corpus with a different annotation standard to the desired target parser, with the supervision by a target corpus annotated in the desired standard. Furthermore, instead of a hand-annotated one, a projected treebank derived from a bilingual corpus is used as the source corpus. This benefits the resource-scarce languages which havent different handannotated treebanks. Experiments show that the target parser gains significant improvement over the baseline parser trained on the target corpus only, when the target corpus is smaller. "
W09-3804 "Abstract We present a biparsing algorithm for Stochastic Bracketing Inversion Transduction Grammars that runs in O(bn 3 ) time instead of O(n 6 ). Transduction grammars learned via an EM estimation procedure based on this biparsing algorithm are evaluated directly on the translation task, by building a phrase-based statistical MT system on top of the alignments dictated by Viterbi parses under the induced bigrammars. Translation quality at different levels of pruning are compared, showing improvements over a conventional word aligner even at heavy pruning levels. "
W09-3805 " Empirical lower bounds studies in which the frequency of alignment configurations that cannot be induced by a particular formalism is estimated, have been important for the development of syntax-based machine translation formalisms. The formalism that has received most attention has been inversion transduction grammars (ITGs) (Wu, 1997). All previous work on the coverage of ITGs, however, concerns parse failure rates (PFRs) or sentence level coverage, which is not directly related to any of the evaluation measures used in machine translation. Sgaard and Kuhn (2009) induce lower bounds on translation unit error rates (TUERs) for a number of formalisms, incl. normal form ITGs, but not for the full class of ITGs. Many of the alignment configurations that cannot be induced by normal form ITGs can be induced by unrestricted ITGs, however. This paper estimates the difference and shows that the average reduction in lower bounds on TUER is 2.48 in absolute difference (16.01 in average parse failure rate). 1 Introduction  "
W09-3806 " Most cellular telephones use numeric keypads, where texting is supported by dictionaries and frequency models. Given a key sequence, the entry system recognizes the matching words and proposes a rankordered list of candidates. The ranking quality is instrumental to an effective entry. This paper describes a new method to enhance entry that combines syntax and language models. We first investigate components to improve the ranking step: language models and semantic relatedness. We then introduce a novel syntactic model to capture the word context, optimize ranking, and then reduce the number of keystrokes per character (KSPC) needed to write a text. We finally combine this model with the other components and we discuss the results. We show that our syntax-based model reaches an error reduction in KSPC of 12.4% on a Swedish corpus over a baseline using word frequencies. We also show that bigrams are superior to all the other models. However, bigrams have a memory footprint that is unfit for most devices. Nonetheless, bigrams can be further improved by the addition of syntactic models with an error reduction that reaches 29.4%. 1 Introduction The 12-key input is the most common keypad layout on cellular telephones. It divides the alphabet into eight lists of characters and each list is mapped onto one key as shown in Figure 1. Since three or four characters are assigned to a key, a single key press is ambiguous. Figure 1: Standard 12-button keypad layout (ISO 9995-8).  "
W09-3807 "V  axj  o University, School of Mathematics and Systems Engineering, Sweden  Uppsala University, Department of Linguistics and Philology, Sweden {jens.nilsson|welf.lowe|johan.hall|joakim.nivre}@vxu.se Abstract Program analysis tools used in software maintenance must be robust and ought to be accurate. Many data-driven parsing approaches developed for natural languages are robust and have quite high accuracy when applied to parsing of software. We show this for the programming languages Java, C/C++, and Python. Further studies indicate that post-processing can almost completely remove the remaining errors. Finally, the training data for instantiating the generic data-driven parser can be generated automatically for formal languages, as opposed to the manually development of treebanks for natural languages. Hence, our approach could improve the robustness of software maintenance tools, probably without showing a significant negative effect on their accuracy. "
W09-3808 " We present an Earley-style parser for simple range concatenation grammar, a formalism strongly equivalent to linear context-free rewriting systems. Furthermore, we present different filters which reduce the number of items in the parsing chart. An implementation shows that parses can be obtained in a reasonable time. 1 I  "
W09-3809 " We present a parsing algorithm for Interaction Grammars using the deductive parsing framework. This approach brings new perspectives to this problem, departing from previous methods which rely on constraint-solving techniques. 1 Introduction An Interaction Grammar (IG) (Guillaume and Perrier, 2008) is a lexicalized grammatical formalism that primarily focuses on valency, explicitly expressed using polarities decorating syntagms. These polarities and the use of underspecified structures naturally lead parsing to be viewed as a constraint-solving problem  for example (Bonfante et al.) reduce the parsing problem to a graphrewriting problem in (2003) . However, in this article we depart from this approach and present an algorithm close to (Earley, 1970) for context-free grammars. We introduce this algorithm using the standard framework of deductive parsing (Shieber et al., 1995). This article is organised as follows: we first present IGs (section 2), then we describe the algorithm (section 3). Finally we discuss some technical points and conclude (sections 4 and 5). 2 Interaction Grammars We briefly introduce IGs as in (Guillaume and Perrier, 2008) 1 . However, we omit polarized feature structures, for the sake of exposition. 2.1 Polarized Tree D  "
W09-3810 " Several formalisms have been proposed for modeling trees with discontinuous phrases. Some of these formalisms allow for synchronous rewriting. However, it is unclear whether synchronous rewriting is a necessary feature. This is an important question, since synchronous rewriting greatly increases parsing complexity. We present a characterization of recursive synchronous rewriting in constituent treebanks with discontinuous annotation. An empirical investigation reveals that synchronous rewriting is actually a necessary feature. Furthermore, we transfer this property to grammars extracted from treebanks. 1 Introduct  "
W09-3811 "Abstract We present an improved training strategy for dependency parsers that use online reordering to handle non-projective trees. The new strategy improves both efficiency and accuracy by reducing the number of swap operations performed on non-projective trees by up to 80%. We present state-ofthe-art results for five languages with the best ever reported results for Czech. "
W09-3812 " The paper describes the overall design of a new two stage constraint based hybrid approach to dependency parsing. We define the two stages and show how different grammatical construct are parsed at appropriate stages. This division leads to selective identification and resolution of specific dependency relations at the two stages. Furthermore, we show how the use of hard constraints and soft constraints helps us build an efficient and robust hybrid parser. Finally, we evaluate the implemented parser on Hindi and compare the results with that of two data driven dependency parsers. 1 Introduct  "
W09-3813 "<NoAbstract>"
W09-3814 "<NoAbstract>"
W09-3815 " We apply the idea of weight pushing (Mohri, 1997) to CKY parsing with fixed context-free grammars. Applied after rule binarization, weight pushing takes the weight from the original grammar rule and pushes it down across its binarized pieces, allowing the parser to make better pruning decisions earlier in the parsing process. This process can be viewed as generalizing weight pushing from transducers to hypergraphs. We examine its effect on parsing efficiency with various binarization schemes applied to tree substitution grammars from previous work. We find that weight pushing produces dramatic improvements in efficiency, especially with small amounts of time and with large grammars. "
W09-3816 "Abstract We present an asymmetric approach to a run-time combination of two parsers where one component serves as a predictor to the other one. Predictions are integrated by means of weighted constraints and therefore are subject to preferential decisions. Previously, the same architecture has been successfully used with predictors providing partial or inferior information about the parsing problem. It has now been applied to a situation where the predictor produces exactly the same type of information at a fully competitive quality level. Results show that the combined system outperforms its individual components, even though their performance in isolation is already fairly high. "
W09-3817 "Abstract We present a method for dependency and case structure analysis that captures the consistency between intra-clause relations (i.e., case structures or predicate-argument structures) and inter-clause relations. We assess intra-clause relations on the basis of case frames and inter-clause relations on the basis of transition knowledge between case frames. Both knowledge bases are automatically acquired from a massive amount of parses of a Web corpus. The significance of this study is that the proposed method selects the best dependency and case structure that are consistent within each clause and between clauses. We confirm that this method contributes to the improvement of dependency parsing of Japanese. "
W09-3818 " This paper describes and compares two algorithms that take as input a shared PCFG parse forest and produce shared forests that contain exactly the n most likely trees of the initial forest. Such forests are suitable for subsequent processing, such as (some types of) reranking or LFG fstructure computation, that can be performed ontop of a shared forest, but that may have a high (e.g., exponential) complexity w.r.t. the number of trees contained in the forest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank. 1 Introduct  "
W09-3819 "srael {yoavg,elhadad}@cs.bgu.ac.il Abstract We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew (constituency) Treebank. We establish some baseline unlabeled dependency parsing performance on Hebrew, based on two state-of-the-art parsers, MST-parser and MaltParser. The evaluation is performed both in an artificial setting, in which the data is assumed to be properly morphologically segmented and POS-tagged, and in a real-world setting, in which the parsing is performed on automatically segmented and POS-tagged text. We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than MaltParser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. "
W09-3820 "Abstract Generative lexicalized parsing models, which are the mainstay for probabilistic parsing of English, do not perform as well when applied to languages with different language-specific properties such as free(r) word order or rich morphology. For German and other non-English languages, linguistically motivated complex treebank transformations have been shown to improve performance within the framework of PCFG parsing, while generative lexicalized models do not seem to be as easily adaptable to these languages. In this paper, we show a practical way to use grammatical functions as first-class citizens in a discriminative model that allows to extend annotated treebank grammars with rich feature sets without having to suffer from sparse data problems. We demonstrate the flexibility of the approach by integrating unsupervised PP attachment and POS-based word clusters into the parser. "
W09-3821 " We present a semi-supervised method to improve statistical parsing performance. We focus on the well-known problem of lexical data sparseness and present experiments of word clustering prior to parsing. We use a combination of lexiconaided morphological clustering that preserves tagging ambiguity, and unsupervised word clustering, trained on a large unannotated corpus. We apply these clusterings to the French Treebank, and we train a parser with the PCFG-LA unlexicalized algorithm of (Petrov et al., 2006). We find a gain in French parsing performance: from a baseline of F 1 =86.76% to F 1 =87.37% using morphological clustering, and up to F 1 =88.29% using further unsupervised clustering. This is the best known score for French probabilistic parsing. These preliminary results are encouraging for statistically parsing morphologically rich languages, and languages with small amount of annotated data. "
W09-3822 "Abstract This paper presents a set of experiments performed on parsing the Basque Dependency Treebank. We have applied feature propagation to dependency parsing, experimenting the propagation of several morphosyntactic feature values. In the experiments we have used the output of a parser to enrich the input of a second parser. Both parsers have been generated by Maltparser, a freely data-driven dependency parser generator. The transformations, combined with the pseudoprojective graph transformation, obtain a LAS of 77.12% improving the best reported results for Basque. "
W09-3823 "NCLT, Dublin City University, Dublin 9, Ireland Abstract Lexical-Functional Grammar (Kaplan and Bresnan, 1982) f-structures are bilexical labelled dependency representations. We show that the Naive Bayes classifier is able to guess missing grammatical function labels (i.e. bilexical dependency labels) with reasonably high accuracy (8291%). In the experiments we use f-structure parser output for English and German Europarl data, automatically broken by replacing grammatical function labels with a generic UNKNOWN label and asking the classifier to restore the label. "
W09-3824 "This paper presents preliminary investigations on the statistical parsing of French by bringing a complete evaluation on French data of the main probabilistic lexicalized and unlexicalized parsers first designed on the Penn Treebank. We adapted the parsers on the two existing treebanks of French (Abeille et al., 2003; Schluter and van Genabith, 2007). To our knowledge, mostly all of the results reported here are state-of-the-art for the constituent parsing of French on every available treebank. Regarding the algorithms, the comparisons show that lexicalized parsing models are outperformed by the unlexicalized Berkeley parser. Regarding the treebanks, we observe that, depending on the parsing model, a tag set with specific features has direct influence over evaluation results. We show that the adapted lexicalized parsers do not share the same sensitivity towards the amount of lexical material used for training, thus questioning the relevance of using only one lexicalized model to study the usefulness of lexicalization for the parsing of French. "
W09-3825 " Transition-based approaches have shown competitive performance on constituent and dependency parsing of Chinese. Stateof-the-art accuracies have been achieved by a deterministic shift-reduce parsing model on parsing the Chinese Treebank 2 data (Wang et al., 2006). In this paper, we propose a global discriminative model based on the shift-reduce parsing process, combined with a beam-search decoder, obtaining competitive accuracies on CTB2. We also report the performance of the parser on CTB5 data, obtaining the highest scores in the literature for a dependencybased evaluation. 1 Int  "
W09-3826 "Abstract In this paper, we propose that grammar error detection be disambiguated in generating the connected parse(s) of optimal merit for the full input utterance, in overcoming the cheapest error. The detected error(s) are described as violated grammatical constraints in a framework for ModelTheoretic Syntax (MTS). We present a parsing algorithm for MTS, which only relies on a grammar of well-formedness, in that the process does not require any extragrammatical resources, additional rules for constraint relaxation or error handling, or any recovery process. "
W09-3827 " We parse the sentences in three parallel error corpora using a generative, probabilistic parser and compare the parse probabilities of the most likely analyses for each grammatical sentence and its closely related ungrammatical counterpart. 1 Intro  "
W09-3828 " In this paper, we propose two methods for analyzing errors in parsing. One is to classify errors into categories which grammar developers can easily associate with defects in grammar or a parsing model and thus its improvement. The other is to discover inter-dependencies among errors, and thus grammar developers can focus on errors which are crucial for improving the performance of a parsing model. The first method uses patterns of errors to associate them with categories of causes for those errors, such as errors in scope determination of coordination, PPattachment, identification of antecedent of relative clauses, etc. On the other hand, the second method, which is based on reparsing with one of observed errors corrected, assesses inter-dependencies among errors by examining which other errors were to be corrected as a result if a specific error was corrected. Experiments show that these two methods are complementary and by being combined, they can provide useful clues as to how to improve a given grammar. "
W09-3829 " We present an approach for deriving syntactic word clusters from parsed text, grouping words according to their unlexicalized syntactic contexts. We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure. In our experiments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy. 1 Int  "
W09-3830 " Chunking is segmenting a text into chunks, sub-sentential segments, that Abney approximately defined as stress groups. Chunking usually uses monolingual resources, most often exhaustive, sometimes partial : function words and punctuations, which often mark beginnings and ends of chunks. But, to extend this method to other languages, monolingual resources have to be multiplied. We present a new method : endogenous chunking, which uses no other resource than the text to be segmented itself. The idea of this method comes from Zipf : to make the least communication effort, speakers are driven to shorten frequent words. A chunk then can be characterized as the period of the periodic correlated functions length and frequency of words on the syntagmatic axis. This original method takes its advantage to be applied to a great number of languages of alphabetic script, with the same algorithm, without any resource. Introduct  "
W09-3831 " In this short paper, an off-the-shelf maximum entropy-based POS-tagger is used as a partial parser to improve the accuracy of an extremely fast linear time dependency parser that provides state-of-the-art results in multilingual unlabeled POS sequence parsing. 1  "
W09-3832 "ng, UK {yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jp Junichi Tsujii  Abstract Supertagging is a widely used speed-up technique for deep parsing. In another aspect, supertagging has been exploited in other NLP tasks than parsing for utilizing the rich syntactic information given by the supertags. However, the performance of supertagger is still a bottleneck for such applications. In this paper, we investigated the relationship between supertagging and parsing, not just to speed up the deep parser; We started from a sequence labeling view of HPSG supertagging, examining how well a supertagger can do when separated from parsing. Comparison of two types of supertagging model, point-wise model and sequential model, showed that the former model works competitively well despite its simplicity, which indicates the true dependency among supertag assignments is far more complex than the crude first-order approximation made in the sequential model. We then analyzed the limitation of separated supertagging by using a CFG-filter. The results showed that big gains could be acquired by resorting to a light-weight parser. "
W09-3833 " We present an approach for smoothing treebank-PCFG lexicons by interpolating treebank lexical parameter estimates with estimates obtained from unannotated data via the Inside-outside algorithm. The PCFG has complex lexical categories, making relative-frequency estimates from a treebank very sparse. This kind of smoothing for complex lexical categories results in improved parsing performance, with a particular advantage in identifying obligatory arguments subcategorized by verbs unseen in the treebank. 1  "
W09-3834 " This paper discusses the performance difference of wide-coverage parsers on small-domain speech transcripts. Two parsers (C&C CCG and RASP) are tested on the speech transcripts of two different domains (parent-child language, and picture descriptions). The performance difference between the domain-independent parsers and two domain-trained parsers (MSTParser and MEGRASP) is substantial, with a difference of at least 30 percent point in accuracy. Despite this gap, some of the grammatical relations can still be recovered reliably. 1  "
W09-3835 " This paper introduces a formal framework that presents a novel Interactive Predictive Parsing schema which can be operated by a user, tightly integrated into the system, to obtain error free trees. This compares to the classical two-step schema of manually post-editing the erroneus constituents produced by the parsing system. We have simulated interaction and calculated evalaution metrics, which established that an IPP system results in a high amount of effort reduction for a manual annotator compared to a two-step system. 1 Introdu  "
W09-3836 "Dept of Computational Linguistics, Saarland University  Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany {chowd,yzhang,kordoni}@coli.uni-sb.de Abstract This paper presents a novel approach of incorporating fine-grained treebanking decisions made by human annotators as discriminative features for automatic parse disambiguation. To our best knowledge, this is the first work that exploits treebanking decisions for this task. The advantage of this approach is that use of human judgements is made. The paper presents comparative analyses of the performance of discriminative models built using treebanking decisions and state-of-the-art features. We also highlight how differently these features scale when these models are tested on out-of-domain data. We show that, features extracted using treebanking decisions are more efficient, informative and robust compared to traditional features. "
W09-3837 "Abstract We present a cognitive process model of human sentence comprehension based on generalized left-corner parsing. A search heuristic based upon previouslyparsed corpora derives garden path effects, garden path paradoxes, and the local coherence effect. "
W09-3838 "<NoAbstract>"
W09-3839 "erlands {f.sangati,zuidema,rens.bod}@uva.nl Abstract We propose a framework for dependency parsing based on a combination of discriminative and generative models. We use a discriminative model to obtain a kbest list of candidate parses, and subsequently rerank those candidates using a generative model. We show how this approach allows us to evaluate a variety of generative models, without needing different parser implementations. Moreover, we present empirical results that show a small improvement over state-of-the-art dependency parsing of English sentences. "
W09-3840 "Abstract We propose a generic method to perform lexical disambiguation in lexicalized grammatical formalisms. It relies on dependency constraints between words. The soundness of the method is due to invariant properties of the parsing in a given grammar that can be computed statically from the grammar. "
W09-4501 "<NoAbstract>"
W09-4502 " Case studies for developing and implementing medical portals based on Semantic Technologies and ontological approach in Knowledge Management, Information Extraction and unstructured text processing are presented in the paper. Keywords Semantic Technologies, multi-lingual information extraction, medical content gathering, drug descriptions processing, RDFstorage, semantic Wiki, knowledge based analytics. 1. Introduction Semantic Technologies and the Semantic Web (SW) as the embodiment of know-how for practical usage of these technologies are widely discussed, and it is already clear that semantic content available within knowledge portals shall lead us to a new generation of the Internet and knowledge intensive applications [1]. Medicine should be considered among the top domains for Semantic Web and intelligent applications due to high (and increasing) volumes of health-related information presented in both unstructured and machine readable form [2, 3, 4, 5]. The aim of this paper is to present one particular approach to this task  the Ontos solution for the Semantic Web in the medical domain. Two types of web applications (semantic portals) are examined, as well as the technology which underlies them.  Knowledge Management , Information Extraction and unstructured text processing are presented in the paper. Keywords Semantic Technologies, multi-lingual information extraction, medical content gathering, drug descriptions processing, RDFstorage, semantic Wiki, knowledge based analytics. 1. Introduction Semantic Technologies and the Semantic Web (SW) as the embodiment of know-how for practical usage of these technologies are widely discussed, and it is already clear that semantic content available within knowledge portals shall lead us to a new generation of the Internet and knowledge intensive applications [1]. Medicine should be considered among the top domains for Semantic Web and intelligent applications due to high (and increasing) volumes of health-related information presented in both unstructured and machine readable form [2, 3, 4, 5]. The aim of this paper is to present one particular approach to this task  the Ontos solution for the Semantic Web in the medical domain. Two types of web applications (semantic portals) are examined, as well as the technology which underlies them. "
W09-4503 "vassil.momtchev @ontotext.com Abstract The aim of gene mention normalization is to propose an appropriate canonical name, or an identifier from a popular database, for a gene or a gene product mentioned in a given piece of text. The task has attracted a lot of research attention for several organisms under the assumption that both the mention boundaries and the target organism are known. Here we extend the task to also recognizing whether the gene mention is valid and to finding the organism it is from. We solve this extended task using a joint model for gene and organism name normalization which allows for instances from different organisms to share features, thus achieving sizable performance gains with different learning methods: Na  ve Bayes, Maximum Entropy, Perceptron and mira, as well as averaged versions of the last two. The evaluation results for our joint classifier show F1 score of over 97%, which proves the potential of the approach. Keywords Gene normalization, gene mention tagging, organism recognition, identity resolution. 1 Introduction Gene mention normalization is one of the emerging tasks in bio-medical text processing along with gene mention tagging, protein-protein interaction, and biomedical event extraction. The objective is to propose an appropriate canonical name, or a unique identifier from a predefined list, for each gene or gene product name mentioned in a given piece of text. Solving this task is important for many practical application, e.g., enriching high precision databases such as the Protein and Interaction Knowledge Base (pikb), part of LinkedLifeData 1 , or compiling gene-related search indexes for large document collections such as LifeSKIM 2 and medie 3 .  Ontotext AD, 135 Tsarigradsko Ch. Sofia 1784, Bulgaria  Department of Computer Science, National Un versity of Singapore, 13 Computin Drive, Singapore 117417  Department of Computer and Information Science, University of Penns ylvania, Philadelphia, PA, USA 1 http://www.linkedlifedata.com 2 http://lifeskim.sirma.bg 3 http://www-tsujii.is.s.u-tokyo.ac.jp/medie/ In this work, we focus on the preparation of good training data and on improving the performance of the normalization classifier rather than on building an integrated solution for gene mention normalization. The remainder of the paper is organized as follows: Section 2 gives an overview of the related work, Section 3 present our method, Section 4 describes the experiments and discusses the results, and Section 5 concludes and suggests directions for future work. "
W09-4504 "<NoAbstract>"
W09-4505 "<NoAbstract>"
W09-4506 "<NoAbstract>"
W09-4507 "Abstract Clinical named entities convey great deal of knowledge in clinical notes. This paper investigates named entity recognition from clinical notes using machine learning approaches. We present a cascading system that uses a Conditional Random Fields model, a Support Vector Machine and a Maximum Entropy to reclassify the identified entities in order to reduce misclassification. Voting strategy was employed to determine the class of the recognised entities between the three classifiers. The experiments were conducted on a corpus of 311 manually annotated admission summaries form an Intensive Care Unit. The recognition of 10 types of clinical named entities using 10 fold cross-validation achieved an overall results of 83.3 F-score. The reclassifier effectively increased the performance over stand-alone CRF models by 3.35 F-score. Keywords Named Entity Recognition, Clinical Information Extraction, Machine Learning, Classifier Ensemble, Two Phase Model 1 Introduction With the rapid growth of clinical data produced by health organisations, efficient information extraction from these free text clinical notes will be valuable for improving the work of clinical wards and gaining greater understanding of patient care as well as progression of disease. Recognising named entities is a key to unlocking the information stored in unstructured clinical text. Named entity recognition is an important subtask of Information Extraction. It involves the recognition of named entity (NE) phrases, and usually the classification of these NEs into particular categories. In the clinical domain, important entity categories are clinical findings, procedures and drugs. In recent years, the recognition of named entities in the biomedical scientific literature has become the focus of much research. A large number of systems have been built to recognise, classify and map biomedical entities to ontologies. On the other side, only a little work have been reported in clinical named entity recognition [14, 8, 17]. NER has achieved high performance in scientific articles and newswire text, whereas the clinical notes written by clinicians are in a less structured and often minimal grammatical form with idosyncratic and cryptic shorthand, which poses challenges in NER. Principally, the clinical named entity recognition systems are rule or pattern based. The rules or patterns may not be generalisable due to the specific writing style of individual clinicians. However, a machine learning approach is not fully advanced in clinical named entity recognition due to a lack of available training data. We have investigated the issues of clinical named entity recognition, by constructing a set of annotation guidelines and manually annotating 311 clinical notes from an Intensive Care Unit (ICU), with inter-annotator agreement of 88%. In this paper we present a named entity recogniser using a cascade of classifiers to find entities. The named entities will serve as a prerequisite for clinical relation extraction, clinical notes indexing and question answering from the ICU database. There have been many approaches to NER in biomedical literature. They roughly fall into three approaches: rule-based approaches, dictionary-based approaches and machine learning based approaches. The state-of-art machine learning based systems focus on selecting effective features for building classifiers. Many machine learners have been used for experimentation, for example, Support Vector Machines (SVMs)[9], Hidden Markov Model (HMM)[16], Maximum Entropy Model (ME) [2] and Conditional Random Fields (CRFs) [12]. Conditional Random Fields have been proven to be the best performing learner for the NER task [3]. The benefit of using a machine learner is that it can utilise both the information form of the entity themselves and the contextual information surrounding the entity. It has better generalisability over pattern based approach as it is able to perform prediction without seeing the entire length of the entity. Nevertheless the performance of biomedical NER systems still trails behind newswire NER systems. It suggests that individual NER system may not cover entity representations with sufficiently rich features due to the great variety and ambiguity in biomedical named entities. This problem also exists in clinical text as it has characteristic of both formal and informal linguistic styles, with many unseen named entities, spelling variations and abbreviations. To overcome these difficulties, we propose a classifier cascade approach to clinical NER. We firstly build a CRF based 42 classifier to identify the boundary and class of the named entities, then we trained a SVM and an ME model to reclassify the class of the named entities using the output of the CRF models and different features. The final class of the entity was determined by a majority voting [18] among the output of the CRF, SVM and ME models. The overall system achieved best performance of 83.26 F-score. The cascading classifiers improved 3.35 F-sore over the stand-alone CRF system. This paper is organised as follows: Section 2 gives an overview of related work in biomedical named entity recognition. Section 3 introduces the data used in our experiments. Section 4 to Section 6 describes the cascading named entity recogniser in detail. Section 7 presents the evaluation of the proposed system as well as discussion of the results. "
W10-0701 "Abstract In this paper we give an introduction to using Amazons Mechanical Turk crowdsourcing platform for the purpose of collecting data for human language technologies. We survey the papers published in the NAACL2010 Workshop. 24 researchers participated in the workshops shared task to create data for speech and language applications with $100. "
W10-0702 "0027, USA {mj2472,jda2129}@columbia.edu, {kapil,sara,kathy}@cs.columbia.edu Abstract This paper explores the task of building an accurate prepositional phrase attachment corpus for new genres while avoiding a large investment in terms of time and money by crowdsourcing judgments. We develop and present a system to extract prepositional phrases and their potential attachments from ungrammatical and informal sentences and pose the subsequent disambiguation tasks as multiple choice questions to workers from Amazons Mechanical Turk service. Our analysis shows that this two-step approach is capable of producing reliable annotations on informal and potentially noisy blog text, and this semi-automated strategy holds promise for similar annotation projects in new genres. "
W10-0703 " Vocabulary tutors need word sense disambiguation (WSD) in order to provide exercises and assessments that match the sense of words being taught. Using expert annotators to build a WSD training set for all the words supported would be too expensive. Crowdsourcing that task seems to be a good solution. However, a first required step is to define what the possible sense labels to assign to word occurrence are. This can be viewed as a clustering task on dictionary definitions. This paper evaluates the possibility of using Amazon Mechanical Turk (MTurk) to carry out that prerequisite step to WSD. We propose two different approaches to using a crowd to accomplish clustering: one where the worker has a global view of the task, and one where only a local view is available. We discuss how we can aggregate multiple workers clusters together, as well as pros and cons of our two approaches. We show that either approach has an interannotator agreement with experts that corresponds to the agreement between experts, and so using MTurk to cluster dictionary definitions appears to be a reliable approach. 1 Introduction  "
W10-0704 ", 15213 {qing,stephan.vogel}@cs.cmu.edu Abstract Word alignment is an important preprocessing step for machine translation. The project aims at incorporating manual alignments from Amazon Mechanical Turk (MTurk) to help improve word alignment quality. As a global crowdsourcing service, MTurk can provide flexible and abundant labor force and therefore reduce the cost of obtaining labels. An easyto-use interface is developed to simplify the labeling process. We compare the alignment results by Turkers to that by experts, and incorporate the alignments in a semi-supervised word alignment tool to improve the quality of the labels. We also compared two pricing strategies for word alignment task. Experimental results show high precision of the alignments provided by Turkers and the semi-supervised approach achieved 0.5% absolute reduction on alignment error rate. "
W10-0705 "Abstract We use Amazon Mechanical Turk to rate computer-generated reading comprehension questions about Wikipedia articles. Such application-specific ratings can be used to train statistical rankers to improve systems final output, or to evaluate technologies that generate natural language. We discuss the question rating scheme we developed, assess the quality of the ratings that we gathered through Amazon Mechanical Turk, and show evidence that these ratings can be used to improve question generation. "
W10-0706 "Abstract Mechanical Turk is useful for generating complex speech resources like conversational speech transcription. In this work, we explore the next step of eliciting narrations of Wikipedia articles to improve accessibility for low-literacy users. This task proves a useful test-bed to implement qualitative vetting of workers based on difficult to define metrics like narrative quality. Working with the Mechanical Turk API, we collected sample narrations, had other Turkers rate these samples and then granted access to full narration HITs depending on aggregate quality. While narrating full articles proved too onerous a task to be viable, using other Turkers to perform vetting was very successful. Elicitation is possible on Mechanical Turk, but it should conform to suggested best practices of simple tasks that can be completed in a streamlined workflow. "
W10-0708 "Educational Testing Service {KEvanini, DHiggins, KZechner}@ets.org Abstract This study investigates the use of Amazon Mechanical Turk for the transcription of nonnative speech. Multiple transcriptions were obtained from several distinct MTurk workers and were combined to produce merged transcriptions that had higher levels of agreement with a gold standard transcription than the individual transcriptions. Three different methods for merging transcriptions were compared across two types of responses (spontaneous and read-aloud). The results show that the merged MTurk transcriptions are as accurate as an individual expert transcriber for the readaloud responses, and are only slightly less accurate for the spontaneous responses. "
W10-0709 "2, USA {mdenkows,alavie}@cs.cmu.edu Abstract This paper discusses a machine translation evaluation task conducted using Amazon Mechanical Turk. We present a translation adequacy assessment task for untrained Arabicspeaking annotators and discuss several techniques for normalizing the resulting data. We present a novel 2-stage normalization technique shown to have the best performance on this task and further discuss the results of all techniques and the usability of the resulting adequacy scores. "
W10-0710 "5213, USA Abstract Corpus based approaches to machine translation (MT) rely on the availability of parallel corpora. In this paper we explore the effectiveness of Mechanical Turk for creating parallel corpora. We explore the task of sentence translation, both into and out of a language. We also perform preliminary experiments for the task of phrase translation, where ambiguous phrases are provided to the turker for translation in isolation and in the context of the sentence it originated from. "
W10-0711 "2, USA {mdenkows,hhaj,alavie}@cs.cmu.edu Abstract This paper describes a semi-automatic paraphrasing task for English-Arabic machine translation conducted using Amazon Mechanical Turk. The method for automatically extracting paraphrases is described, as are several human judgment tasks completed by Turkers. An ideal task type, revised specifically to address feedback from Turkers, is shown to be sophisticated enough to identify and filter problem Turkers while remaining simple enough for non-experts to complete. The results of this task are discussed along with the viability of using this data to combat data sparsity in MT. "
W10-0712 " Amazon's Mechanical Turk service has been successfully applied to many natural language processing tasks. However, the task of named entity recognition presents unique challenges. In a large annotation task involving over 20,000 emails, we demonstrate that a compet itive bonus system and interannotator agree ment can be used to improve the quality of named entity annotations from Mechanical Turk. We also build several statistical named entity recognition models trained with these annotations, which compare favorably to sim ilar models trained on expert annotations.  "
W10-0713 "Abstract We describe our experience using both Amazon Mechanical Turk (MTurk) and CrowdFlower to collect simple named entity annotations for Twitter status updates. Unlike most genres that have traditionally been the focus of named entity experiments, Twitter is far more informal and abbreviated. The collected annotations and annotation techniques will provide a first step towards the full study of named entity recognition in domains like Facebook and Twitter. We also briefly describe how to use MTurk to collect judgements on the quality of word clouds. "
W10-0714 " This paper presents findings on using crowdsourcing via Amazon Mechanical Turk (MTurk) to obtain Arabic nicknames as a contribution to exiting Named Entity (NE) lexicons. It demonstrates a strategy for increasing MTurk participation from Arab countries. The researchers validate the nicknames using experts, MTurk workers, and Google search and then compare them against the Database of Arabic Names (DAN). Additionally, the experiment looks at the effect of pay rate on speed of nickname collection and documents an advertising effect where MTurk workers respond to existing work batches, called Human Intelligence Tasks (HITs), more quickly once similar higher paying HITs are posted. 1  "
W10-0715 "Dept. of Computer Science, Johns Hopkins University Baltimore, MD 21218, USA {ozaidan,juri}@cs.jhu.edu Abstract We propose a framework for improving output quality of machine translation systems, by operating on the level of grammar rule features. Our framework aims to give a boost to grammar rules that appear in the derivations of translation candidates that are deemed to be of good quality, hence making those rules more preferable by the system. To that end, we ask human annotators on Amazon Mechanical Turk to compare translation candidates, and then interpret their preferences of one candidate over another as an implicit preference for one derivation over another, and therefore as an implicit preference for one or more grammar rules. Our framework also allows us to generalize these preferences to grammar rules corresponding to a previously unseen test set, namely rules for which no candidates have been judged. "
W10-0716 " Due to its complexity, meeting speech provides a challenge for both transcription and annotation. While Amazons Mechanical Turk (MTurk) has been shown to produce good results for some types of speech, its suitability for transcription and annotation of spontaneous speech has not been established. We find that MTurk can be used to produce highquality transcription and describe two techniques for doing so (voting and corrective). We also show that using a similar approach, high quality annotations useful for summarization systems can also be produced. In both cases, accuracy is comparable to that obtained using trained personnel. 1 Introduct  "
W10-0717 "In this work we present results from using Amazons Mechanical Turk (MTurk) to annotate translation lexicons between English and a large set of less commonly used languages. We generate candidate translations for 100 English words in each of 42 foreign languages using Wikipedia and a lexicon induction framework. We evaluate the MTurk annotations by using positive and negative control candidate translations. Additionally, we evaluate the annotations by adding pairs to our seed dictionaries, providing a feedback loop into the induction system. MTurk workers are more successful in annotating some languages than others and are not evenly distributed around the world or among the worlds languages. However, in general, we find that MTurk is a valuable resource for gathering cheap and simple annotations for most of the languages that we explored, and these annotations provide useful feedback in building a larger, more accurate lexicon. "
W10-0718 ", 177, planta 9 08018 Barcelona, Spain {bart.mellebeek|francesc.benavent|jens.grivolla|joan.codina| marta.ruiz|rafael.banchs}@barcelonamedia.org Abstract One of the major bottlenecks in the development of data-driven AI Systems is the cost of reliable human annotations. The recent advent of several crowdsourcing platforms such as Amazons Mechanical Turk, allowing requesters the access to affordable and rapid results of a global workforce, greatly facilitates the creation of massive training data. Most of the available studies on the effectiveness of crowdsourcing report on English data. We use Mechanical Turk annotations to train an Opinion Mining System to classify Spanish consumer comments. We design three different Human Intelligence Task (HIT) strategies and report high inter-annotator agreement between non-experts and expert annotators. We evaluate the advantages/drawbacks of each HIT design and show that, in our case, the use of non-expert annotations is a viable and costeffective alternative to expert annotations. "
W10-0719 "Abstract We present a compendium of recent and current projects that utilize crowdsourcing technologies for language studies, finding that the quality is comparable to controlled laboratory experiments, and in some cases superior. While crowdsourcing has primarily been used for annotation in recent language studies, the results here demonstrate that far richer data may be generated in a range of linguistic disciplines from semantics to psycholinguistics. For these, we report a number of successful methods for evaluating data quality in the absence of a correct response for any given data point. "
W10-0720 "Abstract Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Recent studies have found that while there are suggestive connections between topic models and the way humans interpret data, these two often disagree. In this paper, we explore this disagreement from the perspective of the learning process rather than the output. We present a novel task, tag-and-cluster, which asks subjects to simultaneously annotate documents and cluster those annotations. We use these annotations as a novel approach for constructing a topic model, grounded in human interpretations of documents. We demonstrate that these topic models have features which distinguish them from traditional topic models. "
W10-0721 "1-2302 {crashtc2, pyoung2, mhodosh2, juliahmr}@illinois.edu Abstract Crowd-sourcing approaches such as Amazons Mechanical Turk (MTurk) make it possible to annotate or collect large amounts of linguistic data at a relatively low cost and high speed. However, MTurk offers only limited control over who is allowed to particpate in a particular task. This is particularly problematic for tasks requiring free-form text entry. Unlike multiple-choice tasks there is no correct answer, and therefore control items for which the correct answer is known cannot be used. Furthermore, MTurk has no effective built-in mechanism to guarantee workers are proficient English writers. We describe our experience in creating corpora of images annotated with multiple one-sentence descriptions on MTurk and explore the effectiveness of different quality control strategies for collecting linguistic data using Mechanical MTurk. We find that the use of a qualification test provides the highest improvement of quality, whereas refining the annotations through follow-up tasks works rather poorly. Using our best setup, we construct two image corpora, totaling more than 40,000 descriptive captions for 9000 images. "
W10-0722 "Abstract We provide evidence that intrinsic evaluation of summaries using Amazons Mechanical Turk is quite difficult. Experiments mirroring evaluation at the Text Analysis Conferences summarization track show that nonexpert judges are not able to recover system rankings derived from experts. "
W10-0723 "Abstract This paper considers the linguistic indicators of bias in political text. We used Amazon Mechanical Turk judgments about sentences from American political blogs, asking annotators to indicate whether a sentence showed bias, and if so, in which political direction and through which word tokens. We also asked annotators questions about their own political views. We conducted a preliminary analysis of the data, exploring how different groups perceive bias in different blogs, and showing some lexical indicators strongly associated with perceived bias. "
W10-0724 " Efforts to automatically acquire world knowledge from text suffer from the lack of an easy means of evaluating the resulting knowledge. We describe initial experiments using Mechanical Turk to crowdsource evaluation to nonexperts for little cost, resulting in a collection of factoids with associated quality judgements. We describe the method of acquiring usable judgements from the public and the impact of such large-scale evaluation on the task of knowledge acquisition. 1 Int  "
W10-0725 "Abstract This paper describes our experiments of using Amazons Mechanical Turk to generate (counter-)facts from texts for certain namedentities. We give the human annotators a paragraph of text and a highlighted named-entity. They will write down several (counter-)facts about this named-entity in that context. The analysis of the results is performed by comparing the acquired data with the recognizing textual entailment (RTE) challenge dataset. "
W10-0726 " Human listeners can almost instantaneously judge whether or not another speaker is part of their speech community. The basis of this judgment is the speakers accent. Even though humans judge speech accents with ease, it has been tremendously difficult to automatically evaluate and rate accents in any consistent manner. This paper describes an experiment using the Amazon Mechanical Turk to develop an automatic speech accent rating dataset. 1  "
W10-0727 "stin {cgrady,ml}@ischool.utexas.edu Abstract We investigate human factors involved in designing effective Human Intelligence Tasks (HITs) for Amazons Mechanical Turk 1 . In particular, we assess document relevance to search queries via MTurk in order to evaluate search engine accuracy. Our study varies four human factors and measures resulting experimental outcomes of cost, time, and accuracy of the assessments. While results are largely inconclusive, we identify important obstacles encountered, lessons learned, related work, and interesting ideas for future investigation. Experimental data is also made publicly available for further study by the community 2 . "
W10-0728 " Amazons Mechanical Turk (MTurk) service is becoming increasingly popular in Natural Language Processing (NLP) research. In this paper, we report our findings in using MTurk to annotate medical text extracted from clinical trial descriptions with three entity types: medical condition, medication, and laboratory test. We compared MTurk annotations with a gold standard manually created by a domain expert. Based on the good performance results, we conclude that MTurk is a very promising tool for annotating large-scale corpora for biomedical NLP tasks. 1 Int  "
W10-0729 "Abstract To rapidly port speech applications to new languages one of the most difficult tasks is the initial collection of sufficient speech corpora. State-of-the-art automatic speech recognition systems are typical trained on hundreds of hours of speech data. While pre-existing corpora do exist for major languages, a sufficient amount of quality speech data is not available for most world languages. While previous works have focused on the collection of translations and the transcription of audio via Mechanical-Turk mechanisms, in this paper we introduce two tools which enable the collection of speech data remotely. We then compare the quality of audio collected from paid part-time staff and unsupervised volunteers, and determine that basic user training is critical to obtain usable data. "
W10-0730 "ge Park {nmadnani,jbg,resnik}@umiacs.umd.edu Abstract Hopper and Thompson (1980) defined a multi-axis theory of transitivity that goes beyond simple syntactic transitivity and captures how much action takes place in a sentence. Detecting these features requires a deep understanding of lexical semantics and real-world pragmatics. We propose two general approaches for creating a corpus of sentences labeled with respect to the Hopper-Thompson transitivity schema using Amazon Mechanical Turk. Both approaches assume no existing resources and incorporate all necessary annotation into a single system; this is done to allow for future generalization to other languages. The first task attempts to use languageneutral videos to elicit human-composed sentences with specified transitivity attributes. The second task uses an iterative process to first label the actors and objects in sentences and then annotate the sentences transitivity. We examine the success of these techniques and perform a preliminary classification of the transitivity of held-out data. Hopper and Thompson (1980) created a multi-axis theory of Transitivity 1 that describes the volition of the subject, the affectedness of the object, and the duration of the action. In short, this theory goes beyond the simple grammatical notion of transitivity (whether verbs take objects  transitive  or not  intransitive) and captures how much action takes place in a sentence. Such notions of Transitivity are not apparent from surface features alone; identical syntactic constructions can have vastly different Transitivity. This well-established linguistic theory, however, is not useful for real-world applications without a Transitivity-annotated corpus. Given such a substantive corpus, conventional machine learning techniques could help determine the Transitivity of verbs within sentences. Transitivity has been found to play a role in what is called syntactic framing, which expresses implicit sentiment (Greene and Resnik, 2009). 1 We use capital T to differentiate from conventional syntactic transitivity throughout the paper. In these contexts, the perspective or sentiment of the writer is reflected in the constructions used to express ideas. For example, a less Transitive construction might be used to deflect responsibility (e.g. John was killed vs. Benjamin killed John). In the rest of this paper, we review the HopperThompson transitivity schema and propose two relatively language-neutral methods to collect Transitivity ratings. The first asks humans to generate sentences with desired Transitivity characteristics. The second asks humans to rate sentences on dimensions from the HopperThompson schema. We then discuss the difficulties of collecting such linguistically deep data and analyze the available results. We then pilot an initial classifier on the Hopper-Thompson dimensions. "
W10-0731 " Amazon Mechanical Turk (MTurk) is a marketplace for so-called human intelligence tasks (HITs), or tasks that are easy for humans but currently difficult for automated processes. Providers upload tasks to MTurk which workers then complete. Natural language annotation is one such human intelligence task. In this paper, we investigate using MTurk to collect annotations for Subjectivity Word Sense Disambiguation (SWSD), a coarse-grained word sense disambiguation task. We investigate whether we can use MTurk to acquire good annotations with respect to gold-standard data, whether we can filter out low-quality workers (spammers), and whether there is a learning effect associated with repeatedly completing the same kind of task. While our results with respect to spammers are inconclusive, we are able to obtain high-quality annotations for the SWSD task. These results suggest a greater role for MTurk with respect to constructing a large scale SWSD system in the future, promising substantial improvement in subjectivity and sentiment analysis. "
W10-0732 ",adam.gerber@jhu.edu,mharper@umd.edu,mdredze@cs.jhu.edu Abstract We explore a new way to collect human annotated relations in text using Amazon Mechanical Turk. Given a knowledge base of relations and a corpus, we identify sentences which mention both an entity and an attribute that have some relation in the knowledge base. Each noisy sentence/relation pair is presented to multiple turkers, who are asked whether the sentence expresses the relation. We describe a design which encourages user efficiency and aids discovery of cheating. We also present results on inter-annotator agreement. "
W10-0733 "Abstract Building machine translation (MT) test sets is a relatively expensive task. As MT becomes increasingly desired for more and more language pairs and more and more domains, it becomes necessary to build test sets for each case. In this paper, we investigate using Amazons Mechanical Turk (MTurk) to make MT test sets cheaply. We find that MTurk can be used to make test sets much cheaper than professionally-produced test sets. More importantly, in experiments with multiple MT systems, we find that the MTurk-produced test sets yield essentially the same conclusions regarding system performance as the professionally-produced test sets yield. "
W10-0734 "Trento 2 Trento, Italy {negri,mehdad}@fbk.eu Abstract This paper reports on experiments in the creation of a bi-lingual Textual Entailment corpus, using non-experts workforce under strict cost and time limitations ($100, 10 days). To this aim workers have been hired for translation and validation tasks, through the CrowdFlower channel to Amazon Mechanical Turk. As a result, an accurate and reliable corpus of 426 English/Spanish entailment pairs has been produced in a more cost-effective way compared to other methods for the acquisition of translations based on crowdsourcing. Focusing on two orthogonal dimensions (i.e. reliability of annotations made by non experts, and overall corpus creation costs), we summarize the methodology we adopted, the achieved results, the main problems encountered, and the lessons learned. "
W10-1301 " We are building a tool that helps children with Complex Communication Needs 1 (CCN) to create stories about their day at school. The tool uses Natural Language Generation (NLG) technology to create a draft story based on sensor data of the childs activities, which the child can edit. This work is still in its early stages, but we believe it has great potential to support interactive personal narrative which is not well supported by current Augmentative and Alternative Communication (AAC) tools.  "
W10-1302 " We detail the design, development and evaluation of Augmentative and Alternative Communication (AAC) software which encourages rapid conversational interaction. The system uses Natural Language Generation (NLG) technology to automatically generate conversational utterances from a domain knowledge base modelled from content suggested by a small AAC user group. Findings from this work are presented along with a discussion about how NLG might be successfully applied to conversational AAC systems in the future. 1 Int  "
W10-1303 " Utterance-based AAC systems have the potential to significantly speed communication rate for someone who relies on a speech generating device for communication. At the same time, such systems pose interesting challenges including anticipating text needs, remembering what text is stored, and accessing desired text when needed. Moreover, using such systems has profound pragmatic implications as a prestored message may or may not capture exactly what the user wishes to say in a particular discourse situation. In this paper we describe a prototype of an utterance-based AAC system whose design choices are driven by findings from theoretically driven studies concerning pragmatic choices with which the user of such a system is faced. These findings are coupled with cognitive theories to make choices for system design. 1 Introdu  "
W10-1304 "Center for Spoken Language Understanding  Child Development & Rehabilitation Center Oregon Health & Science University {roark,jacques}@cslu.ogi.edu {gibbons,mfo}@ohsu.edu Abstract We present preliminary experiments of a binary-switch, static-grid typing interface making use of varying language model contributions. Our motivation is to quantify the degree to which language models can make the simplest scanning interfaces  such as showing one symbol at a time rather than a scanning a grid  competitive in terms of typing speed. We present a grid scanning method making use of optimal Huffman binary codes, and demonstrate the impact of higher order language models on its performance. We also investigate the scanning methods of highlighting just one cell in a grid at any given time or showing one symbol at a time without a grid, and show that they yield commensurate performance when using higher order n-gram models, mainly due to lower error rate and a lower rate of missed targets. "
W10-1305 " The use of speech production data has been limited by a steep learning curve and the need for laborious hand measurement. We are building a tool set that provides summary statistics for measures designed by clinicians to screen, diagnose or provide training to assistive technology users. This will be achieved by extending an existing shareware software platform with plug-ins that perform specific measures and report results to the user. The common underlying basis for this tool set is a Stevens paradigm of landmarks, points in an utterance around which information about articulatory events can be extracted. 1 Int  "
W10-1306 "<NoAbstract>"
W10-1307 "Abstract Spoken dialogue systems typically do not manage the communication channel, instead using fixed values for such features as the amplitude and speaking rate. Yet, the quality of a dialogue can be compromised if the user has difficulty understanding the system. In this proof-of-concept research, we explore using reinforcement learning (RL) to create policies that manage the communication channel to meet the needs of diverse users. Towards this end, we first formalize a preliminary communication channel model, in which users provide explicit feedback regarding issues with the communication channel, and the system implicitly alters its amplitude to accommodate the users optimal volume. Second, we explore whether RL is an appropriate tool for creating communication channel management strategies, comparing two different hand-crafted policies to policies trained using both a dialogue-length and a novel annoyance cost. The learned policies performed better than hand-crafted policies, with those trained using the annoyance cost learning an equitable tradeoff between users with differing needs and also learning to balance finding a users optimal amplitude against dialoguelength. These results suggest that RL can be used to create effective communication channel management policies for diverse users. Index Terms: communication channel, spoken dialogue systems, reinforcement learning, amplitude, diverse users 1 Introduction Both Spoken Dialog Systems (SDS) and Assistive Technology (AT) tend to have a narrow focus, supporting only a subset of the population. SDS typically aim to support the average man, ignoring wide variations in potential users ability to hear and understand the system. AT aims to support people with a recognized disability, but doesnt support those whose impairment is not severe enough to warrant the available devices or services, or those who are unaware or have not acknowledged that they need assistance. However, SDS should be able to meet the needs of users whose abilities fall within, and between, the extremes of severly impaired and perfectly abled. When aiming to support users with widely differing abilities, the cause of a users difficulty is less important than adapting the communication channel in a manner that aids understanding. For example, speech that is presented more loudly and slowly can help a hearing-impaired elderly person understand the system, and can also help a person with no hearing loss who is driving in a noisy car. Although one users difficulty is due to impairment and the other due to an adverse environment, a similar adaptation may be appropriate to both. During human-human communication, speakers manage the communication channel; implicitly altering their manner of speech to increase the likelihood of being understood while concurrently economizing effort (Lindblom, 1990). In addition to these implicit actions, speakers also make statements referring to breakdowns in the communication chan53 nel, explicitly pointing out potential problems or corrections, (e.g. Could you please speak up?) (Jurafsky et al., 1997). As for human-computer dialogue, SDS are prone to misrecognition of users spoken utterances. Much research has focused on developing techniques for overcoming or avoiding system misunderstandings. Yet, as the quality of automatic speech recognition improves and SDS are deployed to diverse populations and in varied environments, systems will need to better attend to possible human misunderstandings. Future SDS will need to manage the communication channel, in addition to managing the task, to aid in avoiding these misunderstandings. Researchers have explored the use of reinforcement learning (RL) to create dialogue policies that balance and optimize measures of task success (e.g., see (Scheffler and Young, 2002; Levin et al., 2000; Henderson et al., 2008; Walker, 2000)). Along these lines, RL is potentially well suited to creating policies for the subtask of managing the communication channel, as it can learn to adapt to the user while continuing the dialogue. In doing so, RL may choose actions that appear costly at the time, but lead to better overall dialogues. Our long term goal is to learn how to manage the communication channel along with the task, moving away from just what to say and also focusing on how to say it. For this proof-of-concept, our goals are twofold: 1) to formalize a communication channel model that encompasses diverse users, initially focusing just on explicit user actions and implicit system actions, and 2) to determine whether RL is an appropriate tool for learning an effective communication channel management strategy for diverse users. To explore the above issues, we use a simple communication channel model in which the system needs to determine and maintain an amplitude level that is pleasant and effective for users with differing amplitude preferences and needs. As our goal includes decreasing the amount of potentially annoying utterances (i.e., those in which the systems amplitude setting is in discord with the users optimal amplitude), we introduce a user-centric cost metric, which we have termed annoyance cost. We then compare hand-crafted policies against policies trained using both annoyance and more traditional dialogue-length cost components. "
W10-1308 "Existing Augmentative and Alternative Communication vocabularies assign multimodal stimuli to words with multiple meanings. The ambiguity hampers the vocabulary effectiveness when used by people with language disabilities. For example, the noun a missing letter may refer to a character or a written message, and each corresponds to a different picture. A vocabulary with images and sounds unambiguously linked to words can better eliminate misunderstanding and assist communication for people with language disorders. We explore a new approach of creating such a vocabulary via automatically assigning semantically unambiguous groups of synonyms to sound and image labels. We propose an unsupervised word sense disambiguation (WSD) voting algorithm, which combines different semantic relatedness measures. Our voting algorithm achieved over 80% accuracy with a sound label dataset, which significantly outperforms WSD with individual measures. We also explore the use of human judgments of evocation between members of concept pairs, in the label disambiguation task. Results show that evocation achieves similar performance to most of the existing relatedness measures. "
W10-1309 "<NoAbstract>"
W10-1310 "Abstract This paper describes the results of our experiments in building speaker-adaptive recognizers for talkers with spastic dysarthria. We study two modifications  (a) MAP adaptation of speaker-independent systems trained on normal speech and, (b) using a transition probability matrix that is a linear interpolation between fully ergodic and (exclusively) leftto-right structures, for both speaker-dependent and speaker-adapted systems. The experiments indicate that (1) for speaker-dependent systems, left-to-right HMMs have lower word error rate than transition-interpolated HMMs, (2) adapting all parameters other than transition probabilities results in the highest recognition accuracy compared to adapting any subset of these parameters or adapting all parameters including transition probabilities, (3) performing both transition-interpolation and adaptation gives higher word error rate than performing adaptation alone and, (4) dysarthria severity is not a sufficient indicator of the relative performance of speakerdependent and speaker-adapted systems. "
W10-1311 "Abstract Modern automatic speech recognition is ineffective at understanding relatively unintelligible speech caused by neuro-motor disabilities collectively called dysarthria. Since dysarthria is primarily an articulatory phenomenon, we are collecting a database of vocal tract measurements during speech of individuals with cerebral palsy. In this paper, we demonstrate that articulatory knowledge can remove ambiguities in the acoustics of dysarthric speakers by reducing entropy relatively by 18.3%, on average. Furthermore, we demonstrate that dysarthric speech is more precisely portrayed as a noisy-channel distortion of an abstract representation of articulatory goals, rather than as a distortion of non-dysarthric speech. We discuss what implications these results have for our ongoing development of speech systems for dysarthric speakers. "
W10-1312 "<NoAbstract>"
W10-1401 " The term Morphologically Rich Languages (MRLs) refers to languages in which significant information concerning syntactic units and relations is expressed at word-level. There is ample evidence that the application of readily available statistical parsing models to such languages is susceptible to serious performance degradation. The first workshop on statistical parsing of MRLs hosts a variety of contributions which show that despite languagespecific idiosyncrasies, the problems associated with parsing MRLs cut across languages and parsing frameworks. In this paper we review the current state-of-affairs with respect to parsing MRLs and point out central challenges. We synthesize the contributions of researchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. The overarching analysis suggests itself as a source of directions for future investigations. "
W10-1402 "<NoAbstract>"
W10-1403 "<NoAbstract>"
W10-1404 "<NoAbstract>"
W10-1405 "erdam {r.tsarfaty,k.simaan}@uva.nl Abstract We show that na  ve modeling of morphosyntactic agreement in a Constituency-Based (CB) statistical parsing model is worse than none, whereas a linguistically adequate way of modeling inflectional morphology in CB parsing leads to improved performance. In particular, we show that an extension of the Relational-Realizational (RR) model that incorporates agreement features is superior to CB models that treat morphosyntax as statesplits (SP), and that the RR model benefits more from inflectional features. We focus on parsing Hebrew and report the best result to date, F 1 84.13 for parsing off of gold-tagged text, 5% error reduction from previous results. "
W10-1406 "<NoAbstract>"
W10-1407 " Discontinuities occur especially frequently in languages with a relatively free word order, such as German. Generally , due to the longdistance dependencies they induce, they lie beyond the expressivity of Probabilistic CFG, i.e., they cannot be directly reconstructed by a PCFG parser. In this paper, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), a formalism with high expressivity, to directly parse the German NeGra and TIGER treebanks. In both treebanks, discontinuities are annotated with crossing branches. Based on an evaluation using different metrics, we show that an output quality can be achieved which is comparable to the output quality of PCFG-based systems. 1 I  "
W10-1408 " This paper presents a study of the impact of using simple and complex morphological clues to improve the classification of rare and unknown words for parsing. We compare this approach to a language-independent technique often used in parsers which is based solely on word frequencies. This study is applied to three languages that exhibit different levels of morphological expressiveness: Arabic, French and English. We integrate information about Arabic affixes and morphotactics into a PCFG-LA parser and obtain stateof-the-art accuracy. We also show that these morphological clues can be learnt automatically from an annotated corpus. 1 Introductio  "
W10-1409 " We present and discuss experiments in statistical parsing of French, where terminal forms used during training and parsing are replaced by more general symbols, particularly clusters of words obtained through unsupervised linear clustering. We build on the work of Candito and Crabbe (2009) who proposed to use clusters built over slightly coarsened French inflected forms. We investigate the alternative method of building clusters over lemma/part-of-speech pairs, using a raw corpus automatically tagged and lemmatized. We find that both methods lead to comparable improvement over the baseline (we obtain F 1 =86.20% and F 1 =86.21% respectively, compared to a baseline of F 1 =84.10%). Yet, when we replace gold lemma/POS pairs with their corresponding cluster, we obtain an upper bound (F 1 =87.80) that suggests room for improvement for this technique, should tagging/lemmatisation performance increase for French. We also analyze the improvement in performance for both techniques with respect to word frequency. We find that replacing word forms with clusters improves attachment performance for words that are originally either unknown or low-frequency, since these words are replaced by cluster symbols that tend to have higher frequencies. Furthermore, clustering also helps significantly for medium to high frequency words, suggesting that training on word clusters leads to better probability estimates for these words. "
W10-1410 " This paper shows that training a lexicalized parser on a lemmatized morphologically-rich treebank such as the French Treebank slightly improves parsing results. We also show that lemmatizing a similar in size subset of the English Penn Treebank has almost no effect on parsing performance with gold lemmas and leads to a small drop of performance when automatically assigned lemmas and POS tags are used. This highlights two facts: (i) lemmatization helps to reduce lexicon data-sparseness issues for French, (ii) it also makes the parsing process sensitive to correct assignment of POS tags to unknown words. 1 Intro  "
W10-1411 " This paper analyzes the relative importance of different linguistic features for data-driven dependency parsing of Hindi, using a feature pool derived from two state-of-the-art parsers. The analysis shows that the greatest gain in accuracy comes from the addition of morphosyntactic features related to case, tense, aspect and modality. Combining features from the two parsers, we achieve a labeled attachment score of 76.5%, which is 2 percentage points better than the previous state of the art. We finally provide a detailed error analysis and suggest possible improvements to the parsing scheme. 1 I  "
W10-1801 " The exponential growth of the subjective information in the framework of the Web 2.0 has led to the need to create Natural Language Processing tools able to analyse and process such data for multiple practical applications. They require training on specifically annotated corpora, whose level of detail must be fine enough to capture the phenomena involved. This paper presents EmotiBlog  a finegrained annotation scheme for subjectivity. We show the manner in which it is built and demonstrate the benefits it brings to the systems using it for training, through the experiments we carried out on opinion mining and emotion detection. We employ corpora of different textual genres a set of annotated reported speech extracted from news articles, the set of news titles annotated with polarity and emotion from the SemEval 2007 (Task 14) and ISEAR, a corpus of real-life selfexpressed emotion. We also show how the model built from the EmotiBlog annotations can be enhanced with external resources. The results demonstrate that EmotiBlog, through its structure and annotation paradigm, offers high quality training data for systems dealing both with opinion mining, as well as emotion detection. "
W10-1802 " The paper describes a learner corpus of Czech, currently under development. The corpus captures Czech as used by nonnative speakers. We discuss its structure, the layered annotation of errors and the annotation process. 1 I  "
W10-1803 "CCLS Columbia University New York, U.S.A. rambow@ccls.columbia.edu Rebecca J. Passonneau CCLS Columbia University New York, U.S.A. becky@cs.columbia.edu Abstract We are interested in extracting social networks from text. We present a novel annotation scheme for a new type of event, called social event, in which two people participate such that at least one of them is cognizant of the other. We compare our scheme in detail to the ACE scheme. We perform a detailed analysis of interannotator agreement, which shows that our annotations are reliable. "
W10-1804 "Abstract This paper describes work testing agile data annotation by moving away from the traditional, linear phases of corpus creation towards iterative ones and by recognizing the potential for sources of error occurring throughout the annotation process. "
W10-1805 "Abstract This paper explores ways to detect errors in aligned corpora, using very little technology. In the first method, applicable to any aligned corpus, we consider alignment as a string-to-string mapping. Treating the target string as a label, we examine each source string to find inconsistencies in alignment. Despite setting up the problem on a par with grammatical annotation, we demonstrate crucial differences in sorting errors from legitimate variations. The second method examines phrase nodes which are predicted to be aligned, based on the alignment of their yields. Both methods are effective in complementary ways. "
W10-1806 "Manual annotation of natural language to capture linguistic information is essential for NLP tasks involving supervised machine learning of semantic knowledge. Judgements of meaning can be more or less subjective, in which case instead of a single correct label, the labels assigned might vary among annotators based on the annotators knowledge, age, gender, intuitions, background, and so on. We introduce a framework Anveshan, where we investigate annotator behavior to find outliers, cluster annotators by behavior, and identify confusable labels. We also investigate the effectiveness of using trained annotators versus a larger number of untrained annotators on a word sense annotation task. The annotation data comes from a word sense disambiguation task for polysemous words, annotated by both trained annotators and untrained annotators from Amazons Mechanical turk. Our results show that Anveshan is effective in uncovering patterns in annotator behavior, and we also show that trained annotators are superior to a larger number of untrained annotators for this task. 1 Credits This work was supported by a research supplement to the National Science Foundation CRI award 0708952. 2 Introduction Manual annotation of language data in order to capture linguistic knowledge has become increasingly important for semantic and pragmatic annotation tasks. A very short list of a few such tasks illustrates the range of types of annotation, in varying stages of development: predicate argument structure (Palmer et al., 2005b), dialogue acts (Hu et al., 2009), discourse structure (Carbone et al., 2004), opinion (Wiebe and Cardie, 2005), emotion (Alm et al., 2005). The number of efforts to create corpus resources that include manual annotations has also been growing. A common approach in assessing the resulting manual annotations is to report a single quantitative measure reflecting the quality of the annotations, either a summary statistic such as percent agreement, or an agreement coefficient from the family of metrics that include Krippendorffs alpha (Krippendorff, 1980) and Cohens kappa (Cohen, 1960). We present some new assessment methods to use in combination with an agreement coefficient for understanding annotator behavior when there are multiple annotators and many annotation values. Anveshan (Annotation Variance Estimation) 1 is a suite of procedures for analyzing patterns of agreement and disagreement among annotators, as well as the distributions of annotation values across annotators. Anveshan thus makes it possible to explore annotator behavior in more detail. Currently, it includes three types of analysis: interannotator agreement (IA) among all subsets of annotators, leverage of annotation values for outlier detection, and metrics for comparing annotators distributions of annotation values (e.g., KullbachLiebler divergence). As an illustration of the utility of Anveshan, we compare two groups of annotators on the same annotation word sense annotation tasks: a half dozen trained annotators and fourteen Mechanical Turkers. Previous work has argued that it can be cost effective to collect multiple labels from untrained labelers at a low cost per label, and to combine the multiple labels through a voting method, rather than to collect single labels from highly trained l 1 Anveshan is a Sanskrit word which literally means search or exploration. 47 belers (Snow et al., 2008; Sheng et al., 2008; Lam and Stork, 2003). The tasks included in (Snow et al., 2008), for example, include word sense annotation; in contrast to our case, where the average number of senses per word is 9.5, the one word sense annotation task had three senses. We find that the same half dozen trained annotators can agree well or not on sense labels for polysemous words. When they agree less well, we find that it is possible to distinguish between problems in the labels (e.g., confusable senses) and systematic differences of interpretation among annotators. When we use twice the number of Mechanical Turkers as trained annotators for three of our ten polysemous words, we find inconsistent results. The next section of the paper presents the motivation for Anveshan and its relevance to the word sense annotation task, followed by a section on related work. The word sense annotation data is given in section 5. Anveshan is described in the subsequent section, followed by the results of its application to the two data sets. We discuss the comparison of trained annotators and Mechanical Turkers, as well as differences among words, in section 7. Section 7 concludes with a short recap of Anveshan in general, and its application to word sense annotations in particular. 3 Beyond Interannotator Agreement (IA) Assessing the reliability of an annotation typically addresses the question of whether different annotators (effectively) assign the same annotation labels. Various measures can be used to compare different annotators, including agreement coefficients such as Krippendorffs alpha (Krippendorff, 1980). Extensive reviews of the properties of such coefficients have been presented elsewhere, e.g., (Artstein and Poesio, 2008). Briefly, an agreement produce values in the interval [-1,1] indicating how much of the observed agreement is above (or below) agreement that would be predicted by chance (value of 0). To measure reliability in this way is to assume that for most of the instances in the data, there is a single correct response. Here we present the use of reliability metrics and other measures for word sense annotation, and we assume that in some cases there may not be a single correct response. When annotators have less than excellent agreement, we aim to examine possible causes. We take word sense to be a problematic annotation to perform, thus requiring a deeper understanding of the conditions under which annotators might disagree. The many reasons can only be touched on here. For example, word senses are not discrete, atomic units that can be delimited and enumerated. While dictionaries and other lexical resoures, such as WordNet (Miller et al., 1993) or the Hector lexicon (cf. SENSEVAL-1 (Kilgarriff and Palmer, 2000)), do provide enumerations of the senses for a given word, and their interrelations (e.g., a list of senses, a tree of senses), it is widely agreed that this is a convenient abstraction, if for no other reason than the fact that words shift meanings along with the communicative needs of the groups of individuals who use them. The context in which a word is used plays a significant role in restricting the current sense. As a result, it is often argued that the best representation for word meaning would consist in clustering the contexts in which words are used (Kilgarriff, 1997). Yet even this would be insufficient because new communities arise, new behaviors and artifacts emerge along with them, hence new contexts of use and new clusters. At the same time, contexts of use and the senses that go along with them can fade away (cf. the use of handbag discussed in (Kilgarriff, 1997) pertaining to disco dancing). Because an enumeration of word senses is somewhat artificial, annotators might disagree on word senses because they disagree on the boundaries between one sense and another, just as professional lexicographers do. Apart from the artificiality of creating flat or hierarchical sense inventories, the meanings of words can vary in their subjectivity, due to differences in the perception or experience of individuals. This can be true for word senses that are inherently relative, such as cold (as in, turn up the thermostat, its too cold in here); or that derive their meaning from cultural norms that may differ from community to community, such as justice; or that change as one grows older, e.g., whether a long time to wait pertains to hours versus days. Despite the arguments against using word sense inventories, until they are replaced with an equally convenient and more representative abstraction, they are an extremely convenient computational representation. We rely on WordNet senses, which are presented to annotators with a gloss (definition) and with example uses. In order to better un48 derstand reasons for disagreement on senses, we collect labels from multiple annotators. When annotators agree, having multiple annotators is redundant. But when annotators disagree, having multiple annotators is necessary in order to determine whether the disagreement is due to noise based on insufficiently clear sense definitions versus a systematic difference between individuals, e.g., those who see a glass as half empty where others see it as half full. To insure the opportunity to observe how varied the labeling of a single word can be, we collect word sense annotations from multiple annotators. One potential benefit of such investigation might be a better understanding of how to model word meaning. In sum, we hypothesize the following cases:  Outliers: A small proportion of annotators may assign senses in a manner that differs markedly from the remaining annotators.  Confusability of senses: If multiple annotators assign multiple senses in an apparently random fashion, it may be that the senses are not sufficiently distinct.  Systematic differences among subsets of annotators: If the same 50% of annotators always pick sense X where the remaining annotators always pick sense Y, it may be that properties of the annotators, such as their age cohort, account for the disagreement. 4 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V  eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, depending on whether the disagreement is due to noise or systematic behavior. Noise has relatively little impact compared with systematic disagreements. In (Passonneau et al., 2008), a similar lack of correlation between interannotator agreement and machine learning performance is found in an empirical investigation. 5 Word Sense Annotation Data 5.1 Trained Annotator data The Manually Annotated Sub-Corpus (MASC) project (Ide et al., 2010) is creating a small, representative corpus of American English written and spoken texts drawn from the Open American National Corpus (OANC). 2 The MASC corpus includes hand-validated or manual annotations for a variety of linguistic phenomena. The first MASC release, available as of May 2010, consists of 82K words. 3 One of the goals of MASC is to support efforts to harmonize WordNet (Miller et al., 1993) and FrameNet (Ruppenhofer et al., 2006), in order to bring the sense distinctions each makes into better alignment. We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part of speech, with the number of WordNet senses, the number of senses used by the trained annotators (TAs), the number of annotators, and Alpha. We call this the Trained annotator (TA) data. We find that interannotator agreement (IA) among half a dozen annotators varies depending on the word. For ten words nearly balanced with 2 http://www.anc.org 3 http://www.anc.org/MASC/Home.html  http://www.anc.org/MASC/Home.html 49 Senses Word-pos Avail. Used Ann Alpha long-j 9 4 6 0.67 fair-j 10 6 5 0.54 quiet-j 6 5 6 0.49 time-n 10 8 5 0.68 work-n 7 7 5 0.62 land-n 11 9 6 0.49 show-v 12 10 5 0.46 tell-v 8 8 6 0.46 know-v 11 10 5 0.37 say-v 11 10 6 0.37 Table 1: Interannotator agreement on ten polysemous words: three adjectives, three nouns and four verbs among trained annotators respect to part of speech, we find a range of about 0.50 to 0.70 for nouns and adjectives, and about 0.37 to 0.46 for verbs. Table 1 shows the ten words and the alpha scores for the same five or six annotators. The layout of the table illustrates both that verbs have lower agreement than adjectives or nouns, and that within each part of speech, annotators achieve varying levels of agreement, depending on the word. The annotators, their level of training, the number of sense choices, the annotation tool, and other factors remain constant from word to word. Thus we hypothesize that the differences in IA reflect differences in the degree of subjectivity of the sense choices, the sense similarity, or both. Anveshan is a data exploration framework to help understand the differences in the ability of the same annotators to agree well on sense annotation for some words and not others. As shown, annotators achieve respectable agreement on long, time and work, and lower agreement on the remaining words. Verbs have lower agreement overall. Figure 1 shows WordNet senses for long in the form displayed to annotators, who used an annotation GUI developed in Java. The sense number appears in the first column, followed by the glosses, then sample phrases; only three senses are shown, to conserve space. Note that annotators did not see the WordNet synsets (sets of synonymous words) for a given sense. 5.2 Mechanical Turk data Amazons Mechanical Turk is a crowd-sourcing marketplace where Human Intelligence Tasks Senses Word-pos Avail. Used Ann Alpha long-j 9 9 14 0.15 fair-j 10 10 14 0.25 quiet-j 6 6 15 0.08 Table 2: Interannotator agreement on adjectives among Mechanical Turk annotators (HITs) such as sense annotation for words in a sentence, can be set up and results from a large number of annotators (or turkers) can be obtained quickly. We used Mechanical Turk to obtain annotations from 14 annotators on the set of adjectives to analyze IA for a larger set of untrained annotators. The task was set up to get 150 occurrences annotated for each of the three adjectives: fair, long and quiet, by 14 mechanical turk annotators each. 100 of these occurrences were the same as those done by the trained annotators. For each word, the 150 instances were divided into 15 HITs of 10 instances each. The average submit time of a HIT was 200 seconds. We report the IA among the Mechanical Turk annotators using Krippendorffs Alpha in Table 2. As shown, the turkers have poor agreement, particularly on long and quiet, which is at the chance level. 6 Anveshan Anveshan: Annotation Variance Estimation, is our approach to perform a more subtle analysis of inter-annotator agreement. Anveshan uses simple statistical methods to achieve the three goals identified in section 3: outlier detection, confusable senses, and distinct subsets of annotators that agree with each other. 6.1 Method This section uses the following notation to explain Anveshans methodology: We assume that we have n annotators annotating m senses. The probability of annotator a using sense s i is given by P a (S = s i ) = count(s i , a) m j=1 count(s j , a) where, count(s i , a) is number of times s i was used by a. 50 1 primarily temporal sense; being or indicating a relatively great or greater than average duration or passage of time or a duration as specified: a long life; a long boring speech; a long time; a long friendship; a long game; long ago; an hour long 2 primarily spatial sense; of relatively great or greater than average spatial extension or extension as specified: a long road; a long distance; contained many long words; ten miles long 3 of relatively great height: a race of long gaunt men (Sherwood Anderson); looked out the long French windows Figure 1: Three of the WordNet senses for Long Anveshan uses the Kullbach-Liebler divergence (KLD), Jensen-Shannon divergence (JSD) and Leverage to compare probability distributions. The KLD of two probability distributions P and Q is given by: KLD(P, Q) = i P (i) log P (i) Q(i) JSD is a modified version of KLD, it is also known as total divergence to the average, and is given by: JSD(P, Q) = 1 2 KLD(P, M ) + 1 2 KLD(Q, M ) where M = (P + Q)/2 We define Leverage Lev of probability distribution P over Q as: Lev(P, Q) = k |P (k)  Q(k)| We now compute the following statistics:  For each annotator a i , we compute P a i .  We compute P avg , which is ( i P a i )/n.  We compute Lev(P a i , P avg ), i  Then we compute JSD(P a i , P a j ) (i, j), where i, j  n and i = j  Lastly, we compute a distance measure for each annotator, by computing the KLD between each annotator and the average of the remaining annotators, i.e. we get i, D a i = KLD(P a i , Q), where Q = ( j =i P a j )/(n  1) These statistics give us a deeper understanding of annotator behavior. Looking at the sense usage probabilities, we can identify how frequently senses are used by an annotator. We can see how much an annotator deviates from the average sense 0\t\r   0.2\t\r   0.4\t\r   0.6\t\r   0.8\t\r   1\t\r   A107\t\r   A101\t\r   A103\t\r   A102\t\r   A105\t\r   A108 Figure 2: Distance measure (KLD) for Annotators of long in TA Data 0\t\r   0.2\t\r   0.4\t\r   0.6\t\r   0.8\t\r   A101\t\r   A102\t\r   A103\t\r   A105\t\r   A107\t\r   A108\t\r   101\t\r   102\t\r   999\t\r   103\t\r   108\t\r   Figure 3: Sense Usage distribution for long by annotators in TA Data usage distribution by looking at Leverage. JSD between two annotators gives us a measure of how close they are to each other. KLD of an annotator with the remaining annotators shows us how different the annotator is from the rest. In the following section we show results, which illustrate the effectiveness of Anveshan in identifying useful patterns in the data from the trained annotators (TAs) and Mechanical Turkers (MTs). 6.2 Results We used Anveshan on all data from TAs and MTs. We were successful in correctly identifying outliers on many words. Also, analyzing the sense usage patterns and observing the JSD and KLD scores gave us useful insights on annotator differences. In the figures for this section, the six TAs are represented by their unique identifiers (A101, A102, A103, A105, A107, A108). Word senses are identified by adding 100 to the WordNet sense 51 Word Old Alpha Ann Dropped New Alpha long 0.67 1 0.80 land 0.49 1 0.54 know 0.377 1 0.48 tell 0.45 2 0.52 say 0.37 2 0.44 fair 0.54 2 0.63 Table 3: Increase in IA score by dropping annotators (TA Data) 0\t\r   0.05\t\r   0.1\t\r   0.15\t\r   0.2\t\r   0.25\t\r   105\t\r   102\t\r   104\t\r   103\t\r   101\t\r   999\t\r   108\t\r   106\t\r   107\t\r   110\t\r   109\t\r   A102\t\r   A105 Figure 4: Sense usage patterns of annotators 102 and 105 for show in TA Data number. An additional None of the Above label is represented as 999; annotators select this when no sense applies, when the word occurs as part of a large lexical unit (collocation) with a clearly distinct meaning, or when the sentence is not a correct example for other reasons (e.g., wrong part of speech). Figure 2 shows the distance measure (KLD) for each annotator from the rest of the annotators for the word long with respect to the probability for each of the four senses used (cf. Table 1). It can be clearly seen that annotator A108 is an outlier. A108 differs in her excessive use of label 999, as shown in Figure 3. Indeed, by dropping A108, we see that the IA score (Alpha) jumps from 0.67 to 0.8 for long. Similar results were obtained for annotations for other words as well. Table 3 shows the jump in IA score after outlier(s) were dropped. Anveshan helps us differentiate between noisy disagreement versus systematic disagreement. The word show with 5 annotators has a low agreement score of 0.45. By looking at the sense distributions for the various annotators, and observing annotation preferences for each annotator, we can see that annotators A102 and A105 have similar behavior (Figure 4, with a pairwise alpha of 0.52 versus 0.46 for all five 0\t\r   0.05\t\r   0.1\t\r   0.15\t\r   0.2\t\r   0.25\t\r   0.3\t\r   105\t\r   102\t\r   104\t\r   103\t\r   101\t\r   999\t\r   108\t\r   106\t\r   107\t\r   110\t\r   109\t\r   A107\t\r   A108 Figure 5: Sense usage patterns of annotators 107 and 108 for show in TA Data 0\t\r   0.05\t\r   0.1\t\r   0.15\t\r   0.2\t\r   0.25\t\r   0.3\t\r   0.35\t\r   105\t\r   102\t\r   104\t\r   103\t\r   101\t\r   999\t\r   108\t\r   106\t\r   107\t\r   110\t\r   109\t\r   Overall\t\r   A101 Figure 6: Sense usage distribution of annotator 101 vs. the average of all annotators for show in TA Data annotators), and annotators A107 and A108 have similar behavior (Figure 5, with a pairwise alpha of 0.53). In contrast, Annotator A101 has very distinct preferences (Figure 6). This behavior is captured by computing JSD scores among all pairs of annotators. As can be seen in Figure 7, the pairs A102-A105 and A107-A108 have very low JSD values, indicating similarity in annotator behavior. At the same time we also see the pairs having A101 in them have a much higher JSD score, which is attributed to the fact that A101 is different from everyone else. If we look at corresponding Alpha scores, we see that pairs having low JSD values have higher agreement scores and vice versa. Observing the sense usage distributions also helps us identify confusable senses. For example, Figure 8 shows us the differences in sense usage patterns of A101, A103 and the average of all annotators for the word say. We can see that A101 and A103 deviate in distinct ways from the average. A101 prefers sense 101 whereas A103 prefers sense 102. This indicates that sense 101 and 102 might be confusable. Sense 1 is given as expressing words; sense 2 as report or maintain. 52 0\t\r   0.1\t\r   0.2\t\r   0.3\t\r   0.4\t\r   0.5\t\r   0.6\t\r   A105\t\r   A108\t\r   A105\t\r   A102\t\r   A107\t\r   A108\t\r   A102\t\r   A107\t\r   A101\t\r   A101\t\r   A101\t\r   A101\t\r   JSD\t\r   Alpha\t\r   Figure 7: JSD and Alpha scores for pairs of annotators for show in TA Data 0\t\r   0.1\t\r   0.2\t\r   0.3\t\r   0.4\t\r   0.5\t\r   0.6\t\r   Overall\t\r   A101\t\r   A103\t\r   102\t\r   101\t\r   108\t\r   103 Figure 8: Sense usage distribution for say in TA Data for annotators 101 and 103 0\t\r   0.05\t\r   0.1\t\r   0.15\t\r   0.2\t\r   0.25\t\r   0.3\t\r   0.35\t\r   A102\t\r   A105\t\r   A108\t\r   A101\t\r   A107 Figure 9: Distance measure (KLD) for annotators of work in TA Data 0\t\r   0.2\t\r   0.4\t\r   0.6\t\r   0.8\t\r   1\t\r   1.2\t\r   Overall\t\r   \t\r   \t\r   A101\t\r   \t\r   \t\r   A102\t\r   \t\r   \t\r   A104\t\r   \t\r   \t\r   A107\t\r   \t\r   \t\r   A108\t\r   \t\r   \t\r   A111\t\r   \t\r   \t\r   A112\t\r   \t\r   \t\r   A115\t\r   \t\r   \t\r   A116\t\r   \t\r   \t\r   A117\t\r   \t\r   \t\r   A118\t\r   \t\r   \t\r   A119\t\r   \t\r   \t\r   A120\t\r   \t\r   \t\r   A121\t\r   106\t\r   109\t\r   999\t\r   108\t\r   103\t\r   102\t\r   101 Figure 10: Sense usage distribution among MTs for long 0\t\r   0.2\t\r   0.4\t\r   0.6\t\r   0.8\t\r   1\t\r   1.2\t\r   A 1 0 1 T A \t\r   A 1 0 2 T A \t\r   A 1 0 3 T A \t\r   A 1 0 4 T A \t\r   A 1 0 5 T A \t\r   A 1 0 2 M T \t\r   A 1 0 6 M T \t\r   A 1 0 7 M T \t\r   A 1 0 8 M T \t\r   A 1 1 4 M T \t\r   105\t\r   999\t\r   102\t\r   101 Figure 11: Sense usage distribution among TAs and MTs for fair Anveshan not only helps us understand underlying patterns in annotator behavior and remove noise from IA scores, but also helps identify cases where there is no noise and no systematic subsets of annotators that agree with each other. An example can be seen in for the noun work. We observed that the annotators do not have largely different behavior, which is reflected in Figure 9. As none of the annotators are significantly different from the others, the KLD scores are low and the plotted line does not have any steep rises, as seen in Figure 2. Similar to the results for TA data, Anveshan was successful in identifying outliers in Mechanical Turk data as well. In order to compare the agreement among TAs and MTs, we looked at IA scores of all subsets of annotators for the three adjectives in the Mechanical Turk data. We observed that MTs used much more senses than TAs for all words and that there was a lot of noise in sense usage distribution. Figure 10 illustrates the sense usage statistics for long among MTs, for frequently used senses. We also looked at agreement scores among all subsets of MTs to see if there are any subsets of annotators who agree as much as TAs, and we observed that for both long and quiet, there were no 53 subsets of MT annotators whose agreement was comparable or greater than the same number of the TAs, however for fair, we found one set of 5 annotators whose IA score (0.61) was greater than the IA score (0.54) of trained annotators. We also observed that among both these pairs of annotators, the frequently used senses were the same, as illustrated in Figure 11. Still, the two groups of annotators have sufficiently distinct sense usage that the overall IA for the combined set drops to 0.43. 7 Conclusion and Future Work For annotations on a subjective task, there are cases where there is no single correct label. In this paper, we presented Anveshan, an approach to study annotator behavior and to explore datasets with multiple annotators, and with a large set of annotation values. Here we looked at data from half a dozen trained annotators and fourteen untrained Mechanical Turkers on word sense annotation for polysemous words. The analysis using Anveshan provided many insights into sources of disagreement among the annotators. We learn that IA Scores do not give us a complete picture and it is necessary to delve deeper and study annotator behavior in order to identify noise possibly due to sense confusability, to eliminate noise due to outliers, and to identify systematic differences where subsets of annotators have much higher IA than the full set. The results from Anveshan are encouraging and the methodology can be readily extended to study patterns in human behavior. We plan to extend our work by looking at JSD scores of all subsets of annotators instead of pairs, to identify larger subsets of annotators who have similar behavior. We also plan to investigate other statistical methods of outlier detection such as the orthogonalized Gnanadesikan-Kettenring estimator. "
W10-1807 "<NoAbstract>"
W10-1808 "Abstract The common accepted wisdom is that blind double annotation followed by adjudication of disagreements is necessary to create training and test corpora that result in the best possible performance. We provide evidence that this is unlikely to be the case. Rather, the greatest value for your annotation dollar lies in single annotating more data. "
W10-1809 "Abstract Many noun phrases in text are ambiguously quantified: syntax doesnt explicitly tell us whether they refer to a single entity or to several, and what portion of the set denoted by the Nbar actually takes part in the event expressed by the verb. We describe this ambiguity phenomenon in terms of underspecification, or rather underquantification. We attempt to validate the underquantification hypothesis by producing and testing an annotation scheme for quantification resolution, the aim of which is to associate a single quantifier with each noun phrase in our corpus. "
W10-1810 " In this paper, we have addressed the task of PropBank annotation of light verb constructions, which like multi-word expressions pose special problems. To arrive at a solution, we have evaluated 3 different possible methods of annotation. The final method involves three passes: (1) manual identification of a light verb construction, (2) annotation based on the light verb constructions Frame File, and (3) a deterministic merging of the first two passes. We also discuss how in various languages the light verb constructions are identified and can be distinguished from the non-light verb word groupings.  "
W10-1811 "Abstract This paper describes the retrieval of correct semantic boundaries for predicateargument structures annotated by dependency structure. Unlike phrase structure, in which arguments are annotated at the phrase level, dependency structure does not have phrases so the argument labels are associated with head words instead: the subtree of each head word is assumed to include the same set of words as the annotated phrase does in phrase structure. However, at least in English, retrieving such subtrees does not always guarantee retrieval of the correct phrase boundaries. In this paper, we present heuristics that retrieve correct phrase boundaries for semantic arguments, called semantic boundaries, from dependency trees. By applying heuristics, we achieved an F1-score of 99.54% for correct representation of semantic boundaries. Furthermore, error analysis showed that some of the errors could also be considered correct, depending on the interpretation of the annotation. "
W10-1812 ", Am  alia Mendes, S  lvia Pereira, Anabela Gonc  alves and In es Duarte Centro de Lingu  stica da Universidade de Lisboa, Lisboa, Portugal {iris, amalia.mendes}@clul.ul.pt Abstract We present an annotation scheme for the annotation of complex predicates, understood as constructions with more than one lexical unit, each contributing part of the information normally associated with a single predicate. We discuss our annotation guidelines of four types of complex predicates, and the treatment of several difficult cases, related to ambiguity, overlap and coordination. We then discuss the process of marking up the Portuguese CINTIL corpus of 1M tokens (written and spoken) with a new layer of information regarding complex predicates. We also present the outcomes of the annotation work and statistics on the types of CPs that we found in the corpus. "
W10-1813 " Language documentation is important as a tool for preservation of endangered languages and making data available to speakers and researchers of a language. A data base such as TypeCraft is important for typology studies both for well documented languages as well as little documented languages and is a valid tool for comparison of languages. This requires that linguistic elements must be coded in a manner that allows comparability across widely varying language data. In this paper, I discuss how I have used the coding system in TypeCraft for the documentation of data from E do language, a language belonging to the Edoid group of the Benue-Congo subfamily of the Volta-Congo language family and spoken in Mid-Western Nigeria, West Africa. The study shows how syntactic, semantic and morphological properties of multi-verb constructions in E do (Benue-Congo) can be represented in a relational database.  "
W10-1814 "Abstract Methods that re-use existing mono-lingual semantic annotation resources to annotate a new language rely on the hypothesis that the semantic annotation scheme used is cross-lingually valid. We test this hypothesis in an annotation agreement study. We show that the annotation scheme can be applied cross-lingually. "
W10-1815 "Abstract The purpose of this paper is to present an unusual English dataset for affect exploration in text. It describes a corpus of fairy tales from three sources that have been annotated for affect at the sentence level. Special attention is given to data marked by high annotator agreement. A qualitative analysis of characteristics of high agreement sentences from H. C. Andersen reveals several interesting trends, illustrated by examples. "
W10-1816 " In this paper, we introduce our recent work on re-annotating the deep information, which includes both the grammatical functional tags and the traces, in a Chinese scientific treebank. The issues with regard to re-annotation and its corresponding solutions are discussed. Furthermore, the process of the re-annotation work is described. 1  "
W10-1817 " We propose a unified model of syntax and discourse in which text structure is viewed as a tree structure augmented with anaphoric relations and other secondary relations. We describe how the model accounts for discourse connectives and the syntax-discourse-semantics interface. Our model is dependency-based, ie, words are the basic building blocks in our analyses. The analyses have been applied cross-linguistically in the Copenhagen Dependency Treebanks, a set of parallel treebanks for Danish, English, German, Italian, and Spanish which are currently being annotated with respect to discourse, anaphora, syntax, morphology, and translational equivalence. 1 Introductio  "
W10-1818 " This paper reports on a pilot study where two Models of argument were applied to the Discussion sections of a corpus of biomedical research articles. The goal was to identify sources of systematic inter-annotator variation as diagnostics for improving the Models. In addition to showing a need to revise both Models, the results identified problems resulting from limitations in annotator expertise. In future work two types of annotators are required: those with biomedical domain expertise and those with an understanding of rhetorical structure. 1 Introduct  "
W10-1819 "Abstract In this paper, we present a PropBank of clinical Finnish, an annotated corpus of verbal propositions and arguments. The clinical PropBank is created on top of a previously existing dependency treebank annotated in the Stanford Dependency (SD) scheme and covers 90% of all verb occurrences in the treebank. We establish that the PropBank scheme is applicable to clinical Finnish as well as compatible with the SD scheme, with an overwhelming proportion of arguments being governed by the verb. This allows argument candidates to be restricted to direct verb dependents, substantially simplifying the PropBank construction. The clinical Finnish PropBank is freely available at the address http://bionlp.utu.fi  http://bionlp.utu.fi . 1 Introduction Natural language processing (NLP) in the clinical domain has received substantial interest, with applications in decision support, patient managing and profiling, mining trends, and others (see the extensive review by Friedman and Johnson (2006)). While some of these applications, such as document retrieval and trend mining, can rely solely on word-frequency-based methods, others, such as information extraction and summarization require a detailed linguistic analysis capturing some of the sentence semantics. Among the most important steps in this direction is an analysis of verbs and their argument structures. In this work, we focus on the Finnish language in the clinical domain, analyzing its verbs and their argument structures using the PropBank scheme (Palmer et al., 2005). The choice of this particular scheme is motivated by its practical, application-oriented nature. We build the clinical Finnish PropBank on top of the existing dependency treebank of Haverinen et al. (2009). The primary outcome of this study is the PropBank of clinical Finnish itself, consisting of the analyses for 157 verbs with 2,382 occurrences and 4,763 arguments, and covering 90% of all verb occurrences in the underlying treebank. This PropBank, together with the treebank, is an important resource for the further development of clinical NLP applications for the Finnish language. We also establish the applicability of the PropBank scheme to the clinical sublanguage with its many atypical characteristics, and finally, we find that the PropBank scheme is compatible with the Stanford Dependency scheme of de Marneffe and Manning (2008a; 2008b) in which the underlying treebank is annotated. "
W10-1820 "Abstract In this paper, we introduce the NotaBene RDF Annotation Tool free software used to build the Syntactic Reference Corpus of Medieval French. It relies on a dependency-based model to manually annotate Old French texts from the Base de Francais Medieval and the Nouveau Corpus dAmsterdam. NotaBene uses OWL ontologies to frame the terminology used in the annotation, which is displayed in a tree-like view of the annotation. This tree widget allows easy grouping and tagging of words and structures. To increase the quality of the annotation, two annotators work independently on the same texts at the same time and NotaBene can also generate automatic comparisons between both analyses. The RDF format can be used to export the data to several other formats: namely, TigerXML (for querying the data and extracting structures) and graphviz dot format (for quoting syntactic description in research papers). First, we will present the Syntactic Reference Corpus of Medieval French project (SRCMF) (1). Then, we will show how the NotaBene RDF Annotation Tool software is used within the project (2). In our conclusion, we will stress further developments of the tool (3). "
W10-1821 " This paper describes a CoNLL-style chunk representation for the T  ubingen Treebank of Written German, which assumes a flat chunk structure so that each word belongs to at most one chunk. For German, such a chunk definition causes problems in cases of complex prenominal modification. We introduce a flat annotation that can handle these structures via a stranded noun chunk. 1 I  "
W10-1822 ", Am  alia Mendes and Sandra Antunes Centro de Lingu  stica da Universidade de Lisboa, Lisboa, Portugal {iris, amalia.mendes, sandra.antunes}@clul.ul.pt Abstract We present a proposal for the annotation of multi-word expressions in a 1M corpus of contemporary portuguese. Our aim is to create a resource that allows us to study multi-word expressions (MWEs) in their context. The corpus will be a valuable additional resource next to the already existing MWE lexicon that was based on a much larger corpus of 50M words. In this paper we discuss the problematic cases for annotation and proposed solutions, focusing on the variational properties of MWEs. "
W10-1823 " We propose a feature type classification thought to be used in a therapeutic context. Such a scenario lays behind our need for a easily usable and cognitively plausible classification. Nevertheless, our proposal has both a practical and a theoretical outcome, and its applications range from computational linguistics to psycholinguistics. An evaluation through inter-coder agreement has been performed to highlight the strength of our proposal and to conceive some improvements for the future. 1 Intro  "
W10-1824 " This paper presents preliminary work on a corpus-based study of Korean demonstratives. Through the development of an annotation scheme and the use of spoken and written corpora, we aim to determine different functions of demonstratives and to examine their distributional properties. Our corpus study adopts similar features of annotation used in Botley and McEnery (2001) and provides some linguistic hypotheses on grammatical functions of Korean demonstratives to be further explored. 1 I  "
W10-1825 "Abstract This paper describes the creation of a resource of German sentences with multiple automatically created alternative syntactic analyses (parses) for the same text, and how qualitative and quantitative investigations of this resource can be performed using ANNIS, a tool for corpus querying and visualization. Using the example of PP attachment, we show how parsing can benefit from the use of such a resource. "
W10-1826 " The paper presents an architecture for connecting annotated linguistic data with a computational grammar system. Pivotal to the architecture is an annotational interlingua  called the Construction Labeling system (CL) which is notationally very simple, descriptively finegrained, cross-typologically applicable, and formally well-defined enough to map to a state-of-the-art computational model of grammar. In the present instantiation of the architecture, the computational grammar is an HPSG-based system called TypeGram. Underlying the architecture is a research program of enhancing the interconnectivity between linguistic analytic subsystems such as grammar formalisms and text annotation systems. 1 Introducti  "
W10-1827 "Abstract Prepositions are highly polysemous. Yet, little effort has been spent to develop languagespecific annotation schemata for preposition senses to systematically represent and analyze the polysemy of prepositions in large corpora. In this paper, we present an annotation schema for preposition senses in German. The annotation schema includes a hierarchical taxonomy and also allows multiple annotations for individual tokens. It is based on an analysis of usage-based dictionaries and grammars and has been evaluated in an inter-annotatoragreement study. 1 Annotation Schemata for Preposition Senses: A Problem to be Tackled It is common linguistic wisdom that prepositions are highly polysemous. It is thus somewhat surprising that little attention has been paid to the development of specialized annotation schemata for preposition senses. 1 In the present paper, we present a tagset for the annotation of German prepositions. The need for an annotation schema emerged in an analysis of so-called PrepositionNoun Combinations (PNCs), sometimes called determinerless PPs or bare PPs. PNCs minimally consist of a preposition and a count noun in the singular that appear without a determiner. In (1), examples are given from German. (1) auf parlamentarische Anfrage (after being asked in parliament), bei absolut klarer Zielsetzung (given a clearly present aim), unter sanfter Androhung (under gentle threat) The preposition-sense annotation forms part of a larger annotation task of the corpus, where all 1 The Preposition Project is a notable exception (cf. www.clres.com/prepositions.html).  "
W10-1828 "Abstract This paper presents OTTO, a transcription tool designed for diplomatic transcription of historical language data. The tool supports easy and fast typing and instant rendering of transcription in order to gain a look as close to the original manuscript as possible. In addition, the tool provides support for the management of transcription projects which involve distributed, collaborative working of multiple parties on collections of documents. "
W10-1829 "Abstract We propose in this paper a broad-coverage approach for multimodal annotation of conversational data. Large annotation projects addressing the question of multimodal annotation bring together many different kinds of information from different domains, with different levels of granularity. We present in this paper the first results of the OTIM project aiming at developing conventions and tools for multimodal annotation. "
W10-1830 "Abstract This paper describes a new kind of semantic annotation in parallel treebanks. We build French-German parallel treebanks of mountaineering reports, a text genre that abounds with geographical names which we classify and ground with reference to a large gazetteer of Swiss toponyms. We discuss the challenges in obtaining a high recall and precision in automatic grounding, and sketch how we represent the grounding information in our treebank. "
W10-1831 "Abstract We describe the challenges of resource creation for a resource-light system for morphological tagging of fusional languages (Feldman and Hana, 2010). The constraints on resources (time, expertise, and money) introduce challenges that are not present in development of morphological tools and corpora in the usual, resource intensive way. "
W10-1832 "{bozsahin,dezeyrek}@metu.edu.tr Abstract In this paper, we describe an annotation environment developed for the marking of discourse structures in Turkish, and the kinds of discourse relation configurations that led to its design. 1 Introduction The property that distinguishes a discourse from a set of arbitrary sentences is defined as coherence (Halliday and Hasan, 1976). Coherence is established by the relations between the units of discourse. Systematic analysis of coherence requires an annotated corpus in which coherence relations are encoded. Turkish Discourse Bank Project (TDB) aims to produce a large-scale discourse level annotation resource for Turkish (Zeyrek and Weber, 2008). The TDB follows the annotation scheme of the PDTB (Miltsakaki et al, 2004). The lexicalized approach adopted in the TDB assumes that discourse relations are set up by lexical items called discourse connectives. Connectives are considered as discourse level predicates which take exactly two arguments. The arguments are abstract objects like propositions, facts, events, etc. (Asher, 1993). They can be linked either by explicitly realized connectives or by implicit ones recognized by an inferential process. We annotate explicit connectives; implicit connectives are future work. We use the naming convention of the PDTB. Conn stands for the connective, Arg1 and Arg2 for the first and the second argument, respectively. Conn, Arg1 and Arg2 are assumed to be required components of discourse relations. Supplementary materials which are relevant to but not necessary for the interpretation are also annotated. Our main data is METU Turkish Corpus(MTC) (Say et al, 2002). MTC is a written source of Turkish with approximately 2 million words. The original MTC files include informative tags, such as the author of the text, the paragraph boundaries in the text, etc. We removed these tags to obtain raw text files and set the character encoding of the files to UTF-8. These conversions are useful for programming purposes such as visualizing the data in different platforms and the use of third-party libraries. We developed an annotation environment to mark up the discourse relations, which we call DATT (Discourse Annotation Tool for Turkish). DATT produces XML files as annotation data which are generated by the implementation of a stand-off annotation methodology. We present in 2 the data from Turkish discourse, which forced us to use stand-off annotation instead of in-line markup. The key aspect is potential crossing of the markup links. However, stand-off annotation is also advantageous for separate licensing. We present the design of data structure and the functionality of the tool in 3. We report some preliminary results in the conclusion. "
W10-1833 " We present our concept-annotation guidelines for an large multi-institutional effort to create a gold-standard manually annotated corpus of full-text biomedical journal articles. We are semantically annotating these documents with the full term sets of eight large biomedical ontologies and controlled terminologies ranging from approximately 1,000 to millions of terms, and, using these guidelines, we have been able to perform this extremely challenging task with a high degree of interannotator agreement. The guidelines have been designed to be able to be used with any terminology employed to semantically annotate concept mentions in text and are available for external use. 1 Intro  "
W10-1834 "Abstract In this paper, we argue for and demonstrate the use of Prolog as a tool to query annotated corpora. We present a case study based on the German TuBa-D/Z Treebank to show that flexible and efficient corpus querying can be started with a minimal amount of effort. We end this paper with a brief discussion of performance, that suggests that the approach is both fast enough and scalable. "
W10-1835 "Abstract E-Dictor is a tool for encoding, applying levels of editions, and assigning part-ofspeech tags to ancient texts. In short, it works as a WYSIWYG interface to encode text in XML format. It comes from the experience during the building of the Tycho Brahe Parsed Corpus of Historical Portuguese and from consortium activities with other research groups. Preliminary results show a decrease of at least 50% on the overall time taken on the editing process. "
W10-1836 " The revised Arabic PropBank (APB) reflects a number of changes to the data and the process of PropBanking. Several changes stem from Treebank revisions. An automatic process was put in place to map existing annotation to the new trees. We have revised the original 493 Frame Files from the Pilot APB and added 1462 new files for a total of 1955 Frame Files with 2446 framesets. In addition to a heightened attention to sense distinctions this cycle includes a greater attempt to address complicated predicates such as light verb constructions and multi-word expressions. New tools facilitate the data tagging and also simplify frame creation. 1 Introdu  "
W10-1837 "National Lab 1 Bethel Valley Rd Oak Ridge, TN 37831 Abstract Building training data is labor-intensive and presents a major obstacle to advancing machine learning technologies such as machine translators, named entity recognizers (NER), part-of-speech taggers, etc. Training data are often specialized for a particular language or Natural Language Processing (NLP) task. Knowledge captured by a specific set of training data is not easily transferable, even to the same NLP task in another language. Emerging technologies, such as social networks and serious games, offer a unique opportunity to change how we construct training data. While collaborative games have been used in information retrieval, it is an open issue whether users can contribute accurate annotations in a collaborative game context for a problem that requires an exact answer, such as games that would create named entity recognition training data. We present PackPlay, a collaborative game framework that empirically shows players ability to mimic annotation accuracy and thoroughness seen in gold standard annotated corpora. "
W10-1838 " Among the many proposals to promote alternatives to costly to create gold standards, just recently the idea of a fully automatically, and thus cheaply, to set up silver standard has been launched. However, the current construction policy for such a silver standard requires crucial parameters (such as similarity thresholds and agreement cut-offs) to be set a priori, based on extensive testing though, at corpus compile time. Accordingly, such a corpus is static, once it is released. We here propose an alternative policy where silver standards can be dynamically optimized and customized on demand (given a specific goal function) using a gold standard as an oracle. 1 Introductio  "
W10-1839 " In this paper, we present a two-phase, hybrid model for generating training data for Named Entity Recognition systems. In the first phase, a trained annotator labels all named entities in a text irrespective of type. In the second phase, naive crowdsourcing workers complete binary judgment tasks to indicate the type(s) of each entity. Decomposing the data generation task in this way results in a flexible, reusable corpus that accommodates changes to entity type taxonomies. In addition, it makes efficient use of precious trained annotator resources by leveraging highly available and cost effective crowdsourcing worker pools in a way that does not sacrifice quality. Keywords: annotation scheme design, annotation tools and systems, corpus annotation, annotation for machine learning 1 Backg  "
W10-1840 "Abstract In this paper, we apply the annotation scheme design methodology defined in (Bunt, 2010) and demonstrate its use for generating a mapping from an existing annotation scheme to a representation in GrAF format. The most important features of this methodology are (1) the distinction of the abstract and concrete syntax of an annotation language; (2) the specification of a formal semantics for the abstract syntax; and (3) the formalization of the relation between abstract and concrete syntax, which guarantees that any concrete syntax inherits the semantics of the abstract syntax, and thus guarantees meaning-preserving mappings between representation formats. By way of illustration, we apply this mapping strategy to annotations from ISOTimeML, PropBank, and FrameNet. "
W10-1841 "9AB, UK {jniekras,jmoore}@inf.ed.ac.uk Abstract In conversational language, references to people (especially to the conversation participants, e.g., I, you, and we) are an essential part of many expressed meanings. In most conversational settings, however, many such expressions have numerous potential meanings, are frequently vague, and are highly dependent on social and situational context. This is a significant challenge to conversational language understanding systems  one which has seen little attention in annotation studies. In this paper, we present a method for annotating verbal reference to people in conversational speech, with a focus on reference to conversation participants. Our goal is to provide a resource that tackles the issues of vagueness, ambiguity, and contextual dependency in a nuanced yet reliable way, with the ultimate aim of supporting work on summarization and information extraction for conversation. "
W10-1842 " In the area of large French speech corpora, there is a demonstrated need for a common prosodic notation system allowing for easy data exchange, comparison, and automatic annotation. The major questions are: (1) how to develop a single simple scheme of prosodic transcription which could form the basis of guidelines for non-expert manual annotation (NEMA), used for linguistic teaching and research; (2) based on this NEMA, how to establish reference prosodic corpora (RPC) for different discourse genres (Cresti and Moneglia, 2005); (3) how to use the RPC to develop corpus-based learning methods for automatic prosodic labelling in spontaneous speech (Buhman et al., 2002; Tamburini and Caini 2005, Avanzi, et al. 2010). This paper presents two pilot experiments conducted with a consortium of 15 French experts in prosody in order to provide a prosodic transcription framework (transcription methodology and transcription reliability measures) and to establish reference prosodic corpora in French. 1 Introduct  "
W10-1843 " We present a syntactic annotation scheme for spoken French that is currently used in the Rhapsodie project. This annotation is dependency-based and includes coordination and disfluency as analogously encoded types of paradigmatic phenomena. Furthermore, we attempt a thorough definition of the discourse units required by the systematic annotation of other phenomena beyond usual sentence boundaries, which are typical for spoken language. This includes so called macrosyntactic phenomena such as dislocation, parataxis, insertions, grafts, and epexegesis. 1 Introdu  "
W10-1901 "Abstract This paper presents two strong baselines for the BioNLP 2009 shared task on event extraction. First we re-implement a rulebased approach which allows us to explore the task and the effect of domainadapted parsing on it. We then replace the rule-based component with support vector machine classifiers and achieve performance near the state-of-the-art without using any external resources. The good performances achieved and the relative simplicity of both approaches make them reproducible baselines. We conclude with suggestions for future work with respect to the task representation. "
W10-1902 "{aihuang, zxy-dcs}@tsinghua.edu.cn Abstract Linear-chain Conditional Random Fields (CRF) has been applied to perform the Named Entity Recognition (NER) task in many biomedical text mining and information extraction systems. However, the linear-chain CRF cannot capture long distance dependency, which is very common in the biomedical literature. In this paper, we propose a novel study of capturing such long distance dependency by defining two principles of constructing skipedges for a skip-chain CRF: linking similar words and linking words having typed dependencies. The approach is applied to recognize gene/protein mentions in the literature. When tested on the BioCreAtIvE II Gene Mention dataset and GENIA corpus, the approach contributes significant improvements over the linear-chain CRF. We also present in-depth error analysis on inconsistent labeling and study the influence of the quality of skip edges on the labeling performance. "
W10-1903 " We consider the task of automatically extracting post-translational modification events from biomedical scientific publications. Building on the success of event extraction for phosphorylation events in the BioNLP09 shared task, we extend the event annotation approach to four major new post-transitional modification event types. We present a new targeted corpus of 157 PubMed abstracts annotated for over 1000 proteins and 400 post-translational modification events identifying the modified proteins and sites. Experiments with a state-of-the-art event extraction system show that the events can be extracted with 52% precision and 36% recall (42% Fscore), suggesting remaining challenges in the extraction of the events. The annotated corpus is freely available in the BioNLP09 shared task format at the GENIA project homepage. 1 1 Introdu  "
W10-1904 "Manchester, Manchester, UK jari.bjorne@utu.fi,ginter@cs.utu.fi,smp@is.s.u-tokyo.ac.jp tsujii@is.s.u-tokyo.ac.jp,tapio.salakoski@it.utu.fi Abstract We present the first full-scale event extraction experiment covering the titles and abstracts of all PubMed citations. Extraction is performed using a pipeline composed of state-of-the-art methods: the BANNER named entity recognizer, the McCloskyCharniak domain-adapted parser, and the Turku Event Extraction System. We analyze the statistical properties of the resulting dataset and present evaluations of the core event extraction as well as negation and speculation detection components of the system. Further, we study in detail the set of extracted events relevant to the apoptosis pathway to gain insight into the biological relevance of the result. The dataset, consisting of 19.2 million occurrences of 4.5 million unique events, is freely available for use in research at  http://bionlp.utu.fi/   http://bionlp.utu.fi/ . "
W10-1905 " The extraction of bio-molecular events from text is an important task for a number of domain applications such as pathway construction. Several syntactic parsers have been used in Biomedical Natural Language Processing (BioNLP) applications, and the BioNLP 2009 Shared Task results suggest that incorporation of syntactic analysis is important to achieving state-of-the-art performance. Direct comparison of parsers is complicated by to differences in the such as the division between phrase structureand dependencybased analyses and the variety of output formats, structures and representations applied. In this paper, we present a taskoriented comparison of five parsers, measuring their contribution to bio-molecular event extraction using a state-of-the-art event extraction system. The results show that the parsers with domain models using dependency formats provide very similar performance, and that an ensemble of different parsers in different formats can improve the event extraction system. "
W10-1906 " Based on linguistic generalizations, we enhanced an existing semantic processor, SemRep, for effective interpretation of a wide range of patterns used to express arguments of nominalization in clinically oriented biomedical text. Nominalizations are pervasive in the scientific literature, yet few text mining systems adequately address them, thus missing a wealth of information. We evaluated the system by assessing the algorithm independently and by determining its contribution to SemRep generally. The first evaluation demonstrated the strength of the method through an F-score of 0.646 (P=0.743, R=0.569), which is more than 20 points higher than the baseline. The second evaluation showed that overall SemRep results were increased to F-score 0.689 (P=0.745, R=0.640), approximately 25 points better than processing without nominalizations. 1 Introductio  "
W10-1907 "Abstract We describe a concept-based summarization system for biomedical documents and show that its performance can be improved using Word Sense Disambiguation. The system represents the documents as graphs formed from concepts and relations from the UMLS. A degree-based clustering algorithm is applied to these graphs to discover different themes or topics within the document. To create the graphs, the MetaMap program is used to map the text onto concepts in the UMLS Metathesaurus. This paper shows that applying a graph-based Word Sense Disambiguation algorithm to the output of MetaMap improves the quality of the summaries that are generated. "
W10-1908 "Abstract Forums and mailing lists dedicated to particular diseases are increasingly popular online. Automatically inferring the health status of a patient can be useful for both forum users and health researchers who study patients online behaviors. In this paper, we focus on breast cancer forums and present a method to predict the stage of patients cancers from their online discourse. We show that what the patients talk about (content-based features) and whom they interact with (social networkbased features) provide complementary cues to predicting cancer stage and can be leveraged for better prediction. Our methods are extendable and can be applied to other tasks of acquiring contextual information about online health forum participants. "
W10-1909 " Here we explore mining data on gene expression from the biomedical literature and present Gene Expression Text Miner (GETM), a tool for extraction of information about the expression of genes and their anatomical locations from text. Provided with recognized gene mentions, GETM identifies mentions of anatomical locations and cell lines, and extracts text passages where authors discuss the expression of a particular gene in specific anatomical locations or cell lines. This enables the automatic construction of expression profiles for both genes and anatomical locations. Evaluated against a manually extended version of the BioNLP '09 corpus, GETM achieved precision and recall levels of 58.8% and 23.8%, respectively. Application of GETM to MEDLINE and PubMed Central yielded over 700,000 gene expression mentions. This data set may be queried through a web interface, and should prove useful not only for researchers who are interested in the developmental regulation of specific genes of interest, but also for database curators aiming to create structured repositories of gene expression information. The compiled tool, its source code, the manually annotated evaluation corpus and a search query interface to the data set extracted from MEDLINE and PubMed Central is available at http://getmproject.sourceforge.net/. "
W10-1910 "Abstract We investigate the automatic identification of negated and speculative statements in biomedical texts, focusing on the clinical domain. Our goal is to evaluate the performance of simple, Regex-based algorithms that have the advantage of low computational cost, simple implementation, and do not rely on the accurate computation of deep linguistic features of idiosyncratic clinical texts. The performance of the NegEx algorithm with an additional set of Regex-based rules reveals promising results (evaluated on the BioScope corpus). Current and future work focuses on a bootstrapping algorithm for the discovery of new rules from unannotated clinical texts. "
W10-1911 "Human Language Technology Research Unit, Fondazione Bruno Kessler, Trento, Italy  ICT Doctoral School, University of Trento, Italy {chowdhury,lavelli}@fbk.eu Abstract Despite an increasing amount of research on biomedical named entity recognition, there has been not enough work done on disease mention recognition. Difficulty of obtaining adequate corpora is one of the key reasons which hindered this particular research. Previous studies argue that correct identification of disease mentions is the key issue for further improvement of the disease-centric knowledge extraction tasks. In this paper, we present a machine learning based approach that uses a feature set tailored for disease mention recognition and outperforms the state-ofthe-art results. The paper also discusses why a feature set for the well studied gene/protein mention recognition task is not necessarily equally effective for other biomedical semantic types such as diseases. "
W10-1912 " This paper describes our study on identifying semantic relations that exist between diseases and treatments in biomedical sentences. We focus on three semantic relations: Cure, Prevent, and Side Effect. The contributions of this paper consists in the fact that better results are obtained compared to previous studies and the fact that our research settings allow the integration of biomedical and medical knowledge. We obtain 98.55% F-measure for the Cure relation, 100% F-measure for the Prevent relation, and 88.89% F-measure for the Side Effect relation. 1 Intro  "
W10-1913 "Abstract Many practical tasks require accessing specific types of information in scientific literature; e.g. information about the objective, methods, results or conclusions of the study in question. Several schemes have been developed to characterize such information in full journal papers. Yet many tasks focus on abstracts instead. We take three schemes of different type and granularity (those based on section names, argumentative zones and conceptual structure of documents) and investigate their applicability to biomedical abstracts. We show that even for the finest-grained of these schemes, the majority of categories appear in abstracts and can be identified relatively reliably using machine learning. We discuss the impact of our results and the need for subsequent task-based evaluation of the schemes. "
W10-1914 "Abstract The extraction of nested, semantically rich relationships of biological entities has recently gained popularity in the biomedical text mining community. To move toward this objective, a method is proposed for reconstructing original semantic relationship graphs from projections, where each node and edge is mapped to the representative of its equivalence class, by determining the relationship argument combinations that represent real relationships. It generalises the limited postprocessing step of the method of Bj  orne et al. (2010) and hence extends this extraction method to arbitrarily deep relationships with unrestricted primary argument combinations. The viability of the method is shown by successfully extracting nested relationships in BioInfer and the corpus of the BioNLP09 Shared Task on Event Extraction. The reported results, to the best of our knowledge, are the first for the nested relationships in BioInfer on a task in which only named entities are given. "
W10-1915 "Arizona, USA {robert.leaman, whitz, rpsulli, annie.skariah, jian.yang, graciela.gonzalez}@asu.edu Abstract Adverse reactions to drugs are among the most common causes of death in industrialized nations. Expensive clinical trials are not sufficient to uncover all of the adverse reactions a drug may cause, necessitating systems for post-marketing surveillance, or pharmacovigilance. These systems have typically relied on voluntary reporting by health care professionals. However, self-reported patient data has become an increasingly important resource, with efforts such as MedWatch from the FDA allowing reports directly from the consumer. In this paper, we propose mining the relationships between drugs and adverse reactions as reported by the patients themselves in user comments to health-related websites. We evaluate our system on a manually annotated set of user comments, with promising performance. We also report encouraging correlations between the frequency of adverse drug reactions found by our system in unlabeled data and the frequency of documented adverse drug reactions. We conclude that user comments pose a significant natural language processing challenge, but do contain useful extractable information which merits further exploration. "
W10-1916 "Abstract This abstract describes work in progress on semantic role labeling of gene regulation events. We present preliminary results of a supervised semantic role labeler that has been trained and tested on the GREC corpus. "
W10-1917 "<NoAbstract>"
W10-1918 "Early recognition of distinguishing patterns of a novel pandemic disease is important. We introduce a methodological approach based on popular data mining techniques to extract key features and temporal patterns of swine (h1n1) flu that is discriminated from swine flu like symptoms. "
W10-1919 "Abstract Event extraction approaches based on expressive structured representations of extracted information have been a significant focus of research in recent biomedical natural language processing studies. However, event extraction efforts have so far been limited to publication abstracts, with most studies further considering only the specific transcription factor-related subdomain of molecular biology of the GENIA corpus. To establish the broader relevance of the event extraction approach and proposed methods, it is necessary to expand on these constraints. In this study, we propose an adaptation of the event extraction approach to a subdomain related to infectious diseases and present analysis and initial experiments on the feasibility of event extraction from domain full text publications. "
W10-1920 "Abstract We present a preliminary attempt to apply the TARSQI Toolkit to the medical domain, specifically electronic health records, for use in answering temporally motivated questions. 1 Introduction Electronic Health Records are often the most complete records of a patients hospital stay, making them invaluable for retrospective cohort studies. However, the free text nature of these documents makes it difficult to extract complex information such as the relative timing of conditions or procedures. While there have been recent successes in this endeavor (Irvine et al., 2008; Mowery et al., 2009; Zhou et al., 2007), there is still much to be done. We present work done to adapt the TARSQI Toolkit (TTK) to the medical domain. Though the use of the TTK and a set of auxiliary Perl scripts, we perform information extraction over a set of 354 discharge summaries used in the R3i REALIST study to answer the following question: Which patients can be positively identified as being on statins at the time they were admitted to the hospital? 2 TARSQI Toolkit The TARSQI Toolkit, developed as a part of the AQUAINT workshops, is a modular system for automatic temporal and event annotation of natural language in newswire texts (Verhagen and Pustejovsky, 2008). The different modules preprocess the data, label events and times, create links between times and events (called tlinks), and mark subordination relationships. Output from the TTK consists documents annotated in TimeML, an XML specification for event and time annotation (Pustejovsky et al., 2005). Of particular interest for this project are EVITA, the module responsible for finding events in text, and Blinker, the module used to create syntactic rule-based links between events and timexes. 3 Structure of EHRs The bodies of the Electronic Health Records used were segmented, with each section having a header indicating the topic of that section (Medical History, Course of Treatment, Discharge Medications, etc). Header names and sections are not standardized across EHRs, but often give important temporal information about when events described took place (Denny et al., 2008). "
W10-2001 ", Edinburgh EH8 9AB, UK Abstract In this paper we investigate a new source of information for syntactic category acquisition: sentence type (question, declarative, imperative). Sentence type correlates strongly with intonation patterns in most languages; we hypothesize that these intonation patterns are a valuable signal to a language learner, indicating different syntactic patterns. To test this hypothesis, we train a Bayesian Hidden Markov Model (and variants) on child-directed speech. We first show that simply training a separate model for each sentence type decreases performance due to sparse data. As an alternative, we propose two new models based on the BHMM in which sentence type is an observed variable which influences either emission or transition probabilities. Both models outperform a standard BHMM on data from English, Cantonese, and Dutch. This suggests that sentence type information available from intonational cues may be helpful for syntactic acquisition crosslinguistically. "
W10-2002 "Abstract Natural language as well as other communication forms are constrained by cognitive function and evolved through a social process. Here, we examine whether human memory may be uniquely adapted to the social structures prevalent in groups, specifically small-world networks. The emergence of domain languages is simulated using an empirically evaluated ACTR-based cognitive model of agents in a naming game played within communities. Several community structures are examined (grids, trees, random graphs and small-world networks). We present preliminary results from small-scale simulations, showing relative robustness of cognitive models to network structure. "
W10-2003 ", NY, USA {afine, tqian, fjaeger, robbie}@bcs.rochester.edu Abstract In this paper we investigate the manner in which the human language comprehension system adapts to shifts in probability distributions over syntactic structures, given experimentally controlled experience with those structures. We replicate a classic reading experiment, and present a model of the behavioral data that implements a form of Bayesian belief update over the course of the experiment. "
W10-2004 " Hierarchical Hidden Markov Model (HHMM) parsers have been proposed as psycholinguistic models due to their broad coverage within human-like working memory limits (Schuler et al., 2008) and ability to model human reading time behavior according to various complexity metrics (Wu et al., 2010). But HHMMs have been evaluated previously only with very wide beams of several thousand parallel hypotheses, weakening claims to the models efficiency and psychological relevance. This paper examines the effects of varying beam width on parsing accuracy and speed in this model, showing that parsing accuracy degrades gracefully as beam width decreases dramatically (to 2% of the width used to achieve previous top results), without sacrificing gains over a baseline CKY parser. 1  "
W10-2005 "Abstract This paper examines how grammatical and memory constraints explain gradience in superiority violation acceptability. A computational model encoding both categories of constraints is compared to experimental evidence. By formalizing memory capacity as beam-search in the parser, the model predicts gradience evident in human data. To predict attachment behavior, the parser must be sensitive to the types of nominal intervenors that occur between a wh-filler and its head. The results suggest memory is more informative for modeling violation gradience patterns than grammatical constraints. "
W10-2006 "States {tqian,fjaeger}@bcs.rochester.edu Abstract We formally derive a mathematical model for evaluating the effect of context relevance in language production. The model is based on the principle that distant contextual cues tend to gradually lose their relevance for predicting upcoming linguistic signals. We evaluate our model against a hypothesis of efficient communication (Genzel and Charniaks Constant Entropy Rate hypothesis). We show that the development of entropy throughout discourses is described significantly better by a model with cue relevance decay than by previous models that do not consider context effects. "
W10-2007 "Abstract When subjects describe concepts in terms of their characteristic properties, they often produce composite properties, e. g., rabbits are said to have long ears, not just ears. We present a set of simple methods to extract the modifiers of composite properties (in particular: parts) from corpora. We achieve our best performance by combining evidence about the association between the modifier and the part both within the context of the target concept and independently of it. We show that this performance is relatively stable across languages (Italian and German) and for production vs. perception of properties. "
W10-2008 "Abstract This paper presents a data-driven model of eye movement control in reading that builds on earlier work using machine learning methods to model saccade behavior. We extend previous work by modeling the time course of eye movements, in addition to where the eyes move. In this model, the initiation of eye movements is delayed as a function of on-line processing difficulty, and the decision of where to move the eyes is guided by past reading experience, approximated using machine learning methods. In benchmarking the model against held-out previously unseen data, we show that it can predict gaze durations and skipping probabilities with good accuracy. "
W10-2009 "Abstract This paper investigates whether surprisal theory can account for differential processing difficulty in the NP-/S-coordination ambiguity in Dutch. Surprisal is estimated using a Probabilistic Context-Free Grammar (PCFG), which is induced from an automatically annotated corpus. We find that our lexicalized surprisal model can account for the reading time data from a classic experiment on this ambiguity by Frazier (1987). We argue that syntactic and lexical probabilities, as specified in a PCFG, are sufficient to account for what is commonly referred to as an NP-coordination preference. "
W10-3401 "<NoAbstract>"
W10-3402 " SemanticNet is a semantic network of lexicons to hold human pragmatic knowledge. So far Natural Language Processing (NLP) research patronized much of manually augmented lexicon resources such as WordNet. But the small set of semantic relations like Hypernym, Holonym, Meronym and Synonym etc are very narrow to capture the wide variations human cognitive knowledge. But no such information could be retrieved from available lexicon resources. SemanticNet is the attempt to capture wide range of context dependent semantic inference among various themes which human beings perceive in their pragmatic knowledge, learned by day to day cognitive interactions with the surrounding physical world. SemanticNet holds human pragmatics with twenty well established semantic relations for every pair of lexemes. As every pair of relations cannot be defined by fixed number of certain semantic relation labels thus additionally contextual semantic affinity inference in SemanticNet could be calculated by network distance and represented as a probabilistic score. SemanticNet is being presently developed for Bengali language. 1 Historical Moti  "
W10-3403 " In this paper, we present an on-going project aiming at extending the WordNet lexical database by encoding common sense featural knowledge elicited from language speakers. Such extension of WordNet is required in the framework of the STaRS.sys project, which has the goal of building tools for supporting the speech therapist during the preparation of exercises to be submitted to aphasic patients for rehabilitation purposes. We review some preliminary results and illustrate what extensions of the existing WordNet model are needed to accommodate for the encoding of commonsense (featural) knowledge. 1 Intro  "
W10-3404 " When two texts have an inclusion relation, the relationship between them is called entailment. The task of mechanically distinguishing such a relation is called recognising textual entailment (RTE), which is basically a kind of semantic analysis. A variety of methods have been proposed for RTE. However, when the previous methods were combined, the performances were not clear. So, we utilized each method as a feature of machine learning, in order to combine methods. We have dealt with the binary classification problem of two texts exhibiting inclusion, and proposed a method that uses machine learning to judge whether the two texts present the same content. We have built a program capable to perform entailment judgment on the basis of word overlap, i.e. the matching rate of the words in the two texts, mutual information, and similarity of the respective syntax trees (Subpath Set). Word overlap was calclated by utilizing BiLingual Evaluation Understudy (BLEU). Mutual information is based on co-occurrence frequency, and the Subpath Set was determined by using the Japanise WordNet. A ConfidenceWeighted Score of 68.6% was obtained in the mutual information experiment on RTE. Mutual information and the use of three methods of SVM were shown to be effective. 1  "
W10-3405 "Abstract Color affects every aspect of our lives. There has been a considerable interest in the psycholinguistic research area addressing the impact of color on emotions. In the experiments conducted by these studies, subjects have usually been asked to indicate their emotional responses to different colors. On the other side, sensing emotions in text by using NLP techniques has recently become a popular topic in computational linguistics. In this paper, we introduce a semantic similarity mechanism acquired from a large corpus of texts in order to check the similarity of colors and emotions. Then we investigate the correlation of our results with the outcomes of some psycholinguistic experiments. The conclusions are quite interesting. The correlation varies among different colors and globally we achieve very good results. "
W10-3406 "Abstract This paper presents an approach to enrich conceptual classes based on the Web. To test our approach, we first build conceptual classes using syntactic and semantic information provided by a corpus. The concepts can be the input of a dictionary. Our web-mining approach deals with a cognitive process which simulates human reasoning based on the enumeration principle. The experiments reveal the interest of our approach by adding new relevant terms to existing conceptual classes. "
W10-3407 " This paper presents a cross-linguistic analysis of the largest dictionaries currently existing for Romanian, French, and German, and a new, robust and portable method for Dictionary Entry Parsing (DEP), based on SegmentationCohesion-Dependency (SCD) configurations. The SCD configurations are applied successively on each dictionary entry to identify its lexicographic segments (the first SCD configuration), to extract its sense tree (the second configuration), and to parse its atomic sense definitions (the third one). Using previous results on DLR (The Romanian Thesaurus  new format), the present paper adapts and applies the SCD-based technology to other four large and complex thesauri: DAR (The Romanian Thesaurus  old format), TLF (Le Tresor de la Langue Francaise), DWB (Deutsches Worterbuch  GRIMM), and GWB (GotheWorterbuch). This experiment is illustrated on significantly large parsed entries of these thesauri, and proved the following features: (1) the SCD-based method is a completely formal grammarfree approach for dictionary parsing, with efficient (weeks-time adaptable) modeling through sense hierarchies and parsing portability for a new dictionary. (2) SCDconfigurations separate and run sequentially and independently the processes of lexicographic segment recognition, sense tree extraction, and atomic definition parsing. (3) The whole DEP process with SCD-configurations is optimal. (4) SCDconfigurations, through sense marker classes and their dependency hypergraphs, offer an unique instrument of lexicon construction comparison, sense concept design and DEP standardization. "
W10-3408 "LINA Universit  e de Nantes jungyeul.park @univ-nantes.fr Abstract This paper is based on our efforts on automatic multi-word terms extraction and its conceptual structure for multiple languages. At present, we mainly focus on English and the major Romance languages such as French, Spanish, Portuguese, and Italian. This paper is a case study for Italian language. We present how to build automatically conceptual structure of automatically extracted multi-word terms from domain specific corpora for Italian. We show the experimental results for extracting multi-word terms from two domain corpora (natural area and organic agriculture). Since this work is still ongoing, we discuss our future direction at the end of the paper. "
W10-3409 "Abstract Chinese noun classifiers are obligatory as a category in association with nouns. Conventional dictionaries include classifiers as lexical entries but explanations given are very brief and thus hardly helpful for L2 learners. This paper presents a new design of an edictionary of Chinese classifiers. The design is based on both theoretical studies of Chinese classifiers and empirical studies of Chinese classifier acquisition by both children and adults. My main argument with regards to Chinese classifier acquisition is that cognitive strategies with a bottom-up approach are the key to the understanding of the complexity of classifier and noun associations. The noun-dependent semantic features of classifiers are evidence to support my argument. These features are categorically defined and stored in a separated database in an elearning environment linked to the edictionary. The aim of making such a design is to provide a platform for L2 learners to explore and learn with a bottom-up approach the associations of classifiers with nouns. The computational agent-based model that automatically links noun features to that of classifiers is the technical part of the design that will be described in detail in the paper. Future development of the e-dictionary will be discussed as well. "
W10-3410 "Abstract We report on a user needs investigation carried out in the framework of the project EKFRASIS 1 that developed a platform for supporting authoring work in Modern Greek. The platform had as a backbone a conceptually organised dictionary enhanced with rich lexicographic and morphosyntactic information. Organisation of information and encoding drew on Semantic Web technologies (ontologies). Users were all professional authors (of literature, editors, translators, journalists) working for well-established firms. They were all familiar with printed conceptually organized dictionaries while most of them used a computer. They were asked to specify how the platform would be helpful to them when they searched for a word for which they had only vague or few clues, a situation that was familiar to all of them. Users preferred to have, in a first step, easy access to limited but to-the-point lexical information while access to rich semantic information should be provided at a second step. They were interested in rich lexical material although they were not really able to identify the relations that would help them retrieve it. They strongly preferred an organization of material by concept and PoS and appreci1 EKFRASIS http://www  http://www.ilsp.gr/ekfrasi_eng.html was funded by the General Secretariat of Research and Technology / Greece. ated easy access to normative information. 1 Introduction We present an investigation of user needs that was conducted in the process of developing a platform for supporting authoring work in Modern Greek. One main component of this platform is the dictionary EKFRASIS (literally expression). EKFRASIS exploits technology and ideas developed for the Semantic Web (Guarino and Welty, 2002) to encode a conceptually organized dictionary enriched with translations as well as a wealth of lexicographic and morphosyntactic information. The overall organisation of the dictionary and, partially of the platform, aims at helping the user who needs a word but has few clues or just guesses about it and about its way of use. The interviewed users were all professionals: journalists, translators, editors and authors of literature. In this survey, we aimed to map user expectations concerning interaction with the dictionary rather than to look for appropriate ways for populating it. Of course, some of the conclusions reached here may be useful for collecting linguistic material as well. "
W10-3701 "1 Invited Talk Abstract Multiword units (MWUs) are critical in processing and understanding texts and have been extensively studied in relation to their occurrences in texts. MWUs also play an essential role in organising vocabulary, which is most prominently visible in domain-specific terminologies. There has been, however, a limited and mostly theoretical concern with the latter aspect of MWUs; researchers interested in NLP-related applications of terminologies have not paid sufficient attention to this aspect. In this talk I will start by giving the basic framework within which the study of MWUs from the point of view of vocabulary can be carried out, in the process clarifying the relationships between studies of MWUs in texts and those in relation to vocabulary. I will then introduce some of the theoretical studies in terminological structure which I have carried out in recent years. Referring to some of the problems that practically-oriented research in terminology processing is currently facing, I will argue why, how and in what possible ways the understanding of the roles MWUs take in terminological structure constitute a sin qua non condition for making a breakthrough in current text-oriented studies of terminological MWUs. "
W10-3702 "rsaw f.makowiecki@ student.uw.edu.pl Abstract The morphosyntactic treatment of multiword units is particularly challenging in morphologically rich languages. We present a comparative study of two formalisms meant for lexicalized description of MWUs in Polish. We show their expressive power and describe encoding experiments, involving novice and expert lexicographers, and allowing to evaluate the accuracy and efficiency of both implementations. "
W10-3703 " Idioms are not only interesting but also distinctive in a language for its continuity and metaphorical meaning in its context. This paper introduces the construction of a Chinese idiom knowledge base by the Institute of Computational Linguistics at Peking University and describes an experiment that aims at the automatic emotion classification of Chinese idioms. In the process, we expect to know more about how the constituents in a fossilized composition like an idiom function so as to affect its semantic or grammatical properties. As an important Chinese language resource, our idiom knowledge base will play a major role in applications such as linguistic research, teaching Chinese as a foreign language and even as a tool for preserving this non-material Chinese cultural and historical heritage.  "
W10-3704 " In this paper we investigate the automatic acquisition of Arabic Multiword Expressions (MWE). We propose three complementary approaches to extract MWEs from available data resources. The first approach relies on the correspondence asymmetries between Arabic Wikipedia titles and titles in 21 different languages. The second approach collects English MWEs from Princeton WordNet  "
W10-3705 "eneva {Eric.Wehrli, Violeta.Seretan, Luka.Nerima}@unige.ch Abstract Identifying collocations in a sentence, in order to ensure their proper processing in subsequent applications, and performing the syntactic analysis of the sentence are interrelated processes. Syntactic information is crucial for detecting collocations, and vice versa, collocational information is useful for parsing. This article describes an original approach in which collocations are identified in a sentence as soon as possible during the analysis of that sentence, rather than at the end of the analysis, as in our previous work. In this way, priority is given to parsing alternatives involving collocations, and collocational information guide the parser through the maze of alternatives. This solution was shown to lead to substantial improvements in the performance of both tasks (collocation identification and parsing), and in that of a subsequent task (machine translation). "
W10-3706 " This paper presents the automatic extraction of Complex Predicates (CPs) in Bengali with a special focus on compound verbs (Verb + Verb) and conjunct verbs (Noun /Adjective + Verb). The lexical patterns of compound and conjunct verbs are extracted based on the information of shallow morphology and available seed lists of verbs. Lexical scopes of compound and conjunct verbs in consecutive sequence of Complex Predicates (CPs) have been identified. The fine-grained error analysis through confusion matrix highlights some insufficiencies of lexical patterns and the impacts of different constraints that are used to identify the Complex Predicates (CPs). System achieves F-Scores of 75.73%, and 77.92% for compound verbs and 89.90% and 89.66% for conjunct verbs respectively on two types of Bengali corpus. 1 Int  "
W10-3707 " Data preprocessing plays a crucial role in phrase-based statistical machine translation (PB-SMT). In this paper, we show how single-tokenization of two types of multi-word expressions (MWE), namely named entities (NE) and compound verbs, as well as their prior alignment can boost the performance of PB-SMT. Single-tokenization of compound verbs and named entities (NE) provides significant gains over the baseline PB-SMT system. Automatic alignment of NEs substantially improves the overall MT performance, and thereby the word alignment quality indirectly. For establishing NE alignments, we transliterate source NEs into the target language and then compare them with the target NEs. Target language NEs are first converted into a canonical form before the comparison takes place. Our best system achieves statistically significant improvements (4.59 BLEU points absolute, 52.5% relative improvement) on an EnglishBangla translation task. 1 Introduct  "
W10-3708 "Abstract Most word segmentation methods employed in Chinese Information Retrieval systems are based on a static dictionary or a model trained against a manually segmented corpus. These general segmentation approaches may not be optimal because they disregard information within semantic units. We propose a novel method for improving word-based Chinese IR, which performs segmentation according to the tightness of phrases. In order to evaluate the effectiveness of our method, we employ a new test collection of 203 queries, which include a broad distribution of phrases with different tightness values. The results of our experiments indicate that our method improves IR performance as compared with a general word segmentation approach. The experiments also demonstrate the need for the development of better evaluation corpora. "
W10-3709 " In order to accomplish the deep semantic understanding of a language, it is essential to analyze the meaning of predicate phrases, a content word plus functional expressions. In agglutinating languages such as Japanese, however, sentential predicates are multi-morpheme expressions and all the functional expressions including those unnecessary to the meaning of the predicate are merged into one phrase. This triggers an increase in surface forms, which is problematic for NLP systems. We solve this by introducing simplified surface forms of predicates that retain only the crucial meaning of the functional expressions. We construct paraphrasing rules based on syntactic and semantic theories in linguistics. The results of experiments show that our system achieves the high accuracy of 77% while reducing the differences in surface forms by 44%, which is quite close to the performance of manually simplified predicates. 1 Introduction  "
W10-3710 "In linguistic studies, reduplication generally means the repetition of any linguistic unit such as a phoneme, morpheme, word, phrase, clause or the utterance as a whole. The identification of reduplication is a part of general task of identification of multiword expressions (MWE). In the present work, reduplications have been identified from the Bengali corpus of the articles of Rabindranath Tagore. The present rule-based approach is divided into two phases. In the first phase, identification of reduplications has been done mainly at general expression level and in the second phase, their structural and semantics classifications are analyzed. The system has been evaluated with average Precision, Recall and FScore values of 92.82%, 91.50% and 92.15% respectively. "
W10-3711 "Istituto di Linguistica Computazionale Antonio Zampolli (ILC-CNR)  Dipartimento di Informatica, Universit` a di Pisa,  CLIC Language Interaction and Computation Lab {francesca.bonin, felice.dellorletta, giulia.venturi, simonetta.montemagni}@ilc.cnr.it Abstract In this paper we tackle the challenging task of Multi-word term (MWT) extraction from different types of specialized corpora. Contrastive filtering of previously extracted MWTs results in a considerable increment of acquired domainspecific terms. "
W10-3712 "Abstract In this paper we present a hybrid approach for identifying Japanese functional expressions and its application in a Japanese reading assistant. We combine the results of machine learning and patternbased methods to identify several types of functional expressions. We show that by using our approach we can double the coverage of previous approaches and still maintain the high level of performance necessary for our application. "
W10-3713 " The Varro toolkit offers an intuitive mechanism for extracting syntactically motivated multi-word expressions (MWEs) from dependency treebanks by looking for recurring connected subtrees instead of subsequences in strings. This approach can find MWEs that are in varying orders and have words inserted into their components. This paper also proposes description length gain as a statistical correlation measure well-suited to tree structures. 1 Intro  "
W10-4201 " Rating-scale evaluations are common in NLP, but are problematic for a range of reasons, e.g. they can be unintuitive for evaluators, inter-evaluator agreement and self-consistency tend to be low, and the parametric statistics commonly applied to the results are not generally considered appropriate for ordinal data. In this paper, we compare rating scales with an alternative evaluation paradigm, preferencestrength judgement experiments (PJEs), where evaluators have the simpler task of deciding which of two texts is better in terms of a given quality criterion. We present three pairs of evaluation experiments assessing text fluency and clarity for different data sets, where one of each pair of experiments is a rating-scale experiment, and the other is a PJE. We find the PJE versions of the experiments have better evaluator self-consistency and interevaluator agreement, and a larger proportion of variation accounted for by system differences, resulting in a larger number of significant differences being found. 1 Introductio  "
W10-4202 "<NoAbstract>"
W10-4203 "Abstract In this paper we present a reference generation model based on Reference Domain Theory which gives a dynamic account of reference. This reference model assumes that each referring act both relies and updates the reference context. We present a formal definition of a reference domain, a generation algorithm and its instantiation in the GIVE challenge. "
W10-4204 " We present a novel approach to natural language generation (NLG) that applies hierarchical reinforcement learning to text generation in the wayfinding domain. Our approach aims to optimise the integration of NLG tasks that are inherently different in nature, such as decisions of content selection, text structure, user modelling, referring expression generation (REG), and surface realisation. It also aims to capture existing interdependencies between these areas. We apply hierarchical reinforcement learning to learn a generation policy that captures these interdependencies, and that can be transferred to other NLG tasks. Our experimental resultsin a simulated environmentshow that the learnt wayfinding policy outperforms a baseline policy that takes reasonable actions but without optimization. "
W10-4205 "{delson,kathy}@cs.columbia.edu Abstract We describe a method for assigning English tense and aspect in a system that realizes surface text for symbolically encoded narratives. Our testbed is an encoding interface in which propositions that are attached to a timeline must be realized from several temporal viewpoints. This involves a mapping from a semantic encoding of time to a set of tense/aspect permutations. The encoding tool realizes each permutation to give a readable, precise description of the narrative so that users can check whether they have correctly encoded actions and statives in the formal representation. Our method selects tenses and aspects for individual event intervals as well as subintervals (with multiple reference points), quoted and unquoted speech (which reassign the temporal focus), and modal events such as conditionals. "
W10-4206 "Abstract This paper investigates the relationship between the results of an extrinsic, taskbased evaluation of an NLG system and various metrics measuring both surface and deep semantic textual properties, including relevance. The latter rely heavily on domain knowledge. We show that they correlate systematically with some measures of performance. The core argument of this paper is that more domain knowledge-based metrics shed more light on the relationship between deep semantic properties of a text and task performance. 1 Introduction Evaluation methodology in NLG has generated a lot of interest. Some recent work suggested that the relationship between various intrinsic and extrinsic evaluation methods (Sp  arck-Jones and Galliers, 1996) is not straightforward (Reiter and Belz, 2009; Gatt and Belz, to appear), leading to some arguments for more domain-specific intrinsic metrics (Foster, 2008). One reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform system development. Indeed, this is often the stated purpose of evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003), which were originally characterised as evaluation understudies. In this paper we take up these questions in the context of a knowledge-based NLG system, BT-45 (Portet et al., 2009), which summarises medical data for decision support purposes in a Neonatal Intensive Care Unit (NICU). Our extrinsic data comes from an experiment involving complex medical decision making based on automatically generated and human-authored texts (van der Meulen et al., 2009). This gives us the opportunity to directly compare the textual characteristics of generated and human-written summaries and their relationship to decision-making performance. The present work uses data from an earlier study (Gatt and Portet, 2009), which presented some preliminary results along these lines for the system in question. We extend this work in a number of ways. Our principal aim is to test the validity not only of general-purpose metrics which measure surface properties of text, but also of metrics which make use of domain knowledge, in the sense that they attempt to relate the deep semantics of the texts to extrinsic factors, based on an ontology for the BT-45 domain. After an overview of related work in section 2, the BT-45 system, its domain ontology and the extrinsic evaluation are described in section 3. The ontology plays an important role in the evaluation metrics presented in Section 5. Finally, the evaluation of the methods is presented in Section 6, before discussing and concluding in Section 7. "
W10-4207 " We present the situated reference generation module of a hybrid human-robot interaction system that collaborates with a human user in assembling target objects from a wooden toy construction set. The system contains a sub-symbolic goal inference system which is able to detect the goals and errors of humans by analysing their verbal and non-verbal behaviour. The dialogue manager and reference generation components then use situated references to explain the errors to the human users and provide solution strategies. We describe a user study comparing the results from subjects who heard constant references to those who heard references generated by an adaptive process. There was no difference in the objective results across the two groups, but the subjects in the adaptive condition gave higher subjective ratings to the robots abilities as a conversational partner. An analysis of the objective and subjective results found that the main predictors of subjective user satisfaction were the users performance at the assembly task and the number of times they had to ask for instructions to be repeated. "
W10-4208 " In this paper, we propose a general way of constructing an NLG system that permits the systematic exploration of the effects of particular system choices on output quality. We call a system developed according to this model a Programmable Instrumented Generator (PIG). Although a PIG could be designed and implemented from scratch, it is likely that researchers would also want to create PIGs based on existing systems. We therefore propose an approach to instrumenting an NLG system so as to make it PIG-like. To experiment with the idea, we have produced code to support the instrumenting of any NLG system written in Java. We report on initial experiments with instrumenting two existing systems and attempting to tune them to produce text satisfying complex stylistic constraints. 1 Introductio  "
W10-4209 "Abstract The semantic web is a general vision for supporting knowledge-based processing across the WWW and its successors. As such, semantic web technology has potential to support the exchange and processing of complex NLG data. This paper discusses one particular approach to data sharing and exchange that was developed for NLG  the RAGS framework. This was developed independently of the semantic web. RAGS was relatively complex and involved a number of idiosyncratic features. However, we present a rational reconstruction of RAGS in terms of semantic web concepts, which yields a relatively simple approach that can exploit semantic web technology directly. Given that RAGS was motivated by the concerns of the NLG community, it is perhaps remarkable that its aspirations seem to fit so well with semantic web technology. "
W10-4210 " This paper discusses the basic structures necessary for the generation of reference to objects in a visual scene. We construct a study designed to elicit naturalistic referring expressions to relatively complex objects, and find aspects of reference that have not been accounted for in work on Referring Expression Generation (REG). This includes reference to object parts, size comparisons without crisp measurements, and the use of analogies. By drawing on research in cognitive science, neurophysiology, and psycholinguistics, we begin developing the input structure and background knowledge necessary for an algorithm capable of generating the kinds of reference we observe. 1 Intro  "
W10-4211 "Abstract In this paper we present a complete system for automatically generating natural language abstracts of meeting conversations. This system is comprised of components relating to interpretation of the meeting documents according to a meeting ontology, transformation or content selection from that source representation to a summary representation, and generation of new summary text. In a formative user study, we compare this approach to gold-standard human abstracts and extracts to gauge the usefulness of the different summary types for browsing meeting conversations. We find that our automatically generated summaries are ranked significantly higher than human-selected extracts on coherence and usability criteria. More generally, users demonstrate a strong preference for abstract-style summaries over extracts. "
W10-4212 "K Abstract The generation of referring expressions (GRE), an important subtask of Natural Language Generation (NLG) is to generate phrases that uniquely identify domain entities. Until recently, many GRE algorithms were developed using only simple formalisms, which were taylor made for the task. Following the fast development of ontology-based systems, reinterpretations of GRE in terms of description logic (DL) have recently started to be studied. However, the expressive power of these DL-based algorithms is still limited, not exceeding that of older GRE approaches. In this paper, we propose a DL-based approach to GRE that exploits the full power of OWL2. Unlike existing approaches, the potential of reasoning in GRE is explored. "
W10-4213 "Abstract We present a framework for reformulating sentences by applying transfer rules on a typed dependency representation. We specify a list of operations that the framework needs to support and argue that typed dependency structures are currently the most suitable formalism for complex lexico-syntactic paraphrasing. We demonstrate our approach by reformulating sentences expressing the discourse relation of causation using four lexico-syntactic discourse markers  cause as a verb and as a noun, because as a conjunction and because of as a preposition. "
W10-4214 "gy Abstract In the field of referring expression generation, while in the static domain both intrinsic and extrinsic evaluations have been considered, extrinsic evaluation in the dynamic domain, such as in a situated collaborative dialog, has not been discussed in depth. In a dynamic domain, a crucial problem is that referring expressions do not make sense without an appropriate preceding dialog context. It is unrealistic for an evaluation to simply show a human evaluator the whole period from the beginning of a dialog up to the time point at which a referring expression is used. Hence, to make evaluation feasible it is indispensable to determine an appropriate shorter context. In order to investigate the context necessary to understand a referring expression in a situated collaborative dialog, we carried out an experiment with 33 evaluators and a Japanese referring expression corpus. The results contribute to finding the proper contexts for extrinsic evalution in dynamic domains. "
W10-4215 "Abstract This paper proposes a method for extracting high-level rules for expository dialogue generation. The rules are extracted from dialogues that have been authored by expert dialogue writers. We examine the rules that can be extracted by this method, focusing on whether different dialogues and authors exhibit different dialogue styles. "
W10-4216 "Abstract Fluency rankers are used in modern sentence generation systems to pick sentences that are not just grammatical, but also fluent. It has been shown that feature-based models, such as maximum entropy models, work well for this task. Since maximum entropy models allow for incorporation of arbitrary real-valued features, it is often attractive to create very general feature templates, that create a huge number of features. To select the most discriminative features, feature selection can be applied. In this paper we compare three feature selection methods: frequency-based selection, a generalization of maximum entropy feature selection for ranking tasks with realvalued features, and a new selection method based on feature value correlation. We show that the often-used frequency-based selection performs badly compared to maximum entropy feature selection, and that models with a few hundred well-picked features are competitive to models with no feature selection applied. In the experiments described in this paper, we compressed a model of approximately 490.000 features to 1.000 features. "
W10-4217 " Building NLG systems, in particular statistical ones, requires parallel data (paired inputs and outputs) which do not generally occur naturally. In this paper, we investigate the idea of automatically extracting parallel resources for data-to-text generation from comparable corpora obtained from the Web. We describe our comparable corpus of data and texts relating to British hills and the techniques for extracting paired input/output fragments we have developed so far. 1 Introductio  "
W10-4218 "Abstract Critical software most often requires an independent validation and verification (IVV). IVV is usually performed by domain experts, who are not familiar with specific, many times formal, development technologies. In addition, model-based testing (MBT) is a promising testing technique for the verification of critical software. Test cases generated by MBT tools are logical descriptions. The problem is, then, to provide natural language (NL) descriptions of these test cases, making them accessible to domain experts. In this paper, we present ongoing research aimed at finding a suitable method for generating NL descriptions from test cases in a formal specification language. A first prototype has been developed and applied to a real-world project in the aerospace sector. "
W10-4219 "Abstract Today there exist a growing number of framenet-like resources offering semantic and syntactic phrase specifications that can be exploited by natural language generation systems. In this paper we present on-going work that provides a starting point for exploiting framenet information for multilingual natural language generation. We describe the kind of information offered by modern computational lexical resources and discuss how template-based generation systems can benefit from them. "
W10-4220 " We have begun a project to automatically create the lexico-syntactic resources for a microplanner as a side-effect of running a domain-specific language understanding system. The resources are parameterized synchronous TAG Derivation Trees. Since the KB is assembled from the information in the texts that these resources are abstracted from, it will decompose along those same lines when used for generation. As all possible ways of expressing each concept are pre-organized into general patterns known to be linguistically-valid (they were observed in natural text), we obtain an architectural account for expressibility. 1. Expres  "
W10-4221 "Abstract In this paper we describe a cross-linguistic experiment in attribute selection for referring expression generation. We used a graph-based attribute selection algorithm that was trained and cross-evaluated on English and Dutch data. The results indicate that attribute selection can be done in a largely language independent way. "
W10-4222 "Abstract Ontologies and datasets for the Semantic Web are encoded in OWL formalisms that are not easily comprehended by people. To make ontologies accessible to human domain experts, several research groups have developed ontology verbalisers using Natural Language Generation. In practice ontologies are usually composed of simple axioms, so that realising them separately is relatively easy; there remains however the problem of producing texts that are coherent and efficient. We describe in this paper some methods for producing sentences that aggregate over sets of axioms that share the same logical structure. Because these methods are based on logical structure rather than domain-specific concepts or language-specific syntax, they are generic both as regards domain and language. "
W10-4223 "nds {s.wubben,antal.vdnbosch,e.j.krahmer}@uvt.nl Abstract In this paper we investigate the automatic generation and evaluation of sentential paraphrases. We describe a method for generating sentential paraphrases by using a large aligned monolingual corpus of news headlines acquired automatically from Google News and a standard Phrase-Based Machine Translation (PBMT) framework. The output of this system is compared to a word substitution baseline. Human judges prefer the PBMT paraphrasing system over the word substitution system. We demonstrate that BLEU correlates well with human judgements provided that the generated paraphrased sentence is sufficiently different from the source sentence. "
W10-4224 "Abstract The paper presents two models for producing and understanding situationally appropriate referring expressions (REs) during a discourse about large-scale space. The models are evaluated against an empirical production experiment. 1 Introduction and Background For situated interaction, an intelligent system needs methods for relating entities in the world, its representation of the world, and the natural language references exchanged with its user. Human natural language processing and algorithmic approaches alike have been extensively studied for application domains restricted to small visual scenes and other small-scale surroundings. Still, rather little research has addressed the specific issues involved in establishing reference to entities outside the currently visible scene. The challenge that we address here is how the focus of attention can shift over the course of a discourse if the domain is larger than the currently visible scene. The generation of referring expressions (GRE) has been viewed as an isolated problem, focussing on efficient algorithms for determining which information from the domain must be incorporated in a noun phrase (NP) such that this NP allows the hearer to optimally understand which referent is meant. The domains of such approaches usually consist of small, static domains or simple visual scenes. In their seminal work Dale and Reiter (1995) present the Incremental Algorithm (IA) for GRE. Recent extensions address some of its shortcomings, such as negated and disjoined properties (van Deemter, 2002) and an account of salience for generating contextually appropriate shorter REs (Krahmer and Theune, 2002). Other, alternative GRE algorithms exist (Horacek, 1997; Bateman, 1999; Krahmer et al., 2003). However, all these algorithms rely on a given domain of discourse constituting the current context (or focus of attention). The task of the GRE algorithm is then to single out the intended referent against the other members of the context, which act as potential distractors. As long as the domains are such closed-context scenarios, the intended referent is always in the current focus. We address the challenge of producing and understanding of references to entities that are outside the current focus of attention, because they have not been mentioned yet and are beyond the currently observable scene. Our approach relies on the dichotomy between small-scale space and large-scale space for human spatial cognition. Large-scale space is a space which cannot be perceived at once; its global structure must be derived from local observations over time (Kuipers, 1977). In everyday situations, an office environment, ones house, or a university campus are large-scale spaces. A table-top or a part of an office are examples of small-scale space. Despite large-scale space being not fully observable, people can nevertheless have a reasonably complete mental representation of, e.g., their domestic or work environments in their cognitive maps. Details might be missing, and people might be uncertain about particular things and states of affairs that are known to change frequently. Still, people regularly engage in a conversation about such an environment, making successful references to spatially located entities. It is generally assumed that humans adopt a partially hierarchical representation of spatial organization (Stevens and Coupe, 1978; McNamara, 1986). The basic units of such a representation are topological regions (i.e., more or less clearly bounded spatial areas) (Hirtle and Jonides, 1985). Paraboni et al. (2007) are among the few to address the issue of generating references to entities outside the immediate environment, and present an algorithm for context determination in hierar... ... ... ... ... office1 office4 office1 floor1 floor2 building 1A building 3B old campus kitchen office2 helpdesk office3 office5 floor1 floor2 floor1 building 2C building 3B new campus Dienstag, 14. April 2009 (a) Example for a hierarchical representation of space. (b) Illustration of the TA principle: starting from the attentional anchor (a), the smallest sub-hierarchy containing both a and the intended referent (r) is formed incrementally. Figure 1: TA in a spatial hierarchy. chically ordered domains. However, since it is mainly targeted at producing textual references to entities in written documents (e.g., figures and tables in book chapters), they do not address the challenges of physical and perceptual situatedness. Large-scale space can be viewed as a hierarchically ordered domain. To keep track of the referential context in such a domain, in our previous work we propose the principle of topological abstraction (TA, summarized in Fig. 1) for context extension (Zender et al., 2009a), similar to Ancestral Search (Paraboni et al., 2007). In (Zender et al., 2009b), we describe the integration of the approach in an NLP system for situated human-robot dialogues and present two algorithms instantiating the TA principle for GRE and resolving referring expressions (RRE), respectively. It relies on two parameters: the location of the intended referent r, and the attentional anchor a. As discussed in our previous works, for single utterances the anchor is the physical position where it is made (i.e., the utterance situation (Devlin, 2006)). Below, we propose models for attentional anchor-progression for longer discourses about large-scale space, and evaluate them against real-world data. "
W10-4225 "<NoAbstract>"
W10-4226 " There were three GREC Tasks at Generation Challenges 2010: GREC-NER required participating systems to identify all people references in texts; for GRECNEG, systems selected coreference chains for all people entities in texts; and GRECFull combined the NER and NEG tasks, i.e. systems identified and, if appropriate, replaced references to people in texts. Five teams submitted 10 systems in total, and we additionally created baseline systems for each task. Systems were evaluated automatically using a range of intrinsic metrics. In addition, systems were assessed by human judges using preference strength judg ments. This report presents the evaluation results, along with descriptions of the three GREC tasks, the evaluation methods, and the participating systems. "
W10-4227 " The problem of Named Entity Generation is expressed as a conditional probability model over a structured domain. By defining a factor-graph model over the mentions of a text, we obtain a compact parameterization of what is learned using the SampleRank algorithm. 1 Int  "
W10-4228 "<NoAbstract>"
W10-4229 " This paper presents the experiments carried out at Jadavpur University as part of the participation in the GREC Named Entity Generation Challenge 2010. The Baseline system is based on the SEMCAT, SYNCAT and SYNFUNC features of REF and REG08-TYPE and CASE features of REFEX elements. The discourse level system is based on the additional positional features: paragraph number, sentence number, word position in the sentence and mention number of a particular named entity in the document. The inclusion of discourse level features has improved the performance of the system. 1 Baseline  "
W10-4230 "Abstract We present the UMUS (Universit  e du Maine/Universit  at Stuttgart) submission for the NEG task at GREC10. We refined and tuned our 2009 system but we still rely on predicting generic labels and then choosing from the list of expressions that match those labels. We handled recursive expressions with care by generating specific labels for all the possible embeddings. The resulting system performs at a type accuracy of 0.84 an a string accuracy of 0.81 on the development set. "
W10-4231 " This report describes the methods and results of a system developed for the GREC Named Entity Challenge 2010. We detail the refinements made to our 2009 submission and present the output of the selfevaluation on the development data set. 1 Intro  "
W10-4232 " This report describes the methods and results of a system developed for the GREC Named Entity Recognition and GREC Named Entity Regeneration Challenges 2010. We explain our process of automatically annotating surface text, as well as how we use this output to select improved referring expressions for named entities. 1 I  "
W10-4233 "Abstract We describe the second installment of the Challenge on Generating Instructions in Virtual Environments (GIVE-2), a shared task for the NLG community which took place in 2009-10. We evaluated seven NLG systems by connecting them to 1825 users over the Internet, and report the results of this evaluation in terms of objective and subjective measures. "
W10-4234 "and {p.piwek, s.stoyanchev}@open.ac.uk Abstract. The paper briefly describes the First Shared Task Evaluation Challenge on Question Generation that took place in Spring 2010. The campaign included two tasks: Task A  Question Generation from Paragraphs and Task B  Question Generation from Sentences. An overview of each of the tasks is provided. Keywords: question generation, shared task evaluation campaign. "
W10-4235 "Abstract We invite the research community to consider challenges for NLG which arise from uncertainty. NLG systems should be able to adapt to their audience and the generation environment in general, but often the important features for adaptation are not known precisely. We explore generation challenges which could employ simulated environments to study NLG which is adaptive under uncertainty, and suggest possible metrics for such tasks. It would be particularly interesting to explore how different planning approaches to NLG perform in challenges involving uncertainty in the generation environment. "
W10-4236 "Abstract In this paper, we propose a new shared task called HOO: Helping Our Own. The aim is to use tools and techniques developed in computational linguistics to help people writing about computational linguistics. We describe a text-to-text generation scenario that poses challenging research questions, and delivers practical outcomes that are useful in the first case to our own community and potentially much more widely. Two specific factors make us optimistic that this task will generate useful outcomes: one is the availability of the ACL Anthology, a large corpus of the target text type; the other is that CL researchers who are non-native speakers of English will be motivated to use prototype systems, providing informed and precise feedback in large quantity. We lay out our plans in detail and invite comment and critique with the aim of improving the nature of the planned exercise. "
W10-4237 "Abstract In many areas of NLP reuse of utility tools such as parsers and POS taggers is now common, but this is still rare in NLG. The subfield of surface realisation has perhaps come closest, but at present we still lack a basis on which different surface realisers could be compared, chiefly because of the wide variety of different input representations used by different realisers. This paper outlines an idea for a shared task in surface realisation, where inputs are provided in a common-ground representation formalism which participants map to the types of input required by their system. These inputs are derived from existing annotated corpora developed for language analysis (parsing etc.). Outputs (realisations) are evaluated by automatic comparison against the human-authored text in the corpora as well as by human assessors. "
W11-0201 " The extraction of protein-protein interactions (PPIs) reported in scientific publications is one of the most studied topics in Text Mining in the Life Sciences, as such algorithms can substantially decrease the effort for databases curators. The currently best methods for this task are based on analyzing the dependency tree (DT) representation of sentences. Many approaches exploit only topological features and thus do not yet fully exploit the information contained in DTs. We show that incorporating the grammatical information encoded in the types of the dependencies in DTs noticeably improves extraction performance by using a pattern matching approach. We automatically infer a large set of linguistic patterns using only information about interacting proteins. Patterns are then refined based on shallow linguistic features and the semantics of dependency types. Together, these lead to a total improvement of 17.2 percent points in F 1 , as evaluated on five publicly available PPI corpora. More than half of that improvement is gained by properly handling dependency types. Our method provides a general framework for building task-specific relationship extraction methods that do not require annotated training data. Furthermore, our observations offer methods to improve upon relation extraction approaches. "
W11-0202 "Abstract Entailment detection systems are generally designed to work either on single words, relations or full sentences. We propose a new task  detecting entailment between dependency graph fragments of any type  which relaxes these restrictions and leads to much wider entailment discovery. An unsupervised framework is described that uses intrinsic similarity, multi-level extrinsic similarity and the detection of negation and hedged language to assign a confidence score to entailment relations between two fragments. The final system achieves 84.1% average precision on a data set of entailment examples from the biomedical domain. "
W11-0203 " Accurate phenotype mapping will play an important role in facilitating Phenome-Wide Association Studies (PheWAS), and potentially in other phenomics based studies. The PheWAS approach investigates the association between genetic variation and an extensive range of phenotypes in a high-throughput manner to better understand the impact of genetic variations on multiple phenotypes. Herein we define the phenotype mapping problem posed by PheWAS analyses, discuss the challenges, and present a machine-learning solution. Our key ideas include the use of weighted Jaccard features and term augmentation by dictionary lookup. When compared to string similarity metric-based features, our approach improves the F-score from 0.59 to 0.73. With augmentation we show further improvement in F-score to 0.89. For terms not covered by the dictionary, we use transitive closure inference and reach an F-score of 0.91, close to a level sufficient for practical use. We also show that our model generalizes well to phenotypes not used in our training dataset. "
W11-0204 "Abstract In comparative genomics, functional annotations are transferred from one organism to another relying on sequence similarity. With more than 20 million citations in PubMed, text mining provides the ideal tool for generating additional large-scale homology-based predictions. To this end, we have refined a recent dataset of biomolecular events extracted from text, and integrated these predictions with records from public gene databases. Accounting for lexical variation of gene symbols, we have implemented a disambiguation algorithm that uniquely links the arguments of 11.2 million biomolecular events to well-defined gene families, providing interesting opportunities for query expansion and hypothesis generation. The resulting MySQL database, including all 19.2 million original events as well as their homology-based variants, is publicly available at http://bionlp.utu.fi/  http://bionlp.utu.fi/ . "
W11-0205 "Abstract A simple and accurate method for assigning broad semantic classes to text strings is presented. The method is to map text strings to terms in ontologies based on a pipeline of exact matches, normalized strings, headword matching, and stemming headwords. The results of three experiments evaluating the technique are given. Five semantic classes are evaluated against the CRAFT corpus of full-text journal articles. Twenty semantic classes are evaluated against the corresponding full ontologies, i.e. by reflexive matching. One semantic class is evaluated against a structured test suite. Precision, recall, and F-measure on the corpus when evaluating against only the ontologies in the corpus is micro-averaged 67.06/78.49/72.32 and macro-averaged 69.84/83.12/75.31. Accuracy on the corpus when evaluating against all twenty semantic classes ranges from 77.12% to 95.73%. Reflexive matching is generally successful, but reveals a small number of errors in the implementation. Evaluation with the structured test suite reveals a number of characteristics of the performance of the approach. "
W11-0206 "Abstract Traditionally, automated triage of papers is performed using lexical (unigram, bigram, and sometimes trigram) features. This paper explores the use of information extraction (IE) techniques to create richer linguistic features than traditional bag-of-words models. Our classifier includes lexico-syntactic patterns and more-complex features that represent a pattern coupled with its extracted noun, represented both as a lexical term and as a semantic category. Our experimental results show that the IE-based features can improve performance over unigram and bigram features alone. We present intrinsic evaluation results of full-text document classification experiments to determine automatically whether a paper should be considered of interest to biologists at the Mouse Genome Informatics (MGI) system at the Jackson Laboratories. We also further discuss issues relating to design and deployment of our classifiers as an application to support scientific knowledge curation at MGI. "
W11-0207 "<NoAbstract>"
W11-0208 " Named Entity Recognition (NER) is an important first step for BioNLP tasks, e.g., gene normalization and event extraction. Employing supervised machine learning techniques for achieving high performance recent NER systems require a manually annotated corpus in which every mention of the desired semantic types in a text is annotated. However, great amounts of human effort is necessary to build and maintain an annotated corpus. This study explores a method to build a high-performance NER without a manually annotated corpus, but using a comprehensible lexical database that stores numerous expressions of semantic types and with huge amount of unannotated texts. We underscore the effectiveness of our approach by comparing the performance of NERs trained on an automatically acquired training data  training data and on a manually annotated corpus. 1 Introduct  "
W11-0209 " Semantic Role Labeling (SRL) plays a key role in many NLP applications. The development of SRL systems for the biomedical domain is frustrated by the lack of large domainspecific corpora that are labeled with semantic roles. Corpus development has been very expensive and time-consuming. In this paper we propose a method for building frame-based corpus on the basis of domain knowledge provided by ontologies. We believe that ontologies, as a structured and semantic representation of domain knowledge, can instruct and ease the tasks in building the corpora. In the paper we present a corpus built by using the method. We compared it to BioFrameNet, and examined the gaps between the semantic classification of the target words in the domainspecific corpus and in FrameNet and PropBank/VerbNet. "
W11-0210 "Abstract One of the reasons for which the resolution of coreferences has remained a challenging information extraction task, especially in the biomedical domain, is the lack of training data in the form of annotated corpora. In order to address this issue, we developed the HANAPIN corpus. It consists of full-text articles from biochemistry literature, covering entities of several semantic types: chemical compounds, drug targets (e.g., proteins, enzymes, cell lines, pathogens), diseases, organisms and drug effects. All of the coreferring expressions pertaining to these semantic types were annotated based on the annotation scheme that we developed. We observed four general types of coreferences in the corpus: sortal, pronominal, abbreviation and numerical. Using the MASI distance metric, we obtained 84% in computing the inter-annotator agreement in terms of Krippendorffs alpha. Consisting of 20 full-text, open-access articles, the corpus will enable other researchers to use it as a resource for their own coreference resolution methodologies. "
W11-0211 "Abstract The paper discuses problems in annotating a corpus containing Polish clinical data with low level linguistic information. We propose an approach to tokenization and automatic morphologic annotation of data that uses existing programs combined with a set of domain specific rules and vocabulary. Finally we present the results of manual verification of the annotation for a subset of data. "
W11-0212 " We present a bootstrapping approach to infer new proteins, locations and protein-location pairs by combining UniProt seed proteinlocation pairs with dependency paths from a large collection of text. Of the top 20 system proposed protein-location pairs, 18 were in UniProt or supported by online evidence. Interestingly, 3 of the top 20 locations identified by the system were in the UniProt description, but missing from the formal ontology. 1 I  "
W11-0213 " Research in the biomedical domain can have a major impact through open sharing of data produced. In this study, we use machine learning for the automatic identification of data deposition sentences in research articles. Articles containing deposition sentences are correctly identified with 73% f-measure. These results show the potential impact of our method for literature curation. 1 Background Research in the biomedical domain aims at furthering the knowledge of biological processes and improving human health. Major contributions towards this goal can be achieved by sharing the results of research efforts with the community, including datasets produced in the course of the research work. While such sharing behavior is encouraged by funding agencies and scientific journals, recent work has shown that the ratio of data sharing is still modest compared to actual data production. For instance, Ochsner et al. (2008) found the deposition rate of microarray data to be less than 50% for work published in 2007. Information about the declaration of data deposition in research papers can  "
W11-0214 "Abstract The construction of pathways is a major focus of present-day biology. Typical pathways involve large numbers of entities of various types whose associations are represented as reactions involving arbitrary numbers of reactants, outputs and modifiers. Until recently, few information extraction approaches were capable of resolving the level of detail in text required to support the annotation of such pathway representations. We argue that event representations of the type popularized by the BioNLP Shared Task are potentially applicable for pathway annotation support. As a step toward realizing this possibility, we study the mapping from a formal pathway representation to the event representation in order to identify remaining challenges in event extraction for pathway annotation support. Following initial analysis, we present a detailed study of protein association and dissociation reactions, proposing a new event class and representation for the latter and, as a step toward its automatic extraction, introduce a manually annotated resource incorporating the type among a total of nearly 1300 annotated event instances. As a further practical contribution, we introduce the first pathway-to-event conversion software for SBML/CellDesigner pathways and discuss the opportunities arising from the ability to convert the substantial existing pathway resources to events. "
W11-0215 "Abstract Protein modifications, in particular posttranslational modifications, have a central role in bringing about the full repertoire of protein functions, and the identification of specific protein modifications is important for understanding biological systems. This task presents a number of opportunities for the automatic support of manual curation efforts. However, the sheer number of different types of protein modifications is a daunting challenge for automatic extraction that has so far not been met in full, with most studies focusing on single modifications or a few prominent ones. In this work, aim to meet this challenge: we analyse protein modification types through ontologies, databases, and literature and introduce a corpus of 360 abstracts manually annotated in the BioNLP Shared Task event representation for over 4500 mentions of proteins and 1000 statements of modification events of nearly 40 different types. We argue that together with existing resources, this corpus provides sufficient coverage of modification types to make effectively exhaustive extraction of protein modifications from text feasible. "
W11-0216 " Kernel methods are considered the most effective techniques for various relation extraction (RE) tasks as they provide higher accuracy than other approaches. In this paper, we introduce new dependency tree (DT) kernels for RE by improving on previously proposed dependency tree structures. These are further enhanced to design more effective approaches that we call mildly extended dependency tree (MEDT) kernels. The empirical results on the protein-protein interaction (PPI) extraction task on the AIMed corpus show that tree kernels based on our proposed DT structures achieve higher accuracy than previously proposed DT and phrase structure tree (PST) kernels. "
W11-0217 " Increasingly, as full-text scientific papers are becoming available, scientific queries have shifted from looking for facts to looking for arguments. Researchers want to know when their colleagues are proposing theories, outlining evidentiary relations, or explaining discrepancies. We show here that sentence-level annotation with the CISP schema adapts well to a corpus of biomedical articles, and we present preliminary results arguing that the CISP schema is uniquely suited to recovering common types of scientific arguments about hypotheses, explanations, and evidence. 1 I  "
W11-0218 "Abstract In this study we investigate the merits of fast approximate string matching to address challenges relating to spelling variants and to utilise large-scale lexical resources for semantic class disambiguation. We integrate string matching results into machine learning-based disambiguation through the use of a novel set of features that represent the distance of a given textual span to the closest match in each of a collection of lexical resources. We collect lexical resources for a multitude of semantic categories from a variety of biomedical domain sources. The combined resources, containing more than twenty million lexical items, are queried using a recently proposed fast and efficient approximate string matching algorithm that allows us to query large resources without severely impacting system performance. We evaluate our results on six corpora representing a variety of disambiguation tasks. While the integration of approximate string matching features is shown to substantially improve performance on one corpus, results are modest or negative for others. We suggest possible explanations and future research directions. Our lexical resources and implementation are made freely available for research purposes at: http://github.com/  http://github.com/ninjin/ simsem "
W11-0219 "<NoAbstract>"
W11-0220 " Suppose we have a large collection of documents most of which are unlabeled. Suppose further that we have a small subset of these documents which represent a particular class of documents we are interested in, i.e. these are labeled as positive examples. We may have reason to believe that there are more of these positive class documents in our large unlabeled collection. What data mining techniques could help us find these unlabeled positive examples? Here we examine machine learning strategies designed to solve this problem. We find that a proper choice of machine learning method as well as training strategies can give substantial improvement in retrieving, from the large collection, data enriched with positive examples. We illustrate the principles with a real example consisting of multiword UMLS phrases among a much larger collection of phrases from Medline.  "
W11-0221 "Abstract This paper presents our preliminary work on adaptation of parsing technology toward natural language query processing for biomedical domain. We built a small treebank of natural language queries, and tested a state-of-theart parser, the results of which revealed that a parser trained on Wall-Street-Journal articles and Medline abstracts did not work well on query sentences. We then experimented an adaptive learning technique, to seek the chance to improve the parsing performance on query sentences. Despite the small scale of the experiments, the results are encouraging, enlightening the direction for effective improvement. "
W11-0222 " Ontology authoring is a specialised task requiring amongst other things a deep knowledge of the ontology language being used. Understanding and reusing ontologies can thus be difficult for domain experts, who tend not to be ontology experts. To address this problem, we have developed a Natural Language Generation system for transforming the axioms that form the definitions of ontology classes into Natural Language paragraphs. Our method relies on deploying ontology axioms into a top-level Rhetorical Structure Theory schema. Axioms are ordered and structured with specific rhetorical relations under rhetorical structure trees. We describe here an implementation that focuses on a sub-module of SNOMED CT. With some refinements on articles and layout, the resulting paragraphs are fluent and coherent, offering a way for subject specialists to understand an ontologys content without need to understand its logical representation.  "
W11-0223 " Word sense disambiguation (WSD) is an intermediate task within information retrieval and information extraction, attempting to select the proper sense of ambiguous words. Due to the scarcity of training data, semi-supervised learning, which profits from seed annotated examples and a large set of unlabeled data, are worth researching. We present preliminary results of two semi-supervised learning algorithms on biomedical word sense disambiguation. Both methods add relevant unlabeled examples to the training set, and optimal parameters are similar for each ambiguous word. 1 Introdu  "
W11-0224 "Abstract We present MedstractPlus, a resource for mining relations from the Medline bibliographic database. It was built on the remains of Medstract, a previously created resource that included a bio-relation server and an acronym database. MedstractPlus uses simple and scalable natural language processing modules to structure text and is designed with reusability and extendibility in mind. "
W11-0401 "juan-manuel.torres@univavignon.fr Abstract In this article we present the RST Spanish Treebank, the first corpus annotated with rhetorical relations for this language. We describe the characteristics of the corpus, the annotation criteria, the annotation procedure, the inter-annotator agreement, and other related aspects. Moreover, we show the interface that we have developed to carry out searches over the corpus annotated texts. "
W11-0402 " This paper describes the modeling of the morphosyntactic annotations of the MULTEXT-East corpora and lexicons as an OWL/DL ontology. Formalizing annotation schemes in OWL/DL has the advantages of enabling formally specifying interrelationships between the various features and making logical inferences based on the relationships between them. We show that this approach provides us with a top-down perspective on a large set of morphosyntactic specifications for multiple languages, and that this perspective helps to identify and to resolve conceptual problems in the original specifications. Furthermore, the ontological modeling allows us to link the MULTEXT-East specifications with repositories of annotation terminology such as the General Ontology of Linguistics Descriptions or the ISO TC37/SC4 Data Category Registry. "
W11-0403 "lder {vaidyaa,choijd,mpalmer,narasimb}@colorado.edu Abstract This paper makes two contributions. First, we describe the Hindi Proposition Bank that contains annotations of predicate argument structures of verb predicates. Unlike PropBanks in most other languages, the Hind PropBank is annotated on top of dependency structure, the Hindi Dependency Treebank. We explore the similarities between dependency and predicate argument structures, so the PropBank annotation can be faster and more accurate. Second, we present a probabilistic rule-based system that maps syntactic dependents to semantic arguments. With simple rules, we classify about 47% of the entire PropBank arguments with over 90% confidence. These preliminary results are promising; they show how well these two frameworks are correlated. This can also be used to speed up our annotations. "
W11-0404 "Abstract There has been a great deal of excitement recently about using the wisdom of the crowd to collect data of all kinds, quickly and cheaply (Howe, 2008; von Ahn and Dabbish, 2008). Snow et al. (Snow et al., 2008) were the first to give a convincing demonstration that at least some kinds of linguistic data can be gathered from workers on the web more cheaply than and as accurately as from local experts, and there has been a steady stream of papers and workshops since then with similar results. e.g. (Callison-Burch and Dredze, 2010). Many of the tasks which have been successfully crowdsourced involve judgments which are similar to those performed in everyday life, such as recognizing unclear writing (von Ahn et al., 2008), or, for those tasks that require considerable judgment, the responses are usually binary or from a small set of responses, such as sentiment analysis (Mellebeek et al., 2010) or ratings (Heilman and Smith, 2010). Since the FrameNet process is known to be relatively expensive, we were interested in whether the FrameNet process of fine word sense discrimination and marking of dependents with semantic roles could be performed more cheaply and equally accurately using Amazons Mechanical Turk (AMT) or similar resources. We report on a partial success in this respect and how it was achieved. "
W11-0405 " For the implementation of the prosody prediction model, large scale annotated speech corpora have been widely applied. Reliability among transcribers, however, was too low for successful learning of an automatic prosodic prediction. This paper reveals our observations on performance deterioration of the learning model due to inconsistent tagging of prosodic breaks in the established corpora. Then, we suggest a method for consistent prosodic labeling among multiple transcribers. As a result, we obtain a corpus with consistent annotation of prosodic breaks. The estimated pairwise agreement of annotation of the main corpus is between 0.7477 and 0.7916, and the value of K is between 0.7057 and 0.7569. Considering the estimated K, annotation of the main corpus has reliable consistency among multiple transcribers. 1 Introduct  "
W11-0406 ", U.S.A. lherzig, nunesa, bsnir @brandeis.edu Abstract BiasML is a novel annotation scheme with the purpose of identifying the presence as well as nuances of biased language within the subset of Wikipedia articles dedicated to service providers. Whereas Wikipedia currently uses only manual flagging to detect possible bias, our scheme provides a foundation for the automating of bias flagging by improving upon the methodology of annotation schemes in classic sentiment analysis. We also address challenges unique to the task of identifying biased writing within the specific context of Wikipedias neutrality policy. We perform a detailed analysis of inter-annotator agreement, which shows that although the agreement scores for intra-sentential tags were relatively low, the agreement scores on the sentence and entry levels were encouraging (74.8% and 66.7%, respectively). Based on an analysis of our first implementation of our scheme, we suggest possible improvements to our guidelines, in hope that further rounds of annotation after incorporating them could provide appropriate data for use within a machine learning framework for automated detection of bias within Wikipedia. "
W11-0407 "Abstract We describe a new interactive annotation scheme between a human annotator who carries out simplified annotations on CFG trees, and a statistical parser that converts the human annotations automatically into a richly annotated HPSG treebank. In order to check the proposed schemes effectiveness, we performed automatic pseudo-annotations that emulate the systems idealized behavior and measured the performance of the parser trained on those annotations. In addition, we implemented a prototype system and conducted manual annotation experiments on a small test set. "
W11-0408 "Abstract The quality of annotated data is crucial for supervised learning. To eliminate errors in single annotated data, a second round of annotation is often used. However, is it absolutely necessary to double annotate every example? We show that it is possible to reduce the amount of the second round of annotation by more than half without sacrificing the performance. "
W11-0409 " In this paper, we propose a crowdsourcing methodology for a single-step construction of both an empirically-derived sense inventory and the corresponding sense-annotated corpus. The methodology taps the intuitions of non-expert native speakers to create an expertquality resource, and natively lends itself to supplementing such a resource with additional information about the structure and reliability of the produced sense inventories. The resulting resource will provide several ways to empirically measure distances between related word senses, and will explicitly address the question of fuzzy boundaries between them. 1 Intro  "
W11-0410 "Abstract This paper presents an evaluation of an automated quality assurance technique for a type of semantic representation known as a predicate argument structure. These representations are crucial to the development of an important class of corpus known as a proposition bank. Previous work (Cohen and Hunter, 2006) proposed and tested an analytical technique based on a simple discovery procedure inspired by classic structural linguistic methodology. Cohen and Hunter applied the technique manually to a small set of representations. Here we test the feasibility of automating the technique, as well as the ability of the technique to scale to a set of semantic representations and to a corpus many times larger than that used by Cohen and Hunter. We conclude that the technique is completely automatable, uncovers missing sense distinctions and other bad semantic representations, and does scale well, performing at an accuracy of 69% for identifying bad representations. We also report on the implications of our findings for the correctness of the semantic representations in PropBank. "
W11-0411 ", {olivier.galibert,ludovic.quintard}@lne.fr Abstract Within the framework of the construction of a fact database, we defined guidelines to extract named entities, using a taxonomy based on an extension of the usual named entities definition. We thus defined new types of entities with broader coverage including substantivebased expressions. These extended named entities are hierarchical (with types and components) and compositional (with recursive type inclusion and metonymy annotation). Human annotators used these guidelines to annotate a 1.3M word broadcast news corpus in French. This article presents the definition and novelty of extended named entity annotation guidelines, the human annotation of a global corpus and of a mini reference corpus, and the evaluation of annotations through the computation of inter-annotator agreements. Finally, we discuss our approach and the computed results, and outline further work. "
W11-0412 "Human Language Technology Research Unit, Fondazione Bruno Kessler, Trento, Italy  Department of Information Engineering and Computer Science, University of Trento, Italy {chowdhury,lavelli}@fbk.eu Abstract The creation of a gold standard corpus (GSC) is a very laborious and costly process. Silver standard corpus (SSC) annotation is a very recent direction of corpus development which relies on multiple systems instead of human annotators. In this paper, we investigate the practical usability of an SSC when a machine learning system is trained on it and tested on an unseen benchmark GSC. The main focus of this paper is how an SSC can be maximally exploited. In this process, we inspect several hypotheses which might have influenced the idea of SSC creation. Empirical results suggest that some of the hypotheses (e.g. a positive impact of a large SSC despite of having wrong and missing annotations) are not fully correct. We show that it is possible to automatically improve the quality and the quantity of the SSC annotations. We also observe that considering only those sentences of SSC which contain annotations rather than the full SSC results in a performance boost. "
W11-0413 "Abstract Subjectivity and sentiment analysis (SSA) is an area that has been witnessing a flurry of novel research. However, only few attempts have been made to build SSA systems for morphologically-rich languages (MRL). In the current study, we report efforts to partially bridge this gap. We present a newly labeled corpus of Modern Standard Arabic (MSA) from the news domain manually annotated for subjectivity and domain at the sentence level. We summarize our linguisticallymotivated annotation guidelines and provide examples from our corpus exemplifying the different phenomena. Throughout the paper, we discuss expression of subjectivity in natural language, combining various previously scattered insights belonging to many branches of linguistics. "
W11-0414 " We describe our efforts to apply the Penn Discourse Treebank guidelines on a Tamil corpus to create an annotated corpus of discourse relations in Tamil. After conducting a preliminary exploratory study on Tamil discourse connectives, we show our observations and results of a pilot experiment that we conducted by annotating a small portion of our corpus. Our ultimate goal is to develop a Tamil Discourse Relation Bank that will be useful as a resource for further research in Tamil discourse. Furthermore, a study of the behavior of discourse connectives in Tamil will also help in furthering the cross-linguistic understanding of discourse connectives. 1 Introdu  "
W11-0415 "Abstract This paper describes an annotated gold standard sample corpus of Early Modern German containing over 50,000 tokens of text manually annotated with POS tags, lemmas, and normalised spelling variants. The corpus is the first resource of its kind for this variant of German, and represents an ideal test bed for evaluating and adapting existing NLP tools on historical data. We describe the corpus format, annotation levels, and challenges, providing an example of the requirements and needs of smaller humanities-based corpus projects. "
W11-0416 "Abstract MAE and MAI are lightweight annotation and adjudication tools for corpus creation. DTDs are used to define the annotation tags and attributes, including extent tags, link tags, and non-consuming tags. Both programs are written in Java and use a stand-alone SQLite database for storage and retrieval of annotation data. Output is in stand-off XML. "
W11-0417 "Intl Institute of Info. Technology Hyderabad, India prashanth @research.iiit.ac.in Abstract In this paper, we first analyze and classify the empty categories in a Hindi dependency treebank and then identify various discovery procedures to automatically detect the existence of these categories in a sentence. For this we make use of lexical knowledge along with the parsed output from a constraint based parser. Through this work we show that it is possible to successfully discover certain types of empty categories while some other types are more difficult to identify. This work leads to the state-of-the-art system for automatic insertion of empty categories in the Hindi sentence. "
W11-0418 " This paper presents the annotation guidelines and specifications which have been developed for the creation of the Italian TimeBank, a language resource composed of two corpora manually annotated with temporal and event information. In particular, the adaptation of the TimeML scheme to Italian is described, and a special attention is given to the methodology used for the realization of the annotation specifications, which are strategic in order to create good quality annotated resources and to justify the annotated items. The reliability of the It-TimeML guidelines and specifications is evaluated on the basis of the results of the inter-coder agreement performed during the annotation of the two corpora.  "
W11-0419 "Abstract In this paper, we discuss some of the challenges of adequately applying a specification language to an annotation task, as embodied in a specific guideline. In particular, we discuss some issues with TimeML motivated by error analysis on annotated TLINKs in TimeBank. We introduce a document level information structure we call a narrative container (NC), designed to increase informativeness and accuracy of temporal relation identification. The narrative container is the default interval containing the events being discussed in the text, when no explicit temporal anchor is given. By exploiting this notion in the creation of a new temporal annotation over TimeBank, we were able to reduce inconsistencies and increase informativeness when compared to existing TLINKs in TimeBank. "
W11-0601 " Models of the acquisition of word segmentation are typically evaluated using phonemically transcribed corpora. Accordingly, they implicitly assume that children know how to undo phonetic variation when they learn to extract words from speech. Moreover, whereas models of language acquisition should perform similarly across languages, evaluation is often limited to English samples. Using child-directed corpora of English, French and Japanese, we evaluate the performance of state-of-the-art statistical models given inputs where phonetic variation has not been reduced. To do so, we measure segmentation robustness across different levels of segmental variation, simulating systematic allophonic variation or errors in phoneme recognition. We show that these models do not resist an increase in such variations and do not generalize to typologically different languages. From the perspective of early language acquisition, the results strengthen the hypothesis according to which phonological knowledge is acquired in large part before the construction of a lexicon. "
W11-0602 " The mapping from phonetic categories to acoustic cue values is highly flexible, and adapts rapidly in response to exposure. There is currently, however, no theoretical framework which captures the range of this adaptation. We develop a novel approach to modeling phonetic adaptation via a belief-updating model, and demonstrate that this model naturally unifies two adaptation phenomena traditionally considered to be distinct. 1 Introduction In order to understand speech, people map a continuous, acoustic signal onto discrete, linguistic categories, such as words. Despite a long history of research, no invariant mapping from acoustic features to underlying linguistic units has yet been found. Some of this lack of invariance is due to random factors, such as errors in production and perception, but much is due to systematic factors, such as differences between speakers, dialects/accents, and speech conditions. The human speech perception system appears to deal with the lack of invariance in two ways: by storing separate, speaker-, group-, or context-specific representations of the same categories (Goldinger, 1998), and by rapidly adapting phonetic categories to acoustic input. Even though a persons inventory of native language phonetic categories is generally fixed from an early age (Werker and Tees, 1984), the mapping between these categories and their acoustic realizations is flexible. Listeners adapt rapidly to foreign-accented speech (Bradlow and Bent, 2008) and acoustically distorted speech (Davis et al., 2005), showing increased comprehension after little exposure. Such adaptation results in temporary and perhaps speaker-specific changes in phonetic categorization (Norris et al., 2003; Vroomen et al., 2007; Kraljic and Samuel, 2007). To our knowledge, there is no theoretical framework which explains the range and specific patterns of adaptation of phonetic categories. In this paper, we propose a novel framework for understanding phonetic category adaptationrational belief updatingand develop a computational model within this framework which straightforwardly explains two types of phonetic category adaptation which are traditionally considered to be separate. While phonetic category adaptation has not thus far been described in this way, it nevertheless shows many hallmarks of rational inference under uncertainty (Jacobs and Kruschke, 2010). When there is another possible explanation for strange pronunciations (e.g. the speaker has a pen in her mouth), listeners do not show any adaptation (Kraljic et al., 2008). Listeners are more willing to generalize features of a foreign accent to new talkers if they were exposed to multiple talkers initially, rather than a single talker (Bradlow and Bent, 2008). Listeners also show rational patterns of generalizations of perceptual learning for specific phonetic contrasts, generalizing to new speakers only when the adapted phonetic categories of the old and new speakers share similar acoustic cue values (Kraljic and Samuel, 2007). While it is not conclusive, the available evidence suggests that listeners update their beliefs about pho10 1100 1680 F2 at closure (Hz) /aba/ /ada/ Proportion 'ba' responses Proportion /b/ responses /aba/ /ada/ 0 1 Proportion 'ba' responses /aba/ /ada/ Figure 1: Left: approximate distribution of acoustic cue values for /aba/ and /ada/ stimuli from Vroomen et al. (2007). Right: exposure to acoustically ambiguous /aba/ tokens results in recalibration of the /aba/ category, with the classification boundary shifting towards /ada/ (center-right), while exposure to unambiguous /aba/ tokens results in selective adaptation of the /aba/ category, where the classification boundary shifts towards /aba/ (far right). netic categories based on experience in a rational way. We propose that Bayesian belief updating can provide a principled computational framework for understanding rapid adaptation of phonetic categories as optimal inference under uncertainty. Such a framework has the appeal of being successfully applied in other domains (Brenner et al., 2000; Fine et al., 2010). In addition, rational models have also been used within the domain of speech perception to model acquisition of phonetic categories (Vallabha et al., 2007; Feldman et al., 2009a; McMurray et al., 2009), the perceptual magnet effect (Feldman et al., 2009b), and how various cues to the same phonetic contrast can be combined (Toscano and McMurray, 2010). 2 The Phenomena: Perceptual recalibration and selecti  "
W11-0603 ", Edinburgh EH8 9AB, UK Abstract Learning to group words into phrases without supervision is a hard task for NLP systems, but infants routinely accomplish it. We hypothesize that infants use acoustic cues to prosody, which NLP systems typically ignore. To evaluate the utility of prosodic information for phrase discovery, we present an HMMbased unsupervised chunker that learns from only transcribed words and raw acoustic correlates to prosody. Unlike previous work on unsupervised parsing and chunking, we use neither gold standard part-of-speech tags nor punctuation in the input. Evaluated on the Switchboard corpus, our model outperforms several baselines that exploit either lexical or prosodic information alone, and, despite producing a flat structure, performs competitively with a state-of-the-art unsupervised lexicalized parser, with a substantial advantage in precision. Our results support the hypothesis that acoustic-prosodic cues provide useful evidence about syntactic phrases for languagelearning infants. "
W11-0604 "<NoAbstract>"
W11-0605 " This paper defines a normal form for MCFGs that includes strongly equivalent representations of many MG variants, and presents an incremental priority-queue-based TD recognizer for these MCFGs. After introducing MGs with overt phrasal movement, head movement and simple adjunction are added without change in the recognizer. The MG representation can be used directly, so that even rather sophisticated analyses of properly non-CF languages can be defined very succinctly. As with the similar stack-based CFmethods, finite memory suffices for the recognition of infinite languages, and a fully connected left context for probabilistic analysis is available at every point. 1 Introductio  "
W11-0606 "IL 60208 USA Abstract Greater learnability has been offered as an explanation as to why certain properties appear in human languages more frequently than others. Languages with greater learnability are more likely to be accurately transmitted from one generation of learners to the next. We explore whether such a learnability bias is sufficient to result in a property becoming prevalent across languages by formalizing language transmission using a linear model. We then examine the outcome of repeated transmission of languages using a mathematical analysis, a computer simulation, and an experiment with human participants, and show several ways in which greater learnability may not result in a property becoming prevalent. Both the ways in which transmission failures occur and the relative number of languages with and without a property can affect whether the relationship between learnability and prevalence holds. Our results show that simply finding a learnability bias is not sufficient to explain why a particular property is a linguistic universal, or even frequent among human languages. "
W11-0607 "Abstract The aim of this paper is to present a computational model of the dynamic composition and update of verb argument expectations using Distributional Memory , a state-of-the-art framework for distributional semantics. The experimental results conducted on psycholinguistic data sets show that the model is able to successfully predict the changes on the patient argument thematic fit produced by different types of verb agents. "
W11-0608 " This paper presents a study of the effect of working memory load on the interpretation of pronouns in different discourse contexts: stories with and without a topic shift. We present a computational model (in ACT-R, Anderson, 2007) to explain how referring subjects are used and interpreted. We furthermore report on an experiment that tests predictions that follow from simulations. The results of the experiment support the model predictions that WM load only affects the interpretation of pronouns in stories with a topic shift, but not in stories without a topic shift. 1  "
W11-0609 "Abstract Conversational participants tend to immediately and unconsciously adapt to each others language styles: a speaker will even adjust the number of articles and other function words in their next utterance in response to the number in their partners immediately preceding utterance. This striking level of coordination is thought to have arisen as a way to achieve social goals, such as gaining approval or emphasizing difference in status. But has the adaptation mechanism become so deeply embedded in the language-generation process as to become a reflex? We argue that fictional dialogs offer a way to study this question, since authors create the conversations but dont receive the social benefits (rather, the imagined characters do). Indeed, we find significant coordination across many families of function words in our large movie-script corpus. We also report suggestive preliminary findings on the effects of gender and other features; e.g., surprisingly, for articles, on average, characters adapt more to females than to males. "
W11-0610 "20000 NW Walker Rd., Beaverton, Oregon 97006 {emily,roark,lmblack,vansanten}@cslu.ogi.edu Abstract Atypical or idiosyncratic language is a characteristic of autism spectrum disorder (ASD). In this paper, we discuss previous work identifying language errors associated with atypical language in ASD and describe a procedure for reproducing those results. We describe our data set, which consists of transcribed data from a widely used clinical diagnostic instrument (the ADOS) for children with autism, children with developmental language disorder, and typically developing children. We then present methods for automatically extracting lexical and syntactic features from transcripts of childrens speech to 1) identify certain syntactic and semantic errors that have previously been found to distinguish ASD language from that of children with typical development; and 2) perform diagnostic classification. Our classifiers achieve results well above chance, demonstrating the potential for using NLP techniques to enhance neurodevelopmental diagnosis and atypical language analysis. We expect further improvement with additional data, features, and classification techniques. "
W11-0611 "Abstract Since many real-world concepts are associated with colour, for example danger with red, linguistic information is often complimented with the use of appropriate colours in information visualization and product marketing. Yet, there is no comprehensive resource that captures conceptcolour associations. We present a method to create a large wordcolour association lexicon by crowdsourcing. We focus especially on abstract concepts and emotions to show that even though they cannot be physically visualized, they too tend to have strong colour associations. Finally, we show how wordcolour associations manifest themselves in language, and quantify usefulness of co-occurrence and polarity cues in automatically detecting colour associations. 1 "
W11-1701 "a Santa Cruz Abstract A growing body of work has highlighted the challenges of identifying the stance a speaker holds towards a particular topic, a task that involves identifying a holistic subjective disposition. We examine stance classification on a corpus of 4873 posts across 14 topics on ConvinceMe.net, ranging from the playful to the ideological. We show that ideological debates feature a greater share of rebuttal posts, and that rebuttal posts are significantly harder to classify for stance, for both humans and trained classifiers. We also demonstrate that the number of subjective expressions varies across debates, a fact correlated with the performance of systems sensitive to sentimentbearing terms. We present results for identifing rebuttals with 63% accuracy, and for identifying stance on a per topic basis that range from 54% to 69%, as compared to unigram baselines that vary between 49% and 60%. Our results suggest that methods that take into account the dialogic context of such posts might be fruitful. "
W11-1702 " This paper presents a lexicon model for subjectivity description of Dutch verbs that offers a framework for the development of sentiment analysis and opinion mining applications based on a deep syntactic-semantic approach. The model aims to describe the detailed subjectivity relations that exist between the participants of the verbs, expressing multiple attitudes for each verb sense. Validation is provided by an annotation study that shows that these subtle subjectivity relations are reliably identifiable by human annotators. 1 Int  "
W11-1703 "<NoAbstract>"
W11-1704 "Abstract The paper presents a semi-automatic approach to creating sentiment dictionaries in many languages. We first produced high-level goldstandard sentiment dictionaries for two languages and then translated them automatically into third languages. Those words that can be found in both target language word lists are likely to be useful because their word senses are likely to be similar to that of the two source languages. These dictionaries can be further corrected, extended and improved. In this paper, we present results that verify our triangulation hypothesis, by evaluating triangulated lists and comparing them to nontriangulated machine-translated word lists. "
W11-1705 "and Hal Daum  e III Dept. of Computer Science University of Maryland College Park, MD 20742 {amit,hal}@umiacs.umd.edu Abstract We propose a novel method to construct semantic orientation lexicons using large data and a thesaurus. To deal with large data, we use Count-Min sketch to store the approximate counts of all word pairs in a bounded space of 8GB. We use a thesaurus (like Roget) to constrain near-synonymous words to have the same polarity. This framework can easily scale to any language with a thesaurus and a unzipped corpus size  50 GB (12 billion tokens). We evaluate these lexicons intrinsically and extrinsically, and they perform comparable when compared to other existing lexicons. "
W11-1706 " Locating documents carrying positive or negative favourability is an important application within media analysis. This paper presents some empirical results on the challenges facing a machine-learning approach to this kind of opinion mining. Some of the challenges include: the often considerable imbalance in the distribution of positive and negative samples; changes in the documents over time; and effective training and quantification procedures for reporting results. This paper begins with three datasets generated by a media-analysis company, classifying documents in two ways: detecting the presence of favourability, and assessing negative vs. positive favourability. We then evaluate a machine-learning approach to automate the classification process. We explore the effect of using five different types of features, the robustness of the models when tested on data taken from a later time period, and the effect of balancing the input data by undersampling. We find varying choices for the optimum classifier, feature set and training strategy depending on the task and dataset. 1 Introduct  "
W11-1707 " Sentiment analysis is one of the recent, highly dynamic fields in Natural Language Processing. Most existing approaches are based on word-level analysis of texts and are able to detect only explicit expressions of sentiment. In this paper, we present an approach towards automatically detecting emotions (as underlying components of sentiment) from contexts in which no clues of sentiment appear, based on commonsense knowledge. The resource we built towards this aim  EmotiNet is a knowledge base of concepts with associated affective value. Preliminary evaluations show that this approach is appropriate for the task of implicit emotion detection, thus improving the performance of sentiment detection and classification in text.  "
W11-1708 " To assist in the research of social networks in history, we develop machine-learning-based tools for the identification and classification of personal relationships. Our case study focuses on the Dutch social movement between 1870 and 1940, and is based on biographical texts describing the lives of notable people in this movement. We treat the identification and the labeling of relations between two persons into positive, neutral, and negative both as a sequence of two tasks and as a single task. We observe that our machine-learning classifiers, support vector machines, produce better generalization performance on the single task. We show how a complete social network can be built from these classifications, and provide a qualitative analysis of the induced network using expert judgements on samples of the network. 1 Intro  "
W11-1709 "Abstract With the widespread use of email, we now have access to unprecedented amounts of text that we ourselves have written. In this paper, we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in many types of mail. We create a large wordemotion association lexicon by crowdsourcing, and use it to compare emotions in love letters, hate mail, and suicide notes. We show that there are marked differences across genders in how they use emotion words in work-place email. For example, women use many words from the joysadness axis, whereas men prefer terms from the feartrust axis. Finally, we show visualizations that can help people track emotions in their emails. "
W11-1710 " This paper reports the development of Japanese WordNet Affect from the English WordNet Affect lists with the help of English SentiWordNet and Japanese WordNet. Expanding the available synsets of the English WordNet Affect using SentiWordNet, we have performed the translation of the expanded lists into Japanese based on the synsetIDs in the Japanese WordNet. A baseline system for emotion analysis of Japanese sentences has been developed based on the Japanese WordNet Affect. The incorporation of morphology improves the performance of the system. Overall, the system achieves average precision, recall and F-scores of 32.76%, 53% and 40.49% respectively on 89 sentences of the Japanese judgment corpus and 83.52%, 49.58% and 62.22% on 1000 translated Japanese sentences of the SemEval 2007 affect sensing test corpus. Different experimental outcomes and morphological analysis suggest that irrespective of the google translation error, the performance of the system could be improved by enhancing the Japanese WordNet Affect in terms of coverage. 1 Introduct  "
W11-1711 " In this paper, we focus on the impressions that people gain from reading articles in Japanese newspapers, and we propose a method for extracting and quantifying these impressions in real numbers. The target impressions are limited to those represented by three bipolar scales, Happy  Sad, Glad  Angry, and Peaceful  Strained, and the strength of each impression is computed as a real number between 1 and 7. First, we implement a method for computing impression values of articles using an impression lexicon. This lexicon represents a correlation between the words appearing in articles and the influence of these words on the readers impressions, and is created from a newspaper database using a word co-occurrence based method. We considered that some gaps would occur between values computed by such an unsupervised method and those judged by the readers, and we conducted experiments with 900 subjects to identify what gaps actually occurred. Consequently, we propose a new approach that uses regression equations to correct impression values computed by the method. Our investigation shows that accuracy is improved by a range of 23.2% to 42.7% by using regression equations. 1 Introducti  "
W11-1712 " Recent solutions for sentiment analysis have relied on feature selection methods ranging from lexicon-based approaches where the set of features are generated by humans, to approaches that use general statistical measures where features are selected solely on empirical evidence. The advantage of statistical approaches is that they are fully automatic, however, they often fail to separate features that carry sentiment from those that do not. In this paper we propose a set of new feature selection schemes that use a Content and Syntax model to automatically learn a set of features in a review document by separating the entities that are being reviewed from the subjective expressions that describe those entities in terms of polarities. By focusing only on the subjective expressions and ignoring the entities, we can choose more salient features for document-level sentiment analysis. The results obtained from using these features in a maximum entropy classifier are competitive with the state-of-the-art machine learning approaches. "
W11-1713 "Abstract We introduce a new emotion classification task based on Learys Rose, a framework for interpersonal communication. We present a small dataset of 740 Dutch sentences, outline the annotation process and evaluate annotator agreement. We then evaluate the performance of several automatic classification systems when classifying individual sentences according to the four quadrants and the eight octants of Learys Rose. SVM-based classifiers achieve average F-scores of up to 51% for 4-way classification and 31% for 8-way classification, which is well above chance level. We conclude that emotion classification according to the Interpersonal Circumplex is a challenging task for both humans and machine learners. We expect classification performance to increase as context information becomes available in future versions of our dataset. "
W11-1714 "ubingen Wilhelmstr. 1923 72074 Tubingen Germany dale.gerdemann@googl email.com Detmar Meurers University of Tubingen Wilhelmstr. 1923 72074 Tubingen  Germany dm@sfs.unituebingen.de Abstract In   this   paper   we   explore   the   use   of   phrases  occurring   maximally   in   text   as   features   for  sentiment classification of product reviews. The  goal is to find in a statistical way representative  words   and   phrases   used   typically   in   positive  and   negative   reviews.   The  approach   does  not  rely on predefined sentiment lexicons, and the  motivation   for   this   is   that   potentially   every  word   could   be   considered   as   expressing  something positive and/or negative in different  situations, and that the context and the personal  attitude of the opinion holder should be taken  into account when determining the polarity of  the   phrase,   instead   of   doing   this   out   of  particular context. 1 Introduction  "
W11-1715 "Natural Language Engineering Lab ELiRF Departamento de Sistemas Inform  aticos y Computaci  on Universidad Polit  ecnica de Valencia, Spain {areyes,prosso}@dsic.upv.es Abstract The research described in this work focuses on identifying key components for the task of irony detection. By means of analyzing a set of customer reviews, which are considered as ironic both in social and mass media, we try to find hints about how to deal with this task from a computational point of view. Our objective is to gather a set of discriminating elements to represent irony. In particular, the kind of irony expressed in such reviews. To this end, we built a freely available data set with ironic reviews collected from Amazon. Such reviews were posted on the basis of an online viral effect; i.e. contents whose effect triggers a chain reaction on people. The findings were assessed employing three classifiers. The results show interesting hints regarding the patterns and, especially, regarding the implications for sentiment analysis. "
W11-1716 ", F. Javier Ortega, Fernando Enr  quez University of Seville Avda. Reina Mercedes s/n. 41012 Seville, Spain {fcruz,troyano,javierortega,fenros}@us.es Abstract In most tasks related to opinion mining and sentiment analysis, it is necessary to compute the semantic orientation (i.e., positive or negative evaluative implications) of certain opinion expressions. Recent works suggest that semantic orientation depends on application domains. Moreover, we think that semantic orientation depends on the specific targets (features) that an opinion is applied to. In this paper, we introduce a technique to build domainspecific, feature-level opinion lexicons in a semi-supervised manner: we first induce a lexicon starting from a small set of annotated documents; then, we expand it automatically from a larger set of unannotated documents, using a new graph-based ranking algorithm. Our method was evaluated in three different domains (headphones, hotels and cars), using a corpus of product reviews which opinions were annotated at the feature level. We conclude that our method produces feature-level opinion lexicons with better accuracy and recall that domain-independent opinion lexicons using only a few annotated documents. "
W11-1717 "T Bombay 2 Dept. of Computer Science and Engineering, IIT Bombay Mumbai, India 400076 {balamurali,adityaj,pb}@cse.iitb.ac.in Abstract The new trend in sentiment classification is to use semantic features for representation of documents. We propose a semantic space based on WordNet senses for a supervised document-level sentiment classifier. Not only does this show a better performance for sentiment classification, it also opens opportunities for building a robust sentiment classifier. We examine the possibility of using similarity metrics defined on WordNet to address the problem of not finding a sense in the training corpus. Using three popular similarity metrics, we replace unknown synsets in the test set with a similar synset from the training set. An improvement of 6.2% is seen with respect to baseline using this approach. "
W11-1718 " In this paper, we concentrate on the 3 of the tracks proposed in the NTCIR 8 MOAT, concerning the classification of sentences according to their opinionatedness, relevance and polarity. We propose a method for the detection of opinions, relevance, and polarity classification, based on ISR-WN (a resource for the multidimensional analysis with Relevant Semantic Trees of sentences using different WordNet-based information sources). Based on the results obtained, we can conclude that the resource and methods we propose are appropriate for the task, reaching the level of state-of-the-art approaches.  "
W11-1719 " In recent years microblogs have taken on an important role in the marketing sphere, in which they have been used for sharing opinions and/or experiences about a product or service. Companies and researchers have become interested in analysing the content generated over the most popular of these, the Twitter platform, to harvest information critical for their online reputation management (ORM). Critical to this task is the efficient and accurate identification of tweets which refer to a company distinguishing them from those which do not. The aim of this work is to present and compare two different approaches to achieve this. The obtained results are promising while at the same time highlighting the difficulty of this task. 1 Int  "
W11-1720 "<NoAbstract>"
W11-1721 "Dallas {dongwang,yangl}@hlt.utdallas.edu Abstract In this study we investigate using an unsupervised generative learning method for subjectivity detection in text across different domains. We create an initial training set using simple lexicon information, and then evaluate a calibrated EM (expectation-maximization) method to learn from unannotated data. We evaluate this unsupervised learning approach on three different domains: movie data, news resource, and meeting dialogues. We also perform a thorough analysis to examine impacting factors on unsupervised learning, such as the size and self-labeling accuracy of the initial training set. Our experiments and analysis show inherent differences across domains and performance gain from calibration in EM. "
W11-1722 " The aim of this paper is to present an approach to tackle the task of opinion question answering and text summarization. Following the guidelines TAC 2008 Opinion Summarization Pilot task, we propose new methods for each of the major components of the process. In particular, for the information retrieval, opinion mining and summarization stages. The performance obtained improves with respect to the state of the art by approximately 12.50%, thus concluding that the suggested approaches for these three components are adequate. 1 Introduct  "
W11-1723 " News articles have always been a prominent force in the formation of a companys financial image in the minds of the general public, especially the investors. Given the large amount of news being generated these days through various websites, it is possible to mine the general sentiment of a particular company being portrayed by media agencies over a period of time, which can be utilized to gauge the long term impact on the investment potential of the company. However, given such a vast amount of news data, we need to first separate corporate news from other kinds namely, sports, entertainment, science & technology, etc. We propose a system which takes news as, checks whether it is of corporate nature, and then identifies the polarity of the sentiment expressed in the news. The system is also capable of distinguishing the company/organization which is the subject of the news from other organizations which find mention, and this is used to pair the sentiment polarity with the identified company. Introduction With the rapid advancements in the field of information technology, the amount of information available has increased tremendously. News articles constitute the largest available portion of factual information about events happening in the world. Corporate news constitutes a major chunk of these news articles. Sentiment Mining applied to the corporate domain would help in various ways like Automatic Recommendation Systems, to help organizations evaluate their market strategies help them frame their advertisement campaigns. Our system tries to address these issues by automating the entire process of news collection, organization/product detection and sentiment mining. This paper is divided into two main parts. The first part describes a way of identifying corporate news from a collection of news articles and then pairing the news with the organization/company which is being talked about in the article. The second part of our paper works on the output of the first part (corporate news) and detects the valence of the identified corporate news articles. It calculates an overall score and identifies valence a s positive, negative or neutral based on this score. The system is immune to addition/mergers of companies, with regards to their identification, as it does not use any name lists. The model uses a machine learning approach to do this task. We extract a set of features from the news and use them to train a set of classifiers. The best model is then used to classify the test data. One advantage of our approach described below is that it only requires a very small amount of annotated training data. We trained the model on the NewsCorp dataset consisting of 860 annotated news articles. The system has shown promising 175 results on test data with classification accuracy being 92.05% and a f-measure of 92.00. The final average valence detection accuracy measured was 79.93%. Related Work Much work has been done on text classification.(Barak, 2009; Sebastiani,2002) There have been earlier attempts (Research on Sports Game News Information Extraction, Yonggui YANG,et al) However, they had focused mainly on information extraction and not classification. Earlier attempts on web news classification(Krishnlal et al, 2010) concentrated mainly on classification according to the domain of the news articles. Not much work has been done in the field of corporate news-company pairing. This paper tries to address a more general problem of detecting the main organization being talked about in the articles. Sentiment analysis in computational linguistics has focused on examining what textual features contribute to affective content of text and automatically detecting these features to derive a sentiment metric for a word, sentence or whole text. Niederhoffer (1971) after classifying New York Times headlines into 19 categories evaluated how the markets react to good and bad news. Davis et al (2006) investigate the effects of optimistic or pessimistic language used in financial press releases on future firm performance. Sumbaly et al(2009) used k gram models to detect sentiment in large news datasets. Devitt(2007) improves upon and Melville(2009) have done work on sentiment analysis of web blogs PART I : News Classification Steps involved in news classification  "
W11-1724 "Abstract This paper presents two instance-level transfer learning based algorithms for cross lingual opinion analysis by transferring useful translated opinion examples from other languages as the supplementary training data for improving the opinion classifier in target language. Starting from the union of small training data in target language and large translated examples in other languages, the Transfer AdaBoost algorithm is applied to iteratively reduce the influence of low quality translated examples. Alternatively, starting only from the training data in target language, the Transfer Self-training algorithm is designed to iteratively select high quality translated examples to enrich the training data set. These two algorithms are applied to sentenceand document-level cross lingual opinion analysis tasks, respectively. The evaluations show that these algorithms effectively improve the opinion analysis by exploiting small target language training data and large cross lingual training data. "
W11-1801 " The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. 1 Intro  "
W11-1802 " The Genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of BioNLP Shared Task 2011. As its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers. After a 3-month system development period, 15 teams submitted their performance results on test cases. The results show the community has made a significant advancement in terms of both performance improvement and generalization. 1 Introduct  "
W11-1803 "Abstract This paper presents the preparation, resources, results and analysis of the Epigenetics and Post-translational Modifications (EPI) task, a main task of the BioNLP Shared Task 2011. The task concerns the extraction of detailed representations of 14 protein and DNA modification events, the catalysis of these reactions, and the identification of instances of negated or speculatively stated event instances. Seven teams submitted final results to the EPI task in the shared task, with the highest-performing system achieving 53% F-score in the full task and 69% F-score in the extraction of a simplified set of core event arguments. "
W11-1804 " This paper presents the preparation, resources, results and analysis of the Infectious Diseases (ID) information extraction task, a main task of the BioNLP Shared Task 2011. The ID task represents an application and extension of the BioNLP09 shared task event extraction approach to full papers on infectious diseases. Seven teams submitted final results to the task, with the highest-performing system achieving 56% F-score in the full task, comparable to state-of-the-art performance in the established BioNLP09 task. The results indicate that event extraction methods generalize well to new domains and full-text publications and are applicable to the extraction of events relevant to the molecular mechanisms of infectious diseases. 1 Introduct  "
W11-1805 "Biomedical Event Extraction from Abstracts and Full Papers using Search-based Structured Prediction  Andreas Vlachos and Mark Craven  raven Department of Biostatistics and Medical Informatics University of Wisconsin-M adison {vlachos,craven}@biostat.wisc.edu Abstract In this paper we describe our approach to the BioNLP 2011 shared task on biomedical event extraction from abstracts and full papers. We employ a joint inference system developed using the search-based structured prediction framework and show that it improves on a pipeline using the same features and it is better able to handle the domain shift from abstracts to full papers. In addition, we report on experiments using a simple domain adaptation method. "
W11-1806 "CA 94305 {mcclosky,mihais,manning}@stanford.edu Abstract We describe the Stanford entry to the BioNLP 2011 shared task on biomolecular event extraction (Kim et al., 2011a). Our framework is based on the observation that event structures bear a close relation to dependency graphs. We show that if biomolecular events are cast as these pseudosyntactic structures, standard parsing tools (maximum-spanning tree parsers and parse rerankers) can be applied to perform event extraction with minimum domainspecific tuning. The vast majority of our domain-specific knowledge comes from the conversion to and from dependency graphs. Our system performed competitively, obtaining 3rd place in the Infectious Diseases track (50.6% f-score), 5th place in Epigenetics and Post-translational Modifications (31.2%), and 7th place in Genia (50.0%). Additionally, this system was part of the combined system in Riedel et al. (2011) to produce the highest scoring system in three out of the four event extraction tasks. "
W11-1807 "Amherst {riedel,mccallum}@cs.umass.edu Abstract We present a joint model for biomedical event extraction and apply it to four tracks of the BioNLP 2011 Shared Task. Our model decomposes into three sub-models that concern (a) event triggers and outgoing arguments, (b) event triggers and incoming arguments and (c) protein-protein bindings. For efficient decoding we employ dual decomposition. Our results are very competitive: With minimal adaptation of our model we come in second for two of the tasksright behind a version of the system presented here that includes predictions of the Stanford event extractor as features. We also show that for the Infectious Diseases task using data from the Genia track is a very effective way to improve accuracy. "
W11-1808 "d University {riedel,mccallum}@cs.umass.edu {mcclosky,mihais,manning}@stanford.edu Abstract We describe the FAUST entry to the BioNLP 2011 shared task on biomolecular event extraction. The FAUST system explores several stacking models for combination using as base models the UMass dual decomposition (Riedel and McCallum, 2011) and Stanford event parsing (McClosky et al., 2011b) approaches. We show that using stacking is a straightforward way to improving performance for event extraction and find that it is most effective when using a small set of stacking features and the base models use slightly different representations of the input data. The FAUST system obtained 1st place in three out of four tasks: 1st place in Genia Task 1 (56.0% f-score) and Task 2 (53.9%), 2nd place in the Epigenetics and Post-translational Modifications track (35.0%), and 1st place in the Infectious Diseases track (55.6%). "
W11-1809 "Abstract This paper presents the Bacteria Biotope task as part of the BioNLP Shared Tasks 2011. The Bacteria Biotope task aims at extracting the location of bacteria from scientific Web pages. Bacteria location is a crucial knowledge in biology for phenotype studies. The paper details the corpus specification, the evaluation metrics, summarizes and discusses the participant results. "
W11-1810 " We present two related tasks of the BioNLP Shared Tasks 2011: Bacteria Gene Renaming (Rename) and Bacteria Gene Interactions (GI). We detail the objectives, the corpus specification, the evaluation metrics, and we summarize the participants results. Both issued from PubMed scientific literature abstracts, the Rename task aims at extracting gene name synonyms, and the GI task aims at extracting genic interaction events, mainly about gene transcriptional regulations in bacteria. 1 Int  "
W11-1811 "Abstract This paper summarizes the Protein Coreference Resolution task of BioNLP Shared Task 2011. After 7 weeks of system development period, the task received final submissions from 6 teams. Evaluation results show that state-of-the-art performance on the task can find 22.18% of protein coreferences with the precision of 73.26%. Analysis of the submissions shows that several types of anaphoric expressions including definite expressions, which occupies a significant part of the problem, have not yet been solved. "
W11-1812 "Abstract This paper presents the Entity Relations (REL) task, a supporting task of the BioNLP Shared Task 2011. The task concerns the extraction of two types of part-of relations between a gene/protein and an associated entity. Four teams submitted final results for the REL task, with the highest-performing system achieving 57.7% F-score. While experiments suggest use of the data can help improve event extraction performance, the task data has so far received only limited use in support of event extraction. The REL task continues as an open challenge, with all resources available from the shared task website. "
W11-1813 " To participate in the Protein Coreference section of the BioNLP 2011 Shared Task, we use Reconcile, a coreference resolution engine, by replacing some pre-processing components and adding a new mention detector. We got some improvement from training two separate classifiers for detecting anaphora and antecedent mentions. Our system yielded the highest score in the task, F-score 34.05% in partial mention, protein links, and system recall mode. We witnessed that specialized mention detection is crucial for coreference resolution in the biomedical domain.  Protein Coreference section of the BioNLP 2011 Shared Task, we use Reconcile, a coreference resolution engine, by replacing some pre-processing components and adding a new mention detector. We got some improvement from training two separate classifiers for detecting anaphora and antecedent mentions. Our system yielded the highest score in the task, F-score 34.05% in partial mention, protein links, and system recall mode. We witnessed that specialized mention detection is crucial for coreference resolution in the biomedical domain. "
W11-1814 "92 Japan {nthnhung,tsuruoka}@jaist.ac.jp Abstract This paper describes our event extraction system that participated in the bacteria biotopes task in BioNLP Shared Task 2011. The system performs semi-supervised named entity recognition by leveraging additional information derived from external resources including a large amount of raw text. We also perform coreference resolution to deal with events having a large textual scope, which may span over several sentences (or even paragraphs). To create the training data for coreference resolution, we have manually annotated the corpus with coreference links. The overall F-score of event extraction was 33.2 at the official evaluation of the shared task, but it has been improved to 33.8 thanks to the refinement made after the submission deadline. "
W11-1815 " This paper describes the system of the INRA Bibliome research group applied to the Bacteria Biotope (BB) task of the BioNLP 2011 shared tasks. Bacteria, geographical locations and host entities were processed by a pattern-based approach and domain lexical resources. For the extraction of environment locations, we propose a framework based on semantic analysis supported by an ontology of the biotope domain. Domain-specific rules were developed for dealing with Bacteria anaphora. Official results show that our Alvis system achieves the best performance of participating systems. 1 Introduct  "
W11-1816 "Abstract This paper describes the supporting resources provided for the BioNLP Shared Task 2011. These resources were constructed with the goal to alleviate some of the burden of system development from the participants and allow them to focus on the novel aspects of constructing their event extraction systems. With the availability of these resources we also seek to enable the evaluation of the applicability of specific tools and representations towards improving the performance of event extraction systems. Additionally we supplied evaluation software and services and constructed a visualisation tool, stav, which visualises event extraction results and annotations. These resources helped the participants make sure that their final submissions and research efforts were on track during the development stages and evaluate their progress throughout the duration of the shared task. The visualisation software was also employed to show the differences between the gold annotations and those of the submitted results, allowing the participants to better understand the performance of their system. The resources, evaluation tools and visualisation tool are provided freely for research purposes and can be found at http://sites.google.com/site/bionlpst/  http://sites.google.com/site/bionlpst/  "
W11-1817 "Abstract The Bacteria Gene Renaming (RENAME) task is a supporting task in the BioNLP Shared Task 2011 (BioNLP-ST11). The task consists in extracting gene renaming acts and gene synonymy reminders in scientific texts about bacteria. In this paper, we present in details our method in three main steps: 1) the document segmentation into sentences, 2) the removal of the sentences exempt of renaming act (false positives) using both a gene nomenclature and supervised machine learning (feature selection and SVM), 3) the linking of gene names by the target renaming relation in each sentence. Our system ranked third at the official test with 64.4% of F-measure. We also present here an effective post-competition improvement: the representation as SVM features of regular expressions that detect combinations of trigger words. This increases the F-measure to 73.1%. "
W11-1818 " Building on technical advances from the BioNLP 2009 Shared Task Challenge, the 2011 challenge sets forth to generalize techniques to other complex biological event extraction tasks. In this paper, we present the implementation and evaluation of a signaturebased machine-learning technique to predict events from full texts of infectious disease documents. Specifically, our approach uses novel signatures composed of traditional linguistic features and semantic knowledge to predict event triggers and their candidate arguments. Using a leave-one out analysis, we report the contribution of linguistic and shallow semantic features in the trigger prediction and candidate argument extraction. Lastly, we examine evaluations and posit causes for errors in our complex biological event extraction. 1 Introductio  "
W11-1819 " In this paper we describe a rule-based system developed for the BioNLP 2011 GENIA event detection task. The system applies Kybots (Knowledge Yielding Robots) on annotated texts to extract bio-events involving proteins or genes. The main goal of this work is to verify the usefulness and portability of the Kybot technology to the domain of biomedicine. 1 Introdu  "
W11-1820 " This paper describes a novel approach presented to the BioNLP11 Shared Task on GENIA event extraction. The approach consists of three steps. First, a dictionary is automatically constructed based on training datasets which is then used to detect candidate triggers and determine their event types. Second, we apply a set of heuristic algorithms which use syntactic patterns and candidate triggers detected in the first step to extract biological events. Finally, a post-processing is used to resolve regulatory events. We achieved an F-score of 43.94% using the online evaluation system. 1 Intro  "
W11-1821 " Recently, the focus in the BioNLP domain has shifted from binary relations to more expressive event representations, largely owing to the international popularity of the BioNLP Shared Task (ST) of 2009. This year, the ST11 provides a further generalization on three key aspects: text type, subject domain, and targeted event types. One of the supporting tasks established to provide more finegrained text predictions is the extraction of entity relations. We have implemented an extraction system for such non-causal relations between named entities and domain terms, applying semantic spaces and machine learning techniques. Our system ranks second of four participating teams, achieving 37.04% precision, 47.48% recall and 41.62% F-score. 1 Introductio  "
W11-1822 " We describe our approach for the GENIA Event Extraction in the Main Task of BioNLP Shared Task 2011. There are two important parts in our method: Event Trigger Annotation and Event Extraction. We use rules and dictionary to annotate event triggers. Event extraction is based on patterns created from dependent graphs. We apply UIMA Framework to support all stages in our system. 1 Int  "
W11-1823 "Switzerland {tuggener,klenner,gschneid,siclemat,rinaldi}@cl.uzh.ch Abstract We introduce our incremental coreference resolution system for the BioNLP 2011 Shared Task on Protein/Gene interaction. The benefits of an incremental architecture over a mentionpair model are: a reduction of the number of candidate pairs, a means to overcome the problem of underspecified items in pair-wise classification and the natural integration of global constraints such as transitivity. A filtering system takes into account specific features of different anaphora types. We do not apply Machine Learning, instead the system classifies with an empirically derived salience measure based on the dependency labels of the true mentions. The OntoGene pipeline is used for preprocessing. "
W11-1824 "Abstract This paper presents our approach (referred to as BioEvent) for protein-level complex event extraction, developed for the GENIA task (Kim et al., 2011b) of the BioNLP Shared Task 2011 (Kim et al., 2011a). We developed a double layered machine learning approach which utilizes a state-of-the-art minimized feature set for each of the event types. We improved the best performing system of BioNLP 2009 overall, and ranked first amongst 15 teams in finding Localization events in 2011 12 . BioEvent is available at http://bioevent.sourceforge.net/  "
W11-1825 " We describe the system from the Natural Language Processing group at Microsoft Research for the BioNLP 2011 Shared Task. The task focuses on event extraction, identifying structured and potentially nested events from unannotated text. Our approach follows a pipeline, first decorating text with syntactic information, then identifying the trigger words of complex events, and finally identifying the arguments of those events. The resulting system depends heavily on lexical and syntactic features. Therefore, we explored methods of maintaining ambiguities and improving the syntactic representations, making the lexical information less brittle through clustering, and of exploring novel feature combinations and feature reduction. The system ranked 4th in the GENIA task with an F-measure of 51.5%, and 3rd in the EPI task with an F-measure of 64.9%.  Language Processing group at Microsoft Research for the BioNLP 2011 Shared Task. The task focuses on event extraction, identifying structured and potentially nested events from unannotated text. Our approach follows a pipeline, first decorating text with syntactic information, then identifying the trigger words of complex events, and finally identifying the arguments of those events. The resulting system depends heavily on lexical and syntactic features. Therefore, we explored methods of maintaining ambiguities and improving the syntactic representations, making the lexical information less brittle through clustering, and of exploring novel feature combinations and feature reduction. The system ranked 4th in the GENIA task with an F-measure of 51.5%, and 3rd in the EPI task with an F-measure of 64.9%. "
W11-1826 " We participated in the BioNLP Shared Task 2011, addressing the GENIA event extraction (GE) and the Epigenetics and Post-translational Modifications (EPI) tasks. A graph-based approach is employed to automatically learn rules for detecting biological events in the life-science literature. The event rules are learned by identifying the key contextual dependencies from full syntactic parsing of annotated text. Event recognition is performed by searching for an isomorphism between event rules and the dependency graphs of sentences in the input texts. While we explored methods such as performance-based rule ranking to improve precision, we merged rules across multiple event types in order to increase recall. We achieved a 41.13% F-score in detecting events of nine types in the Task 1 of the GE task, and a 52.67% F-score in identifying events across fifteen types in the core task of the EPI task. Our performance on both tasks is comparable to the state-of-the-art systems. Our approach does not require any external domain-specific resources. The consistent performance on the two tasks supports the claim that the method generalizes well to extract events from different domains where training data is available. 1 Introduct  "
W11-1827 "Halil Kilicoglu and Sabine Bergler Department of Computer Science and Software Engineering Concordia University 1455 de Maisonneuve Blvd. West Montr  eal, Canada {h kilico,bergler}@cse.concordia.ca Abstract The second BioNLP Shared Task on Event Extraction (BioNLP-ST11) follows up the previous shared task competition with a focus on generalization with respect to text types, event types and subject domains. In this spirit, we re-engineered and extended our event extraction system, emphasizing linguistic generalizations and avoiding domain-, event typeor text type-specific optimizations. Similar to our earlier system, syntactic dependencies form the basis of our approach. However, diverging from that systems more pragmatic nature, we more clearly distinguish the shared task concerns from a general semantic composition scheme, that is based on the notion of embedding. We apply our methodology to core bio-event extraction and speculation/negation detection tasks in three main tracks. Our results demonstrate that such a general approach is viable and pinpoint some of its shortcomings. "
W11-2101 "Abstract The Workshop on Statistical Machine Translation (WMT) has become one of ACLs flagship workshops, held annually since 2006. In addition to soliciting papers from the research community, WMT also features a shared translation task for evaluating MT systems. This shared task is notable for having manual evaluation as its cornerstone. The Workshops overview paper, playing a descriptive and administrative role, reports the main results of the evaluation without delving deep into analyzing those results. The aim of this paper is to investigate and explain some interesting idiosyncrasies in the reported results, which only become apparent when performing a more thorough analysis of the collected annotations. Our analysis sheds some light on how the reported results should (and should not) be interpreted, and also gives rise to some helpful recommendation for the organizers of WMT. "
W11-2102 "106-6126 {talbot, och}@google.com {kazawa, ichikawa}@google.com {jasonkb, seno}@google.com Abstract Reordering is a major challenge for machine translation between distant languages. Recent work has shown that evaluation metrics that explicitly account for target language word order correlate better with human judgments of translation quality. Here we present a simple framework for evaluating word order independently of lexical choice by comparing the systems reordering of a source sentence to reference reordering data generated from manually word-aligned translations. When used to evaluate a system that performs reordering as a preprocessing step our framework allows the parser and reordering rules to be evaluated extremely quickly without time-consuming endto-end machine translation experiments. A novelty of our approach is that the translations used to generate the reordering reference data are generated in an alignment-oriented fashion. We show that how the alignments are generated can significantly effect the robustness of the evaluation. We also outline some ways in which this framework has allowed our group to analyze reordering errors for English to Japanese machine translation. "
W11-2103 "Center for Language and Speech Processing Johns Hopkins University Abstract This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot tunable metrics task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality. "
W11-2104 "Abstract We present a pilot study on an evaluation method which is able to rank translation outputs with no reference translation, given only their source sentence. The system employs a statistical classifier trained upon existing human rankings, using several features derived from analysis of both the source and the target sentences. Development experiments on one language pair showed that the method has considerably good correlation with human ranking when using features obtained from a PCFG parser. "
W11-2105 " This paper proposes a new automatic machine translation evaluation metric: AMBER, which is based on the metric BLEU but incorporates recall, extra penalties, and some text processing variants. There is very little linguistic information in AMBER. We evaluate its system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance. 1 Int  "
W11-2106 "ingapore {danielhe,liuchan1,nght}@comp.nus.edu.sg Abstract This paper describes the submission from the National University of Singapore to the WMT 2011 Shared Evaluation Task and the Tunable Metric Task. Our entry is TESLA in three different configurations: TESLA-M, TESLA-F, and the new TESLA-B. "
W11-2107 "232, USA {mdenkows,alavie}@cs.cmu.edu Abstract This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks. New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system. "
W11-2108 "Matou s Mach  a cek and Ond rej Bojar Charles University in Prague Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics Malostransk  e n  am est   25, Prague {bojar,machacek}@ufal.mff.cuni.cz Abstract SemPOS is an automatic metric of machine translation quality for Czech and English focused on content words. It correlates well with human judgments but it is computationally costly and hard to adapt to other languages because it relies on a deep-syntactic analysis of the system output and the reference. To remedy this, we attempt at approximating SemPOS using only tagger output and a few heuristics. At a little expense in correlation to human judgments, we can evaluate MT systems much faster. Additionally, we describe our submission to the Tunable Metrics Task in WMT11. "
W11-2109 " Current metrics for evaluating machine translation quality have the huge drawback that they require human-quality reference translations. We propose a truly automatic evaluation metric based on IBM1 lexicon probabilities which does not need any reference translations. Several variants of IBM1 scores are systematically explored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. "
W11-2110 " We propose the use of morphemes for automatic evaluation of machine translation output, and systematically investigate a set of F score and BLEU score based metrics calculated on words, morphemes and POS tags along with all corresponding combinations. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Machine translation outputs in five different European languages are used: English, Spanish, French, German and Czech. The results show that the F scores which take into account morphemes and POS tags are the most promising metrics. 1 Intro  "
W11-2111 "Abstract We describe our submissions to the WMT11 shared MT evaluation task: MTeRater and MTeRater-Plus. Both are machine-learned metrics that use features from e-rater R , an automated essay scoring engine designed to assess writing proficiency. Despite using only features from e-rater and without comparing to translations, MTeRater achieves a sentencelevel correlation with human rankings equivalent to BLEU. Since MTeRater only assesses fluency, we build a meta-metric, MTeRaterPlus, that incorporates adequacy by combining MTeRater with other MT evaluation metrics and heuristics. This meta-metric has a higher correlation with human rankings than either MTeRater or individual MT metrics alone. However, we also find that e-rater features may not have significant impact on correlation in every case. "
W11-2112 "V1 1SB, UK {m.rios, w.aziz, l.specia}@wlv.ac.uk Abstract We describe TINE, a new automatic evaluation metric for Machine Translation that aims at assessing segment-level adequacy. Lexical similarity and shallow-semantics are used as indicators of adequacy between machine and reference translations. The metric is based on the combination of a lexical matching component and an adequacy component. Lexical matching is performed comparing bagsof-words without any linguistic annotation. The adequacy component consists in: i) using ontologies to align predicates (verbs), ii) using semantic roles to align predicate arguments (core arguments and modifiers), and iii) matching predicate arguments using distributional semantics. TINEs performance is comparable to that of previous metrics at segment level for several language pairs, with average Kendalls tau correlation from 0.26 to 0.29. We show that the addition of the shallow-semantic component improves the performance of simple lexical matching strategies and metrics such as BLEU. "
W11-2113 "The Department of Computer Science University of Sheffield Sheffield, S1 4DP. UK {xsong2,t.cohn}@shef.ac.uk Abstract Automatic evaluation metrics are fundamentally important for Machine Translation, allowing comparison of systems performance and efficient training. Current evaluation metrics fall into two classes: heuristic approaches, like BLEU, and those using supervised learning trained on human judgement data. While many trained metrics provide a better match against human judgements, this comes at the cost of including lots of features, leading to unwieldy, non-portable and slow metrics. In this paper, we introduce a new trained metric, ROSE, which only uses simple features that are easy portable and quick to compute. In addition, ROSE is sentence-based, as opposed to document-based, allowing it to be used in a wider range of settings. Results show that ROSE performs well on many tasks, such as ranking system and syntactic constituents, with results competitive to BLEU. Moreover, this still holds when ROSE is trained on human judgements of translations into a different language compared with that use in testing. "
W11-2114 "Abstract The past few years have seen an increasing interest in using Amazons Mechanical Turk for purposes of collecting data and performing annotation tasks. One such task is the mass evaluation of system output in a variety of tasks. In this paper, we present MAISE, a package that allows researchers to evaluate the output of their AI system(s) using human judgments collected via Amazons Mechanical Turk, greatly streamlining the process. MAISE is open source, easy to run, and platform-independent. The core of MAISEs codebase was used for the manual evaluation of WMT10, and the completed package is being used again in the current evaluation for WMT11. In this paper, we describe the main features, functionality, and usage of MAISE, which is now available for download and use. "
W11-2115 "Abstract This paper describes the development operated into MANY for the 2011 WMT system combination evaluation campaign. Hypotheses from French/English and English/French MT systems were combined with a new version of MANY, an open source system combination software based on confusion networks decoding currently developed at LIUM. MANY has been updated in order to optimize decoder parameters with MERT, which proves to find better weights. The system combination yielded significant improvements in BLEU score when applied on system combination data from two languages. "
W11-2116 " This paper presents the submissions of the pattern recognition and human language technology (PRHLT) group to the system combination task of the sixth workshop on statistical machine translation (WMT 2011). Each submissions is generated by a multi-system minimum Bayes risk (MBR) technique. Our technique uses the MBR decision rule and a linear combination of the component systems probability distributions to search for the minimum risk translation among all the sentences in the target language. 1 Introductio  "
W11-2117 "PA, USA {heafield,alavie}@cs.cmu.edu Abstract This paper describes our submissions, cmu-heafield-combo, to the ten tracks of the 2011 Workshop on Machine Translations system combination task. We show how the combination scheme operates by flexibly aligning system outputs then searching a space constructed from the alignments. Humans judged our combination the best on eight of ten tracks. "
W11-2118 ", Germany {leusch,freitag,ney}@cs.rwth-aachen.de Abstract RWTH participated in the System Combination task of the Sixth Workshop on Statistical Machine Translation (WMT 2011). For three language pairs, we combined 6 to 14 systems into a single consensus translation. A three-level metacombination scheme combining six different system combination setups with three different engines was applied on the FrenchEnglish language pair. Depending on the language pair, improvements versus the best single system are in the range of +1.9% and +2.5% abs. on BLEU, and between 1.8% and 2.4% abs. on TER. Novel techniques compared with RWTHs submission to WMT 2010 include two additional system combination engines, an additional word alignment technique, meta combination, and additional optimization techniques. "
W11-2119 "02138, USA {arosti,bzhang,smatsouk,schwartz}@bbn.com Abstract BBN submitted system combination outputs for Czech-English, German-English, SpanishEnglish, and French-English language pairs. All combinations were based on confusion network decoding. The confusion networks were built using incremental hypothesis alignment algorithm with flexible matching. A novel bi-gram count feature, which can penalize bi-grams not present in the input hypotheses corresponding to a source sentence, was introduced in addition to the usual decoder features. The system combination weights were tuned using a graph based expected BLEU as the objective function while incrementally expanding the networks to bi-gram and 5-gram contexts. The expected BLEU tuning described in this paper naturally generalizes to hypergraphs and can be used to optimize thousands of weights. The combination gained about 0.5-4.0 BLEU points over the best individual systems on the official WMT11 language pairs. A 39 system multisource combination achieved an 11.1 BLEU point gain. "
W11-2120 "Abstract This paper describes the UZH system that was used for the WMT 2011 system combination shared task submission. We participated in the system combination task for the translation directions DEEN and ENDE. The system uses Moses as a backbone, with the outputs of the 23 best individual systems being integrated through additional phrase tables. The system compares well to other system combination submissions, with no other submission being significantly better. A BLEU-based comparison to the individual systems, however, indicates that it achieves no significant gains over the best individual system. "
W11-2121 "Abstract This paper describes the JHU system combination scheme used in WMT-11. The JHU system combination is based on confusion network alignment, and inherited the framework developed by (Karakos et al., 2008). We improved our core system combination algorithm by making use of TER-plus, which was originally designed for string alignment, for alignment of confusion networks. Experimental results on French-English, GermanEnglish, Czech-English and Spanish-English combination tasks show significant improvements on BLEU and TER by up to 2 points on average, compared to the best individual system output, and improvements compared with the results produced by ITG which we used in WMT-10. "
W11-2122 " We consider using online language models for translating multiple streams which naturally arise on the Web. After establishing that using just one stream can degrade translations on different domains, we present a series of simple approaches which tackle the problem of maintaining translation performance on all streams in small space. By exploiting the differing throughputs of each stream and how the decoder translates prior test points from each stream, we show how translation performance can equal specialised, per-stream language models, but do this in a single language model using far less space. Our results hold even when adding three billion tokens of additional text as a background language model. 1 Introdu  "
W11-2123 "Abstract We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs. The PROBING data structure uses linear probing hash tables and is designed for speed. Compared with the widelyused SRILM, our PROBING model is 2.4 times as fast while using 57% of the memory. The TRIE data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption. TRIE simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline. Our code is open-source 1 , thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations. "
W11-2124 "Abstract In past Evaluations for Machine Translation of European Languages, it could be shown that the translation performance of SMT systems can be increased by integrating a bilingual language model into a phrase-based SMT system. In the bilingual language model, target words with their aligned source words build the tokens of an n-gram based language model. We analyzed the effect of bilingual language models and show where they could help to better model the translation process. We could show improvements of translation quality on German-to-English and Arabic-to-English. In addition, for the Arabic-to-English task, training an extra bilingual language model on the POS tags instead of the surface word forms led to further improvements. "
W11-2125 " We describe an approach for generating a ranked list of candidate document translation pairs without the use of bilingual dictionary or machine translation system. We developed this approach as an initial, filtering step, for extracting parallel text from large, multilingualbut non-parallel corpora. We represent bilingual documents in a vector space whose basis vectors are the overlapping tokens found in both languages of the collection. Using this representation, weighted by tfidf, we compute cosine document similarity to create a ranked list of candidate document translation pairs. Unlike cross-language information retrieval, where a ranked list in the target language is evaluated for each source query, we are interested in, and evaluate, the more difficult task of finding translated document pairs. We first perform a feasibility study of our approach on parallel collections in multiple languages, representing multiple language families and scripts. The approach is then applied to a large bilingual collection of around 800k books. To avoid the computational cost of ) ( 2 n O document pair comparisons, we employ locality sensitive hashing (LSH) approximation algorithm for cosine similarity, which reduces our time complexity to ) log ( n n O . "
W11-2126 " Languages with rich inflectional morphology pose a difficult challenge for statistical machine translation. To address the problem of morphologically inconsistent output, we add unification-based constraints to the target-side of a string-to-tree model. By integrating constraint evaluation into the decoding process, implausible hypotheses can be penalised or filtered out during search. We use a simple heuristic process to extract agreement constraints for German and test our approach on an English-German system trained on WMT data, achieving a small improvement in translation accuracy as measured by BLEU. 1 Introdu  "
W11-2127 "{habash,rambow}@ccls.columbia.edu Abstract The quality of Arabic-English statistical machine translation often suffers as a result of standard phrase-based SMT systems inability to perform long-range re-orderings, specifically those needed to translate VSO-ordered Arabic sentences. This problem is further exacerbated by the low performance of Arabic parsers on subject and subject span detection. In this paper, we present two parse fuzzification techniques which allow the translation system to select among a range of possible SV re-orderings. With this approach, we demonstrate a 0.3-point improvement in BLEU score (69% of the maximum possible using gold parses), and a corresponding improvement in the percentage of syntactically well-formed subjects under a manual evaluation. "
W11-2128 "iversity {akholy,habash}@ccls.columbia.edu Abstract Paraphrases are useful for statistical machine translation (SMT) and natural language processing tasks. Distributional paraphrase generation is independent of parallel texts and syntactic parses, and hence is suitable also for resource-poor languages, but tends to erroneously rank antonyms, trend-contrasting, and polarity-dissimilar candidates as good paraphrases. We present here a novel method for improving distributional paraphrasing by filtering out such candidates. We evaluate it in simulated low and mid-resourced SMT tasks, translating from English to two quite different languages. We show statistically significant gains in English-to-Chinese translation quality, up to 1 BLEU from nonfiltered paraphrase-augmented models (1.6 BLEU from baseline). We also show that yielding gains in translation to Arabic, a morphologically rich language, is not straightforward. "
W11-2129 "Abstract In many languages the use of compound words is very productive. A common practice to reduce sparsity consists in splitting compounds in the training data. When this is done, the system incurs the risk of translating components in non-consecutive positions, or in the wrong order. Furthermore, a post-processing step of compound merging is required to reconstruct compound words in the output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order. We also propose new heuristic methods for merging components that outperform all known methods, and a learning-based method that has similar accuracy as the heuristic method, is better at producing novel compounds, and can operate with no background linguistic resources. "
W11-2130 "Abstract Statistical machine translation systems are normally optimised for a chosen gain function (metric) by using MERT to find the best model weights. This algorithm suffers from stability problems and cannot scale beyond 20-30 features. We present an alternative algorithm for discriminative training of phrasebased MT systems, SampleRank, which scales to hundreds of features, equals or beats MERT on both small and medium sized systems, and permits the use of sentence or document level features. SampleRank proceeds by repeatedly updating the model weights to ensure that the ranking of output sentences induced by the model is the same as that induced by the gain function. "
W11-2131 "Abstract We present an empirical study of instance selection techniques for machine translation. In an active learning setting, instance selection minimizes the human effort by identifying the most informative sentences for translation. In a transductive learning setting, selection of training instances relevant to the test set improves the final translation quality. After reviewing the state of the art in the field, we generalize the main ideas in a class of instance selection algorithms that use feature decay. Feature decay algorithms increase diversity of the training set by devaluing features that are already included. We show that the feature decay rate has a very strong effect on the final translation quality whereas the initial feature values, inclusion of higher order features, or sentence length normalizations do not. We evaluate the best instance selection methods using a standard Moses baseline using the whole 1.6 million sentence English-German section of the Europarl corpus. We show that selecting the best 3000 training sentences for a specific test sentence is sufficient to obtain a score within 1 BLEU of the baseline, using 5% of the training data is sufficient to exceed the baseline, and a  2 BLEU improvement over the baseline is possible by optimally selected subset of the training data. In out-of-domain translation, we are able to reduce the training set size to about 7% and achieve a similar performance with the baseline. "
W11-2132 "Abstract Most of the freely available parallel data to train the translation model of a statistical machine translation system comes from very specific sources (European parliament, United Nations, etc). Therefore, there is increasing interest in methods to perform an adaptation of the translation model. A popular approach is based on unsupervised training, also called self-enhancing. Both only use monolingual data to adapt the translation model. In this paper we extend the previous work and provide new insight in the existing methods. We report results on the translation between French and English. Improvements of up to 0.5 BLEU were observed with respect to a very competitive baseline trained on more than 280M words of human translated parallel data. "
W11-2133 "Abstract This work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing Probabilistic Latent Semantic Analysis (PLSA) during model training. During inference, documents containing only the source language can be used to infer a full topic-word distribution on all words in the target languages vocabulary, from which we perform Minimum Discrimination Information (MDI) adaptation on a background language model (LM). We apply our approach on the English-French IWSLT 2010 TED Talk exercise, and report a 15% reduction in perplexity and relative BLEU and NIST improvements of 3% and 2.4%, respectively over a baseline only using a 5-gram background LM over the entire translation task. Our topic modeling approach is simpler to construct than its counterparts. "
W11-2134 " This paper presents the Linguatec submission to the WMT 2011 sixth workshop on statistical machine translation. It describes the architecture of our machine translation system Personal Translator (hereinafter also referred to as PT), developed by Linguatec, which is a rule-based translation system, enriched by statistical approaches. We participate for the German-English translation direction. For the current submission we have chosen the latest commercial version of the system, PT14. The translation quality improvement for the submission was done mainly by lexicon tuning: detection of unknown words, extracting of possible translations, partly from the wmt11 training corpora, and enlarging the lexicon by manually coding the chosen transfer candidates.  "
W11-2135 "LIMSI-CNRS B.P. 133, 91403 Orsay cedex, France Abstract This paper describes LIMSIs submissions to the Sixth Workshop on Statistical Machine Translation. We report results for the FrenchEnglish and German-English shared translation tasks in both directions. Our systems use n-code, an open source Statistical Machine Translation system based on bilingual n-grams. For the French-English task, we focussed on finding efficient ways to take advantage of the large and heterogeneous training parallel data. In particular, using a simple filtering strategy helped to improve both processing time and translation quality. To translate from English to French and German, we also investigated the use of the SOUL language model in Machine Translation and showed significant improvements with a 10-gram SOUL model. We also briefly report experiments with several alternatives to the standard n-best MERT procedure, leading to a significant speed-up. "
W11-2136 "V1 1SB, UK {w.aziz, m.rios, l.specia}@wlv.ac.uk Abstract We present a translation model enriched with shallow syntactic and semantic information about the source language. Base-phrase labels and semantic role labels are incorporated into an hierarchical model by creating shallow semantic trees. Results show an increase in performance of up to 6% in BLEU scores for English-Spanish translation over a standard phrase-based SMT baseline. "
W11-2137 "Abstract We present the results we obtain using our RegMT system, which uses transductive regression techniques to learn mappings between source and target features of given parallel corpora and use these mappings to generate machine translation outputs. Our training instance selection methods perform feature decay for proper selection of training instances, which plays an important role to learn correct feature mappings. RegMT uses L 2 regularized regression as well as L 1 regularized regression for sparse regression estimation of target features. We present translation results using our training instance selection methods, translation results using graph decoding, system combination results with RegMT, and performance evaluation with the F 1 measure over target features as a metric for evaluating translation quality. "
W11-2138 " We use target-side monolingual data to extend the vocabulary of the translation model in statistical machine translation. This method called reverse self-training improves the decoders ability to produce grammatically correct translations into languages with morphology richer than the source language esp. in small-data setting. We empirically evaluate the gains for several pairs of European languages and discuss some approaches of the underlying back-off techniques needed to translate unseen forms of known words. We also provide a description of the systems we submitted to WMT11 Shared Task. 1 Introdu  "
W11-2139 "5213, USA {cdyer,kgimpel,jhclark,nasmith}@cs.cmu.edu Abstract This paper describes the German-English translation system developed by the ARK research group at Carnegie Mellon University for the Sixth Workshop on Machine Translation (WMT11). We present the results of several modeling and training improvements to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of OOVs; and using development set translations into other languages to create additional pseudoreferences for training. "
W11-2140 "UMIACS Laboratory for Computational Linguistics and Information Processing  Department of Linguistics University of Maryland, College Park {vlad,hollingk,resnik}@umiacs.umd.edu Abstract This paper presents the system we developed for the 2011 WMT Haitian CreoleEnglish SMS featured translation task. Applying standard statistical machine translation methods to noisy real-world SMS data in a low-density language setting such as Haitian Creole poses a unique set of challenges, which we attempt to address in this work. Along with techniques to better exploit the limited available training data, we explore the benefits of several methods for alleviating the additional noise inherent in the SMS and transforming it to better suite the assumptions of our hierarchical phrase-based model system. We show that these methods lead to significant improvements in BLEU score over the baseline. "
W11-2141 "Abstract In this paper we describe our hybrid machine translation system with which we participated in the WMT11 shared translation task for the EnglishGerman language pair. Our system was able to outperform its RBMT baseline and turned out to be the best-scored participating system in the manual evaluation. To achieve this, we extended an existing, rule-based MT system with a module for stochastic selection of analysis parse trees that allowed to better cope with parsing errors during the systems analysis phase. Due to the integration into the analysis phase of the RBMT engine, we are able to preserve the benefits of a rule-based translation system such as proper generation of target language text. Additionally, we used a statistical tool for terminology extraction to improve the lexicon of the RBMT system. We report results from both automated metrics and human evaluation efforts, including examples which show how the proposed approach can improve machine translation quality. "
W11-2142 " This paper describes the joint QUAERO submission to the WMT 2011 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT GermanEnglish task. Each group translated the data sets with their own systems. Then RWTH system combination combines these translations to a better one. In this paper, we describe the single systems of each group. Before we present the results of the system combination, we give a short description of the RWTH Aachen system combination approach. 1 Overvie  "
W11-2143 " We present the Carnegie Mellon University Stat-XFER group submission to the WMT 2011 shared translation task. We built a hybrid syntactic MT system for FrenchEnglish using the Joshua decoder and an automatically acquired SCFG. New work for this year includes training data selection and grammar filtering. Expanded training data selection significantly increased translation scores and lowered OOV rates, while results on grammar filtering were mixed. 1 I  "
W11-2144 "Abstract This paper presents our submissions to the shared translation task at WMT 2011. We created two largely independent systems for English-to-French and Haitian Creole-toEnglish translation to evaluate different features and components from our ongoing research on these language pairs. Key features of our systems include anaphora resolution, hierarchical lexical reordering, data selection for language modelling, linear transduction grammars for word alignment and syntaxbased decoding with monolingual dependency information. "
W11-2145 " This paper describes the phrase-based SMT systems developed for our participation in the WMT11 Shared Translation Task. Translations for EnglishGerman and EnglishFrench were generated using a phrase-based translation system which is extended by additional models such as bilingual and fine-grained POS language models, POS-based reordering, lattice phrase extraction and discriminative word alignment. Furthermore, we present a special filtering method for the English-French Giga corpus and the phrase scoring step in the training is parallelized. 1 Introduction In this paper we describe our systems for the EMNLP 2011 Sixth Workshop on Statistical Machine Translation. We participated in the Shared Translation Task and submitted translations for EnglishGerman and EnglishFrench. We use a phrase-based decoder that can use lattices as input and developed several models that extend the standard log-linear model combination of phrase-based MT. These include advanced reordering models and corresponding adaptations to the phrase extraction process as well as extension to the translation and language model in form of discriminative word alignment and a bilingual language model to extend source word context. For English-German, language models based on fine-grained part-of-speech tags were used to address the difficult target language generation due to the rich morphology of German. We also present a filtering method directly addressing the problems of web-crawled corpora, which enabled us to make use of the French-English Giga corpus. Another novelty in our systems this year is the parallel phrase scoring method that reduces the time needed for training which is especially convenient for such big corpora as the Giga corpus. 2 System Descript  "
W11-2146 "A 15213, USA {sanjika,nbach,qing,vamshi,vogel+}@cs.cmu.edu Abstract This paper describes the statistical machine translation system submitted to the WMT11 Featured Translation Task, which involves translating Haitian Creole SMS messages into English. In our experiments we try to address the issue of noise in the training data, as well as the lack of parallel training data. Spelling normalization is applied to reduce out-of-vocabulary words in the corpus. Using Semantic Role Labeling rules we expand the available training corpus. Additionally we investigate extracting parallel sentences from comparable data to enhance the available parallel data. "
W11-2147 "Abstract This paper presents the LIU system for the WMT 2011 shared task for translation between German and English. For English German we attempted to improve the translation tables with a combination of standard statistical word alignments and phrase-based word alignments. For GermanEnglish translation we tried to make the German text more similar to the English text by normalizing German morphology and performing rule-based clause reordering of the German text. This resulted in small improvements for both translation directions. "
W11-2148 "Abstract MonoTrans2 is a translation system that combines machine translation (MT) with human computation using two crowds of monolingual source (Haitian Creole) and target (English) speakers. We report on its use in the WMT 2011 Haitian Creole to English translation task, showing that MonoTrans2 translated 38% of the sentences well compared to Google Translates 25%. "
W11-2149 " This paper describes the statistical machine translation (SMT) systems developed by RWTH Aachen University for the translation task of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation. Both phrasebased and hierarchical SMT systems were trained for the constrained German-English and French-English tasks in all directions. Experiments were conducted to compare different training data sets, training methods and optimization criteria, as well as additional models on dependency structure and phrase reordering. Further, we applied a system combination technique to create a consensus hypothesis from several different systems. "
W11-2150 "94242 1090 GE Amsterdam, The Netherlands {m.khalilov,k.simaan}@uva.nl Abstract In this paper we describe the Institute for Logic, Language and Computation (University of Amsterdam) phrase-based statistical machine translation system for Englishto-German translation proposed within the EMNLP-WMT 2011 shared task. The main novelty of the submitted system is a syntaxdriven pre-translation reordering algorithm implemented as source string permutation via transfer of the source-side syntax tree. "
W11-2151 " This paper describes the UPM system for translation task at the EMNLP 2011 workshop on statistical machine translation  http://www.statmt.org/wmt11/   http://www.statmt.org/wmt11/ ), and it has been used for both directions: Spanish-English and English-Spanish. This system is based on Moses with two new modules for pre and post processing the sentences. The main contribution is the method proposed (based on the similarity with the source language test set) for selecting the sentences for training the models and adjusting the weights. With system, we have obtained a 23.2 BLEU for Spanish-English and 21.7 BLEU for EnglishSpanish. 1  "
W11-2152 " This paper describes an experiment in which we try to automatically correct mistakes in grammatical agreement in English to Czech MT outputs. We perform several rule-based corrections on sentences parsed to dependency trees. We prove that it is possible to improve the MT quality of majority of the systems participating in WMT shared task. We made both automatic (BLEU) and manual evaluations. 1  "
W11-2153 " Accuracy of dependency parsers is one of the key factors limiting the quality of dependencybased machine translation. This paper deals with the influence of various dependency parsing approaches (and also different training data size) on the overall performance of an English-to-Czech dependency-based statistical translation system implemented in the Treex framework. We also study the relationship between parsing accuracy in terms of unlabeled attachment score and machine translation quality in terms of BLEU. 1 Introduction In the last years, statistical n-gram models dominated the field of Machine Translation (MT). However, their results are still far from perfect. Therefore we believe it makes sense to investigate alternative statistical approaches. This paper is focused on an analysis-transfer-synthesis translation system called TectoMT whose transfer representation has a shape of a deep-syntactic dependency tree. The system has been introduced by  Zabokrtsk  y et al. (2008). The translation direction under consideration is Englishto-Czech. It has been shown by Popel (2009) that the current accuracy of the dependency parser employed in this translation system is one of the limiting factors from the viewpoint of its output quality. In other words, the parsing phase is responsible for a large portion of translation errors. The biggest source of translation errors in the referred study was (and probably still is) the transfer phase, however the proportion has changed since and the relative importance of the parsing phase has grown, because the tranfer phase errors have already been addressed by improvements based on Hidden Markov Tree Models for lexical and syntactic choice as shown by  Zabokrtsk  y and Popel (2009), and by context sensitive translation models based on maximum entropy as described by Mare cek et al. (2010). Our study proceeds along two directions. First, we train two state-of-the-art dependency parsers on training sets with varying size. Second, we use five parsers based on different parsing techniques. In both cases we document the relation between parsing accuracy (in terms of Unlabeled Attachment Score, UAS) and translation quality (estimated by the well known BLEU metric). The motivation behind the first set of experiments is that we can extrapolate the learning curve and try to predict how new advances in dependency parsing can affect MT quality in the future. The second experiment series is motivated by the hypothesis that parsers based on different approaches are likely to have a different distribution of errors, even if they can have competitive performance in parsing accuracy. In dependency parsing metrics, all types of incorrect edges typically have the same weight, 1 but some incorrect edges can be more harmful than others from the MT viewpoint. For instance, an incorrect attachment of an adverbial node is usually harmless, while incorrect attachment of a subject node might have several negative cons 1 This issue has been tackled already in the parsing literature; for example, some authors disregard placement of punctuation nodes within trees in the evaluation (Zeman, 2004). 433 quences such as:  unrecognized finiteness of the governing verb, which can lead to a wrong syntactization on the target side (an infinitive verb phrase instead of a finite clause),  wrong choice of the target-side verb form (because of unrecognized subject-predicate agreement),  missing punctuation (because of wrongly recognized finite clause boundaries),  wrong placement of clitics (because of wrongly recognized finite clause boundaries),  wrong form of pronouns (personal and possessive pronouns referring to the clauses subject should have reflexive forms in Czech). Thus it is obvious that the parser choice is important and that it might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge. 2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence of the selected parsers on the MT quality in terms of BLEU. Section 5 concludes. 2 However, the parser bottleneck of the dependency-based MT approach was observed also by other researchers (Robert Moore, personal communication). 2 Dependency-based Translation in Treex We have implemented our experiments in the Treex software framework (formerly TectoMT, introduced by  Zabokrtsk  y et al. (2008)), which already offers tool chains for analysis and synthesis of Czech and English sentences. We use the tectogrammatical (deep-syntactic) layer of language representation as the transfer layer in the presented MT experiments. Tectogrammatics was introduced by Sgall (1967) and further elaborated within the Prague Dependency Treebank project (Haji c et al., 2006). On this layer, each sentence is represented as a tectogrammatical tree, whose main properties (from the MT viewpoint) are the following: 1. nodes represent autosemantic words, 2. edges represent semantic dependencies (a node is an argument or a modifier of its parent), 3. there are no functional words (prepositions, auxiliary words) in the tree, and the autosemantic words appear only in their base forms (lemmas). Morphologically indispensable categories (such as number with nouns or tense with verbs, but not number with verbs as it is only imposed by agreement) are stored in separate node attributes (grammatemes). The intuitions behind the decision to use tectogrammatics for MT are the following: we believe that (1) tectogrammatics largely abstracts from language-specific means (inflection, agglutination, functional words etc.) of expressing non-lexical meanings and thus tectogrammatical trees are supposed to be highly similar across languages, (2) it enables a natural transfer factorization, 3 (3) and local tree contexts in tectogrammatical trees carry more information (especially for lexical choice) than local linear contexts in the original sentences. The translation scenario is outlined in the rest of this section. 3 Morphological categories can be translated almost independently from lemmas, which makes parallel training data denser, especially when translating from/to a language with rich inflection such as Czech. 434 "
W11-2154 " We describe our system for the news commentary translation task of WMT 2011. The submitted run for the French-English direction is a combination of two MOSES-based systems developed at LIG and LIA laboratories. We report experiments to improve over the standard phrase-based model using statistical post-edition, information retrieval methods to subsample out-of-domain parallel corpora and ROVER to combine n-best list of hypotheses output by different systems. 1 I  "
W11-2155 "Unsupervised word clustering algorithms  which form word clusters based on a measure of distributional similarity  have proven to be useful in providing beneficial features for various natural language processing tasks involving supervised learning. This work explores the utility of such word clusters as factors in statistical machine translation. Although some of the language pairs in this work clearly benefit from the factor augmentation, there is no consistent improvement in translation accuracy across the board. For all language pairs, the word clusters clearly improve translation for some proportion of the sentences in the test set, but has a weak or even detrimental effect on the rest. It is shown that if one could determine whether or not to use a factor when translating a given sentence, rather substantial improvements in precision could be achieved for all of the language pairs evaluated. While such an oracle method is not identified, evaluations indicate that unsupervised word cluster are most beneficial in sentences without unknown words. "
W11-2156 "Abstract This work describes the Haitian-Cr  eole to English statistical machine translation system built by Barcelona Media Innovation Center (BM) and Institute for Infocomm Research (I2R) for the 6th Workshop on Statistical Machine Translation (WMT 2011). Our system carefully processes the available data and uses it in a standard phrase-based system enhanced with a source context semantic feature that helps conducting a better lexical selection and a feature orthogonalization procedure that helps making MERT optimization more reliable and stable. Our system was ranked first (among a total of 9 participant systems) by the conducted human evaluation. "
W11-2157 " This paper describes the machine translation (MT) system developed by the Transducens Research Group, from Universitat dAlacant, Spain, for the WMT 2011 shared translation task. We submitted a hybrid system for the SpanishEnglish language pair consisting of a phrase-based statistical MT system whose phrase table was enriched with bilingual phrase pairs matching transfer rules and dictionary entries from the Apertium shallowtransfer rule-based MT platform. Our hybrid system outperforms, in terms of BLEU, GTM and METEOR, a standard phrase-based statistical MT system trained on the same corpus, and received the second best BLEU score in the automatic evaluation. 1 Introdu  "
W11-2158 "Abstract This paper describes the development of FrenchEnglish and EnglishFrench statistical machine translation systems for the 2011 WMT shared task evaluation. Our main systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only, but we also performed initial experiments with hierarchical systems. Additional, new features this year include improved translation model adaptation using monolingual data, a continuous space language model and the treatment of unknown words. "
W11-2159 "Abstract We report results on translation of SMS messages from Haitian Creole to English. We show improvements by applying spell checking techniques to unknown words and creating a lattice with the best known spelling equivalents. We also used a small cleaned corpus to train a cleaning model that we applied to the noisy corpora. "
W11-2160 " We present progress on Joshua, an opensource decoder for hierarchical and syntaxbased machine translation. The main focus is describing Thrax, a flexible, open source synchronous context-free grammar extractor. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. It is built on Apache Hadoop for efficient distributed performance, and can easily be extended with support for new grammars, feature functions, and output formats. 1 Introduction Joshua is an open-source 1 toolkit for hierarchical machine translation of human languages. The original version of Joshua (Li et al., 2009) was a reimplementation of the Python-based Hiero machinetranslation system (Chiang, 2007); it was later extended (Li et al., 2010) to support richer formalisms, such as SAMT (Zollmann and Venugopal, 2006). The main focus of this paper is to describe this past years work in developing Thrax (Weese, 2011), an open-source grammar extractor for Hiero and SAMT grammars. Grammar extraction has shown itself to be something of a black art, with decoding performance depending crucially on a variety of features and options that are not always clearly described in papers. This hindered direct comparison both between and within grammatical formalisms. Thrax standardizes Joshuas grammar e 1 http://github.com/joshua-decoder/joshua  http://github.com/joshua-decoder/joshua traction procedures by providing a flexible and configurable means of specifying these settings. Section 3 presents a systematic comparison of the two grammars using identical feature sets. In addition, Joshua now includes a single parameterized script that implements the entire MT pipeline, from data preparation to evaluation. This script is built on top of a module called CachePipe. CachePipe is a simple wrapper around shell commands that uses SHA-1 hashes and explicitlyprovided lists of dependencies to determine whether a command needs to be run, saving time both in running and debugging machine translation pipelines. "
W11-2161 ", Language Technology Lab Stuhlsatzenhausweg 3 D-66123 Saarbr  ucken Germany {Jia.Xu,uszkoreit,David.Vilar}@dfki.de, {bakuzen,xiaojun.zhang.iiken}@gmail.com Abstract We present the DFKI hybrid translation system at the WMT workshop 2011. Three SMT and two RBMT systems are combined at the level of the final translation output. The translation results show that our hybrid system significantly outperformed individual systems by exploring strengths of both rule-based and statistical translations. "
W11-2162 "Abstract This paper describes the system presented for the English-Spanish translation task by the collaboration between CEU-UCH and UPV for 2011 WMT. A comparison of independent phrase-based translation models interpolation for each available training corpora were tested, giving an improvement of 0.4 BLEU points over the baseline. Output N -best lists were rescored via a target Neural Network Language Model. An improvement of one BLEU point over the baseline was obtained adding the two features, giving 31.5 BLEU and 57.9 TER for the primary system, computed over lowercased and detokenized outputs. The system was positioned second in the final ranking. "
W11-2163 " We describe our experiments with hierarchical phrase-based machine translation for the WMT 2011 Shared Task. We trained a system for all 8 translation directions between English on one side and Czech, German, Spanish or French on the other side, though we focused slightly more on the English-to-Czech direction. We provide a detailed description of our configuration and data so the results are replicable. 1 I  "
W11-2164 " In this paper, we propose that MT is an important technology in crisis events, something that can and should be an integral part of a rapid-response infrastructure. By integrating MT services directly into a messaging infrastructure (whatever the type of messages being serviced, e.g., text messages, Twitter feeds, blog postings, etc.), MT can be used to provide first pass translations into a majority language, which can be more effectively triaged and then routed to the appropriate aid agencies. If done right, MT can dramatically increase the speed by which relief can be provided. To ensure that MT is a standard tool in the arsenal of tools needed in crisis events, we propose a preliminary Crisis Cookbook, the contents of which could be translated into the relevant language(s) by volunteers immediately after a crisis event occurs. The resulting data could then be made available to relief groups on the ground, as well as to providers of MT services. We also note that there are significant contributions that our community can make to relief efforts through continued work on our research, especially that research which makes MT more viable for under-resourced languages. "
W11-2165 "213, USA {kgimpel,nasmith}@cs.cmu.edu Abstract A growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011). Typically, these gappy patterns are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily many gaps. We exploit Bayesian nonparametrics and collapsed Gibbs sampling to discover salient patterns in a corpus. We evaluate the patterns qualitatively and also add them as features to an MT system, reporting promising preliminary results. "
W11-2166 "<NoAbstract>"
W11-2167 "<NoAbstract>"
W11-2301 "Abstract Aphasia treatment for the recovery of lost communication functionalities is possible through frequent and intense speech therapy sessions. In this sense, speech and language technology may provide important support in improving the recovery process. The aim of the project Vithea (Virtual Therapist for Aphasia Treatment) is to develop an on-line system designed to behave as a virtual therapist, guiding the patient in performing training exercises in a simple and intuitive fashion. In this paper, the fundamental components of the Vithea system are presented, with particular emphasis on the speech recognition module. Furthermore, we report encouraging automatic word naming recognition results using data collected from speech therapy sessions. "
W11-2302 "Abstract This paper describes modifications to acoustic speech signals produced by speakers with dysarthria in order to make those utterances more intelligible to typical listeners. These modifications include the correction of tempo, the adjustment of formant frequencies in sonorants, the removal of aberrant voicing, the deletion of phoneme insertion errors, and the replacement of erroneously dropped phonemes. Through simple evaluations of intelligibility with na  ve listeners, we show that the correction of phoneme errors results in the greatest increase in intelligibility and is therefore a desirable mechanism for the eventual creation of augmentative application software for individuals with dysarthria. "
W11-2303 "Center for Spoken Language Understanding  Child Development & Rehabilitation Center Oregon Health & Science University {roark,fowlera,sproatr}@cslu.ogi.edu {gibbons,mfo}@ohsu.edu Abstract In this paper, we examine the idea of technology-assisted co-construction, where the communication partner of an AAC user can make guesses about the intended messages, which are included in the users word completion/prediction interface. We run some human trials to simulate this new interface concept, with subjects predicting words as the users intended message is being generated in real time with specified typing speeds. Results indicate that people can provide substantial keystroke savings by providing word completion or prediction, but that the savings are not as high as n-gram language models. Interestingly, the language model and human predictions are complementary in certain key ways  humans doing a better job in some circumstances on contextually salient nouns. We discuss implications of the enhanced coconstruction interface for real-time message generation in AAC direct selection devices. "
W11-2304 "Abstract Advances in natural language generation and speech processing techniques, combined with changes in the commercial landscape, have brought within reach dramatic improvements in Augmentative Alternative Communication (AAC). These improvements, though overwhelmingly positive, amplify a family of personal data use problems. This paper argues that the AAC design and implementation process needs to identify and address personal data use problems. Accordingly, this paper explores personal data management problems and proposes responses. This paper is situated in the context of AAC technology but the responses could be generalised for other communities affected by low digital literacy, low literacy levels and cognitive challenges. "
W11-2305 "Center for Spoken Language Understanding, Oregon Health & Science University {beckleyr,roark}@cslu.ogi.edu Abstract In this paper, we examine several methods for including dynamic, contextually-sensitive binary codes within indirect selection typing methods using a grid with fixed symbol positions. Using Huffman codes derived from a character n-gram model, we investigate both synchronous (fixed latency highlighting) and asynchronous (self-paced using long versus short press) scanning. Additionally, we look at methods that allow for scanning past a target and returning to it versus methods that remove unselected items from consideration. Finally, we investigate a novel method for displaying the binary codes for each symbol to the user, rather than using cell highlighting, as the means for identifying the required input sequence for the target symbol. We demonstrate that dynamic coding methods for fixed position grids can be tailored for very diverse user requirements. "
W11-2306 " This paper describes our work on improving access to the content of multimodal documents containing line graphs in popular media for people with visual impairments. We provide an overview of our implemented system, including our method for recognizing and conveying the intended message of a line graph. The textual description of the graphic generated by our system is presented at the most relevant point in the document. We also describe ongoing work into obtaining additional propositions that elaborate on the intended message, and examine the potential benefits of analyzing the text and graphical content together in order to extend our system to produce summaries of entire multimodal documents. "
W11-2307 " This paper describes the integration of commonly used screen readers, namely, NVDA [ [NVDA 2011] ] and ORCA [ [ORCA 2011] ] with Text to Speech (TTS) systems for Indian languages. A participatory design approach was followed in the development of the integrated system to ensure that the expectations of visually challenged people are met. Given that India is a multilingual country (22 official languages), a uniform framework for an integrated text-to-speech synthesis systems with screen readers across six Indian languages are developed, which can be easily extended to other languages as well. Since Indian languages are syllable centred, syllable-based concatenative speech synthesizers are built. This paper describes the development and evaluation of syllable-based Indian language Text-To-Speech (TTS) synthesis system (around festival TTS) with ORCA and NVDA, for Linux and Windows environments respectively. TTS systems for six Indian Languages, namely, Hindi, Tamil, Marathi, Bengali, Malayalam and Telugu were built. Usability studies of the screen readers were performed. The system usability was evaluated by a group of visually challenged people based on a questionnaire provided to them. And a Mean Opinion Score(MoS) of 62.27% was achieved. "
W11-2308 ", 1  Pisa (Italy) {felice.dellorletta,simonetta.montemagni,giulia.venturi}@ilc.cnr.it Abstract In this paper, we propose a new approach to readability assessment with a specific view to the task of text simplification: the intended audience includes people with low literacy skills and/or with mild cognitive impairment. READIT represents the first advanced readability assessment tool for what concerns Italian, which combines traditional raw text features with lexical, morpho-syntactic and syntactic information. In READIT readability assessment is carried out with respect to both documents and sentences where the latter represents an important novelty of the proposed approach creating the prerequisites for aligning the readability assessment step with the text simplification process. READIT shows a high accuracy in the document classification task and promising results in the sentence classification scenario. "
W11-2309 " This paper describes a categorization module for improving the performance of a Spanish into Spanish Sign Language (LSE) translation system. This categorization module replaces Spanish words with associated tags. When implementing this module, several alternatives for dealing with non-relevant words have been studied. Nonrelevant words are Spanish words not relevant in the translation process. The categorization module has been incorporated into a phrase-based system and a Statistical Finite State Transducer (SFST). The evaluation results reveal that the BLEU has increased from 69.11% to 78.79% for the phrase-based system and from 69.84% to 75.59% for the SFST. Keywords: Source language categorization, Speech into Sign Language translation. Lengua de Signos Espanola (LSE). "
W11-2310 "<NoAbstract>"
W11-2311 "Abstract This paper describes a machine translation system that offers many deaf and hearingimpaired people the chance to access published information in Arabic by translating text into their first language, Arabic Sign Language (ArSL). The system was created under the close guidance of a team that included three deaf native signers and one ArSL interpreter. We discuss problems inherent in the design and development of such translation systems and review previous ArSL machine translation systems, which all too often demonstrate a lack of collaboration between engineers and the deaf community. We describe and explain in detail both the adapted translation approach chosen for the proposed system and the ArSL corpus that we collected for this purpose. The corpus has 203 signed sentences (with 710 distinct signs) with content restricted to the domain of instructional language as typically used in deaf education. Evaluation shows that the system produces translated sign sentences outputs with an average word error rate of 46.7% and an average position error rate of 29.4% using leave-oneout cross validation. The most frequent source of errors is missing signs in the corpus; this could be addressed in future by collecting more corpus material. "
W11-2312 "Talkamatic AB Gothenburg, Sweden {alex,fredrik}@talkamatic.se Abstract This paper describes an ongoing project where we develop and evaluate a setup involving a communication board and a toy robot, which can communicate with each other via synthesised speech. The purpose is to provide children with communicative disabilities with a toy that is fun and easy to use together with peers, with and without disabilities. When the child selects a symbol on the communication board, the board speaks and the robot responds. This encourages the child to use language and learn to cooperate to reach a common goal. Throughout the project, three children with cerebral palsy and their peers use the robot and provide feedback for further development. The multimodal interaction with the robot is video recorded and analysed together with observational data in activity diaries. "
W11-2313 "Abstract A corpus of easy-to-read texts in combination with a base vocabulary pool for Swedish was used in order to build a basic vocabulary. The coverage of these entries by symbols in an existing AAC database was then assessed. We finally suggest a method for enriching the expressive power of the AAC language by combining existing symbols and in this way illustrate additional concepts. "
W11-2314 "s MK76AA, UK {r.power,s.h.williams}@open.ac.uk Abstract Numerical information is very common in all kinds of documents from newspapers and magazines to household bills and wage slips. However, many people find it difficult to understand, particularly people with poor education and disabilities. Sometimes numerical information is presented with hedges that modify the meaning. A numerical hedge is a word or phrase employed to indicate explicitly that some loss of precision has taken place (e.g., around) and it may also indicate the direction of approximation (e.g., more than). This paper presents a study of the use of numerical hedges that is part of research investigating the process of rewriting difficult numerical expressions in simpler ways. We carried out a survey in which experts in numeracy were asked to simplify a range of proportion expressions and analysed the results to obtain guidelines for automating the simplification task. "
W11-2315 " The Simple English Wikipedia provides a simplified version of Wikipedia's English articles for readers with special needs. However, there are fewer efforts to make information in Wikipedia in other languages accessible to a large audience. This work proposes the use of a syntactic simplification engine with high precision rules to automatically generate a Simple Portuguese Wikipedia on demand, based on user interactions with the main Portuguese Wikipedia. Our estimates indicated that a human can simplify about 28,000 occurrences of analysed patterns per million words, while our system can correctly simplify 22,200 occurrences, with estimated f-measure 77.2%.  "
W11-2801 "<NoAbstract>"
W11-2802 " We present a framework for text simplification based on applying transformation rules to a typed dependency representation produced by the Stanford parser. We test two approaches to regeneration from typed dependencies: (a) gen-light, where the transformed dependency graphs are linearised using the word order and morphology of the original sentence, with any changes coded into the transformation rules, and (b) gen-heavy, where the Stanford dependencies are reduced to a DSyntS representation and sentences are generating formally using the RealPro surface realiser. The main contribution of this paper is to compare the robustness of these approaches in the presence of parsing errors, using both a single parse and an n-best parse setting in an overgenerate and rank approach. We find that the gen-light approach is robust to parser error, particularly in the n-best parse setting. On the other hand, parsing errors cause the realiser in the genheavy approach to order words and phrases in ways that are disliked by our evaluators. 1 Introdu  "
W11-2803 "ngdom {s.mahamood, e.reiter}@abdn.ac.uk Abstract This paper presents several affective NLG strategies for generating medical texts for parents of pre-term neonates. Initially, these were meant to be personalised according to a model of the recipients level of stress. However, our evaluation showed that all recipients preferred texts generated with the affective strategies, regardless of predicted stress level. "
W11-2804 " Evaluations of NLG systems generally are quantiative, that is, based on corpus comparison statistics and/or results of experiments with people. Outcomes of such evaluations are important in demonstrating whether or not an NLG system is successful, but leave gaps in understanding why this is the case. Alternatively, qualitative evaluations carried out by experts provide knowledge on where a system needs to be improved. In this paper we describe two such evaluations carried out for the BT-Nurse system, using two different methodologies (content analysis and discourse analysis). The outcomes of such evaluations are discussed in comparison to what was learnt from a quantitiave evaluation of BT-Nurse. Implications for the role of similar evaluations in NLG are also discussed.  "
W11-2805 " We describe the application of a framework for salience metrics and linguistic variability with respect to the contextually adequate choice of referring expressions and grammatical roles: Where multiple meaning-equivalent candidate realizations are available that differ in one of these aspects, NLG systems can apply salience metrics to predict contextually adequate realization preferences. We evaluate this claim and a number of parameters of salience metrics found in the theoretical literature on two German newspaper corpora. Key features of the approach described here include the application of a two-dimensional model of salience, how its theoretical predictions can be exploited to develop salience metrics for a particular phenomenon, and that these salience metrics can be subsequently applied to other phenomena. This approach can be applied to develop classifiers to predict packaging preferences for phenomena where little training data is available. 1 Motivation  "
W11-2806 "Traditional approaches to referring expression generation (REG) have taken as a fundamental requirement the need to distinguish the intended referent from other entities in the context. It seems obvious that this should be a necessary condition for successful reference; but we suggest that a number of recent investigations cast doubt on the significance of this aspect of reference. In the present paper, we look at the role of visual context in determining the content of a referring expression, and come to the conclusion that, at least in the referential scenarios underlying our data, visual context appears not to be a major factor in content determination for reference. We discuss the implications of this surprising finding. 1 Introduction Traditional approaches to referring expression generation are based on the idea of distinguishing the intended referent from the other entities in the context (Dale and Reiter, 1995; Gardent, 2002; Krahmer and Theune, 2002; Krahmer et al., 2003; Gatt and van Deemter, 2006). The task is generally characterised as involving the construction of a distinguishing description consisting of those attributes of the intended referent that distinguish it from the other entities with which it might be confused; building a referring expression thus requires us to have an appropriate formalisation of the notion of context. Earlier work (for example, (Dale, 1989)) took its cue from work on discourse structure (in particular, (Grosz and Sidner, 1986)), and defined the context in terms of the set of discourseaccessible referents; more recent work has tended to focus on visual scenes (for example, (Viethen and Dale, 2006; Gatt et al., 2008; Gatt et al., 2009)), with the context being defined as the set of all the objects in the scene. Most of the early approaches to REG (Dale, 1989; Dale and Haddock, 1991; Dale and Reiter, 1995; Krahmer et al., 2003) were proposed without the support of rigorous empirical testing. Probably the most fundamental shift in the field in the last five years has been the move towards the development of algorithms that attempt to replicate corpora of human-produced referring expressions. This work has only really become possible with the advent of a number of publicly-available corpora of human-produced referring expressions collected under controlled circumstances: these include the TUNA Corpus (van der Sluis et al., 2006), the Drawer Corpus (Viethen and Dale, 2006), and the GRE3D3 and GRE3D7 Corpora (Viethen and Dale, 2008; Viethen and Dale, 2011). All of these corpora contain descriptions of target referents using a small number of attributes in simple visual scenes containing only a very small number of distractor objects. The descriptions in all these cases were elicited in isolation, with no preceding discourse: the reference task they represent has sometimes been called one-shot reference. So there is no discourse context that provides a set of potential distractors, but there is a visual context of potential distractors. The idea that the process of constructing a reference to an object in a visual scene needs to take account of the other entities in that scene in order to ensure that the reference is successful seems so obvious that it might be thought ridiculous to doubt it. However, our exploration of a dataset that contains referring expressions for objects in visual scenes of somewhat greater complexity and involving dialogic discourse calls this fundamental assumption into question. In (Viethen et al., 2011), we presented a machinelearning approach to REG, and distinguished two main kinds of features that might play a role in subsequent reference: traditional REG features, which are concerned with distinguishing the intended referent from visual and discourse distractors; and alignment features, representing aspects of the discourse history (Clark and WilkesGibbs, 1986; Pickering and Garrod, 2004). We used feature ablation in a decision tree approach to investigate the role of the traditional features, and found that the impact of these features was negligible compared to that of the alignment features. The bad performance of these features caused us to ask whether the method of determining 44 the visual distractors that were taken into account was to be blamed. In the present paper, we explore this question by trying out two different ways of determining the set of visual distractors and by varying the size of this set. In Section 2 we provide some background by situating the investigation presented here with respect to the literature. In Section 3, we describe the corpus we work with, and in Section 4, we describe our machine-learning framework for exploring the data this corpus provides. In Section 5, we present the results of some experiments that attempt to determine the role of visual context in REG, and in Section 6 we draw some conclusions. 2 Background Some of the earliest work in REG (for example, (Dale, 1989)) adopted what we might think of as an extreme rationalist characterisation of the task: build a description that has no more and no less information than is required to distinguish the intended referent (a minimal distinguishing description). It was soon recognised that this was not a good characterisation of what people did, in particular because human-produced descriptions are often over-specified, rather than being minimal in the sense just described. The incremental algorithm (IA; (Dale and Reiter, 1995)) diluted the extreme position with the acknowledgement that something akin to habit also played a role in REG: the basic idea here was that, on the basis of experience, people learn preference orders for properties that tend to work well, and when faced with the need to create a new description, they use these preference orders to guide the search for an appropriate description. The IA still hung on to the need to build a distinguishing description, but the preference order mechanism meant that some descriptions might be longer than necessary, containing redundant information. In (Dale and Viethen, 2010), we proposed a further weakening of the traditional model, suggesting that attributes in a referring expression might be chosen independently, rather in a fashion whereby each depends on the attributes previously chosen (a characteristic of earlier algorithms that we refer to as serial dependency). But even this attribute-centric model takes the view that the discriminatory power of the individual attributes plays a role in decision-making. The requirement that we should take account of the context in determining how to refer to something has thus been kept more or less centre-stage in computational work through the last 20 years or so. Meanwhile, work in psycholinguistics has explored the idea that quite orthogonal factors are at play in choosing the content of descriptions. Starting with the early work of Carroll (1980), a distinct strand of research has explored how a speakers form of reference to an entity is impacted by the way that entity has been previously referred to in the discourse or dialogue. The general idea behind what we will call the alignment approach is that a conversational participant will often adopt the same semantic, syntactic and lexical alternatives as the other party in a dialogue. This perspective is most strongly associated with the work of Pickering and Garrod (2004). With respect to reference in particular, speakers are said to form conceptual pacts in their use of language (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996). The implication of much of this work is that one speaker introduces an entity by means of some description, and then (perhaps after some negotiation) both conversational participants share this form of reference, or a form of reference derived from it, when they subsequently refer to that entity. Recent work by Goudbeek and Krahmer (2010) supports the view that subconscious alignment does indeed take place at the level of content selection for referring expressions: the participants in their study were more likely to use a dispreferred attribute to describe a target referent if this attribute had recently been used in a description by a confederate. One way of characterising these developments is that, on the one hand, the original very precise and somewhat rigid computational approaches to REG have been progressively weakened in the face of real human data; and on the other hand, work in a distinct discipline has offered a quite separate view of how reference works. Of course, these two broad approaches may not be incompatible. The truth may lie in-between, involving insights and ideas from both ways of thinking about the problem. In the present paper we aim to put one of the remaining fundamental tenets of the computational approaches to the test: does visual context really matter when we construct a referring expression? 3 Referring Expressions in the iMAP Corpus The iMAP Corpus (Louwerse et al., 2007) is a collection of 256 dialogues between 32 participant-pairs who contributed 8 dialogues each. Both participants had a map of the same environment, but one participants map showed a route winding its way between the landmarks on the map (see Figure 1 for examples). The task was for this participant, the instruction giver (IG), to describe the route in such a way that their partner, the instruction follower (IF), could draw it onto their map; this was complicated by some discrepancies between the two maps, such as missing landmarks, the unavailability of colour in some regions due to ink stains, and small differences between some landmarks. Note that the maps contain a relatively large number of objects compared to the visual stimuli used in other REG corpora. 45 (a) An example pair of IG and IF maps of the type bird+house. (b) An IG map of type fish+car. Figure 1: Three example maps. There are eight types of landmarks, grouped into pairs of one animate and one inanimate type each: alien+trafficsign, bird+house, fish+car, and bugs+trees. Each of these pairs defines a map type, which contains landmarks which are mostly of one of the two types of the pair. Half of the maps contain a few landmarks of types other than the main type; for example, a bird+house map contains mostly birds or houses, but might also contain a small number of other landmarks. The maps in Figure 1(a) are bird+house maps containing mainly birds with a few landmarks of other types mixed in, and the map in Figure 1(b) is an unmixed fishcar map for the IG, containing only fish landmarks. Note the high density of landmarks on the map in Figure 1(b) compared to those in Figure 1(a) (each cluster of same-coloured bugs on the bird maps counts as a single landmark). Overall there are 32 maps, which differ by the map type (four levels), the animatedness of the landmark types (two levels, e.g. fish vs. cars), the mixedness of the landmark types (two levels: only the main landmark type or also a few landmarks of different types), and the shape of the ink blots on the IFs map (two levels: one large blot or several smaller ones). Apart from their type, the landmarks differ in colour, and one other attribute, which is different for each type of landmark. For example, there are different kinds of birds and houses (eagle, ostrich, penguin, . . . ; church, castle, . . . ); fish and cars differ by their patterns (dotted, checkered, plain, . . . ), aliens and traffic signs have different shapes (circular, hexagonal, . . . ), and bugs and trees appear in small clusters of differing numbers. In addition to these three inherent attributes of the landmarks, participants used spatial relations to other items on the map. Each of the 34,403 referring expression in the corpus is annotated with the semantic values of the attributes that it contains. This collection of annotations forms the basic data we use in our experiments. We removed from the data all referring expressions that made reference to more than one landmark and thosein particular, pronounsthat did not contain any of the four main landmark attributes, type, colour, relation, or the landmarks other distinguishing attribute. However, all filtered expressions are taken into account in the computation of the features for the machine learner. The final data set contains 22,727 referring expressions, of which 6,369 are initial references and 16,358 are subsequent references. We can think of each referring expression as being realised from a content pattern: this is the collection of attributes that are used in that description. The attributes can be derived from the property-level annotation given in the corpus. So, for example, if a particular reference appears as the noun phrase the blue penguin, annotated semantically as blue, penguin, then the corresponding content pattern is colour, kind. Our aim is to replicate the content pattern of each referring expression in the corpus. Table 1 lists the 15 content patterns that occur in our data set in order of frequency. The high frequency of the other pattern is in part due to the annotation of the kind of birds and houses as other, which could also be argued to be a more fine-grained type attribute. We accepted this annotation as it was provided in the corpus, but we may alter it in future studies. 46 Content Pattern Count Proportion other 7561 33.27% other, type 5975 26.29% other, colour 2364 10.40% other, colour, type 1954 8.60% colour 1029 4.53% relation 796 3 50 % other, relation 738 3.25% type 662 2.91% colour, type 596 2.62% other, relation, type 463 2.04% relation, type 262 1.15% other, colour, relation 124 0.55% colour, relation 101 0.44% other, colour, relation, type 82 0.36% colour, relation, type 20 0.09% total 22,727 Table 1: The 15 different content patterns that occur in our data and their frequencies. 4 A Machine Learning Approach to Content Determination The number of factors that can be hypothesised as having an impact on the form of a referring expression in a dialogic setting associated with a visual domain is very large. Attempting to incorporate all of these factors into parameters for a rule-based system, and then experimenting with different settings for these parameters, is prohibitively complex. Instead, we here capture a wide range of factors as features that can be used by a machine learning algorithm to automatically induce from the data a classifier that predicts for a given set of feature values the attributes that should be used in a referring expression. The features we extracted from the data set are outlined in Tables 24. 1 They fall into a number of subsets. Map features capture design characteristics of the mappair the current dialogue is about; Speaker features capture the identity and role of the participants; and LMprop features capture the inherent visual properties of the target referent. The TradREG features allow the machine learner to capture factors that the traditional computational approaches to referring expression generation take account of. Of particular interest for our present considerations are the Visual TradREG features, which represent knowledge about the visual context. Alignment features capture factors that we would expect to play a role in the psycholinguistic models of alignment and conceptual pacts. When we refer to the complete feature set, we use the abbreviation allF. 1 In these tables, att is an abbreviatory variable that is instantiated once for each of the four attributes type, colour, relation, and the other distinguishing attribute of the landmark. The abbreviation LM stands for landmark. Map Features Main Map type most frequent type of LM on this map Main Map other other attribute if the most frequent type of LM Mixedness are other LM types present on this map? Ink Orderliness shape of the ink blot(s) on the IFs map LMprop Features other Att type of the other attribute of the target [att] Value value for each att of target [att] Difference was att of target different between the two maps? Missing was target missing one of the maps? Inked Out was target inked] out on the IGs map? Speaker Features Dyad ID ID of the pair of participant-pair Speaker ID ID of the person who uttered this RE Speaker Role was the speaker the IG or the IF? Table 2: The Map, LMProp and Speaker feature sets. Visual TradREG Features Count Vis Distractors number of visual distractors Prop Vis Same [att] proportion of visual distractors with same att Dist Closest distance to the closest visual distractor Closest Same [att] has the closest distractor the same att? Dist Closest Same [att] distance to the closest distractor of same att as target Cl Same type Same [att] has the closest distractor of the same type also the same att? Discourse TradREG Features Count Intervening LMs number of other LMs mentioned since the last mention of the target Prop Intervening [att] proportion of intervening LMs for which att was used AND which have the same att as target Table 3: The TradREG feature set. For our experiments, we use the Weka Toolkit (Witten and Frank, 2005) to learn one decision tree for each of the four attributes which decides whether or not to include that attribute. We then combine the attributes for which a positive decision was made into a content pattern that can be compared to the content pattern found in the corpus for the same instance. 2 In (Viethen et al., 2011) we showed that dropping the complete TradREG feature set from allF does not decrease the performance of this model on subsequent reference. The relevant numbers from that experiment are shown in italics in the first two lines of Table 5. One question this kind of work raises is: just what gets included in the visual context? Considering that most of the TradREG features depend on the visual context, it might be possible that the lack of impact of this feature set was due to the size of the visual context having been chosen incorrectly. A second consideration is that the TradREG features might have more of an impact on 2 We also tried an alternative approach of learning the whole content pattern at once with very similar results, which we do not report here due to space limitations. 47 Alignment Features  Recency Last Men Speaker Same who made the last mention of target? Last Mention [att] was att used in the last mention of target? Dist Last Mention Utts distance to the last mention of target in utterances Dist Last Mention REs distance to the last mention of target in REs Dist Last [att] LM Utts distance in utterances to last use of att for target Dist Last [att] LM REs distance in REs to last use of att for target Dist Last [att] Dial Utts distance in utterances to last use of att Dist Last [att] Dial REs distance in REs to last use of att Dist Last RE Utts distance to last RE in utterances Last RE [att] was att mentioned in the last RE? Alignment Features  Frequency Count [att] Dial how often has att been used in the dialogue? Count [att] LM how often has att been used for target? Quartile quartile of the dialogue the RE was uttered in Dial No number of dialogues already completed +1 Mention No number of previous mentions of target +1 Table 4: The Alignment feature set. initial reference than on the subsequent referring expressions that were at focus in our previous work. We explore these possibilities next. 5 The Effects of Variation in Visual Context In (Viethen et al., 2011), the size of the visual context was set for each map type in such a way that each landmark on any map of that type would have six distractors on average. We will refer to this way of setting the visual context size as average6. Because we are here particularly interested in the performance of the features that depend on the visual context (i.e., the Visual TradREG features), we performed two more ablation steps, in which we separately excluded only the Visual TradREG features and the Discourse TradREG features for both subsequent and initial references. Table 5 confirms that, using the average6 method to determine the visual context, the Visual TradREG features have no significant effect for either subsequent or initial referring expressions on the Accuracy with which the model replicates the referring expressions in our corpus. Perhaps surprisingly, this is true not only for subsequent reference, but also for initial reference, where one might expect that distinguishing from the visual context would be of more importance. Considering the difference in density and uniformity of landmarks on the different types of maps (compare Figure 1(a) with 42 diversely shaped landmarks in the IG map to Figure 1(b) with 59 uniformly shaped landmarks), we wondered whether the average6 method of setting the visual context might be too inflexible. For exall initial subseq. allF 61.5% 68.6% 58.8% allF  TradREG 61.3% 69.4% 58.2% allF  Discourse TradREG 61.3% 68.6% 58.4% allF  Visual TradREG 61.6% 69.4% 58.5% no of REs 22727 6369 16358 Table 5: Ablation of Discourse and Visual TradREG features using average6 to determine the visual context. Performance is measured in percentage of perfect matches. Numbers in italics were prevously reported in (Viethen et al., 2011). ample, one might hypothesise that fewer surrounding objects might get taken into account in describing the blue penguin marked by a circle in the left map in Figure 1(a) than in describing the purple fish marked by a circle in Figure 1(b). We therefore split our data into four sets according to the four different map types and tried out a range of different visual context sizes for each type separately. Two different ways of determining the visual context might be at play. One possibility is that people might indeed be taking into account (roughly) the same number of surrounding objects for each landmark, while this number might be different for different map types due to their different landmark densities. We call this the count method of determining the visual context. Alternatively, one might draw an imaginary circle around each landmark, and consider all objects whose centres fall within the radius of this circle to be distractors. We call this the distance method of determining the visual context. In order to explore whether there is one correct size of visual context for each map type, we tried all distances from 0 to 675 pixels in 15 pixel steps (each map is 488 675 pixels) and all possible distractor counts from 0 to 61 (the maximum number of landmarks on the most dense map pair is 61). If the bad performance of the Visual TradREG features so far was indeed due to the visual context being too inflexible or set incorrectly, we would expect to find at least one visual context size for each map type that outperforms all others. There should also be a peak of performance around that size, with the performance falling if the size grows or shrinks from the ideal size (if the visual context is set too small, we might expect to see references containing too many attributes; if the visual context is set too large, we might expect to see references with too few attributes). We trained the decision trees on 80% of the data for each map type and tested on the remaining 20%. The trainingtest splits were stratified for the content patterns of the referring expressions, the Speaker IDs of the participants who produced the expressions, and the Quartiles of the dialogue in which the references occurred. Table 6 48 map type train test total alien+sign 4,425 967 5,392 fish+car 4,021 813 4,834 bird+house 5,492 1,264 6,756 tree+bug 4,703 1,042 5,745 total 18,641 4,086 22,727 Table 6: Sizes of the training and test sets for the different map types. best all best initial best subseq. maptype sizes REs sizes REs sizes REs alien+sign 43 63.5% 5 68.3% 43 62.5% fish+car 44, 46 59.2% 43 60.6% 13 59.0% house+ 3, 22 72.6% 22 75.6% 13, 71.8% bird 19, 28 trees+ 3 70.5% 0, 1, 3, 74.8% 33 68.4% bugs 11, 12 weighted 67.1% 71.1% 65.9% average all maps 61.5% 68.6% 58.8% average-6 Table 7: Maximum possible Accuracy using all features achieved by choosing the best performing visual context by the count method for each map type, compared to the performance of the average-6 visual contexts. shows the sizes of the four different trainingtest splits. Table 7 shows that if we choose the best performing count of distractors for each map type, the overall performance (weighted average over all map types) does indeed improve over the old average-6 method of choosing the visual context. Table 8 shows the same results for the distance method of determining the visual context. (For both methods p 0.01, using the  2 statistic with df = 1 for all, initial, and subsequent references.) However, Figures 2 to 5 demonstrate that there is no consistent effect of the size of the visual context on the performance of our model using the number method of setting visual context sizes. None of the graphs show a clear performance peak around one particular visual context; instead, performance oscillates in a fairly narrow percentage band both when using all features and when using only the Visual TradREG features that are directly impacted by the visual context. For most map types it becomes clear that even a model using only the features that are not affected by the visual context (the flat lines labelled noVisualTrad) outperforms allF with many of the settings for visual context size. This means that, unless we are certain that we are using the best performing setting for visual context, using the Visual TradREG features is risky, as choosing the wrong visual context can easily lead to a worse match with human behaviour. best all best initial best subsequ. maptype sizes REs sizes REs sizes REs alien+sign 90, 105 59.5% 90 65.1% 240, 285 57.9% fish+car 75 57.3% 75, 180 62.4% 75 55.9% house+ 150 73.3% 300, 74.8% 480 73.4% bird 540-675 trees+ 210 70.4% 585, 76.6% 210, 67.2% bugs 660, 675 420, 525 weighted 65.9% 70.9% 64.3% average all maps 61.5% 68.6% 58.8% average-6 Table 8: Maximum possible Accuracy using all features achieved by choosing the best performing visual context by the distance method for each map type, compared to the performance of the average-6 visual contexts. For space reasons we do not show all four graphs for the distance method. However, Figure 6 shows the performance for all map types when using all feature sets. Again, the performance oscillates as the size of the visual context varies, rather than showing a real peak around an ideal context size. Although the performance of the overall system can be increased over the old average-6 method by setting the visual context to a map type-specific optimum, these results show that this increase is somewhat a matter of luck. Short of trying out (almost) all possible sizes of the visual context, as we did here, there is no systematic way in which to determine the size of the visual context that gives the best performance; and by using features dependent on the visual context one might just as likely hit on a visual context that decreases performance. The oscillations in the graphs in Figures 2 to 6 indicate that it is unlikely that people are taking the visual content into account in the way that our model suggests. 6 Discussion In this paper we have put forward what might be considered a rather heretical position: that during the construction of a referring expression, contrary to what is assumed by much work in the field, a speaker does not seem to take account of the visual context of reference. Using a collection of human-produced referring expressions of landmarks on moderately complex maps, we have shown that there is no principled way in which to determine a visual context that might make a significant difference to the ability of a machine-learned algorithm to replicate the human data. The implication of this would seem to be that humans generate referring expressions with little regard for the visual context, or at least that the role of visual context is masked by other factors (such as alignment) that play a bigger role. So, we might conclude that 49 \t\n \n \n \n \n \t\n \n \n \n \n \t\n \n \n \n \n \t\n \r \t\n \r \r \r \r \r\t\n \r\t\n Figure 2: Accuracy for different visual contexts (determined by the count method) for the alien+sign maps. Figure 3: Accuracy for different visual contexts (determined by the count method) for the fish+car maps . the view that reference is about deliberately constructing distinguishing descriptions should be considered suspect. It could be argued that this is a somewhat plausible position if we look only at subsequent reference as we did in (Viethen et al., 2011): once an entity has been introduced into the discourse, perhaps how it is referred to subsequently depends more on the preceding discourse than it does on the visual context at the time of reference. Indeed, once an entity has been referred to, the description that has been constructed factors in the visual context, and so any subsequent reference to that entity does not require re-computation of the description; referring to the entity in the way that it was referred to before should still do the job (unless, of course, the context has changed in some relevant way). Such a model has the twin appeals of being both more computationally efficient, and consistent Figure 4: Accuracy for different visual contexts (determined by the count method) for the bird+house maps. Figure 5: Accuracy for different visual contexts (determined by the count method) for the bugs+trees maps. with explanations based on the alignment approach. But surely, we would want to say, context must still be taken account of when constructing an initial reference; and if the context is a visual one, then that first reference constructed needs to distinguish the intended referent from the other entities in the scene. Surprisingly, even here, our experimental results support the view that visual context doesnt matter. So whats going on? Intuition suggests that, in real world scenes, we do take account of the distinguishing ability of our referring expressions; when we describe an intended referent, we do not do so blindly without considering whether the referring expression might be confusing or ambiguous. But our data suggests, at least in the scenarios we have looked at, that this is not the case. One possible explanation is that neither of the two ways of determining the visual context that we tried out in our experiments accurately models the visual context that the speakers in our corpus take into account. Firstly, while acknowledging that there are differences between the different types of maps that might influence the number of distractors to be taken into account, we still kept 50 \t \n \t \t \t \t \n \t \t \n \n \n\t \n \r\r \t \t\n \t \r\t \r\t \r\t\n \r\t \r\t\n \t \t\n \t \r\t Figure 6: Accuracy for different visual contexts determined by the distance method for all map types. the size of the visual context constant for all landmarks on a given map. It is conceivable that this is still too simplistic an assumption and that distractor numbers have to be determined on a landmark-by-landmark basis instead. For instance, it is likely that, at least for the IG, the course of the path influences the shape of the visual context, with objects along the path being more likely to be taken into account than those further away. This is a consideration that was taken into account to some extent by Guhe (2007; 2009). Similarly, what counts as the visual context is probably influenced by the linguistic context as well. For example, in uttering as well as resolving an instruction such as go left until you get to the red alien, the red alien has to be distinguished mostly from objects to its right and not so much from anything that lies beyond it to its left. To explore these kinds of hypotheses, a lot more preparatory work would be necessary. The dialogues would need to be annotated with information about the point on the path that the IG and IF have reached, and with possibly relevant information in the dialogue context. However, to obtain a more definite answer to the question of which landmarks are taken into account when people refer in dialogue, we will ultimately have to look beyond the text of the dialogue transcriptions. With technologies such as eye-tracking it might be possible to reveal which other landmarks speakers look at while or before they construct a referring expression. Another possible explanation for the surprising outcome of our experiment is that our scenarios are too simple: they do not reflect the complexity of real-world visual scenes, and so the complex mechanisms we think are required for REG more generally are simply not required in these simple scenes. Rather than compute a reference that takes account of the context, the subjects in the iMAP Task perhaps recognise that the scenes are simple enough to use referring expressions that are not carefully computed on the basis of context. But this then raises a methodological issue. An assumption implicit in much recent work on evaluation in REG is that, by initially using simplistic domains and tasks, the in-principle capabilities of algorithms can be tested before scaling up to more complex real-world settings. The visual scenarios that are represented by the TUNA Corpus, the Drawer Corpus, and the GRE3D3 and GRE3D7 Corpora are very abstract and arguably quite unlike any real-world scenes where a speaker needs to construct a reference. For the work presented here, we attempted to consider more realistic scenes involving speakers discussing larger numbers of objects in a distinct task; but even here, the scenario is still very simple with much fewer attributes to choose from than speaker are usually presented with when referring in the wild. But if this is the case, then what do we learn by developing algorithms that work in these simple scenarios? We do not believe that the idea that human speakers deliberately build distinguishing descriptions in order to uniquely identify their intended referents should be abandoned: this seems to us a fundamentally important aspect of successful referential behaviour. But if we want to understand how it is that people do this, we should be wary of thinking we can learn about these processes by looking at how people refer in vastly simplified models of the real world. To move forward, we need to focus on the complexity of real-world reference scenarios. 7 Conclusions Traditional REG algorithms are based on the aim of distinguishing the target referent from the other objects in its context. However, using a corpus of maptask dialogues, we found in earlier work that using features based on the same considerations as those underlying the traditional REG algorithms does not help in machine learning which attributes people use in a given situation. In this paper, we used two different methods of varying the size of the visual context that gets taken into account in computing the values for these features. We found that it is not possible to systematically determine an ideal context size using these methods, which seems to point to the conclusion that, for the speakers in our corpus, visual context was not an important consideration. Alternatively, even more fine-grained methods of determining the visual context than those we tried might be necessary, or the scenarios on the maps underlying our corpus are too simplistic to elicit real-world behaviour from the speakers. This points to the conclusion that it might be time for the field to move on to more complex visual scenes when researching content selection mechanisms for referring expression generation. 51 "
W11-2807 "Abstract This paper presents a cross-linguistic data elicitation study on fully realised referring expressions (REs) in a dialogue context. A webbased experiment was set up in which participants were asked to choose REs to be uttered by one of two agents for identifying five targets in a scripted dialogue. Participants were told that the agent would point at the referents while uttering their chosen linguistic descriptions. The study was conducted in English, Japanese, Portuguese and Dutch and yielded a total of 1190 referring expressions. Our hypotheses concern sets of objects that need to be considered for identification depending on the effect of the pointing gesture. Results show interesting and significant differences between the language groups. "
W11-2808 " This paper offers a solution to a small problem within a much larger problem. We focus on modelling how people use size in reference, words like big and tall, which is one piece within the much larger problem of how people refer to visible objects. Examining size in isolation allows us to begin untangling a few of the complex and interacting features that affect reference, and we isolate a set of features that may be used in a hand-coded algorithm or a machine learning approach to generate one of six basic size types. The hand-coded algorithm generates a modifier type with a high correspondence to those observed in human data, and achieves 81.3% accuracy in an entirely new domain. This trails oracle accuracy for this task by just 8%. Features used by the hand-coded algorithm are added to a larger set of features in the machine learning approach, and we do not find a statistically significant difference between the precision and recall of the two systems. The input and output of these systems are a novel characterization of the factors that affect referring expression generation, and the methods described here may serve as one building block in future work connecting vision to language. 1 Introduct  "
W11-2809 "<NoAbstract>"
W11-2810 " We present an approach to content selection that works on an ontology-based knowledge base developed independently from the task at hand, i.e., Natural Language Generation. Prior to content selection, a stage akin to signal analysis and data assessment used in the generation from numerical data is performed for identifying and abstracting patterns and trends, and identifying relations between individuals. This new information is modeled as an extended ontology on top of the domain ontology which is populated via inference rules. Content selection leverages the ontology-based description of the domain and is performed throughout the text planning at increasing levels of granularity. It includes a main topic selection phase that takes into account a simple user model, a set of heuristics, and semantic relations that link individuals of the KB. The heuristics are based on weights determined empirically by supervised learning on a corpus of summaries aligned with data. The generated texts are short football match summaries that take into account the user perspective. 1 Introduct  "
W11-2811 "Abstract This paper investigates to what extent rhetorical relations can be assigned purely on the basis of propositional content, without any reference to speaker goals or other pragmatic information. This task confronts any NLG system designed to generate coherent text from a set of formally represented statements; we consider it here in the context of an ontology verbaliser, for which the input is a set of axioms encoded in the web ontology language OWL. A simple set-theoretical model of the possible semantic relationships between two statements is proposed; this model allows 46 logically consistent relationships, of which we hypothesise that 11 are rhetorically coherent. This hypothesis is tested through an empirical survey which also provides evidence on how the coherent patterns are expressed linguistically. "
W11-2812 "<NoAbstract>"
W11-2813 "ed Kingdom {v.rieser,s.keizer,x.liu,o.lemon}@hw.ac.uk Abstract We present evaluation results with human subjects for a novel data-driven approach to Natural Language Generation in spoken dialogue systems. We evaluate a trained Information Presentation (IP) strategy in a deployed tourist-information spoken dialogue system. The IP problem is formulated as statistical decision making under uncertainty using Reinforcement Learning, where both content planning and attribute selection are jointly optimised based on data collected in a Wizard-ofOz study. After earlier work testing and training this model in simulation, we now present results from an extensive online user study, involving 131 users and more than 800 test dialogues, which explores its contribution to overall global task success. We find that the trained Information Presentation strategy significantly improves dialogue task completion, with up to a 9.7% increase (30% relative) compared to the deployed dialogue system which uses conventional, hand-coded presentation prompts. We also present subjective evaluation results and discuss the implications of these results for future work in dialogue management and NLG. "
W11-2814 " Language generators in situated domains face a number of content selection, utterance planning and surface realisation decisions, which can be strictly interdependent. We therefore propose to optimise these processes in a joint fashion using Hierarchical Reinforcement Learning. To this end, we induce a reward function for content selection and utterance planning from data using the PARADISE framework, and suggest a novel method for inducing a reward function for surface realisation from corpora. It is based on generation spaces represented as Bayesian Networks. Results in terms of task success and humanlikeness suggest that our unified approach performs better than a baseline optimised in isolation or a greedy or random baseline. It receives human ratings close to human authors. "
W11-2815 "rmany {garoufi, akoller}@uni-potsdam.de Abstract We present an approach to the generation of referring expressions (REs) which computes the unique RE that it predicts to be fastest for the hearer to resolve. The system operates by learning a maximum entropy model for referential success from a corpus and using the models weights as costs in a metric planning problem. Our system outperforms the baselines both on predicted RE success and on similarity to human-produced successful REs. A task-based evaluation in the context of the GIVE-2.5 Challenge on Generating Instructions in Virtual Environments verifies the higher RE success scores of the system. "
W11-2816 "<NoAbstract>"
W11-2817 "Abstract This paper describes SimpleNLG for German, a surface realisation engine for German based on SimpleNLG (Gatt and Reiter, 2009). Several features of the syntax of German and their implementation within the current framework are discussed, with a special focus on word order phenomena. Grammatical coverage of the system is demonstrated by means of selected examples. "
W11-2818 "Abstract This paper introduces EasyText, a fully operational NLG system. This application processes numerical data (in tables) in order to generate specific analytical commentaries of these tables. We start by describing the context of this particular NLG application (communicative goal, user profiles, etc.). We then shortly present the theoretical background which underlies EasyText, before describing its implementation, realization and evaluation. "
W11-2819 "Abstract We argue that Discourse Representation Structures form a suitable level of languageneutral meaning representation for micro planning and surface realisation. DRSs can be viewed as the output of macro planning, and form the rough plan and structure for generating a text. We present the first ideas of building a large DRS corpus that enables the development of broad-coverage, robust text generators. A DRS-based generator imposes various challenges on micro-planning and surface realisation, including generating referring expressions, lexicalisation and aggregation. "
W11-2820 "24 5UA, UK {t.bouttaz, e.pignotti, c.mellish, p.edwards}@abdn.ac.uk Abstract This paper presents a method for tailoring Natural Language Generation according to context in a web-based Virtual Research Environment. We discuss a policy-driven framework for capturing user, project and organisation preferences and describe how it can be used to control the generation of textual descriptions of RDF resources. "
W11-2821 "Abstract The SWAT TOOLS ontology verbaliser generates a hierarchically organised hypertext designed for easy comprehension and navigation. The document structure, inspired by encyclopedias and glossaries, is organised at a number of levels. At the top level, a heading is generated for every concept in the ontology; at the next level, each entry is subdivided into logically-based headings like Definition and Examples; at the next, sentences are aggregated when they have parts in common; at the lowest level, phrases are hyperlinked to concept headings. One consequence of this organisation is that some statements are repeated because they are relevant to more than one entry; this means that the text is longer than one in which statements are simply listed. This trade-off between organisation and brevity is investigated in a user study. "
W11-2822 " This paper describes preliminary analysis on the influence of the semantic roles in summary generation. The proposed method involves three steps: first, the named entities in the original text are identified using a named entity recognizer; secondly, the sentences are parsed and semantic roles are extracted; thirdly, selection of the sentences containing specific semantic roles for the most relevant entities in text. Although the method is language independent, in order to check its viability, we tested the proposed approach for Romanian summaries.  "
W11-2823 "Abstract This paper presents an ongoing work about the implementation of a CCG grammar for Italian Sign Language. This grammar is part of a generation system used for Italian-LIS translation. 1 Introduction Italian Sign Language (Lingua Italiana dei Segni, henceforth LIS) is the sign language used by the Italian deaf (signing) community. LIS is a natural language that has a specific lexicon, morphology and syntax (Volterra, 2004). In the last years the computational linguistic community showed a growing interest toward sign languages (SLs), and a number of projects concerning the translation into a SL have recently started. Some of these projects adopt statistical techniques based on developing parallel corpora: English to Irish SL (Morrissey et al., 2007), Chinese to Chinese SL (Su and Wu, 2009). Some other projects adopt symbolic techniques: English to British SL (Bangham et al., 2000), English to American SL (Zhao et al., 2000; Huenerfauth, 2006). Recently a new project started for automatic translation from Italian to LIS: in this paper we present some features of the generation module adopted for the interlingua translation in this project. The challenge of Italian-LIS translation depends on the complexity of the translation task as well as on the peculiar features of the LIS. Sign languages mix standard linguistics of vocal languages with a number of typical phenomena. Among others: there is a spatial organization of the sentence that interacts with the word order to determine syntactic/semantic dependencies and plays a role in the coordination; the presence of many articulators (two hands, eyebrow, eye gaze, torso etc.) allows for some form of parallelism; there are no prepositions, articles; finally, LIS is a poorly studied language and linguists often do not agree on basic linguistic properties (e.g. sentence word order). In order to reduce the difficulties of our ambitious project we concentrate on a specific application domain, i.e. weather forecasts. As starting point, the project is producing a parallel corpus of Italian-LIS sentence extracted from TV news and concerning weather forecasts. Our interlingua 1 translation system has four distinct modules, that are: (1) a dependency parser for Italian; (2) an ontology based semantic interpreter; (3) a grammar based generator; (4) a virtual actor that performs the synthesis of the final LIS sentence. Here we give some details about the parser and the semantic interpreter, in the Section 2 we describe the generator. In the first step, the syntactic structure of the source language is produced by the TUP parser (Lesmo, 2007). It uses a morphological dictionary of Italian (about 25, 000 lemmata) and a rule-based grammar. The final result is a dependency tree, that makes clear the structural syntactic relationships occurring between the words of the sentence (Hudson, 1984). Each word in the source sentence is associated with a node of the tree, and the nodes are linked via labeled arcs that specify the 1 Our system can be defines as a knowledge based restricted interlingua, since it uses extra-linguistic information and deals with just two languages (Hutchins and Somer, 1992) 170 Figure 1: The syntactic structure of the sentence Le temperature superano la media in Puglia e in Sicilia (The temperature exceeds the average in Puglia and Sicilia). syntactic role of the dependents with respect to their head (the parent node). Consider the dependency tree n Fig. 1: temperatura (temperature) is the subject of the verb superare (exceed), while media (average) is the object; the coordinated words Puglia and Sicilia are modifiers of the verb. The second step of the translation is the semantic interpretation: the syntax-semantics interface used in the interpretation is based on ontologies (Lesmo et al., 2011a; Nirenburg and Raskin, 2004). The knowledge in the ontology concerns the application domain, i.e. weather forecasts, as well as more general common knowledge about the world. Starting from the lexical semantics of the words in the sentence and on the basis of the dependency structure, a recursive function searches in the ontology providing a number of connection paths that represent the meaning of the sentence. Indeed, the final sentence meaning consists of a complex fragment of the ontology: semantic roles and other kind of semantic relations are contained in this fragment and could be extracted by translating it into First Order Logic (FoL) predicates. However, similar to other approaches (among others (Bunt et al., 2007)), our ontological meaning representation is unscoped. In Fig. 2 we report the semantic interpretation of the sentence Le temperature superano la media in Puglia e in Sicilia in terms of FoL predicates. The predicate onto expresses the lexical meaning of the words by using the ontology concepts: it assigns the concept instances exceed, temperature, average, Puglia, Sicilia to the FoL variables l1, l2, l3, l4, l5 respectively. Moreover, onto explicitly denotes the classes which these instances belong to: meteostatus is the ontological class of the events regard! ! !\" # $!\"#\"$%'#(&)!\"#!!$ !\" % $\"*'%\"+#,#-&%!&'!()%*(! !\" #& '()*+,-&\" % !\" . $\"*'%\"+#,#-+),!()-! !\" #& ',/*0*-&\" . !\" 1 $.\"$%'/\"'&.*-/0) !\" #& '\"23(,42+-&\" 1 !\" 5 $.\"$%'/\"'&10#0/0) !\" 1 &'6*,-&\" 5 2+,27\" # $!\"#\"$%'#(&0!\"#!!$8 *9*+,7* : ;\" # 8 2+,27\" % $\"*'%\"+#,#-;%!&'!()%*(!8 ()*+,7\" # ;\" % 8 2+,27\" . $\"*'%\"+#,#-;),!()-!8 ,/*0*7\" # ;\" . 8 2+,27\" 1 2.\"$%'/\"'0.*-/0)8 \"23(,42+7\" # ;\" 1 8 2+,27\" 5 2.\"$%'/\"'010#0/0)8 6*,7\" 1 ;\" 5 8 Figure 2: The semantic interpretation of the sentence Le temperature superano la media in Puglia e in Sicilia given in terms of FoL predicates. ing the meteo; geo-area is the ontological class of the geographical areas; eva-entity is the ontological class of the evaluable entities. The predicates event, agent, theme, location express the semantics of the event in terms of predicate-arguments by using semantic roles (we adopt the set of semantic roles defined in the LIRICS project (Petukhova and Bunt, 2008)). Finally, the predicate set expresses a semantic relation that groups entities: this predicate allows to specify the cumulative reading, w.r.t. the distributive reading corresponding to have two not related locations. 2 A generator for LIS Natural language generation can be described as a three steps process: text planning, sentence planning and realization (Reiterand and Dale, 2000). Text planning determines which messages to communicate and how to rhetorically structure these messages; sentence planning converts the text plan into a number of sentence plans; realization converts the sentence plans into the final sentences produced. Anyway, in the context of interlingua translation we simplify by assuming that generation needs only for the realization step. Our working hypothesis is that source and target sentences have as much as possible the same text and the same sentence plans. This hypothesis is reasonable in our projects since we are working on a very peculiar sub-language (weather forecasts) where the rhetorical structure is usually very simple. In our architecture we use the OpenCCG realizer (White, 2006), an open source tool that has several appealing features with respect to our approach. OpenCCG is based on combinatory categorial grammars (CCG) (Steedman, 2000), a mildly context171 sensitive formalism that is theoretically adequate to describe the complexity of natural language syntax (e.g. cross-serial dependencies, non-constituency coordination) and it has a very straight syntaxsemantic interface. Moreover, OpenCCG adheres to the bidirectional grammar approach, i.e. there is one grammar for both realisation and parsing. It means that derivation and generation have the same structure and that we can develop a grammar by testing its correctness in realization in terms of parsing: as a result, we obtain a speed-up in the process of grammar development (White et al., 2010). Realization usually accounts for a standard number of morphosyntactic phenomena, that are inflection, agreement, word order, function words. LIS has few function words but, similar to all SLs, it has a peculiar and rich system of inflection and agreement. OpenCCG allows to encode an inflectional system by using feature structures, which are part of the syntactic categories. The integration in one single elementary structure of the morphology-syntax-semantic information is appealing for sign languages where the absence of function words increases the importance of morpho-syntactic features to express the correct meaning of the sentence. Now we first give some specifications about the input/output of the generator (Section 2.1) and secondly we describe the treatment of some linguistic constructions by using a fragment of the CCG for LIS (Section 2.2). "
W11-2824 "Abstract We present a study that investigates that factors that determine what makes a good lexical substitution. We begin by observing that there is a correlation between the corpus frequency of words and the number of WordNet senses they have, and hypothesise that readers might prefer common, but more ambiguous words over less ambiguous but also less common ones. We identify four properties of a word that determine whether it is a suitable substitution in a given context, and ask volunteers to rank their preferences between two common but ambiguous lexical substitutions, and two uncommon but also unambiguous ones. Preliminary results suggest a slight preference towards the unambiguous. "
W11-2825 "<NoAbstract>"
W11-2826 " This paper addresses the task of using natural language generation (NLG) techniques to generate sentences with formal and with informal style. We studied the main characteristics of each style, which helped us to choose parameters that can produce sentences in one of the two styles. We collected some ready-made parallel list of formal and informal words and phrases, from different sources. In addition, we added two more parallel lists: one that contains most of the contractions in English (short forms) and their full forms, and another one that consists in some common abbreviations and their full forms. These parallel lists might help to generate sentences in the preferred style, by changing words or expressions for that style. Our NLG system is built on top of the SimpleNLG package (Gatt and Reiter, 2009). We used templates from which we generated valid English texts with formal or informal style. In order to evaluate the quality of the generated sentences and their level of formality, we used human judges. The evaluation results show that our system can generate formal and informal style successfully, with high accuracy. The main contribution of our work consists in designing a set of parameters that led to good results for the task of generating texts with different formality levels.  "
W11-2827 "Abstract This paper shows how glue rules can be used to increase the robustness of statistical chart realization in a manner inspired by dependency realization. Unlike the use of glue rules in MTbut like previous work with XLE on improving robustness with hand-crafted grammarsthey are invoked here as a fallback option when no grammatically complete realization can be found. The method works with Combinatory Categorial Grammar (CCG) and has been implemented in OpenCCG. As the techniques are not overly tied to CCG, they are expected to be applicable to other grammar-based chart realizers where robustness is a common problem. Unlike an earlier robustness technique of greedily assembling fragments, glue rules enable nbest outputs and are compatible with disjunctive inputs. Experimental results indicate that glue rules yield improved realizations in comparison to greedy fragment assembly, though a sizeable gap remains between the quality of grammatically complete realizations and fragmentary ones. "
W11-2828 "Abstract Hand-crafted approaches to content determination are expensive to port to new domains. Machine-learned approaches, on the other hand, tend to be limited to relatively simple selection of items from data sets. We observe that in time series domains, textual descriptions often aggregate a series of events into a compact description. We present a simple technique for automatically determining sequences of events that are worth reporting, and evaluate its effectiveness. "
W11-2829 "<NoAbstract>"
W11-2830 "www.macs.hw.ac.uk/InteractionLab Abstract We propose a shared task based around generation of instructions for pedestrian users navigating in open-world virtual environments. An important variant of this task involves handling uncertainty about the users location (as would happen in the real world with a standard GPS system). We motivate and explain the task, propose metrics for evaluation of systems, describe the planned software setup, and propose a timeline for the challenge. "
W11-2831 "Abstract We propose a competitive shared evaluation task for Surface Realization in Spanish. The task would be carried out in 2012. It would involve the generation of text in Spanish from a common ground input shared by all systems. Separate corpora for training/development (composed of pairs of common ground input and expected string result) and testing (only common ground input) will be provided. Automatic evaluation procedures will be provided. Submitted results will also be subject to human evaluation. The present proposal is tentative in two different ways. First, the authors intend to revise the proposal in view of the experience and feedback of the Surface Realization Pilot Task currently in process for English, once its results are made public (due in September, 2011). Second, the authors are willing to colaborate both with organizers of equivalent tasks for other languages or more researchers interested in surface realization for Spanish. "
W11-2832 "Abstract The Surface Realisation (SR) Task was a new task at Generation Challenges 2011, and had two tracks: (1) Shallow: mapping from shallow input representations to realisations; and (2) Deep: mapping from deep input representations to realisations. Five teams submitted six systems in total, and we additionally evaluated human toplines. Systems were evaluated automatically using a range of intrinsic metrics. In addition, systems were assessed by human judges in terms of Clarity, Readability and Meaning Similarity. This report presents the evaluation results, along with descriptions of the SR Task Tracks and evaluation methods. For descriptions of the participating systems, see the separate system reports in this volume, immediately following this results report. "
W11-2833 " In this paper we describe our system and experimental results on the development set of the Surface Realisation Shared Task. DCU submitted 1-best outputs for the Shallow subtask of the shared task, using a surface realisation technique based on dependency-based n-gram models. The surface realiser achieved BLEU and NIST scores of 0.8615 and 13.6841 respectively on the SR development set. 1 Int  "
W11-2834 "<NoAbstract>"
W11-2835 "<NoAbstract>"
W11-2836 "rsity {raja,espinosa,mwhite}@ling.osu.edu Abstract This report documents our efforts to develop a Generation Challenges 2011 surface realization system by converting the shared task deep inputs to ones compatible with OpenCCG. Although difficulties in conversion led us to employ machine learning for relation mapping and to introduce several robustness measures into OpenCCGs grammar-based chart realizer, the percentage of grammatically complete realizations still remained well below results using native OpenCCG inputs on the development set, with a corresponding drop in output quality. We discuss known conversion issues and possible ways to improve performance on shared task inputs. "
W11-2837 "<NoAbstract>"
W11-2838 "Abstract The aim of the Helping Our Own (HOO) Shared Task is to promote the development of automated tools and techniques that can assist authors in the writing task, with a specific focus on writing within the natural language processing community. This paper reports on the results of a pilot run of the shared task, in which six teams participated. We describe the nature of the task and the data used, report on the results achieved, and discuss some of the things we learned that will guide future versions of the task. "
W11-2839 " This paper reports about our work in the HOO shared task 2011. The task is to automatically correct the English of a given document. For that, we have developed a hybrid system of a statistical CRF based model along with a rule-based technique has been used. The system has been trained on the H OO shared task training datasets and run on the test set given by the organizer of HOO. We have submitted one run, which has been demonstrated F-score of 0.204, 0.178 and 0.167 for detection, recognition and correction respectively. "
W11-2840 "Abstract This article describes the experiments we performed during our participation in the HOO Challenge. We present the adaption we made on two systems, mainly designing new grammatical rules and completing a lexicon. We focused our work on some of the most common errors in the corpus: missing punctuation and inaccurate prepositions. Our best experiment achieved a 0.1097 detection score, a 0.0820 recognition score, and a 0.0557 correction score on the test corpus. "
W11-2841 "ingapore {danielhe,nght,thanhphu}@comp.nus.edu.sg Abstract This paper describes the submission of the National University of Singapore (NUS) to the Helping Our Own (HOO) Pilot Shared Task. Our system targets spelling, article, and preposition errors in a sequential processing pipeline. 1 Introduction Helping Our Own (HOO) (Dale and Kilgarriff, 2010) is a new shared task for automatic grammatical error correction, a task which has attracted increasing attention recently. Instead of correcting errors in a general domain, e.g., essays written by second language learners of English, HOO focuses on papers written by non-native authors of English within the natural language processing community. In this paper, we describe the participating system from the National University of Singapore (NUS). The system targets spelling, article, and preposition errors. The core of our system is built on linear classification models and a large language model filter. We present experimental results on the HOO development and test data. The next section describes the system in more detail. Section 3 describes the data sets used. Section 4 reports experimental results on the HOO development and test data. "
W11-2842 "Abstract This paper describes the UKP Lab system participating in the Helping Our Own Challenge 2011. We focus on the correction of realword spelling errors (RWSEs) that are especially hard to detect. Our highly flexible system architecture is based on UIMA (Ferrucci and Lally, 2004) and integrates state-of-the-art approaches for detecting RWSEs. "
W11-2843 "61801 {rozovska,mssammon,gioja,danr}@illinois.edu Abstract In this paper, we describe the University of Illinois system that participated in Helping Our Own (HOO), a shared task in text correction. We target several common errors, such as articles, prepositions, word choice, and punctuation errors, and we describe the approaches taken to address each error type. Our system is based on a combination of classifiers, combined with adaptation techniques for article and preposition detection. We ranked first in all three evaluation metrics (Detection, Recognition and Correction) among six participating teams. We also present type-based scores on preposition and article error correction and demonstrate that our approach achieves best performance in each task. "
W11-2844 "<NoAbstract>"
W11-2845 "Abstract GIVE-2.5 evaluates eight natural language generation (NLG) systems that guide human users through solving a task in a virtual environment. The data is collected via the Internet, and to date, 536 interactions of subjects with one of the NLG systems have been recorded. The systems are compared using both task performance measures and subjective ratings by human users. "
W11-2846 " These notes describe a contribution to the 2011 GIVE Challenge from the University of Aberdeen. Our contribution focuses on an attempt to increase the extent to which participants felt engaged in the direction giving/following game on which the GIVE challenge focuses.  "
W11-2847 " This paper presents the Bremen system for the GIVE-2.5 challenge. It is based on decision trees learnt from new annotations of the GIVE corpus augmented with manually specified rules. Surface realisation is based on context-free grammars. The paper will address advantages and shortcomings of the approach and discuss how the present system can serve as a baseline for a future evaluation with an improved version using hierarchical reinforcement learning with graphical models. 1 Introdu  "
W11-2848 "Universidad Nacional de C  ordoba Facultad de Matem  atica, Astronom  a y F  sica C  ordoba, Argentina {david.racca, luciana.benotti, pablo.duboue}@gmail.com Abstract In this paper we describe the C generation system from the Universidad Nacional de C  ordoba (Argentina) as embodied during the 2011 GIVE 2.5 challenge. The C system has two distinguishing characteristics. First, its navigation and referring strategies are based on the area visible to the player, making the system independent of GIVEs internal representation of areas (such as rooms). As a result, the system portability to other virtual environments is enhanced. Second, the system adapts classical grounding models to the task of instruction giving in virtual worlds. The simple grounding processes implemented (for referents, game concepts and game progress) seem to have an impact on the evaluation results. "
W11-2849 "Abstract The CL system uses an algorithm that, given a task-based corpus situated in a virtual world, which contains human instructors speech acts and the users responses as physical actions, generates a virtual instructor that helps a user achieve a given task in the virtual world. In this report, we explain how this algorithm can be used for generating a virtual instructor for a game-like, task-oriented virtual world such as GIVEs. "
W11-2850 "Abstract This paper presents the instruction generation system L submitted by the LORIA and TALARIS team to the GIVE challenge 2011 (GIVE 2.5). The system L takes the same approach to instruction generation than its predecessor the system NA that participated to the GIVE challenge 2010 (GIVE 2), the two systems are almost the same except minor modifications. We present the strategy of these systems, namely a directive, low level, navigation strategy (Go left) and a referring strategy based on focus and sub-contexts (Denis, 2010) (Not this one! Look for the other one). These strategies were successful, as shown by the GIVE 2 challenge, but also had some deficiences we tried to fix for GIVE 2.5. We explain these deficiencies and how we fixed them in GIVE 2.5. We eventually present the preliminary results that show that the system L, like the system NA, achieved a very good result both in objective and in subjective metrics. "
W11-2851 "rmany {garoufi, akoller}@uni-potsdam.de Abstract We present the Potsdam natural language generation systems P1 and P2 of the GIVE-2.5 Challenge. The systems implement two different referring expression generation models from Garoufi and Koller (2011) while behaving identically in all other respects. In particular, P1 combines symbolic and corpus-based methods for the generation of successful referring expressions, while P2 is based on a purely symbolic model which serves as a qualified baseline for comparison. We describe how the systems operated in the challenge and discuss the results, which indicate that P1 outperforms P2 in terms of several measures of referring expression success. "
W11-2852 "Abstract This paper describes the Thumbs Up! Twente system, a natural language generation system designed for the GIVE 2.5 Challenge. The purpose of the system is to guide a user through a virtual 3D environment by generating instructions in real-time. Our system focuses on motivating the user to keep him playing the game and trying to find the trophy. "
W11-2901 "<NoAbstract>"
W11-2902 "<NoAbstract>"
W11-2903 "<NoAbstract>"
W11-2904 " The problem of finding the most probable string for a distribution generated by a weighted finite automaton or a probabilistic grammar is related to a number of important questions: computing the distance between two distributions or finding the best translation (the most probable one) given a probabilistic finite state transducer. The problem is undecidable with general weights and is N P-hard if the automaton is probabilistic. We give a pseudo-polynomial algorithm which computes the most probable string in time polynomial in the inverse of the probability of the most probable string itself, both for probabilistic finite automata and probabilistic context-free grammars. We also give a randomised algorithm solving the same problem. 1 Introductio  "
W11-2905 " We present a simple and effective way to perform out-of-domain statistical parsing by drastically reducing lexical data sparseness in a PCFG-LA architecture. We replace terminal symbols with unsupervised word clusters acquired from a large newspaper corpus augmented with biomedical targetdomain data. The resulting clusters are effective in bridging the lexical gap between source-domain and target-domain vocabularies. Our experiments combine known self-training techniques with unsupervised word clustering and produce promising results, achieving an error reduction of 21% on a new evaluation set for biomedical text with manual bracketing annotations. 1 Intro  "
W11-2906 " Instance-weighting has been shown to be effective in statistical machine translation (Foster et al., 2010), as well as crosslanguage adaptation of dependency parsers (Sgaard, 2011). This paper presents new methods to do instance-weighting in stateof-the-art dependency parsers. The methods are evaluated on Danish and English data with consistent improvements over unadapted baselines. 1 Intro  "
W11-2907 " This paper discusses the difficulties in Chinese deep parsing, by comparing the accuracy of a Chinese HPSG parser to the accuracy of an English HPSG parser and the commonly used Chinese syntactic parsers. Analysis reveals that deep parsing for Chinese is more challenging than for English, due to the shortage of syntactic constraints of Chinese verbs, the widespread pro-drop, and the large distribution of ambiguous constructions. Moreover, the inherent ambiguities caused by verbal coordination and relative clauses make semantic analysis of Chinese more difficult than the syntactic analysis of Chinese. 1 Int  "
W11-2908 "Institut f  ur Maschinelle Sprachverarbeitung University of Stuttgart {seeker,jonas}@ims.uni-stuttgart.de Abstract We investigate the question whether an explicit feature representation for morphological features is necessary when parsing German with a fully lexicalized, statistical dependency parser. We use two morphosyntactic phenomena of German to show that while lexicalization does indeed suffice to a large extent when recovering the internal structure of noun phrases, an accurate explicit representation can support the correct selection of its grammatical function. "
W11-2909 "Abstract This paper proposes a framework which unifies graphical model theory and formal language theory through automata theory. Specifically, we propose Bayesian Network Automata (BNAs) as a formal framework for specifying graphical models of arbitrarily large structures, or equivalently, specifying probabilistic grammars in terms of graphical models. BNAs use a formal automaton to specify how to construct an arbitrarily large Bayesian Network by connecting multiple copies of a bounded Bayesian Network. Using a combination of results from graphical models and formal language theory, we show that, for a large class of automata, the complexity of inference with a BNA is bounded by the complexity of inference in the bounded Bayesian Network times the complexity of inference for the equivalent stochastic automaton. This illustrates that BNAs provide a useful framework for developing and analysing models and algorithms for structure prediction. "
W11-2910 "LIFO Universit  e dOrl  eans Orl  eans, France Abstract In this paper, we present a model-theoretic description of Property Grammar (PG) with features. Our approach is based on previous work of Duchier et al. (2009), and extends it by giving a model-theoretic account of feature-based properties, which was lacking in the description of Duchier et al. On top of providing a formal definition of the semantics of feature-based PG, this paper also discusses the various possible interpretations of features (e.g., within the requirement and agreement properties), and show how these interpretations are represented in our framework. This work opens the way for a constraint-based implementation of a parser for PG with features. "
W11-2911 "Abstract Using semi-supervised EM, we learn finegrained but sparse lexical parameters of a generative parsing model (a PCFG) initially estimated over the Penn Treebank. Our lexical parameters employ supertags, which encode complex structural information at the pre-terminal level, and are particularly sparse in labeled data  our goal is to learn these for words that are unseen or rare in the labeled data. In order to guide estimation from unlabeled data, we incorporate both structural and lexical priors from the labeled data. We get a large error reduction in parsing ambiguous structures associated with unseen verbs, the most important case of learning lexico-structural dependencies. We also obtain a statistically significant improvement in labeled bracketing score of the treebank PCFG, the first successful improvement via semi-supervised EM of a generative structured model already trained over large labeled data. "
W11-2912 " In this paper, we describe and compare two statistical parsing approaches for the hybrid dependency-constituency syntactic representation used in the Quranic Arabic Treebank (Dukes and Buckwalter, 2010). In our first approach, we apply a multi-step process in which we use a shift-reduce algorithm trained on a pure dependency preprocessed version of the treebank. After parsing, the dependency output is converted into the hybrid representation. This is compared to a novel one-step parser that is able to learn the hybrid representation without preprocessing. We define an extended labelled attachment score (ELAS) as our performance metric for hybrid parsing, and report 87.47% (F1 score) for the multi-step approach, and 89.03% (F1 score) for the onestep integrated algorithm. We also consider the effect of using different sets of morphological features for parsing the Quran, comparing our results to recent work on Modern Standard Arabic. "
W11-2913 " This paper proposes a direct parsing of non-local dependencies in English. To this end, we use probabilistic linear context-free rewriting systems for data-driven parsing, following recent work on parsing German. In order to do so, we first perform a transformation of the Penn Treebank annotation of non-local dependencies into an annotation using crossing branches. The resulting treebank can be used for PLCFRS-based parsing. Our evaluation shows that, compared to PCFG parsing with the same techniques, PLCFRS parsing yields slightly better results. In particular when evaluating only the parsing results concerning long-distance dependencies, the PLCFRS approach with discontinuous constituents is able to recognize about 88% of the dependencies of type *T* and *T*-PRN encoded in the Penn Treebank. Even the evaluation results concerning local dependencies, which can in principle be captured by a PCFG-based model, are better with our PLCFRS model. This demonstrates that by discarding information on non-local dependencies the PCFG model loses important information on syntactic dependencies in general. "
W11-2914 "Abstract Among human cognitive abilities, language is singular in the diversity of its manifestations: over 6000 languages are spoken in the world today. Some of the major challenges in modelling how language is processed by the human brain thus lie in explaining (a) how this diversity is handled, and (b) whether there are nevertheless some underlying generalisations that recur across languages of different types. Furthermore, an adequate model should be neurobiologically plausible, i.e., respect what we know about the structure and function of the human brain. In this presentation, I will describe a line of research in which we have attempted to take up these challenges at the level of sentence comprehension. Based on the results of neurophysiological experiments in a range of typologically varied languages, I will argue for a comprehension architecture that is actor-centred, i.e., focused on identifying the participant primarily responsible for the state of affairs under discussion. I will introduce the latest version of a comprehension model (extended Argument Dependency Model, eADM; Bornkessel and Schlesewsky, 2006), the architecture of which is built around actor-centrality as a design principle, and will describe how it accounts for potential universals of comprehension and critical dimensions of variation. "
W11-2915 ", Hans Uszkoreit, Sebastian Krause DFKI, LT-Lab, Germany {feiyu,lihong,Yi.Zhang,uszkoreit,sebastian.krause}@dfki.de Abstract The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted headdriven phrase structure grammar (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less generic HPSG treebank. It will be shown how the estimated confidence of RE rules learned from the n best parses can be exploited for parse reranking. The acquired reranking model improves the performance of RE in both training and test phases with the new first parses. The obtained significant boost of recall does not come from an overall gain in parsing performance but from an application-driven selection of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. "
W11-2916 "{abmayne, tdeoskar, steedman}@inf.ed.ac.uk Abstract Prepositional phrase attachment is an important subproblem of parsing, performance on which suffers from limited availability of labelled data. We present a semi-supervised approach. We show that a discriminative lexical model trained from labelled data, and a generative lexical model learned via Expectation Maximization from unlabelled data can be combined in a product model to yield a PP-attachment model which is better than either is alone, and which outperforms the modern parser of Petrov and Klein (2007) by a significant margin. We show that, when learning from unlabelled data, it can be beneficial to model the generation of modifiers of a head collectively, rather than individually. Finally, we suggest that our pair of models will be interesting to combine using new techniques for discriminatively constraining EM. "
W11-2917 " Current successful probabilistic parsers require large treebanks which are difficult, time consuming, and expensive to produce. Some parts of these data do not contain any useful information for training a parser. Active learning strategies allow to select the most informative samples for annotation. Most existing active learning strategies for parsing rely on selecting uncertain sentences for annotation. We show in this paper that selecting full sentences is not an optimal solution and propose a way to select only subparts of sentences.  "
W11-2918 "<NoAbstract>"
W11-2919 "Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy Abstract We present a novel method for the computation of prefix probabilities for linear context-free rewriting systems. Our approach streamlines previous procedures to compute prefix probabilities for context-free grammars, synchronous context-free grammars and tree adjoining grammars. In addition, the methodology is general enough to be used for a wider range of problems involving, for example, several prefixes. "
W11-2920 "Center for Spoken Language Understanding Oregon Health & Science University Portland, OR [aaron.dunlop,bodenstab,roarkbr]@gmail.com Abstract We present a matrix encoding of contextfree grammars, motivated by hardware-level efficiency considerations. We find efficiency gains of 2.59 for exhaustive inference and approximately 2 for pruned inference, resulting in high-accuracy parsing at over 20 sentences per second. Our grammar encoding allows fine-grained parallelism during chart cell population; we present a controlled study of several methods of parallel parsing, and find nearoptimal latency reductions as core-count increases. "
W11-2921 " Low-latency solutions for syntactic parsing are needed if parsing is to become an integral part of user-facing natural language applications. Unfortunately, most state-of-theart constituency parsers employ large probabilistic context-free grammars for disambiguation, which renders them impractical for real-time use. Meanwhile, Graphics Processor Units (GPUs) have become widely available, offering the opportunity to alleviate this bottleneck by exploiting the finegrained data parallelism found in the CKY algorithm. In this paper, we explore the design space of parallelizing the dynamic programming computations carried out by the CKY algorithm. We use the Compute Unified Device Architecture (CUDA) programming model to reimplement a state-of-theart parser, and compare its performance on two recent GPUs with different architectural features. Our best results show a 26-fold speedup compared to a sequential C implementation. "
W11-2922 " We present a deterministic HPSG parser capable of processing text incrementally with very fast parsing times. Our system demonstrates an efficient data-driven approach that achieves a high level of precision. Through a series of experiments in different configurations, we evaluate our system and compare it to current state-of-the-art within the field, and show that high quality deterministic parsing is realistic even for a deep unification-based precision grammar. 1 Motivat  "
W11-2923 "Abstract We present a novel corpus-driven approach towards grammar approximation for a linguistically deep Head-driven Phrase Structure Grammar. With an unlexicalized probabilistic context-free grammar obtained by Maximum Likelihood Estimate on a largescale automatically annotated corpus, we are able to achieve parsing accuracy higher than the original HPSG-based model. Different ways of enriching the annotations carried by the approximating PCFG are proposed and compared. Comparison to the state-of-the-art latent-variable PCFG shows that our approach is more suitable for the grammar approximation task where training data can be acquired automatically. The best approximating PCFG achieved ParsEval F 1 accuracy of 84.13%. The high robustness of the PCFG suggests it is a viable way of achieving full coverage parsing with the hand-written deep linguistic grammars. "
W11-2924 "ttgart {farkas,berndbh,schmid}@ims.uni-stuttgart.de Abstract Radically different approaches have been proved to be effective for phrase-structure and dependency parsers in the last decade. Here, we aim to exploit the divergence in these approaches and show the utility of features extracted from the automatic dependency parses of sentences for a discriminative phrase-structure parser. Our experiments show a significant improvement over the state-of-the-art German discriminative constituent parser. "
W11-2925 " We compare the use of edited text in the form of newswire and unedited text in the form of discussion forum posts as sources for training material in a self-training experiment involving the Brown reranking parser and a test set of sentences from an online sports discussion forum. We find that grammars induced from the two automatically parsed corpora achieve similar Parseval fscores, with the grammars induced from the discussion forum material being slightly superior. An error analysis reveals that the two types of grammars do behave differently. 1 Intro  "
W11-2926 "Abstract In this paper, we give a summary of various dependency chart parsing algorithms in terms of the use of parsing histories for a new dependency arc decision. Some parsing histories are closely related to the target dependency arc, and it is necessary for the parsing algorithm to take them into consideration. Each dependency treebank may have some unique characteristics, and it requires for the parser to model them by certain parsing histories. We show in experiments that proper selection of the parsing algorithm which reflect the dependency annotation of the coordinate structures improves the overall performance. "
W11-2927 " We present a perspective on parser evaluation in a context where the goal of parsing is to extract meaning from a sentence. Using this perspective, we show why current parser evaluation metrics are not suitable for evaluating parsers that produce logical-form semantics and present an evaluation metric that is suitable, analysing some of the characteristics of this new metric. 1 Int  "
W11-2928 " We consider the problem of parsing a sentence that is partially annotated with information about where phrases start and end. The application domain is interactive parse selection with probabilistic grammars. It is explained that the main obstacle is spurious ambiguity. The proposed solution is first described in terms of appropriately constrained synchronous grammars, and then in terms of a computational model for parsing. Experiments show the feasibility for a practical grammar. 1 Introdu  "
W11-3701 " Pragmatics the aspects of text that signal interpersonal and situational information, complementing semantics has been almost totally ignored in Natural Language Processing. But in the past five to eight years there has been a surge of research on the general topic of opinion, also called sentiment. Generally, research focuses on the determining the authors opinion/sentiment about some topic within a given fragment of text. Since opinions may differ, it is granted that the authors opinion is subjective, and the effectiveness of an opiniondetermination system is measured by comparing against a gold-standard set of human annotations. But what does subjectivity actually mean? What are opinion and sentiment? Lately, researchers are also starting to talk about affect, and even emotion. What are these notions, and how do they differ from one another? Unfortunately, a survey of the research done to date shows a disturbing lack of clarity on these questions. Very few papers bother to define their terms, but simply take a set of valences such as GoodNeutralBad to be sufficient. More recent work acknowledges the need to specify what the opinion actually applies to, and attempts also to determine the theme. Lately, several identify the holder of the opinion. Some even try to estimate the strength of the expressed opinion. The trouble is, the same aspect of the same object can be considered Good by one person and Bad by another, and we can often understand both their points of view. There is much more to opinion/sentiment than simply matching words and phrases that attach to the theme, and computing a polarity score. People give reasons why they like or dislike something, and these reasons pertain to their goals and plans in the case of opinions) or their deeper emotional states (in the case of affect). In this talk I outline a model of sentiment/opinion and of affect, and show that they appear in text in a fairly structured way, with various components. I show how proper understanding requires the reader to build some kind of person profile of the author, and claim that for systems to do adequate understanding of sentiments, opinions, and affects, they will need to do so as well. This is not a trivial challenge, and it opens the door to a whole new line of research with many fascinating and practical aspects. "
W11-3702 " The body of content available on Twitter undoubtedly contains a diverse range of political insight and commentary. But, to what extent is this representative of an electorate? Can we model political sentiment effectively enough to capture the voting intentions of a nation during an election capaign? We use the recent Irish General Election as a case study for investigating the potential to model political sentiment through mining of social media. Our approach combines sentiment analysis using supervised learning and volume-based measures. We evaluate against the conventional election polls and the final election result. We find that social analytics using both volume-based measures and sentiment analysis are predictive and we make a number of observations related to the task of monitoring public sentiment during an election campaign, including examining a variety of sample sizes, time periods as well as methods for qualitatively exploring the underlying content. "
W11-3703 " Most recent studies on emotion analysis and detection focus on how writers express their emotions through textual information. In this paper, we model emotion generation on the Plurk microblogging platform from both writer and reader perspectives. Support Vector Machine (SVM)-based classifiers are used for emotion prediction. To better model emotion generation on such a social network, three types of non-linguistic features are used: social relation, user behavior, and relevance degree, along with textual features. We found that each of the non-linguistic features can be combined with linguistic features to achieve higher performance. In fact, the combination of linguistic, social, and behavioral features performs the best. 1  "
W11-3704 " The automatic analysis of emotional content of text has become pervasive and has been applied in many fields of research. The work reported in this paper is in particular interested in modeling antisocial behavior and the emotional states that define it. We introduce the antisocial behavior detection (ASBD) model for portraying the emotions pertaining to antisocial behavior. In addition to describing negative affective states, our model uses the concepts of action tendencies and evidences in order to predict possible acts of antisocial behavior based on input texts. We outline a design for an antisocial behavior detection system based on the ASBD model. 1 Introdu  "
W11-3705 "Abstract This paper concentrates on pairing opinion analysis with argument extraction in order to identify why opinions about a certain feature are positive or negative. The objective is to have a better grasp at the underlying elements that support the analysis. In a second stage, given customer recommendations, the goal is to identify the preferences or priorities of customers, e.g. fares over welcome attitude. This induces customers value systems. Finally, we give elements of the implementation based on the platform, dedicated to discourse analysis. 1 Introduction Nowadays, there is an increasing need for an opinion analysis tool. While politicians may find it useful to analyze the popularity of new proposals or the overall public reaction to certain events, companies are definitely interested in consumer attitudes towards a product and the reasons and motivations of these attitudes. It is therefore essential to accurately and quickly analyze opinion intensity on a particular object. In addition to finding a quantitative and qualitative rating, it provides different information on the object like the most important features for people and the weaknesses of the object. The conjunction of efforts in language processing and artificial intelligence is a new promising way to address the problem. Argument analysis (Walton et al. 2008), (Reed, 1998) is a central challenge that has seldom been carried out in such a framework in particular paired with opinion analysis, where the semantics of evaluative expressions remains by large an open issue. The introduction of domain or common-sense knowledge (Breck et al. 2004) for the interpretation of these expressions is also an open issue. In order to be able to analyze opinion, besides the language processing aspects and semantic interpretation challenges, we need to define efficient models for aggregating the different opinions reported on the web (Ashley et al. 2002), (Amgoud et al. 2005). We will take advantage of existing works in social choice theory, namely on judgment aggregation. The output would be a final rating of the object as well as a global rating of each feature and a list of key features. The main difficulty will be the choice of the aggregation function. Different kinds of simulations can also be made, in particular in order to know which feature(s) should be improved in order to alter the global rating of an object (Amgoud et al. 2001) (Keil, 2000) (Pollock 1974). Finally, we may help a user to get an opinion on an object. The idea is to ask the user to give her preferences on the set of features, then using an efficient multiple-criteria decision system, we could give an appropriate recommendation. In this paper we first address the language point of view focusing on argument identification and extraction. Then, we introduce the main formal aspects of an aggregation system that allows to efficiently and accurately compute opinion values and their arguments, as found in various texts. The project is now in a development stage, implemented within the platform and the Dislog language (under submission). "
W11-3706 "chris.eckl@sitekit.net Abstract In a world in which web users are continuously blasted by ads and often compelled to deal with user-unfriendly interfaces, we sometimes feel like we want to evade from the sensory overload of standard web pages and take refuge in a safe web corner, in which contents and design are in harmony with our current frame of mind. Sentic Corner is an intelligent user interface that dynamically collects audio, video, images and text related to the users current feelings and activities as an interconnected knowledge base, which is browsable through a multi-faceted classification website. "
W11-3707 "Abstract This paper explores the ability of senses aligned across languages to carry coherent subjectivity information. We start out with a manual annotation study, and then seek to create an automatic framework to determine subjectivity labeling for unseen senses. We identify two methods that are able to incorporate subjectivity information originating from different languages, namely co-training and multilingual vector spaces, and show that for this task the latter method is better suited and obtains superior results. 1 Introduction Following the terminology proposed by (Wiebe et al., 2005), subjectivity and sentiment analysis focuses on the automatic identification of private states (opinions, emotions, sentiments, etc.) in natural language. While subjectivity classification labels text as either subjective or objective, sentiment or polarity classification further classifies subjective text as either positive, negative or neutral. To date, a large number of text processing applications have used techniques for automatic sentiment and subjectivity analysis, including automatic expressive text-to-speech synthesis (Alm et al., 1990), tracking sentiment timelines in on-line forums and news (Balog et al., 2006; Lloyd et al., 2005), and mining opinions from product reviews (Hu and Liu, 2004). In many natural language processing tasks, subjectivity and sentiment classification has been used as a first phase filtering to generate more viable data. Research that benefited from this additional layering ranges from question answering (Yu and Hatzivassiloglou, 2003), to conversation summarization (Carenini et al., 2008), text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006a) and lexical substitution (Su and Markert, 2010). While research in English has underlined that the most robust subjectivity delineation occurs at sense and not at word level (Wiebe and Mihalcea, 2006), we are not aware of this consideration impacting research in other languages. For this reason, in this work we seek to analyze how subjectivity is maintained across sense aligned resources, and identify ways in which subjectivity at sense level may be employed in a multilingual framework to provide a strengthened automatic senselevel classification. "
W11-3708 " A method for multilingual review classification is described. In this classification task, machine translation techniques are used to remove language gaps in the dataset, but many translation errors occur as a side-effect. These errors cause a decrease in the review classification performance. To resolve this problem, we introduce a sentiment-oriented sentence filtering module to the process of multilingual review classification. Experimental results showed that the proposed method achieved 81.7% classification accuracy for the evaluation data. 1 Introductio  "
W11-3709 " The present task collects different statistics of emotions based on the combinations of general variables (intensity, timing and longevity) and physiological variables (psycho-physiological arousals) from the situational statements of the ISEAR (International Survey on Emotion Antecedents and Reactions) dataset. The individual as well as combinational roles of different variables are analyzed. Some interesting observations and insights are found with respect to emotions. The statements of similar emotions are clustered according to different combinations of the variables. Each of the statements of a cluster is passed through two types of emotion tagging systems, a lexicon based baseline system followed by a supervised system. Due to the difficulty of incorporating knowledge regarding physiological variables, the supervised system only considers the roles of general variables from textual statements. The roles of the general variables are played by intensifiers, modifiers and explicitly specified temporal and causal discourse markers. The evaluation indicates that the supervised system based on general variables produces satisfactory results in identifying emotions. 1 Introduction  "
W11-3710 " Online communication is one of the key value propositions of mobile devices. While a variety of instant messaging clients offer users the ability to communicate with other users in real-time, the user experience remains dominated by a basic exchange of textual content. When compared to face-to-face communication, this experience is significantly poorer. In our proposed solution, we seek to enhance the chat experience by using an intelligent adaptive user interface that exploits semantics and sentics, that is the cognitive and affective information, associated with the ongoing communication. In particular, our approach leverages sentiment analysis techniques to process communication content and context and, hence, enable the interface to be adaptive in order to offer users a richer and more immersive chat experience. "
W11-3711 " The information overload experienced by people who use online services and read usergenerated content (e.g. product reviews and ratings) to make their decisions has led to the development of the so-called recommender systems. We address the problem of the large increase in the user-generated reviews, which are added to each day and consequently make it difficult for the user to obtain a clear picture of the quality of the facility in which they are interested. In this paper, we describe the TWIN (Tell me What I Need) personality-based recommender system, the aim of which is to select for the user reviews which have been written by like-minded individuals. We focus in particular on the task of User Profile construction. We apply the system in the travelling domain, to suggest hotels from the TripAdvisor 1 site by filtering out reviews produced by people with similar, or like-minded views, to those of the user. In order to establish the similarity between people we construct a user profile by modelling the users personality (according to the "
W11-3712 "Abstract The repetition of names of persons, places, ideas and events, is used sometimes for emphasis. The same is true of the repetition of affect words repeated preferentially to show negative/positive sentiment. During an election campaign, this repetition may have a bearing on the electability of politicians and on the reputation of political parties. News media covering an election may be involved in endorsing political parties, attempting to set aspects of election agenda, and may have gender bias. Using Rocksteady, an affect analysis system, we have analyzed samples of news published nationally and regionally by Irish media between 21 st December 2010 and 20 th Feb. 2011 in the run up to the Irish General Election on 25 th February 2011. Our results show that a diachronic study of the coverage, based on named-entity dictionary crafted from electoral lists and with key financial and economic terms added, supplemented by a General Inquirer type dictionary of affect, helped us to distinguish between the winners (two opposition parties that have subsequently formed a coalition government) from the loser (the incumbent party). "
W11-3713 "ePulze Sdn Bhd, C-41-2, Block C, Jaya One, No 72a, Jalan Universiti, 46200, Petaling Jaya, Selangor Darul Ehsan, Malaysia. hemnaath@live.com Abstract This paper presents the use of Maximum Entropy technique for Chinese sentiment analysis. Berger, Vincent and Stephen (1996) prove that Maximum Entropy is a technique that is effective in a number of natural language processing applications. In this paper, Maximum Entropy classification is used to estimating the polarity of given comments of from electronic product. These messages are classified into either positive or negative. Apart from presenting the results obtained via Maximum Entropy technique, we also analyze the feature selection and pre-processing of the comments for training and testing purpose. "
W11-3714 "Abstract Two typical approaches to sentiment analysis are lexicon look up and machine learning. Even though recent studies have shown that machine learning approaches in general outperform the lexicon look up approaches, completely ignoring the knowledge encoded in sentiment lexicons may not be optimal. We present an alternative method that incorporates sentiment lexicons as prior knowledge with machine learning approaches such as SVM to improve the accuracy of sentiment analysis. This paper also describes a method to automatically generate domain specific sentiment lexicons for this learning purpose. Our experiment results show that the domain specific lexicons we constructed lead to a significant accuracy improvement for our sentiment analysis task. "
W11-3801 "This paper gives two contributions to dependency parsing in Korean. First, we build a Korean dependency Treebank from an existing constituent Treebank. For a morphologically rich language like Korean, dependency parsing shows some advantages over constituent parsing. Since there is not much training data available, we automatically generate dependency trees by applying head-percolation rules and heuristics to the constituent trees. Second, we show how to extract useful features for dependency parsing from rich morphology in Korean. Once we build the dependency Treebank, any statistical parsing approach can be applied. The challenging part is how to extract features from tokens consisting of multiple morphemes. We suggest a way of selecting important morphemes and use only these as features to avoid sparsity. Our parsing approach is evaluated on three different genres using both gold-standard and automatic morphological analysis. We also test the impact of fine vs. coarse-grained morphologies on dependency parsing. With automatic morphological analysis, we achieve labeled attachment scores of 80% + . To the best of our knowledge, this is the first time that Korean dependency parsing has been evaluated on labeled edges with such a large variety of data. "
W11-3802 "Abstract We investigate how morphological features in the form of part-of-speech tags impact parsing performance, using Arabic as our test case. The large, fine-grained tagset of the Penn Arabic Treebank (498 tags) is difficult to handle by parsers, ultimately due to data sparsity. However, ad-hoc conflations of treebank tags runs the risk of discarding potentially useful parsing information. The main contribution of this paper is to describe several automated, language-independent methods that search for the optimal feature combination to help parsing. We first identify 15 individual features from the Penn Arabic Treebank tagset. Either including or excluding these features results in 32,768 combinations, so we then apply heuristic techniques to identify the combination achieving the highest parsing performance. Our results show a statistically significant improvement of 2.86% for vocalized text and 1.88% for unvocalized text, compared with the baseline provided by the BikelBies Arabic POS mapping (and an improvement of 2.14% using product models for vocalized text, 1.65% for unvocalized text), giving state-of-the-art results for Arabic constituency parsing. "
W11-3803 "Abstract This article evaluates the integration of data extracted from a French syntactic lexicon, the Lexicon-Grammar (Gross, 1994), into a probabilistic parser. We show that by applying clustering methods on verbs of the French Treebank (Abeill  e et al., 2003), we obtain accurate performances on French with a parser based on a Probabilistic Context-Free Grammar (Petrov et al., 2006). "
W11-3804 "Abstract This paper presents a set of experiments performed on parsing Basque, a morphologically rich and agglutinative language, studying the effect of using the morphological analyzer for Basque together with the morphological disambiguation module, in contrast to using the gold standard tags taken from the treebank. The objective is to obtain a first estimate of the effect of errors in morphological analysis and disambiguation on the parsers. We tested two freely available and state of the art dependency parser generators, MaltParser, and MST, which represent the two dominant approaches in data-driven dependency parsing. "
W11-3805 "Abstract Recent advances in parsing technology have made treebank parsing with discontinuous constituents possible, with parser output of competitive quality (Kallmeyer and Maier, 2010). We apply Data-Oriented Parsing (DOP) to a grammar formalism that allows for discontinuous trees (LCFRS). Decisions during parsing are conditioned on all possible fragments, resulting in improved performance. Despite the fact that both DOP and discontinuity present formidable challenges in terms of computational complexity, the model is reasonably efficient, and surpasses the state of the art in discontinuous parsing. "
W11-3806 " In this paper, we investigated the impact of extracting different types of multiword expressions (MWEs) in improving the accuracy of a data-driven dependency parser for a morphologically rich language (Turkish). We showed that in the training stage, the unification of MWEs of a certain type, namely compound verb and noun formations, has a negative effect on parsing accuracy by increasing the lexical sparsity. Our results gave a statistically significant improvement by using a variant of the treebank excluding this MWE type in the training stage. Our extrinsic evaluation of an ideal MWE recognizer (for only extracting MWEs of type named entities, duplications, numbers, dates and some predefined list of compound prepositions) showed that the preprocessing of the test data would improve the labeled parsing accuracy by 1.5%. 1 Introduct  "
W11-3807 "<NoAbstract>"
W11-4201 " Natural language processing of biomedical text benefits from the ability to recognize broad semantic classes, but the number of semantic types is far bigger than is usually treated in newswire text. A method for broad semantic class assignment using lightweight linguistic analysis is described and evaluated using traditional and novel methods. 1  "
W11-4202 "Abstract The problem of providing effective computer support for clinical coding has been the target of many research efforts. A recently introduced approach, based on statistical data on co-occurrences of words in clinical notes and assigned diagnosis codes, is here developed further and improved upon. The ability of the word space model to detect and appropriately handle the function of negations is demonstrated to be important in accurately correlating words with diagnosis codes, although the data on which the model is trained needs to be sufficiently large. Moreover, weighting can be performed in various ways, for instance by giving additional weight to clinically significant words or by filtering code candidates based on structured patient records data. The results demonstrate the usefulness of both weighting techniques, particularly the latter, yielding 27% exact matches for a general model (across clinic types); 43% and 82% for two domain-specific models (ear-nosethroat and rheumatology clinics). "
W11-4203 "<NoAbstract>"
W11-4204 "Texts containing personal health information reveal enough data for a third party to be able to identify an individual and his health condition. Detection of personal health information in electronic health records is an essential part of record deidentification. Performance evaluation in use today focuses on methods ability to identify whether a word reveals personal health information or not. In this study, we propose and show that the multi-label classification measures better serve the final goal of the record de-identification. "
W11-4205 " We describe experiments with building a recognizer for disease names in Bulgarian clinical epicrises, where both the language and the domain are different from those in mainstream research, which has focused on PubMed articles in English. We show that using a general framework such as GATE and an appropriate pragmatic methodology can yield significant speed up of the manual annotation: we achieve F1=0.81 in just three days. This is the first step towards our ultimate goal: named entity normalization with respect to ICD-10. 1 Int  "
W11-4206 "<NoAbstract>"
W11-4207 " This paper describes the latest developments in the design of a tool to monitor Patient Discharge Summaries to detected pieces of evidences related to Hospital Acquired Infections. Anonymization, Named Entity detection, Temporal Expressions analysis and Causality detection methods have been developed and evaluated. They are embedded in a tool designed to work in a Hospital Information Workflow. 1 Inf  "
W11-4301 "<NoAbstract>"
W11-4302 "versity {m.amoia,k.kunz,e.lapshinova}@mx.uni-saarland.de Abstract In this paper, we discuss some linguistic phenomena that pose potential problems for multilevel linguistic annotation of parallel corpora in general and specifically for data encoding with state-of-art multilevel corpus querying tools such as CQP. We describe the strategy we use for integrating the standard hierarchical XML representation used to annotate such phenomena in our aligned bilingual corpus GECCo into a timeline-based format as used in CQP. Thus , our framework supports efficient multilevel representation as well as corpus exploitation and querying of linguistic data of arbitrary complexity. "
W11-4303 " In this paper, we propose the creation of a tagged and aligned corpus for the study of a linguistic phenomenon, the translation of proper names. We try to modify the hypothesis according to which proper names cannot be translated and should therefore appear as borrowings in a targetlanguage. To do so, we introduce a parallel multilingual corpus made of eleven versions in ten different languages of a novel. One of these versions, the French one, which appears to be the source-text, undergoes named entity extraction so as to localize more easily the phenomenon we try to study. We focus on the tools used for the creation of our corpus and present some results refuting the idea that proper names are not translatable. 1  "
W11-4304 "Abstract The paper introduces an ongoing project for the development of a parallel treebank for Italian, English and French annotated in the pure dependency format of the Turin University Treebank, i.e. ParallelTUT. We hypothesize that the major features of this annotation format can be of some help in addressing the typical issues related to parallel corpora, e.g. alignment at various levels. Therefore, benefitting from the tools previously used for TUT, we applied the TUT format to a multilingual sample set of sentences from the JR Acquis Multilingual Parallel Corpus and the whole text of the Universal Declaration of Human Rights.  "
W11-4305 " The paper describes the basic strategies behind the word and semantic level alignment in the Bulgarian-English treebank. The word level alignment has taken into consideration the experience within other NLP groups in the context of the Bulgarian language specific features. The semantic level alignment builds on the word level alignment and is represented in the framework of the Minimal Recursion Semantics. 1 Intro  "
W11-4306 "<NoAbstract>"
W11-4307 "<NoAbstract>"
W12-0301 " Keystroke-logging tools are widely used in writing process research. These applications are designed to capture each character and mouse movement as isolated events as an indicator of cognitive processes. The current research project explores the possibilities of aggregating the logged process data from the letter level (keystroke) to the word level by merging them with existing lexica and using NLP tools. Linking writing process data to lexica and using NLP tools enables researchers to analyze the data on a higher, more complex level. In this project the output data of Inputlog are segmented on the sentence level and then tokenized. However, by definition writing process data do not always represent clean and grammatical text. Coping with this problem was one of the main challenges in the current project. Therefore, a parser has been developed that extracts three types of data from the S-notation: word-level revisions, deleted fragments, and the final writing product. The within-word typing errors are identified and excluded from further analyses. At this stage the Inputlog process data are enriched with the following linguistic information: part-ofspeech tags, lemmas, chunks, syllable boundaries and word frequencies. 1  "
W12-0302 "Abstract This paper reports on the development of methods for the automated detection of violations of style guidelines for legislative texts, and their implementation in a prototypical tool. To this aim, the approach of error modelling employed in automated style checkers for technical writing is enhanced to meet the requirements of legislative editing. The paper identifies and discusses the two main sets of challenges that have to be tackled in this process: (i) the provision of domain-specific NLP methods for legislative drafts, and (ii) the concretisation of guidelines for legislative drafting so that they can be assessed by machine. The project focuses on German-language legislative drafting in Switzerland. "
W12-0303 " This essay provides a summary of research related to My Reviewers, a web-based application that can be used for teaching and assessment purposes. The essay concludes with speculation about ongoing development efforts, including a social helpfulness algorithm, a badging system, and Natural Language Processing (NLP) features. 1 Int  "
W12-0304 "Spain {rogelio.nazar,irene.renau}@upf.edu Abstract In this research we explore the possibility of using a large n-gram corpus (Google Books) to derive lexical transition probabilities from the frequency of word n-grams and then use them to check and suggest corrections in a target text without the need for grammar rules. We conduct several experiments in Spanish, although our conclusions also reach other languages since the procedure is corpus-driven. The paper reports on experiments involving different types of grammar errors, which are conducted to test different grammar-checking procedures, namely, spotting possible errors, deciding between different lexical possibilities and filling-in the blanks in a text. "
W12-0305 " This short paper relates the main features of LELIE, phase 1, which detects errors made by technical writers when producing procedures or requirements. This results from ergonomic observations of technical writers in various companies. 1 Objectives The main goal of the LELIE project is to produce an analysis and a piece of software based on language processing and artificial intelligence that detects and analyses potential risks of different kinds (first health and ecological, but also social and economical) in technical documents. We concentrate on procedural documents and on requirements (Hull et al. 2011) which are, by large, the main types of technical documents used in companies. Given a set of procedures (e.g., production launch, maintenance) over a certain domain produced by a company, and possibly given some domain knowledge (ontology, terminology, lexical), the goal is to process these procedures and to annotate them wherever potential risks are identified. Procedure authors are then invited to revise these documents. Similarly, requirements, in particular those related to safety, often exhibit complex structures (e.g., public regulations, to cite the worse case): several embedded conditions, negation, pronouns, etc., which make their use difficult, especially in emergency situations. Indeed, procedures as well as safety requirements are dedicated to action: little space should be left to personal interpretations. Risk analysis and prevention in LELIE is based on three levels of analysis, each of them potentially leading to errors made by operators in action: 1. Detection of inappropriate ways of writing: complex expressions, implicit elements, complex references, scoping difficulties (connectors, conditionals), inappropriate granularity level, involving lexical, semantic and pragmatic levels, inappropriate domain style, 2. Detection of domain incoherencies in procedures: detection of unusual ways of realizing an action (e.g., unusual instrument, equipment, product, unusual value such as temperature, length of treatment, etc.) with respect to similar actions in other procedures or to data extracted from technical documents, 3. Confrontation of domain safety requirements with procedures to check if the required safety constraints are met. Most industrial areas have now defined authoring recommendations on the way to elaborate, structure and write procedures of various kinds. However, our experience with technical writers shows that those recommendations are not very strictly followed in most situations. Our objective is to develop a tool that checks ill-formed structures with respect to these recommendations and general style considerations in procedures and requirements when they are written. In addition, authoring guidelines do not specify all the aspects of document authoring: our investigations on author practices have indeed identified a number of recurrent errors which are linguistic or conceptual which are usually not specified in authoring guidelines. These errors are basically identified from the comprehension difficulties encountered by technicians in operation using these documents to realize a task or from technical writers themselves which are aware of the errors they should avoid. 35 2 The Situation and our contribution Risk management and prevention is now a major issue. It is developed at several levels, in particular via probabilistic analysis of risks in complex situations (e.g., oil storage in natural caves). Detecting potential risks by analyzing business errors on written documents is a relatively new approach. It requires the taking into account of most of the levels of language: lexical, grammatical and style and discourse. Authoring tools for simplified language are not a new concept; one of the first checkers was developed at Boeing 1 , initially for their own simplifyed English and later adapted for the ASD Simplified Technical English Specification 2 . A more recent language checking system is Acrolinx IQ by Acrolinx 3 . Some technical writing environments also include language checking functionality, e.g., MadPak 4 . Ament (2002) and Weiss (2000) developed a number of useful methodological elements for authoring technical documents and error identification and correction. The originality of our approach is as follows. Authoring recommendations are made flexible and context-dependent, for example if negation is not allowed in instructions in general, there are, however, cases where it cannot be avoided because the positive counterpart cannot so easily be formulated, e.g., do not dispose of the acid in the sewer. Similarly, references may be allowed if the referent is close and non-ambiguous. However, this requires some knowledge. Following observations in cognitive ergonomics in the project, a specific effort is realized concerning the well-formedness (following grammatical and cognitive standards) of discourse structures and their regularity over entire documents (e.g., instruction or enumerations all written in the same way). The production of procedures includes some controls on contents, in particular action verb arguments, as indicated in the second objective above, via the Arias domain knowledge base, e.g., avoiding typos or confusions among syntactically and semantically well-identified entities such as instruments, products, equipments, values, etc. 1 http://www.boeing.com/phantom/sechecker/ 2 ASD-STE100, http://www.asd-ste100.org/ 3 http://www.acrolinx.com/ 4 http://www.madcapsoftware.com/products/ madpak/  http://www.madcapsoftware.com/products/ madpak/ There exists no real requirement analysis system based on language that can check the quality and the consistency of large sets of authoring recommendations. The main products are IBM Doors and Doors Trek 5 , Objecteering 6 , and Reqtify 7 , which are essentially textual databases with advanced visual and design interfaces, query facilities for retrieving specific requirements, and some traceability functions carried out via predefined attributes. These three products also include a formal language (essentially based on attribute-value pairs) that is used to check some simple forms of coherence among large sets of requirements. The authoring tool includes facilities for Frenchspeaking authors who need to write in English, supporting typical errors they make via language transfer (Garnier, 2011). We will not address this point here. This project, LELIE, is based on the TextCoop system (Saint-Dizier, 2012), a system dedicated to language analysis, in particular discourse (including the taking into account of long-distance dependencies). This project also includes the Arias action knowledge base that stores prototypical actions in context, and can update them. It also includes an ASP (Answer Set Programming) solver 8 to check for various forms of incoherence and incompleteness. The kernel of the system is written in SWI Prolog, with interfaces in Java. The project is currently realized for French, an English version is under development. The system is based on the following principles. First, the system is parameterized: the technical writer may choose the error types he wants to be checked, and the severity level for each error type when there are several such levels (e.g., there are several levels of severity associated with fuzzy terms which indeed show several levels of fuzziness). Second, the system simply tags elements identified as errors, the correction is left to the author. However, some help or guidelines are offered. For example, guidelines for reformulating a negative sentence into a positive one are proposed. Third, the way errors are displayed can be customized to the writers habits. We present below a kernel system that deals 5 http://www.ibm.com/software/awdtools/ doors/ 6 http://www.objecteering.com/ 7 http://www.geensoft.com/ 8 For an overview of ASP see Brewka et al. (2011).  http://www.geensoft.com/ 8 For an overview of ASP see Brewka et al. (2011). 36 with the most frequent and common errors made by technical writers independently of the technical domain. This kernel needs an in-depth customization to the domain at stake. For example, the verbs used or the terminological preferences must be implemented for each industrial context. Our system offers the control operations, but these need to be associated with domain data. Finally, to avoid the variability of document formats, the system input is an abstract document with a minimal number of XML tags as required by the error detection rules. Managing and transforming the original text formats into this abstract format is not dealt with here. "
W12-1501 "<NoAbstract>"
W12-1502 "<NoAbstract>"
W12-1503 "Abstract One important subtask of Referring Expression Generation (REG) algorithms is to select the attributes in a definite description for a given object. In this paper, we study how much training data is required for algorithms to do this properly. We compare two REG algorithms in terms of their performance: the classic Incremental Algorithm and the more recent Graph algorithm. Both rely on a notion of preferred attributes that can be learned from human descriptions. In our experiments, preferences are learned from training sets that vary in size, in two domains and languages. The results show that depending on the algorithm and the complexity of the domain, training on a handful of descriptions can already lead to a performance that is not significantly different from training on a much larger data set. "
W12-1504 "Sociable Agents Group  CITEC, Bielefeld University, Germany {hbuschme,skopp}@techfak.uni-bielefeld.de Abstract Commonly, the result of referring expression generation algorithms is a single noun phrase. In interactive settings with a shared workspace, however, human dialog partners often split referring expressions into installments that adapt to changes in the context and to actions of their partners. We present a corpus of humanhuman interactions in the GIVE-2 setting in which instructions are spoken. A first study of object descriptions in this corpus shows that references in installments are quite common in this scenario and suggests that contextual factors partly determine their use. We discuss what new challenges this creates for NLG systems. "
W12-1505 "Aberdeen {n.tintarev, y.melero, yaji.sripada, elizbeth.tait, r.vanderwal, c.mellish@abdn.ac.uk}@abdn.ac.uk Abstract We describe preliminary work on generating contextualized text for nature conservation volunteers. This Natural Language Generation (NLG) differs from other ways of describing spatio-temporal data, in that it deals with abstractions on data across large geographical spaces (total projected area 20,600 km 2 ), as well as temporal trends across longer time frames (ranging from one week up to a year). We identify challenges at all stages of the classical NLG pipeline. "
W12-1506 "Universit  at Stuttgart IMS, Pfaffenwaldring 5b Stuttgart, 70569, Germany bohnet@ims.unistuttgart.de Abstract Until recently, deep stochastic surface realization has been hindered by the lack of semantically annotated corpora. This is about to change. Such corpora are increasingly available, e.g., in the context of CoNLL shared tasks. However, recent experiments with CoNLL 2009 corpora show that these popular resources, which serve well for other applications, may not do so for generation. The attempts to adapt them for generation resulted so far in a better performance of the realizers, but not yet in a genuinely semantic generationoriented annotation schema. Our goal is to initiate a debate on how a generation suitable annotation schema should be defined. We define some general principles of a semantic generation-oriented annotation and propose an annotation schema that is based on these principles. Experiments shows that making the semantic corpora comply with the suggested principles does not need to have a negative impact on the quality of the stochastic generators trained on them. "
W12-1507 " While in Computer Science, grammar engineering has led to the development of various tools for checking grammar coherence, completion, underand over-generation, in Natural Langage Processing , most approaches developed to improve a grammar have focused on detecting under-generation and to a much lesser extent, over-generation. We argue that generation can be exploited to address other issues that are relevant to grammar engineering such as in particular, detecting grammar incompleteness, identifying sources of overgeneration and analysing the linguistic coverage of the grammar. We present an algorithm that implements these functionalities and we report on experiments using this algorithm to analyse a Feature-Based Lexicalised Tree Adjoining Grammar consisting of roughly 1500 elementary trees. "
W12-1508 "Abstract Variation in language style can lead to different perceptions of the interaction, and different behaviour outcomes. Using the CRAG 2 language generation system we examine how accurately judges can perceive character personality from short, automatically generated dialogues, and how alignment (similarity between speakers) alters judge perceptions of the characters relationship. Whilst personality perception of our dialogues is consistent with perceptions of human behaviour, we find that the introduction of alignment leads to negative perceptions of the dialogues and the interlocutors relationship. A follow up evaluation study of the perceptions of different forms of alignment in the dialogues reveals that while similarity at polarity, topic and construction levels is viewed positively, similarity at the word level is regarded negatively. We discuss our findings in relation to the literature and in the context of dialogue systems. "
W12-1509 " Recent studies have shown that incremental systems are perceived as more reactive, natural, and easier to use than non-incremental systems. However, previous work on incremental NLG has not employed recent advances in statistical optimisation using machine learning. This paper combines the two approaches, showing how the update, revoke and purge operations typically used in incremental approaches can be implemented as state transitions in a Markov Decision Process. We design a model of incremental NLG that generates output based on micro-turn interpretations of the users utterances and is able to optimise its decisions using statistical machine learning. We present a proof-of-concept study in the domain of Information Presentation (IP), where a learning agent faces the trade-off of whether to present information as soon as it is available (for high reactiveness) or else to wait until input ASR hypotheses are more reliable. Results show that the agent learns to avoid long waiting times, fillers and self-corrections, by re-ordering content based on its confidence. 1 Introduction  "
W12-1510 " Linguists Assistant (LA) is a large scale semantic analyzer and multi-lingual natural language generator designed and developed entirely from a linguists perspective. The system incorporates extensive typological, semantic, syntactic, and discourse research into its semantic representational system and its transfer and synthesizing grammars. LA has been tested with English, Korean, Kewa (Papua New Guinea), Jula (Cote dIvoure), and North Tanna (Vanuatu), and proof-of-concept lexicons and grammars have been developed for Spanish, Urdu, Tagalog, Chinantec (Mexico), and Angas (Nigeria). This paper will summarize the major components of the NLG system, and then present the results of experiments that were performed to determine the quality of the generated texts. The experiments indicate that when experienced mothertongue translators use the drafts generated by LA, their productivity is typically quadrupled without any loss of quality. 1 Introduction  "
W12-1511 "Abstract Despite their flat, semantics-free structure, ontology identifiers are often given names or labels corresponding to natural language words or phrases which are very dense with information as to their intended referents. We argue that by taking advantage of this information density, NLG systems applied to ontologies can guide the choice and construction of sentences to express useful ontological information, solely through the verbalisations of identifier names, and that by doing so, they can replace the extremely fussy and repetitive texts produced by ontology verbalisers with shorter and simpler texts which are clearer and easier for human readers to understand. We specify which axioms in an ontology are defining axioms for linguistically-complex identifiers and analyse a large corpus of OWL ontologies to identify common patterns among all defining axioms. By generating texts from ontologies, and selectively including or omitting these defining axioms, we show by surveys that human readers are typically capable of inferring information implicitly encoded in identifier phrases, and that texts which do not make such obvious information explicit are preferred by readers and yet communicate the same information as the longer texts in which such information is spelled out explicitly. "
W12-1512 " During the last decade, there has been a shift from developing natural language generation systems to developing generic systems that are capable of producing natural language descriptions directly from Web ontologies. To make these descriptions coherent and accessible in different languages, a methodology is needed for identifying the general principles that would determine the distribution of referential forms. Previous work has proved through crosslinguistic investigations that strategies for building coreference are language dependent. However, to our knowledge, there is no language generation methodology that makes a distinction between languages about the generation of referential chains. To determine the principles governing referential chains, we gathered data from three languages: English, Swedish and "
W12-1513 "Abstract Human-written, good quality extractive summaries pay great attention to the text intermixing the extracts. In this work, we focused on the lexical choice for verbs introducing quoted text. We analyzed 4000+ high quality summaries for a high traffic mailing list and manually assembled 39 quotation-introducing verb classes that cover the majority of the verb occurrences. A significant amount of the data is covered by on-going work on e-mail speech acts. However, we found that one third of the tail is composed by risky verbs that most likely will be beyond the state of the art for longer time. We used this fact to highlight the trade-offs of risk taking in NLG, where interesting prose might come at the cost of unsettling some of the readers. "
W12-1514 "10115 {akholy,habash}@ccls.columbia.edu Abstract We present an approach for generation of morphologically rich languages using statistical machine translation. Given a sequence of lemmas and any subset of morphological features, we produce the inflected word forms. Testing on Arabic, a morphologically rich language, our models can reach 92.1% accuracy starting only with lemmas, and 98.9% accuracy if all the gold features are provided. "
W12-1515 "15260, USA Abstract While some recent work in tutorial dialogue has touched upon tutor reformulations of student contributions, there has not yet been an attempt to characterize the intentions of reformulations in this educational context nor an attempt to determine which types of reformulation actually contribute to student learning. In this paper we take an initial look at tutor reformulations of student contributions in naturalistic tutorial dialogue in order to characterize the range of pedagogical intentions that may be associated with these reformulations. We further outline our plans for implementing reformulation in our tutorial dialogue system, Rimac, which engages high school physics students in post problem solving reflective discussions. By implementing reformulations in a tutorial dialogue system we can begin to test their impact on student learning in a more controlled way in addition to testing whether our approximation of reformulation is adequate. "
W12-1516 "ngdom {s.mahamood, e.reiter}@abdn.ac.uk Abstract NLG developers must work closely with domain experts in order to build good NLG systems, but relatively little has been published about this process. In this paper, we describe how NLG developers worked with clinicians (nurses) to improve an NLG system which generates information for parents of babies in a neonatal intensive care unit, using a structured revision process. We believe that such a process can significantly enhance the quality of many NLG systems, in medicine and elsewhere. "
W12-1517 "Abstract This paper concerns the architecture of a generator for Italian Sign Language. In particular we describe a microplanner based on an expert-system and a combinatory categorial grammar used in realization. "
W12-1518 "Kingdom {t.nguyen,r.power,p.piwek,s.h.williams}@open.ac.uk Abstract A useful enhancement of an NLG system for verbalising ontologies would be a module capable of explaining undesired entailments of the axioms encoded by the developer. This task raises interesting issues of content planning. One approach, useful as a baseline, is simply to list the subset of axioms relevant to inferring the entailment; however, in many cases it will still not be obvious, even to OWL experts, why the entailment follows. We suggest an approach in which further statements are added in order to construct a proof tree, with every step based on a relatively simple deduction rule of known difficulty; we also describe an empirical study through which the difficulty of these simple deduction patterns has been measured. "
W12-1519 "Abstract Question answering is an age old AI challenge. How we approach this challenge is determined by decisions regarding the linguistic and domain knowledge our system will need, the technical and business acumen of our users, the interface used to input questions, and the form in which we should present answers to a users questions. Our approach to question answering involves the interactive construction of natural language queries. We describe and evaluate a question answering system that provides a point-and-click, webbased interface in conjunction with a semantic grammar to support user-controlled natural language question generation. A preliminary evaluation is performed using a selection of 12 questions based on the Adventure Works sample database. "
W12-1520 "f Aberdeen Abstract This paper proposes the use of NLG to enhance public engagement during the course of species reintroductions. We examine whether ecological insights can be effectively communicated through blogs about satellite-tagged individuals, and whether such blogs can help create a positive perception of the species in readers minds, a requirement for successful reintroduction. We then discuss the implications for NLG systems that generate blogs from satellite-tag data. "
W12-1521 "<NoAbstract>"
W12-1522 " In this paper we introduce an automatic system that generates textual summaries of Internet-style video clips by first identifying suitable high-level descriptive features that have been detected in the video (e.g. visual concepts, recognized speech, actions, objects, persons, etc.). Then a natural language generator is constructed using SimpleNLG to compile the high-level features into a textual form. The generated summary contains information from both visual and acoustic sources, intending to give a general review and summary of the video. To reduce the complexity of the task, we restrict ourselves to work with videos that show a limited number of events. In this demo paper, we describe the design of the system and present example outputs generated by the video summarization system. 1 Introdu  "
W12-1523 "Abstract We demonstrate a novel, robust vision-tolanguage generation system called Midge. Midge is a prototype system that connects computer vision to syntactic structures with semantic constraints, allowing for the automatic generation of detailed image descriptions. We explain how to connect vision detections to trees in Penn Treebank syntax, which provides the scaffolding necessary to further refine data-driven statistical generation approaches for a variety of end goals. "
W12-1524 "<NoAbstract>"
W12-1525 "Abstract The Surface Realisation Shared Task was first run in 2011. Two common-ground input representations were developed and for the first time several independently developed surface realisers produced realisations from the same shared inputs. However, the input representations had several shortcomings which we have been aiming to address in the time since. This paper reports on our work to date on improving the input representations and on our plans for the next edition of the SR Task. We also briefly summarise other related developments in NLG shared tasks and outline how the different ideas may be usefully brought together in the future. "
W12-1526 "<NoAbstract>"
W12-1527 "Abstract So far, there has been little success in Natural Language Generation in coming up with general models of the content selection process. Nonetheless, there has been some work on content selection that employ Machine learning or heuristic search. On the other side, there is a clear tendency in NLG towards the use of resources encoded in standard Semantic Web representation formats. For these reasons, we believe that time has come to propose an initial challenge on content selection from Semantic Web data. In this paper, we briefly outline the idea and plan for the execution of this task. "
W12-1601 " Conversations in poster sessions in academic events, referred to as poster conversations, pose interesting and challenging topics on multi-modal analysis of multi-party dialogue. This article gives an overview of our project on multi-modal sensing, analysis and understanding of poster conversations. We focus on the audiences feedback behaviors such as non-lexical backchannels (reactive tokens) and noddings as well as joint eye-gaze events by the presenter and the audience. We investigate whether we can predict when and who will ask what kind of questions, and also interest level of the audience. Based on these analyses, we design a smart posterboard which can sense human behaviors and annotate interactions and interest level during poster sessions. 1 Introduct  "
W12-1602 "Abstract We present and evaluate two state-of-the art dialogue systems developed to support dialog with French speaking virtual characters in the context of a serious game: one hybrid statistical/symbolic and one purely statistical. We conducted a quantitative evaluation where we compare the accuracy of the interpreter and of the dialog manager used by each system; a user based evaluation based on 22 subjects using both the statistical and the hybrid system; and a corpus based evaluation where we examine such criteria as dialog coherence, dialog success, interpretation and generation errors in the corpus of Human-System interactions collected during the user-based evaluation. We show that although the statistical approach is slightly more robust, the hybrid strategy seems to be better at guiding the player through the game. "
W12-1603 "University {yww, slfink, aeo, awb, justine}@cs.cmu.edu Abstract One challenge of implementing spoken dialogue systems for long-term interaction is how to adapt the dialogue as user and system become more familiar. We believe this challenge includes evoking and signaling aspects of long-term relationships such as rapport. For tutoring systems, this may additionally require knowing how relationships are signaled among non-adult users. We therefore investigate conversational strategies used by teenagers in peer tutoring dialogues, and how these strategies function differently among friends or strangers. In particular, we use annotated and automatically extracted linguistic devices to predict impoliteness and positivity in the next turn. To take into account the sparse nature of these features in real data we use models including Lasso, ridge estimator, and elastic net. We evaluate the predictive power of our models under various settings, and compare our sparse models with standard non-sparse solutions. Our experiments demonstrate that our models are more accurate than non-sparse models quantitatively, and that teens use unexpected kinds of language to do relationship work such as signaling rapport, but friends and strangers, tutors and tutees, carry out this work in quite different ways from one another. "
W12-1604 "Abstract The ability to monitor the communicative success of its utterances and, if necessary, provide feedback and repair is useful for a dialog system. We show that in situated communication, eyetracking can be used to reliably and efficiently monitor the hearers reference resolution process. An interactive system that draws on hearer gaze to provide positive or negative feedback after referring to objects outperforms baseline systems on metrics of referential success and user confusion. "
W12-1605 "Abstract We present a token-level decision summarization framework that utilizes the latent topic structures of utterances to identify summaryworthy words. Concretely, a series of unsupervised topic models is explored and experimental results show that fine-grained topic models, which discover topics at the utterance-level rather than the document-level, can better identify the gist of the decisionmaking process. Moreover, our proposed token-level summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary. "
W12-1606 "2 Abstract This paper proposes an unsupervised approach to user simulation in order to automatically furnish updates and assessments of a deployed spoken dialog system. The proposed method adopts a dynamic Bayesian network to infer the unobservable true user action from which the parameters of other components are naturally derived. To verify the quality of the simulation, the proposed method was applied to the Lets Go domain (Raux et al., 2005) and a set of measures was used to analyze the simulated data at several levels. The results showed a very close correspondence between the real and simulated data, implying that it is possible to create a realistic user simulator that does not necessitate human intervention. "
W12-1607 "PA 15213 {emayfiel, dadamson, cprose}@cs.cmu.edu Abstract Conversational practices do not occur at a single unit of analysis. To understand the interplay between social positioning, information sharing, and rhetorical strategy in language, various granularities are necessary. In this work we present a machine learning model for multi-party chat which predicts conversation structure across differing units of analysis. First, we mark sentence-level behavior using an information sharing annotation scheme. By taking advantage of Integer Linear Programming and a sociolinguistic framework, we enforce structural relationships between sentence-level annotations and sequences of interaction. Then, we show that clustering these sequences can effectively disentangle the threads of conversation. This model is highly accurate, performing near human accuracy, and performs analysis on-line, opening the door to real-time analysis of the discourse of conversation. "
W12-1608 " We herein propose a method for the rapid development of a spoken dialogue system based on collaboratively constructed semantic resources and compare the proposed method with a conventional method that is based on a relational database. Previous development frameworks of spoken dialogue systems, which presuppose a relational database management system as a background application, require complex data definition, such as making entries in a task-dependent language dictionary, templates of semantic frames, and conversion rules from user utterances to the query language of the database. We demonstrate that a semantic web oriented approach based on collaboratively constructed semantic resources significantly reduces troublesome rule descriptions and complex configurations in the rapid development process of spoken dialogue systems.  "
W12-1609 "Abstract In recent years statistical dialogue systems have gained significant attention due to their potential to be more robust to speech recognition errors. However, these systems must also be robust to changes in user behaviour caused by cognitive loading. In this paper, a statistical dialogue system providing restaurant information is evaluated in a set-up where the subjects used a driving simulator whilst talking to the system. The influences of cognitive loading were investigated and some clear differences in behaviour were discovered. In particular, it was found that users chose to respond to different system questions and use different speaking styles, which indicate the need for an incremental dialogue approach. "
W12-1610 "Abstract Recent work on consultations between outpatients with schizophrenia and psychiatrists has shown that adherence to treatment can be predicted by patterns of repair  specifically, the pro-activity of the patient in checking their understanding, i.e. patient clarification. Using machine learning techniques, we investigate whether this tendency can be predicted from high-level dialogue features, such as backchannels, overlap and each participants proportion of talk. The results indicate that these features are not predictive of a patients adherence to treatment or satisfaction with the communication, although they do have some association with symptoms. However, all these can be predicted if we allow features at the word level. These preliminary experiments indicate that patient adherence is predictable from dialogue transcripts, but further work is necessary to develop a meaningful, general and reliable feature set. "
W12-1611 ", {kgeorgila,leuski,traum}@ict.usc.edu Abstract We use Reinforcement Learning (RL) to learn question-answering dialogue policies for a real-world application. We analyze a corpus of interactions of museum visitors with two virtual characters that serve as guides at the Museum of Science in Boston, in order to build a realistic model of user behavior when interacting with these characters. A simulated user is built based on this model and used for learning the dialogue policy of the virtual characters using RL. Our learned policy outperforms two baselines (including the original dialogue policy that was used for collecting the corpus) in a simulation setting. "
W12-1612 " Convergence is thought to be an important phenomenon in dialogue through which interlocutors adapt to each other. Yet, its mechanisms and relationship to dialogue outcomes are not fully understood. This paper explores convergence in textual task-oriented dialogue during a longitudinal study. The results suggest that over time, convergence between interlocutors increases with successive dialogues. Additionally, for the tutorial dialogue domain at hand, convergence metrics were found to be significant predictors of dialogue outcomes such as learning, mental effort, and emotional states including frustration, boredom, and confusion. The results suggest ways in which dialogue systems may leverage convergence to enhance their interactions with users.  "
W12-1613 "rsity {aasish, air}@cs.cmu.edu Abstract A robust system that understands route instructions should be able to process instructions generated naturally by humans. Also desirable would be the ability to handle repairs and other modifications to existing instructions. To this end, we collected a corpus of spoken instructions (and modified instructions) produced by subjects provided with an origin and a destination. We found that instructions could be classified into four categories, depending on their intent such as imperative, feedback, or meta comment. We asked a different set of subjects to follow these instructions to determine the usefulness and comprehensibility of individual instructions. Finally, we constructed a semantic grammar and evaluated its coverage. To determine whether instructiongiving forms a predictable sub-language, we tested the grammar on three corpora collected by others and determined that this was largely the case. Our work suggests that predictable sub-languages may exist for well-defined tasks. Index Terms: Robot Navigation, Spoken Instructions 1 Introduction Generating and interpreting instructions is a topic of enduring interest. Cognitive psychologists have examined how people perceive spatial entities and structure route instructions (Daniel and Denis, 1998; Allen, 1997). Linguists and others have investigated how people articulate route instructions in conversation with people or agents (Eberhard et al., 2010; Gargett et al., 2010; Stoia et al., 2008; Marge and Rudnicky, 2010). Artificial intelligence researchers have shown that under supervised conditions autonomous agents can learn to interpret route instructions (Kollar et al., 2010; MacMahon et al., 2006; Matuszek et al., 2010; Bugmann et al., 2004; Chen and Mooney, 2010). While the subject has been approached from different perspectives, it has been generally held that the language of directions is mostly limited and only parts of the vocabulary (such as location names) will vary from case to case. We are interested in being able to interpret natural directions, as might be given to a robot, and generating corresponding trajectory. But natural directions contain different types of information, some (more-or-less) easily interpreted (e.g., \"go to the end of the hall\") while others seem daunting (e.g., \"walk past the abstract mural with birds\"). So the question might actually be \"is there enough interpretable data in human directions to support planning a usable trajectory?\". The language of instructions contains a variety of relevant propositions: a preface to a route, an imperative statement, or a description of a landmark. Previous work has proposed both coarse and fine-grained instruction taxonomies. (Bugmann et al., 2004) proposed a taxonomy of 15 primitive categories in a concrete action framework. In contrast, (Daniel and Denis, 1998) suggested a five-way categorization based on cognitive properties of instructions. Instructions vary greatly and can include superfluous detail. (Denis et al., 1999) found that when people were asked to read and assess a set of instructions some of the instructions were deemed unnecessary and could be discarded. There is some evidence (Lovelace et al., 1999; Caduff and Timpf, 2008) that only the mention of significant landmarks along the route leads to better-quality instructions. Computational (rather than descriptive) approaches to this problem include: using sequence labeling approach to capture spatial relations, landmarks, and action verbs (Kollar et al., 2010), generating a frame structure for an instruction (MacMahon et al., 2006), or using statistical machine translation techniques to translate instructions into actions (Matuszek et al., 2010). We describe a new instructions corpus, its analysis in terms of a taxonomy suitable for automated understanding and a verification that the instructions are in fact usable by humans. With a view to automating understanding, we also constructed a grammar capable of processing this language, and show that it provides good coverage 99 for both our corpus and three other corpora (Kollar et al., 2010; Marge and Rudnicky, 2010; Bugmann et al., 2004) This paper is organized as following: Section 2 describes the corpus collection study. Then in Section 3, we discuss the taxonomy of route instructions. Section 4 focuses on which categories are important for navigation. In Section 5, we report our results and error analysis on parsing instructions from our corpus and three other corpora containing route instructions, followed by lessons learned and future work. "
W12-1614 "Abstract We provide a systematic study of previously proposed features for implicit discourse relation identification, identifying new feature combinations that optimize F 1 -score. The resulting classifiers achieve the best F 1 -scores to date for the four top-level discourse relation classes of the Penn Discourse Tree Bank: COMPARISON, CONTINGENCY, EXPANSION, and TEMPORAL. We further identify factors for feature extraction that can have a major impact on performance and determine that some features originally proposed for the task no longer provide performance gains in light of more powerful, recently discovered features. Our results constitute a new set of baselines for future studies of implicit discourse relation identification. "
W12-1615 "Abstract Developing sophisticated turn-taking behavior is necessary for next-generation dialogue systems. However, incorporating real users into the development cycle is expensive and current simulation techniques are inadequate. As a foundation for advancing turn-taking behavior, we present a temporal simulator that models the interaction between the user and the system, including speech, voice activity detection, and incremental speech recognition. We describe the details of the simulator and demonstrate it on a sample domain. "
W12-1616 "Abstract In this work we study the effectiveness of speaker adaptation for dialogue act recognition in multiparty meetings. First, we analyze idiosyncracy in dialogue verbal acts by qualitatively studying the differences and conflicts among speakers and by quantitively comparing speaker-specific models. Based on these observations, we propose a new approach for dialogue act recognition based on reweighted domain adaptation which effectively balance the influence of speaker specific and other speakers data. Our experiments on a realworld meeting dataset show that with even only 200 speaker-specific annotated dialogue acts, the performances on dialogue act recognition are significantly improved when compared to several baseline algorithms. To our knowledge, this work is the first 1 to tackle this promising research direction of speaker adaptation for dialogue act recogntion. "
W12-1617 "W.Minker Inst. of Communications Engineering, University of Ulm, Germany wolfgang.minker@ uni-ulm.de Abstract This work investigates to what degree speakers with different verbal intelligence may adapt to each other. The work is based on a corpus consisting of 100 descriptions of a short film (monologues), 56 discussions about the same topic (dialogues), and verbal intelligence scores of the test participants. Adaptation between two dialogue partners was measured using cross-referencing, proportion of I, You and We words, between-subject correlation and similarity of texts. It was shown that lower verbal intelligence speakers repeated more nouns and adjectives from the other and used the same linguistic categories more often than higher verbal intelligence speakers. In dialogues between strangers, participants with higher verbal intelligence showed a greater level of adaptation. "
W12-1618 "<NoAbstract>"
W12-1619 "Abstract We demonstrate a spoken dialogue-based information system for pedestrians. The system is novel in combining geographic information system (GIS) modules such as a visibility engine with a question-answering (QA) system, integrated within a dialogue system architecture. Users of the demonstration system can use a web-based version (simulating pedestrian movement using StreetView) to engage in a variety of interleaved conversations such as navigating from A to B, using the QA functionality to learn more about points of interest (PoI) nearby, and searching for amenities and tourist attractions. This system explores a variety of research questions involving the integration of multiple information sources within conversational interaction. "
W12-1620 "4, USA {morbini,forbell,devault,sagae,traum,rizzo}@ict.usc.edu Abstract We present a mixed initiative conversational dialogue system designed to address primarily mental health care concerns related to military deployment. It is supported by a new information-state based dialogue manager, FLoReS (Forward-Looking, Reward Seeking dialogue manager), that allows both advanced, flexible, mixed initiative interaction, and efficient policy creation by domain experts. To easily reach its target population this dialogue system is accessible as a web application. "
W12-1621 "<NoAbstract>"
W12-1622 "Abstract A coherently related group of sentences may be referred to as a discourse. In this paper we address the problem of parsing coherence relations as defined in the Penn Discourse Tree Bank (PDTB). A good model for discourse structure analysis needs to account both for local dependencies at the token-level and for global dependencies and statistics. We present techniques on using inter-sentential or sentence-level (global), data-driven, nongrammatical features in the task of parsing discourse. The parser model follows up previous approach based on using tokenlevel (local) features with conditional random fields for shallow discourse parsing, which is lacking in structural knowledge of discourse. The parser adopts a twostage approach where first the local constraints are applied and then global constraints are used on a reduced weighted search space (n-best). In the latter stage we experiment with different rerankers trained on the first stage n-best parses, which are generated using lexico-syntactic local features. The two-stage parser yields significant improvements over the best performing model of discourse parser on the PDTB corpus. "
W12-1623 "292, Japan {bachnx,nguyenml,shimazu}@jaist.ac.jp Abstract This paper presents a discriminative reranking model for the discourse segmentation task, the first step in a discourse parsing system. Our model exploits subtree features to rerank Nbest outputs of a base segmenter, which uses syntactic and lexical features in a CRF framework. Experimental results on the RST Discourse Treebank corpus show that our model outperforms existing discourse segmenters in both settings that use gold standard Penn Treebank parse trees and Stanford parse trees. "
W12-1624 ", CA 94043 {araux,dramachandran, rgupta}@hra.com Abstract Many modern spoken dialog systems use probabilistic graphical models to update their belief over the concepts under discussion, increasing robustness in the face of noisy input. However, such models are ill-suited to probabilistic reasoning about spatial relationships between entities. In particular, a car navigation system that infers users intended destination using nearby landmarks as descriptions must be able to use distance measures as a factor in inference. In this paper, we describe a belief tracking system for a location identification task that combines a semantic belief tracker for categorical concepts based on the DPOT framework (Raux and Ma, 2011) with a kernel density estimator that incorporates landmark evidence from multiple turns and landmark hypotheses, into a posterior probability over candidate locations. We evaluate our approach on a corpus of destination setting dialogs and show that it significantly outperforms a deterministic baseline. "
W12-1625 "Abstract Probabilistic models such as Bayesian Networks are now in widespread use in spoken dialogue systems, but their scalability to complex interaction domains remains a challenge. One central limitation is that the state space of such models grows exponentially with the problem size, which makes parameter estimation increasingly difficult, especially for domains where only limited training data is available. In this paper, we show how to capture the underlying structure of a dialogue domain in terms of probabilistic rules operating on the dialogue state. The probabilistic rules are associated with a small, compact set of parameters that can be directly estimated from data. We argue that the introduction of this abstraction mechanism yields probabilistic models that are easier to learn and generalise better than their unstructured counterparts. We empirically demonstrate the benefits of such an approach learning a dialogue policy for a human-robot interaction domain based on a Wizard-of-Oz data set. "
W12-1626 "2 Abstract This paper proposes the use of unsupervised approaches to improve components of partition-based belief tracking systems. The proposed method adopts a dynamic Bayesian network to learn the user action model directly from a machine-transcribed dialog corpus. It also addresses confidence score calibration to improve the observation model in a unsupervised manner using dialog-level grounding information. To verify the effectiveness of the proposed method, we applied it to the Lets Go domain (Raux et al., 2005). Overall system performance for several comparative models were measured. The results show that the proposed method can learn an effective user action model without human intervention. In addition, the calibrated confidence score was verified by demonstrating the positive influence on the user action model learning process and on overall system performance. "
W12-1627 "<NoAbstract>"
W12-1628 "Abstract Models of dialog state are important, both scientifically and practically, but todays best build strongly on tradition. This paper presents a new way to identify the important dimensions of dialog state, more bottomup and empirical than previous approaches. Specifically, we applied Principal Component Analysis to a large number of low-level prosodic features to find the most important dimensions of variation. The top 20 out of 76 dimensions accounted for 81% of the variance, and each of these dimensions clearly related to dialog states and activities, including turn taking, topic structure, grounding, empathy, cognitive processes, attitude and rhetorical structure. "
W12-1629 "Abstract Addressee identification is an element of all language-based interactions, and is critical for turn-taking. We examine the particular problem of identifying when each child playing an interactive game in a small group is speaking to an animated character. After analyzing child and adult behavior, we explore a family of machine learning models to integrate audio and visual features with temporal group interactions and limited, task-independent language. The best model performs identification about 20% better than the model that uses the audio-visual features of the child alone. "
W12-1630 " We evaluate a wizard-of-oz spoken dialogue system that adapts to multiple user affective states in real-time: user disengagement and uncertainty. We compare this version with the prior version of our system, which only adapts to user uncertainty. Our analysis investigates how iteratively adding new affect adaptation to an existing affect-adaptive system impacts global and local performance. We find a significant increase in motivation for users who most frequently received the disengagement adaptation. Moreover, responding to disengagement breaks its negative correlations with task success and user satisfaction, reduces uncertainty levels, and reduces the likelihood of continued disengagement. 1 Int  "
W12-1631 "56, Japan {bessho, harada, kuniyosh}@isi.imi.i.u-tokyo.ac.jp Abstract We propose a dialog system that creates responses based on a large-scale dialog corpus retrieved from Twitter and real-time crowdsourcing. Instead of using complex dialog management, our system replies with the utterance from the database that is most similar to the user input. We also propose a realtime crowdsourcing framework for handling the case in which there is no adequate response in the database. "
W12-1632 "Abstract We present a model for automatically predicting information status labels for German referring expressions. We train a CRF on manually annotated phrases, and predict a fine-grained set of labels. We achieve an accuracy score of 69.56% on our most detailed label set, 76.62% when gold standard coreference is available. "
W12-1633 "-8550, Japan {take,ryu-i}@cl.cs.titech.ac.jp Abstract This paper proposes a probabilistic approach to the resolution of referring expressions for task-oriented dialogue systems. The approach resolves descriptions, anaphora, and deixis in a unified manner. In this approach, the notion of reference domains serves an important role to handle context-dependent attributes of entities and references to sets. The evaluation with the REX-J corpus shows promising results. "
W12-1635 "Abstract Ambiguous or open-ended requests to a dialogue system result in more complex dialogues. We present a semantic-specificity metric to gauge this complexity for dialogue systems that access a relational database. An experiment where a simulated user makes requests to a dialogue system shows that semantic specificity correlates with dialogue length. "
W12-1637 "90094 {leuski,devault}@ict.usc.edu Abstract This paper presents an analysis of how the level of performance achievable by an NLU module can affect the optimal modular design of a dialogue system. We present an evaluation that shows how NLU accuracy levels impact the overall performance of a system that includes an NLU module and a rule-based dialogue policy. We contrast these performance levels with the performance of a direct classification design that omits a separate NLU module. We conclude with a discussion of the potential for a hybrid architecture incorporating the strengths of both approaches. "
W12-1638 "Abstract The goal of this paper is to present a first step toward integrating Incremental Speech Recognition (ISR) and Partially-Observable Markov Decision Process (POMDP) based dialogue systems. The former provides support for advanced turn-taking behavior while the other increases the semantic accuracy of speech recognition results. We present an Incremental Interaction Manager that supports the use of ISR with strictly turn-based dialogue managers. We then show that using a POMDP-based dialogue manager with ISR substantially improves the semantic accuracy of the incremental results. "
W12-1639 "<NoAbstract>"
W12-1640 "Abstract With the aim of investigating how humans understand each other through language and gestures, this paper focuses on how people understand incomplete sentences. We trained a system based on interrupted but resumed sentences, in order to find plausible completions for incomplete sentences. Our promising results are based on multi-modal features. "
W12-1641 "Abstract Participants in a conversation are normally receptive to their surroundings and their interlocutors, even while they are speaking and can, if necessary, adapt their ongoing utterance. Typical dialogue systems are not receptive and cannot adapt while uttering. We present combinable components for incremental natural language generation and incremental speech synthesis and demonstrate the flexibility they can achieve with an example system that adapts to a listeners acoustic understanding problems by pausing, repeating and possibly rephrasing problematic parts of an utterance. In an evaluation, this system was rated as significantly more natural than two systems representing the current state of the art that either ignore the interrupting event or just pause; it also has a lower response time. "
W12-1642 "Abstract We present a novel unsupervised framework for focused meeting summarization that views the problem as an instance of relation extraction. We adapt an existing in-domain relation learner (Chen et al., 2011) by exploiting a set of task-specific constraints and features. We evaluate the approach on a decision summarization task and show that it outperforms unsupervised utterance-level extractive summarization baselines as well as an existing generic relation-extraction-based summarization method. Moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score. "
W12-1701 "o, Canada {libbyb,afsaneh,suzanne}@cs.toronto.edu Abstract Children acquire mental state verbs (MSVs) much later than other, lower-frequency, words. One factor proposed to contribute to this delay is that children must learn various semantic and syntactic cues that draw attention to the difficult-to-observe mental content of a scene. We develop a novel computational approach that enables us to explore the role of such cues, and show that our model can replicate aspects of the developmental trajectory of MSV acquisition. "
W12-1702 " For a given concrete noun concept, humans are usually able to cite properties (e.g., elephant is animal, car has wheels) of that concept; cognitive psychologists have theorised that such properties are fundamental to understanding the abstract mental representation of concepts in the brain. Consequently, the ability to automatically extract such properties would be of enormous benefit to the field of experimental psychology. This paper investigates the use of semi-supervised learning and support vector machines to automatically extract concept-relation-feature triples from two large corpora (Wikipedia and UKWAC) for concrete noun concepts. Previous approaches have relied on manually-generated rules and hand-crafted resources such as WordNet; our method requires neither yet achieves better performance than these prior approaches, measured both by comparison with a property norm-derived gold standard as well as direct human evaluation. Our technique performs particularly well on extracting features relevant to a given concept, and suggests a number of promising areas for future focus. 1 Introductio  "
W12-1703 "Abstract Some of the most robust effects of linguistic variables on eye movements in reading are those of word length. Their leading explanation states that they are caused by visual acuity limitations on word recognition. However, Bicknell (2011) presented data showing that a model of eye movement control in reading that includes visual acuity limitations and models the process of word identification from visual input (Bicknell & Levy, 2010) does not produce humanlike word length effects, providing evidence against the visual acuity account. Here, we argue that uncertainty about word length in early word identification can drive word length effects. We present an extension of Bicknell and Levys model that incorporates word length uncertainty, and show that it produces more humanlike word length effects. "
W12-1704 "69-1020 Abstract We describe a computational framework for language learning and parsing in which dynamical systems navigate on fractal sets. We explore the predictions of the framework in an artificial grammar task in which humans and recurrent neural networks are trained on a language with recursive structure. The results provide evidence for the claim of the dynamical systems models that grammatical systems continuously metamorphose during learning. The present perspective permits structural comparison between the recursive representations in symbolic and neural network models. "
W12-1705 " Probabilistic context-free grammars (PCFGs) are a popular cognitive model of syntax (Jurafsky, 1996). These can be formulated to be sensitive to human working memory constraints by application of a right-corner transform (Schuler, 2009). One side-effect of the transform is that it guarantees at most a single expansion (push) and at most a single reduction (pop) during a syntactic parse. The primary finding of this paper is that this property of right-corner parsing can be exploited to obtain a dramatic reduction in the number of random variables in a probabilistic sequence model parser. This yields a simpler structure that more closely resembles existing simple recurrent network models of sentence comprehension. 1 Introductio  "
W12-1706 "Experimental evidence demonstrates that syntactic structure influences human online sentence processing behavior. Despite this evidence, open questions remain: which type of syntactic structure best explains observed behaviorhierarchical or sequential, and lexicalized or unlexicalized? Recently, Frank and Bod (2011) find that unlexicalized sequential models predict reading times better than unlexicalized hierarchical models, relative to a baseline prediction model that takes wordlevel factors into account. They conclude that the human parser is insensitive to hierarchical syntactic structure. We investigate these claims and find a picture more complicated than the one they present. First, we show that incorporating additional lexical n-gram probabilities estimated from several different corpora into the baseline model of Frank and Bod (2011) eliminates all differences in accuracy between those unlexicalized sequential and hierarchical models. Second, we show that lexicalizing the hierarchical models used in Frank and Bod (2011) significantly improves prediction accuracy relative to the unlexicalized versions. Third, we show that using stateof-the-art lexicalized hierarchical models further improves prediction accuracy. Our results demonstrate that the claim of Frank and Bod (2011) that sequential models predict reading times better than hierarchical models is premature, and also that lexicalization matters for prediction accuracy. "
W12-1707 "Abstract Logical metonymies (The student finished the beer) represent a challenge to compositionality since they involve semantic content not overtly realized in the sentence (covert events  drinking the beer). We present a contrastive study of two classes of computational models for logical metonymy in German, namely a probabilistic and a distributional, similarity-based model. These are built using the SDEWAC corpus and evaluated against a dataset from a self-paced reading and a probe recognition study for their sensitivity to thematic fit effects via their accuracy in predicting the correct covert event in a metonymical context. The similarity-based models allow for better coverage while maintaining the accuracy of the probabilistic models. "
W12-2401 "<NoAbstract>"
W12-2402 "Abstract We describe an open information extraction system for biomedical text based on NELL (the Never-Ending Language Learner) (Carlson et al., 2010), a system designed for extraction from Web text. NELL uses a coupled semi-supervised bootstrapping approach to learn new facts from text, given an initial ontology and a small number of seeds for each ontology category. In contrast to previous applications of NELL, in our task the initial ontology and seeds are automatically derived from existing resources. We show that NELLs bootstrapping algorithm is susceptible to ambiguous seeds, which are frequent in the biomedical domain. Using NELL to extract facts from biomedical text quickly leads to semantic drift. To address this problem, we introduce a method for assessing seed quality, based on a larger corpus of data derived from the Web. In our method, seed quality is assessed at each iteration of the bootstrapping process. Experimental results show significant improvements over NELLs original bootstrapping algorithm on two types of tasks: learning terms from biomedical categories, and named-entity recognition for biomedical entities using a learned lexicon. "
W12-2403 "Abstract The identification of semantically similar linguistic expressions despite their formal difference is an important task within NLP applications (information retrieval and extraction, terminology structuring...) We propose to detect the semantic relatedness between biomedical terms from the pharmacovigilance area. Two approaches are exploited: semantic distance within structured resources and terminology structuring methods applied to a raw list of terms. We compare these methods and study their complementarity. The results are evaluated against the reference pharmacovigilance data and manually by an expert. "
W12-2404 "Abstract We investigate the task of assigning medical events in clinical narratives to discrete time-bins. The time-bins are defined to capture when a medical event occurs relative to the hospital admission date in each clinical narrative. We model the problem as a sequence tagging task using Conditional Random Fields. We extract a combination of lexical, section-based and temporal features from medical events in each clinical narrative. The sequence tagging system outperforms a system that does not utilize any sequence information modeled using a Maximum Entropy classifier. We present results with both handtagged as well as automatically extracted features. We observe over 8% improvement in overall tagging accuracy with the inclusion of sequence information. "
W12-2405 "{Nicola.Stokes, Joe.Carthy, John.Dunnion}@ucd.ie Abstract The growth of digital clinical data has raised questions as to how best to leverage this data to aid the world of healthcare. Promising application areas include Information Retrieval and Question-Answering systems. Such systems require an in-depth understanding of the texts that are processed. One aspect of this understanding is knowing if a medical condition outlined in a patient record is recent, or if it occurred in the past. As well as this, patient records often discuss other individuals such as family members. This presents a second problem determining if a medical condition is experienced by the patient described in the report or some other individual. In this paper, we investigate the suitability of a machine learning (ML) based system for resolving these tasks on a previously unexplored collection of Patient History and Physical Examination reports. Our results show that our novel Score-based feature approach outperforms the standard Linguistic and Contextual features described in the related literature. Specifically, near-perfect performance is achieved in resolving if a patient experienced a condition. While for the task of establishing when a patient experienced a condition, our ML system significantly outperforms the ConText system (87% versus 69% f-score, respectively). "
W12-2406 "Abstract We present an algorithm for extracting abbreviation definitions from biomedical text. Our approach is based on an alignment HMM, matching abbreviations and their definitions. We report 98% precision and 93% recall on a standard data set, and 95% precision and 91% recall on an additional test set. Our results show an improvement over previously reported methods and our model has several advantages. Our model: (1) is simpler and faster than a comparable alignment-based abbreviation extractor; (2) is naturally generalizable to specific types of abbreviations, e.g., abbreviations of chemical formulas; (3) is trained on a set of unlabeled examples; and (4) associates a probability with each predicted definition. Using the abbreviation alignment model we were able to extract over 1.4 million abbreviations from a corpus of 200K full-text PubMed papers, including 455,844 unique definitions. "
W12-2407 "Abstract In the English clinical and biomedical text domains, negation and certainty usage are two well-studied phenomena. However, few studies have made an in-depth characterization of uncertainties expressed in a clinical setting, and compared this between different annotation efforts. This preliminary, qualitative study attempts to 1) create a clinical uncertainty and negation taxonomy, 2) develop a translation map to convert annotation labels from an English schema into a Swedish schema, and 3) characterize and compare two data sets using this taxonomy. We define a clinical uncertainty and negation taxonomy and a translation map for converting annotation labels between two schemas and report observed similarities and differences between the two data sets. "
W12-2408 " As Electronic Health Records are growing exponentially along with large quantities of unstructured clinical information that could be used for research purposes, protecting patient privacy becomes a challenge that needs to be met. In this paper, we present a novel hybrid system designed to improve the current strategies used for person names de-identification. To overcome this task, our system comprises several components designed to accomplish two separate goals: 1) achieve the highest recall (no patient data can be exposed); and 2) create methods to filter out false positives. As a result, our system reached 92.6% F 2 measure when de-identifying person names in Veterans Health Administration clinical notes, and considerably outperformed other existing out-of-the-box de-identification or named entity recognition systems. 1 Introdu  "
W12-2409 " Active learning can lower the cost of annotation for some natural language processing tasks by using a classifier to select informative instances to send to human annotators. It has worked well in cases where the training instances are selected one at a time and require minimal context for annotation. However, coreference annotations often require some context and the traditional active learning approach may not be feasible. In this work we explore various active learning methods for coreference resolution that fit more realistically into coreference annotation workflows. 1 Introdu  "
W12-2410 " Recent efforts in biomolecular event extraction have mainly focused on core event types involving genes and proteins, such as gene expression, protein-protein interactions, and protein catabolism. The BioNLP11 Shared Task extended the event extraction approach to sub-protein events and relations in the Epigenetics and Post-translational Modifications (EPI) and Protein Relations (REL) tasks. In this study, we apply the Turku Event Extraction System, the best-performing system for these tasks, to all PubMed abstracts and all available PMC full-text articles, extracting 1.4M EPI events and 2.2M REL relations from 21M abstracts and 372K articles. We introduce several entity normalization algorithms for genes, proteins, protein complexes and protein components, aiming to uniquely identify these biological entities. This normalization effort allows direct mapping of the extracted events and relations with posttranslational modifications from UniProt, epigenetics from PubMeth, functional domains from InterPro and macromolecular structures from PDB. The extraction of such detailed protein information provides a unique text mining dataset, offering the opportunity to further deepen the information provided by existing PubMed-scale event extraction efforts. The methods and data introduced in this study are freely available from bionlp.utu.fi. "
W12-2411 " The latest discoveries on diseases and their diagnosis/treatment are mostly disseminated in the form of scientific publications. However, with the rapid growth of the biomedical literature and a high level of variation and ambiguity in disease names, the task of retrieving disease-related articles becomes increasingly challenging using the traditional keywordbased approach. An important first step for any disease-related information extraction task in the biomedical literature is the disease mention recognition task. However, despite the strong interest, there has not been enough work done on disease name identification, perhaps because of the difficulty in obtaining adequate corpora. Towards this aim, we created a large-scale disease corpus consisting of 6900 disease mentions in 793 PubMed citations, derived from an earlier corpus. Our corpus contains rich annotations, was developed by a team of 12 annotators (two people per annotation) and covers all sentences in a PubMed abstract. Disease mentions are categorized into Specific Disease, Disease Class, Composite Mention and Modifier categories. When used as the gold standard data for a state-of-the-art machine-learning approach, significantly higher performance can be found on our corpus than the previous one. Such characteristics make this disease name corpus a valuable resource for mining disease-related information from biomedical text. The NCBI corpus is available for download at http://www.ncbi.nlm.nih.gov/CBBresearch/Fe llows/Dogan/disease.html. 1 Introduction  "
W12-2412 " Event extraction is a major focus of recent work in biomedical information extraction. Despite substantial advances, many challenges still remain for reliable automatic extraction of events from text. We introduce a new biomedical event extraction resource consisting of analyses automatically created by systems participating in the recent BioNLP Shared Task (ST) 2011. In providing for the first time the outputs of a broad set of state-ofthe-art event extraction systems, this resource opens many new opportunities for studying aspects of event extraction, from the identification of common errors to the study of effective approaches to combining the strengths of systems. We demonstrate these opportunities through a multi-system analysis on three BioNLP ST 2011 main tasks, focusing on events that none of the systems can successfully extract. We further argue for new perspectives to the performance evaluation of domain event extraction systems, considering a document-level, off-the-page representation and evaluation to complement the mentionlevel evaluations pursued in most recent work. "
W12-2413 "Division of Applied Mathematics M  alardalen University V  aster  as, Sweden Abstract The acquisition of semantic resources and relations is an important task for several applications, such as query expansion, information retrieval and extraction, machine translation. However, their validity should also be computed and indicated, especially for automatic systems and applications. We exploit the compositionality based methods for the acquisition of synonymy relations and of indicators of these synonyms. We then apply pagerank-derived algorithm to the obtained semantic graph in order to filter out the acquired synonyms. Evaluation performed with two independent experts indicates that the quality of synonyms is systematically improved by 10 to 15% after their filtering. "
W12-2414 "Abstract In this paper we explore the applicability of existing coreference resolution systems to a biomedical genre: radiology reports. Analysis revealed that, due to the idiosyncrasies of the domain, both the formulation of the problem of coreference resolution and its solution need significant domain adaptation work. We reformulated the task and developed an unsupervised algorithm based on heuristics for coreference resolution in radiology reports. The algorithm is shown to perform well on a test dataset of 150 manually annotated radiology reports. "
W12-2415 "02453, USA {tet, mshafir, mcrivaro, bborukhov, mmeteer}@brandeis.edu Abstract The goal of this work is to apply NLP techniques to the field of BioNLP in order to gain a better insight into the field and show connections and trends that might not otherwise be apparent. The data we analyzed was the proceedings from last decade of BioNLP workshops. Our findings reveal the prominent research problems and techniques in the field, their progression over time, the approaches that researchers are using to solve those problems, insightful ways to categorize works in the field, and the prominent researchers and groups whose works are influencing the field. "
W12-2416 "<NoAbstract>"
W12-2417 " The application of natural language processing (NLP) in the biology and medical domain crosses many fields from Healthcare Information to Bioinformatics to NLP itself. In order to make sense of how these fields relate and intersect, we have created MedLingMap (www.medlingmap.org) which is a compilation of references with a multi-faceted index. The initial focus has been creating the infrastructure and populating it with references annotated with facets such as topic, resources used (ontologies, tools, corpora), and organizations. Simultaneously we are applying NLP techniques to the text to find clusters, key terms and other relationships. The goal for this paper is to introduce MedLingMap to the community and show how it can be a powerful tool for research and exploration in the field. 1 Introductio  "
W12-2418 " Many genetic epidemiological studies of human diseases have multiple variables related to any given phenotype, resulting from different definitions and multiple measurements or subsets of data. Manually mapping and harmonizing these phenotypes is a timeconsuming process that may still miss the most appropriate variables. Previously, a supervised learning algorithm was proposed for this problem. That algorithm learns to determine whether a pair of phenotypes is in the same class. Though that algorithm accomplished satisfying F-scores, the need to manually label training examples becomes a bottleneck to improve its coverage. Herein we present a novel active learning solution to solve this challenging phenotype-mapping problem. Active learning will make phenotype mapping more efficient and improve its accuracy. 1 Introductio  "
W12-2419 "Abstract Block-LDA is a topic modeling approach to perform data fusion between entity-annotated text documents and graphs with entity-entity links. We evaluate Block-LDA in the yeast biology domain by jointly modeling PubMed R articles and yeast protein-protein interaction networks. The topic coherence of the emergent topics and the ability of the model to retrieve relevant scientific articles and proteins related to the topic are compared to that of a text-only approach that does not make use of the protein-protein interaction matrix. Evaluation of the results by biologists show that the joint modeling results in better topic coherence and improves retrieval performance in the task of identifying top related papers and proteins. "
W12-2420 "Abstract This paper presents a machine learning approach that selects and, more generally, ranks sentences containing clear relations between genes and terms that are related to them. This is treated as a binary classification task, where preference judgments are used to learn how to choose a sentence from a pair of sentences. Features to capture how the relationship is described textually, as well as how central the relationship is in the sentence, are used in the learning process. Simplification of complex sentences into simple structures is also applied for the extraction of the features. We show that such simplification improves the results by up to 13%. We conducted three different evaluations and we found that the system significantly outperforms the baselines. "
W12-2421 "ridge UK {yan, jhkim, croset, rebholz}@ebi.ac.uk Abstract The relationship between small molecules and proteins has attracted attention from the biomedical research community. In this paper a text mining method of extracting smallmolecule and protein pairs from natural text is presented, based on a semi-supervised machine learning approach. The technique has been applied to the complete collection of MEDLINE abstracts and pairs were extracted and evaluated. The results show the feasibility of the bootstrapping system, which will subsequently be further investigated and improved. "
W12-2422 "Abstract Evidence Based Medicine (EBM) is the practice of using the knowledge gained from the best medical evidence to make decisions in the effective care of patients. This medical evidence is extracted from medical documents such as research papers. The increasing number of available medical documents has imposed a challenge to identify the appropriate evidence and to access the quality of the evidence. In this paper, we present an approach for the automatic grading of evidence using the dataset provided by the 2011 Australian Language Technology Association (ALTA) shared task competition. With the feature sets extracted from publication types, Medical Subject Headings (MeSH), title, and body of the abstracts, we obtain a 73.77% grading accuracy with a stacking based approach, a considerable improvement over previous work. "
W12-2423 " Gene name identification is a fundamental step to solve more complicated text mining problems such as gene normalization and protein-protein interactions. However, state-ofthe-art name identification methods are not yet sufficient for use in a fully automated system. In this regard, a relaxed task, gene/protein sentence identification, may serve more effectively for manually searching and browsing biomedical literature. In this paper, we set up a new task, gene/protein sentence classification and propose an ensemble approach for addressing this problem. Wellknown named entity tools use similar goldstandard sets for training and testing, which results in relatively poor performance for unknown sets. We here explore how to combine diverse high-precision gene identifiers for more robust performance. The experimental results show that the proposed approach outperforms BANNER as a stand-alone classifier for newly annotated sets as well as previous gold-standard sets. "
W12-2424 "Abstract Datasets that answer difficult clinical questions are expensive in part due to the need for medical expertise and patient informed consent. We investigate the effect of small sample size on the performance of a text categorization algorithm. We show how to determine whether the dataset is large enough to train support vector machines. Since it is not possible to cover all aspects of sample size calculation in one manuscript, we focus on how certain types of data relate to certain properties of support vector machines. We show that normal vectors of decision hyperplanes can be used for assessing reliability and internal cross-validation can be used for assessing stability of small sample data. "
W12-2425 " There has been an active development of corpora and annotations in the BioNLP community. As those resources accumulate, a new issue arises about the reusability. As a solution to improve the reusability of corpora and annotations, we present PubAnnotation, a persistent and sharable repository, where various corpora and annotations can be stored together in a stable and comparable way. As a position paper, it explains the motivation and the core concepts of the repository and presents a prototype repository as a proof-of-concept. 1 Introdu  "
W12-2426 " The package insert (aka drug product label) is the only publicly-available source of information on drug-drug interactions (DDIs) for some drugs, especially newer ones. Thus, an automated method for identifying DDIs in drug package inserts would be a potentially important complement to methods for identifying DDIs from other sources such as the scientific literature. To develop such an algorithm, we created a corpus of Federal Drug Administration approved drug package insert statements that have been manually annotated for pharmacokinetic DDIs by a pharmacist and a drug information expert. We then evaluated three different machine learning algorithms for their ability to 1) identify pharmacokinetic DDIs in the package insert corpus and 2) classify pharmacokinetic DDI statements by their modality (i.e., whether they report a DDI or no interaction between drug pairs). Experiments found that a support vector machine algorithm performed best on both tasks with an F-measure of 0.859 for pharmacokinetic DDI identification and 0.949 for modality assignment. We also found that the use of syntactic information is very helpful for addressing the problem of sentences containing both interacting and non-interacting pairs of drugs. 1 Introduct  "
W12-2427 "Publications that report genotype-drug interaction findings, as well as manually curated databases such as DrugBank and PharmGKB are essential to advancing pharmacogenomics, a relatively new area merging pharmacology and genomic research. Natural language processing (NLP) methods can be very useful for automatically extracting knowledge such as gene-drug interactions, offering researchers immediate access to published findings, and allowing curators a shortcut for their work. We present a corpus of gene-drug interactions for evaluating and training systems to extract those interactions. The corpus includes 551 sentences that have a mention of a drug and a gene from about 600 journals found to be relevant to pharmacogenomics through an analysis of gene-drug relationships in the PharmGKB knowledgebase. We evaluated basic approaches to automatic extraction, including gene and drug cooccurrence, co-occurrence plus interaction terms, and a linguistic pattern-based method. The linguistic pattern method had the highest precision (96.61%) but lowest recall (7.30%), for an f-score of 13.57%. Basic co-occurrence yields 68.99% precision, with the addition of an interaction term precision increases slightly (69.60%), though not as much as could be expected. Co-occurrence is a reasonable baseline method, with pattern-based being a promising approach if enough patterns can be generated to address recall. The corpus is available at http://diego.asu.edu/index.php/projects  http://diego.asu.edu/index.php/projects  "
W12-2428 "<NoAbstract>"
W12-2429 "ted Kingdom {W.Cheng, J.Preiss, M.Stevenson}@dcs.shef.ac.uk Abstract The most accurate approaches to Word Sense Disambiguation (WSD) for biomedical documents are based on supervised learning. However, these require manually labeled training examples which are expensive to create and consequently supervised WSD systems are normally limited to disambiguating a small set of ambiguous terms. An alternative approach is to create labeled training examples automatically and use them as a substitute for manually labeled ones. This paper describes a large scale WSD system based on automatically labeled examples generated using information from the UMLS Metathesaurus. The labeled examples are generated without any use of labeled training data whatsoever and is therefore completely unsupervised (unlike some previous approaches). The system is evaluated on two widely used data sets and found to outperform a state-of-the-art unsupervised approach which also uses information from the UMLS Metathesaurus. "
W12-2901 "Abstract We present a simple tool that enables the computer to read subtitles of movies and TV shows aloud. The tool extracts information from subtitle files, which can be freely downloaded from the Internet, and reads the text aloud through a speech synthesizer. There are three versions of the tool, one for Windows and Linux, another for Mac OS X, and the third is a browser-based HTML5 prototype. The tools are freely available and open-source. The target audience is people who have trouble reading subtitles while watching a movie, including elderly, people with visual impairments, people with reading difficulties and people who wants to learn a second language. The application is currently being evaluated together with user from these groups. "
W12-2902 "blin, Ireland {eva.szekely|zeeshan.ahmed}@ucdconnect.ie,{joao.cabral|julie.berndsen}@ucd.ie Abstract This paper describes a demonstration of the WinkTalk system, which is a speech synthesis platform using expressive synthetic voices. With the help of a webcamera and facial expression analysis, the system allows the user to control the expressive features of the synthetic speech for a particular utterance with their facial expressions. Based on a personalised mapping between three expressive synthetic voices and the users facial expressions, the system selects a voice that matches their face at the moment of sending a message. The WinkTalk system is an early research prototype that aims to demonstrate that facial expressions can be used as a more intuitive control over expressive speech synthesis than manual selection of voice types, thereby contributing to an improved communication experience for users of speech generating devices. "
W12-2903 "Abstract This paper presents a method for an AAC system to predict a whole response given features of the previous utterance from the interlocutor. It uses a large corpus of scripted dialogs, computes a variety of lexical, syntactic and whole phrase features for the previous utterance, and predicts features that the response should have, using an entropy-based measure. We evaluate the system on a held-out portion of the corpus. We find that for about 3.5% of cases in the held-out corpus, we are able to predict a response, and among those, over half are either exact or at least reasonable substitutes for the actual response. We also present some results on keystroke savings. Finally we compare our approach to a state-of-the-art chatbot, and show (not surprisingly) that a system like ours, tuned for a particular style of conversation, outperforms one that is not. Predicting possible responses automatically by mining a corpus of dialogues is a novel contribution to the literature on whole utterance-based methods in AAC. Also useful, we believe, is our estimate that about 3.5-4.0% of utterances in dialogs are in principle predictable given previous context. "
W12-2904 "<NoAbstract>"
W12-2905 "Abstract Most icon-based augmentative and alternative communication (AAC) devices require users to formulate messages in syntactic order in order to produce syntactic utterances. Reliance on syntactic ordering, however, may not be appropriate for individuals with limited or emerging literacy skills. Some of these users may benefit from unordered message formulation accompanied by automatic message expansion to generate syntactically correct messages. Facilitating communication via unordered message formulation, however, requires new methods of prediction. This paper describes a novel approach to word prediction using semantic grams, or sem-grams, which provide relational information about message components regardless of word order. Performance of four word-level prediction algorithms, two based on sem-grams and two based on n-grams, were compared on a corpus of informal blogs. Results showed that sem-grams yield accurate word prediction, but lack prediction coverage. Hybrid methods that combine n-gram and sem-gram approaches may be viable for unordered prediction in AAC. "
W12-2906 " The number of people with dementia of the Alzheimer's type (DAT) continues to grow. One of the significant impacts of this disease is a decline in the ability to communicate using natural language. This decline in language facility often results in decreased social interaction and life satisfaction for persons with DAT and their caregivers. One possible strategy to lessen the effects of this loss of language facility is for the unaffected conversational partner (Facilitator) to \"co-construct\" short autobiographical stories from the life of the DATaffected conversational partner (Storyteller). It has been observed that a skilled conversational partner can facilitate co-constructed narrative with individuals who have mild to moderate DAT. Developing a computational model of this type of co-constructed narrative would enable assistive technology to be developed that can monitor a conversation between a Storyteller and Facilitator. This technology could provide context-sensitive suggestions to an unskilled Facilitator to help maintain the flow of conversation. This paper describes a framework in which the necessary computational model of co-constructed narrative can be developed. An analysis of the fundamental elements of such a model will be presented. 1 Introductio  "
W12-2907 "a Canada Abstract Currently, health care costs associated with aging at home can be prohibitive if individuals require continual or periodic supervision or assistance because of Alzheimers disease. These costs, normally associated with human caregivers, can be mitigated to some extent given automated systems that mimic some of their functions. In this paper, we present inaugural work towards producing a generic automated system that assists individuals with Alzheimers to complete daily tasks using verbal communication. Here, we show how to improve rates of correct speech recognition by preprocessing acoustic noise and by modifying the vocabulary according to the task. We conclude by outlining current directions of research including specialized grammars and automatic detection of confusion. "
W12-2908 " Tactile maps are important substitutes for visual maps for blind and visually impaired people and the efficiency of tactile-map reading can largely be improved by giving assisting utterances that make use of spatial language. In this paper, we elaborate earlier ideas for a system that generates such utterances and present a prototype implementation based on a semantic conceptualization of the movements that the map user performs. A worked example shows the plausibility of the solution and the output that the prototype generates given input derived from experimental data. 1 Introdu  "
W12-2909 "<NoAbstract>"
W12-3101 " Human assessment is often considered the gold standard in evaluation of translation systems. But in order for the evaluation to be meaningful, the rankings obtained from human assessment must be consistent and repeatable. Recent analysis by Bojar et al. (2011) raised several concerns about the rankings derived from human assessments of English-Czech translation systems in the 2010 Workshop on Machine Translation. We extend their analysis to all of the ranking tasks from 2010 and 2011, and show through an extension of their reasoning that the ranking is naturally cast as an instance of finding the minimum feedback arc set in a tournament, a wellknown NP-complete problem. All instances of this problem in the workshop data are efficiently solvable, but in some cases the rankings it produces are surprisingly different from the ones previously published. This leads to strong caveats and recommendations for both producers and consumers of these rankings. 1 Introductio  "
W12-3102 "<NoAbstract>"
W12-3103 " This paper describes the system used for our participation in the WMT12 Machine Translation evaluation shared task. We also present a new approach to Machine Translation evaluation based on the recently defined task Semantic Textual Similarity. This problem is addressed using a textual entailment engine entirely based on WordNet semantic features. We described results for the Spanish-English, Czech-English and German-English language pairs according to our submission on the Eight Workshop on Statistical Machine Translation. Our first experiments reports a competitive score to system level. 1 Int  "
W12-3104 " A recent paper described a new machine translation evaluation metric, AMBER. This paper describes two changes to AMBER. The first one is incorporation of a new ordering penalty; the second one is the use of the downhill simplex algorithm to tune the weights for the components of AMBER. We tested the impact of the two changes, using data from the WMT metrics task. Each of the changes by itself improved the performance of AMBER, and the two together yielded even greater improvement, which in some cases was more than additive. The new version of AMBER clearly outperforms BLEU in terms of correlation with human judgment.  "
W12-3105 "Abstract We present TerrorCat, a submission to the WMT12 metrics shared task. TerrorCat uses frequencies of automatically obtained translation error categories as base for pairwise comparison of translation hypotheses, which is in turn used to generate a score for every translation. The metric shows high overall correlation with human judgements on the system level and more modest results on the level of individual sentences. "
W12-3106 "Abstract We investigate the use of error classification results for automatic evaluation of machine translation output. Five basic error classes are taken into account: morphological errors, syntactic (reordering) errors, missing words, extra words and lexical errors. In addition, linear combinations of these categories are investigated. Correlations between the class error rates and human judgments are calculated on the data of the third, fourth, fifth and sixth shared tasks of the Statistical Machine Translation Workshop. Machine translation outputs in five different European languages are used: English, Spanish, French, German and Czech. The results show that the following combinations are the most promising: the sum of all class error rates, the weighted sum optimised for translation into English and the weighted sum optimised for translation from English. "
W12-3107 "05 USA {mengqiu,manning}@cs.stanford.edu Abstract This paper describes Stanford Universitys submission to the Shared Evaluation Task of WMT 2012. Our proposed metric (SPEDE) computes probabilistic edit distance as predictions of translation quality. We learn weighted edit distance in a probabilistic finite state machine (pFSM) model, where state transitions correspond to edit operations. While standard edit distance models cannot capture long-distance word swapping or cross alignments, we rectify these shortcomings using a novel pushdown automaton extension of the pFSM model. Our models are trained in a regression framework, and can easily incorporate a rich set of linguistic features. Evaluated on two different prediction tasks across a diverse set of datasets, our methods achieve state-of-the-art correlation with human judgments. "
W12-3108 " We describe a submission to the WMT12 Quality Estimation task, including an extensive Machine Learning experimentation. Data were augmented with features from linguistic analysis and statistical features from the SMT search graph. Several Feature Selection algorithms were employed. The Quality Estimation problem was addressed both as a regression task and as a discretised classification task, but the latter did not generalise well on the unseen testset. The most successful regression methods had an RMSE of 0.86 and were trained with a feature set given by Correlation-based Feature Selection. Indications that RMSE is not always sufficient for measuring performance were observed. 1 Introduct  "
W12-3109 "<NoAbstract>"
W12-3110 "Abstract This paper describes a study on the contribution of linguistically-informed features to the task of quality estimation for machine translation at sentence level. A standard regression algorithm is used to build models using a combination of linguistic and non-linguistic features extracted from the input text and its machine translation. Experiments with EnglishSpanish translations show that linguistic features, although informative on their own, are not yet able to outperform shallower features based on statistics from the input text, its translation and additional corpora. However, further analysis suggests that linguistic information is actually useful but needs to be carefully combined with other features in order to produce better results. "
W12-3111 "<NoAbstract>"
W12-3112 "<NoAbstract>"
W12-3113 " In this paper we present the system we submitted to the WMT12 shared task on Quality Estimation. Each translated sentence is given a score between 1 and 5. The score is obtained using several numerical or boolean features calculated according to the source and target sentences. We perform a linear regression of the feature space against scores in the range [1:5]. To this end, we use a Support Vector Machine. We experiment with two kernels: linear and radial basis function. In our submission we use the features from the shared task baseline system and our own features. This leads to 66 features. To deal with this large number of features, we propose an in-house feature selection algorithm. Our results show that a lot of information is already present in baseline features, and that our feature selection algorithm discards features which are linearly correlated. 1 Introduction  "
W12-3114 "Abstract We present the approach we took for our participation to the WMT12 Quality Estimation Shared Task: our main goal is to achieve reasonably good results without appeal to supervised learning. We have used various similarity measures and also an external resource (Google N -grams). Details of results clarify the interest of such an approach. "
W12-3115 "Meritxell Gonz  alez Llu  s M` arquez Universitat Polit` ecnica de Catalunya, Barcelona {pighin,mgonzalez,lluism}@lsi.upc.edu Abstract In this paper, we describe the UPC system that participated in the WMT 2012 shared task on Quality Estimation for Machine Translation. Based on the empirical evidence that fluencyrelated features have a very high correlation with post-editing effort, we present a set of features for the assessment of quality estimation for machine translation designed around different kinds of n-gram language models, plus another set of features that model the quality of dependency parses automatically projected from source sentences to translations. We document the results obtained on the shared task dataset, obtained by combining the features that we designed with the baseline features provided by the task organizers. "
W12-3116 "Abstract We present a method we used for the quality estimation shared task of WMT 2012 involving IBM1 and language model scores calculated on morphemes and POS tags. The IBM1 scores calculated on morphemes and POS-4grams of the source sentence and obtained translation output are shown to be competitive with the classic evaluation metrics for ranking of translation systems. Since these scores do not require any reference translations, they can be used as features for the quality estimation task presenting a connection between the source language and the obtained target language. In addition, target language model scores of morphemes and POS tags are investigated as estimates for the obtained target language quality. "
W12-3117 "Abstract This paper describes the features and the machine learning methods used by Dublin City University (DCU) and SYMANTEC for the WMT 2012 quality estimation task. Two sets of features are proposed: one constrained, i.e. respecting the data limitation suggested by the workshop organisers, and one unconstrained, i.e. using data or tools trained on data that was not provided by the workshop organisers. In total, more than 300 features were extracted and used to train classifiers in order to predict the translation quality of unseen data. In this paper, we focus on a subset of our feature set that we consider to be relatively novel: features based on a topic model built using the Latent Dirichlet Allocation approach, and features based on source and target language syntax extracted using part-of-speech (POS) taggers and parsers. We evaluate nine feature combinations using four classification-based and four regression-based machine learning techniques. "
W12-3118 "CA, USA {rsoricut,nbach,zwang}@sdl.com Abstract We present in this paper the system submissions of the SDL Language Weaver team in the WMT 2012 Quality Estimation shared-task. Our MT quality-prediction systems use machine learning techniques (M5P regression-tree and SVM-regression models) and a feature-selection algorithm that has been designed to directly optimize towards the official metrics used in this shared-task. The resulting submissions placed 1st (the M5P model) and 2nd (the SVM model), respectively, on both the Ranking task and the Scoring task, out of 11 participating teams. "
W12-3119 "Abstract We in this paper describe the regression system for our participation in the quality estimation task of WMT12. This paper focuses on exploiting special phrases, or word sequences, to estimate translation quality. Several feature templates on this topic are put forward subsequently. We train a SVM regression model for predicting the scores and numerical results show the effectiveness of our phrase indicators and method in both ranking and scoring tasks. "
W12-3120 ", France {firstname.lastname}@limsi.fr Abstract This paper describes our work with the data distributed for the WMT12 Confidence Estimation shared task. Our contribution is twofold: i) we first present an analysis of the data which highlights the difficulty of the task and motivates our approach; ii) we show that using non-linear models, namely random forests, with a simple and limited feature set, succeeds in modeling the complex decisions required to assess translation quality and achieves results that are on a par with the second best results of the shared task. "
W12-3121 "Abstract This paper presents techniques for referencefree, automatic prediction of Machine Translation output quality at both sentenceand document-level. In addition to helping with document-level quality estimation, sentencelevel predictions are used for system selection, improving the quality of the output translations. We present three system selection techniques and perform evaluations that quantify the gains across multiple domains and language pairs. "
W12-3122 "Fondazione Bruno Kessler, FBK-irst Trento , Italy {mehdad|negri|federico}@fbk.eu Abstract We address two challenges for automatic machine translation evaluation: a) avoiding the use of reference translations, and b) focusing on adequacy estimation. From an economic perspective, getting rid of costly hand-crafted reference translations (a) permits to alleviate the main bottleneck in MT evaluation. From a system evaluation perspective, pushing semantics into MT (b) is a necessity in order to complement the shallow methods currently used overcoming their limitations. Casting the problem as a cross-lingual textual entailment application, we experiment with different benchmarks and evaluation settings. Our method shows high correlation with human judgements and good results on all datasets without relying on reference translations. "
W12-3123 "Abstract Post-editing performed by translators is an increasingly common use of machine translated texts. While high quality MT may increase productivity, post-editing poor translations can be a frustrating task which requires more effort than translating from scratch. For this reason, estimating whether machine translations are of sufficient quality to be used for post-editing and finding means to reduce post-editing effort are an important field of study. Post-editing effort consists of different aspects, of which temporal effort, or the time spent on post-editing, is the most visible and involves not only the technical effort needed to perform the editing, but also the cognitive effort required to detect and plan necessary corrections. Cognitive effort is difficult to examine directly, but ways to reduce the cognitive effort in particular may prove valuable in reducing the frustration associated with postediting work. In this paper, we describe an experiment aimed at studying the relationship between technical post-editing effort and cognitive post-editing effort by comparing cases where the edit distance and a manual score reflecting perceived effort differ. We present results of an error analysis performed on such sentences and discuss the clues they may provide about edits requiring great cognitive effort compared to the technical effort, on one hand, or little cognitive effort, on the other. "
W12-3124 "on Street, Cambridge, MA 02138 {smatsouk,bzhang}@bbn.com Abstract Confusion network decoding has proven to be one of the most successful approaches to machine translation system combination. The hypothesis alignment algorithm is a crucial part of building the confusion networks and many alternatives have been proposed in the literature. This paper describes a systematic comparison of five well known hypothesis alignment algorithms for MT system combination via confusion network decoding. Controlled experiments using identical pre-processing, decoding, and weight tuning methods on standard system combination evaluation sets are presented. Translation quality is assessed using case insensitive BLEU scores and bootstrapping is used to establish statistical significance of the score differences. All aligners yield significant BLEU score gains over the best individual system included in the combination. Incremental indirect hidden Markov model and a novel incremental inversion transduction grammar with flexible matching consistently yield the best translation quality, though keeping all things equal, the differences between aligners are relatively small.  The work reported in this paper was carried out while the authors were at Raytheon BBN Technologies and  RWTH Aachen University. "
W12-3125 "<NoAbstract>"
W12-3126 "Abstract Statistical phrase-based machine translation requires no linguistic information beyond word-aligned parallel corpora (Zens et al., 2002; Koehn et al., 2003). Unfortunately, this linguistic agnosticism often produces ungrammatical translations. Syntax, or sentence structure, could provide guidance to phrasebased systems, but the non-constituent word strings that phrase-based decoders manipulate complicate the use of most recursive syntactic tools. We address these issues by using Combinatory Categorial Grammar, or CCG, (Steedman, 2000), which has a much more flexible notion of constituency, thereby providing more labels for putative nonconstituent multiword translation phrases. Using CCG parse charts, we train a syntactic analogue of a lexicalized reordering model by labelling phrase table entries with multiword labels and demonstrate significant improvements in translating between Urdu and English, two language pairs with divergent sentence structure. "
W12-3127 "rsity Abstract Adding syntactic labels to synchronous context-free translation rules can improve performance, but labeling with phrase structure constituents, as in GHKM (Galley et al., 2004), excludes potentially useful translation rules. SAMT (Zollmann and Venugopal, 2006) introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar. We introduce a labeling scheme based on categorial grammar, which allows syntactic labeling of many rules with a minimal, well-motivated label set. We show that our labeling scheme performs comparably to SAMT on an UrduEnglish translation task, yet the label set is an order of magnitude smaller, and translation is twice as fast. "
W12-3128 "Abstract Chiangs hierarchical phrase-based (HPB) translation model advances the state-of-the-art in statistical machine translation by expanding conventional phrases to hierarchical phrases  phrases that contain sub-phrases. However, the original HPB model is prone to overgeneration due to lack of linguistic knowledge: the grammar may suggest more derivations than appropriate, many of which may lead to ungrammatical translations. On the other hand, limitations of glue grammar rules in the original HPB model may actually prevent systems from considering some reasonable derivations. This paper presents a simple but effective translation model, called the Head-Driven HPB (HD-HPB) model, which incorporates head information in translation rules to better capture syntax-driven information in a derivation. In addition, unlike the original glue rules, the HD-HPB model allows improved reordering between any two neighboring non-terminals to explore a larger reordering search space. An extensive set of experiments on Chinese-English translation on four NIST MT test sets, using both a small and a large training set, show that our HDHPB model consistently and statistically significantly outperforms Chiangs model as well as a source side SAMT-style model. "
W12-3129 "Abstract We introduce the first fully automatic, fully semantic frame based MT evaluation metric, MEANT, that outperforms all other commonly used automatic metrics in correlating with human judgment on translation adequacy. Recent work on HMEANT, which is a human metric, indicates that machine translation can be better evaluated via semantic frames than other evaluation paradigms, requiring only minimal effort from monolingual humans to annotate and align semantic frames in the reference and machine translations. We propose a surprisingly effective Occams razor automation of HMEANT that combines standard shallow semantic parsing with a simple maximum weighted bipartite matching algorithm for aligning semantic frames. The matching criterion is based on lexical similarity scoring of the semantic role fillers through a simple context vector model which can readily be trained using any publicly available large monolingual corpus. Sentence level correlation analysis, following standard NIST MetricsMATR protocol, shows that this fully automated version of HMEANT achieves significantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR, PER, CDER, WER, or TER. Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. Despite its high performance, fully automated MEANT is still able to preserve HMEANTs virtues of simplicity, representational transparency, and inexpensiveness. "
W12-3130 "Charles University in Prague, Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics Malostransk  e n  am. 25, Praha 1, CZ-118 00, Czech Republic {bojar,jawaid,kamran}@ufal.mff.cuni.cz Abstract We introduce a taxonomy of factored phrasebased translation scenarios and conduct a range of experiments in this taxonomy. We point out several common pitfalls when designing factored setups. The paper also describes our WMT12 submissions CU-BOJAR and CU-POOR-COMB. "
W12-3131 "213, USA {mdenkows,ghannema,alavie}@cs.cmu.edu Abstract This paper describes the French-English translation system developed by the Avenue research group at Carnegie Mellon University for the Seventh Workshop on Statistical Machine Translation (NAACL WMT12). We present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building. "
W12-3132 ", Martin Majlis, Michal Novak, and David Mare cek Charles University in Prague, Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics Malostranske nam esti 25, Prague {odusek,zabokrtsky,popel,majlis,mnovak,marecek}@ufal.mff.cuni.cz Abstract One of the most notable recent improvements of the TectoMT English-to-Czech translation is a systematic and theoretically supported revision of formemesthe annotation of morpho-syntactic features of content words in deep dependency syntactic structures based on the Prague tectogrammatics theory. Our modifications aim at reducing data sparsity, increasing consistency across languages and widening the usage area of this markup. Formemes can be used not only in MT, but in various other NLP tasks. "
W12-3133 "TALP Research Centre Universitat Polit` ecnica de Catalunya Barcelona, Spain {lluis.formiga,carlos.henriquez,adolfo.hernandez jose.marino,enric.monte,jose.fonollosa}@upc.edu Abstract This paper describes the UPC participation in the WMT 12 evaluation campaign. All systems presented are based on standard phrasebased Moses systems. Variations adopted several improvement techniques such as morphology simplification and generation and domain adaptation. The morphology simplification overcomes the data sparsity problem when translating into morphologicallyrich languages such as Spanish by translating first to a morphology-simplified language and secondly leave the morphology generation to an independent classification task. The domain adaptation approach improves the SMT system by adding new translation units learned from MT-output and reference alignment. Results depict an improvement on TER, METEOR, NIST and BLEU scores compared to our baseline system, obtaining on the official test set more benefits from the domain adaptation approach than from the morphological generalization method. "
W12-3134 "University Abstract We present Joshua 4.0, the newest version of our open-source decoder for parsing-based statistical machine translation. The main contributions in this release are the introduction of a compact grammar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization, J-PRO. We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases. "
W12-3135 " We present a variant of phrase-based SMT that uses source-side parsing and a constituent reordering model based on word alignments in the word-aligned training corpus to predict hierarchical block-wise reordering of the input. Multiple possible translation orders are represented compactly in a source order lattice. This source order lattice is then annotated with phrase-level translations to form a lattice of tokens in the target language. Various feature functions are combined in a log-linear fashion to evaluate paths through that lattice. 1 Intro  "
W12-3136 ", floor 10, PO box 5825 Doha, Qatar {fguzman,pnakov,ahawad,svogel}@qf.org.qa Abstract We describe the systems developed by the team of the Qatar Computing Research Institute for the WMT12 Shared Translation Task. We used a phrase-based statistical machine translation model with several non-standard settings, most notably tuning data selection and phrase table combination. The evaluation results show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English. "
W12-3137 "Abstract This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the translation task of the NAACL 2012 Seventh Workshop on Statistical Machine Translation (WMT 2012). We participated in the evaluation campaign for the French-English and German-English language pairs in both translation directions. Both hierarchical and phrase-based SMT systems are applied. A number of different techniques are evaluated, including an insertion model, different lexical smoothing methods, a discriminative reordering extension for the hierarchical system, reverse translation, and system combination. By application of these methods we achieve considerable improvements over the respective baseline systems. "
W12-3138 "Abstract We describe a substitution-based system for hybrid machine translation (MT) that has been extended with machine learning components controlling its phrase selection. The approach is based on a rule-based MT (RBMT) system which creates template translations. Based on the rule-based generation parse tree and target-to-target alignments, we identify the set of interesting translation candidates from one or more translation engines which could be substituted into our translation templates. The substitution process is either controlled by the output from a binary classifier trained on feature vectors from the different MT engines, or it is depending on weights for the decision factors, which have been tuned using MERT. We are able to observe improvements in terms of BLEU scores over a baseline version of the hybrid system. "
W12-3139 "ingdom {pkoehn,bhaddow}@inf.ed.ac.uk Abstract We report on findings of exploiting large data sets for translation modeling, language modeling and tuning for the development of competitive machine translation systems for eight language pairs. 1 Introduction We report on experiments carried out for the development of competitive systems on the datasets of the 2012 Workshop on Statistical Machine Translation. Our main focus was directed on the effective use of all the available training data during training of translation and language models and tuning. We use the open source machine translation system Moses (Koehn et al., 2007) and other standard open source tools, hence all our experiments are straightforwardly replicable 1 . Compared to all single system submissions by participants of the workshop we achieved the best BLEU scores for four language pairs (es-en, en-es, cs-en, en-cs), the 2 nd best results for two language pairs (fr-en, de-en), as well as a 3 rd place (en-de) and a 5 th place (en-fr) for the remaining pairs. We improved upon this in the post-evaluation period for some of the language pairs by more systematically applying our methods. During the development of our system, we saw most gains from using large corpora for translation model training, especially when using subsampling techniques for out-of-domain sets, using large corpora for language model training, and larger tuning sets. We also observed mixed results with alternative tuning methods. We also experimented with hierarchical models and semi-supervised training, but did not achieve any improvements. 1 Configuration files and instructions are available at http: //www.statmt.org/wmt12/uedin/.  "
W12-3140 " This paper describes the joint QUAERO submission to the WMT 2012 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT GermanEnglish task. Each group translated the data sets with their own systems and finally the RWTH system combination combined these translations in our final submission. Experimental results show improvements of up to 1.7 points in BLEU and 3.4 points in TER compared to the best single system. 1 Introdu  "
W12-3141 "1,2 , Franc  ois Yvon 1,2 Univ. Paris-Sud 1 and LIMSI-CNRS 2 rue John von Neumann, 91403 Orsay cedex, France {firstname.lastname}@limsi.fr Abstract This paper describes LIMSIs submissions to the shared translation task. We report results for French-English and German-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams. In this approach, both the translation and target language models are estimated as conventional smoothed n-gram models; an approach we extend here by estimating the translation probabilities in a continuous space using neural networks. Experimental results show a significant and consistent BLEU improvement of approximately 1 point for all conditions. We also report preliminary experiments using an on-the-fly translation model. "
W12-3142 " This paper describes the UPM system for the Spanish-English translation task at the NAACL 2012 workshop on statistical machine translation. This system is based on Moses. We have used all available free corpora, cleaning and deleting some repetitions. In this paper, we also propose a technique for selecting the sentences for tuning the system. This technique is based on the similarity with the sentences to translate. With our approach, we improve the BLEU score from 28.37% to 28.57%. And as a result of the WMT12 challenge we have obtained a 31.80% BLEU with the 2012 test set. Finally, we explain different experiments that we have carried out after the competition. 1 Intro  "
W12-3143 " This paper describes the PROMT submission for the WMT12 shared translation task. We participated in two language pairs: EnglishFrench and English-Spanish. The translations were made using the PROMT DeepHybrid engine, which is the first hybrid version of the PROMT system. We report on improvements over our baseline RBMT output both in terms of automatic evaluation metrics and linguistic analysis. 1  "
W12-3144 " This paper describes the phrase-based SMT systems developed for our participation in the WMT12 Shared Translation Task. Translations for EnglishGerman and EnglishFrench were generated using a phrase-based translation system which is extended by additional models such as bilingual, fine-grained part-of-speech (POS) and automatic cluster language models and discriminative word lexica. In addition, we explicitly handle out-of-vocabulary (OOV) words in German, if we have translations for other morphological forms of the same stem. Furthermore, we extended the POS-based reordering approach to also use information from syntactic trees.  "
W12-3145 "School of Computing Science Simon Fraser University 8888 University Drive Burnaby BC. V5A 1S6. Canada {razmara, baskaran, aca69, anoop}@cs.sfu.ca Abstract This paper describes our submissions for the WMT-12 translation task using Kriya our hierarchical phrase-based system. We submitted systems in French-English and English-Czech language pairs. In addition to the baseline system following the standard MT pipeline, we tried ensemble decoding for French-English. The ensemble decoding method improved the BLEU score by 0.4 points over the baseline in newstest-2011. For English-Czech, we segmented the Czech side of the corpora and trained two different segmented models in addition to our baseline system. "
W12-3146 ", David Mare cek and Ond rej Du sek Charles University in Prague, Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics Malostransk  e n  am est   25, Prague {rosa,marecek,odusek}@ufal.mff.cuni.cz Abstract We present an improved version of DEPFIX (Mare cek et al., 2011), a system for automatic rule-based post-processing of Englishto-Czech MT outputs designed to increase their fluency. We enhanced the rule set used by the original DEPFIX system and measured the performance of the individual rules. We also modified the dependency parser of McDonald et al. (2005) in two ways to adjust it for the parsing of MT outputs. We show that our system is able to improve the quality of the state-of-the-art MT systems. "
W12-3147 "Abstract This paper describes the development of FrenchEnglish and EnglishFrench statistical machine translation systems for the 2012 WMT shared task evaluation. We developed phrase-based systems based on the Moses decoder, trained on the provided data only. Additionally, new features this year included improved language and translation model adaptation using the cross-entropy score for the corpus selection. "
W12-3148 "Abstract We provide a few insights on data selection for machine translation. We evaluate the quality of the new CzEng 1.0, a parallel data source used in WMT12. We describe a simple technique for reducing out-of-vocabulary rate after phrase extraction. We discuss the benefits of tuning towards multiple reference translations for English-Czech language pair. We introduce a novel approach to data selection by full-text indexing and search: we select sentences similar to the test set from a large monolingual corpus and explore several options of incorporating them in a machine translation system. We show that this method can improve translation quality. Finally, we describe our submitted system CU-TAMCH-BOJ. "
W12-3149 "Abstract We describe DFKIs statistical based submission to the 2012 WMT evaluation. The submission is based on the freely available machine translation toolkit Jane, which supports phrase-based and hierarchical phrase-based translation models. Different setups have been tested and combined using a sentence selection method. "
W12-3150 " We developed a string-to-tree system for EnglishGerman, achieving competitive results against a hierarchical model baseline. We provide details of our implementation of GHKM rule extraction and scope-3 parsing in the Moses toolkit. We compare systems trained on the same data using different grammar extraction methods. 1 I  "
W12-3151 " We describe our experiments with phrasebased machine translation for the WMT 2012 Shared Task. We trained one system for 14 translation directions between English or Czech on one side and English, Czech, German, Spanish or French on the other side. We describe a set of results with different training data sizes and subsets. 1 Int  "
W12-3152 " Recent work has established the efficacy of Amazons Mechanical Turk for constructing parallel corpora for machine translation research. We apply this to building a collection of parallel corpora between English and six languages from the Indian subcontinent: Bengali, Hindi, Malayalam, Tamil, Telugu, and Urdu. These languages are low-resource, under-studied, and exhibit linguistic phenomena that are difficult for machine translation. We conduct a variety of baseline experiments and analysis, and release the data to the community. 1 Intro  "
W12-3153 " Microblogging services such as Twitter have become popular media for real-time usercreated news reporting. Such communication often happens in parallel in different languages, e.g., microblog posts related to the same events of the Arab spring were written in Arabic and in English. The goal of this paper is to exploit this parallelism in order to eliminate the main bottleneck in automatic Twitter translation, namely the lack of bilingual sentence pairs for training SMT systems. We show that translation-based cross-lingual information retrieval can retrieve microblog messages across languages that are similar enough to be used to train a standard phrasebased SMT pipeline. Our method outperforms other approaches to domain adaptation for SMT such as language model adaptation, meta-parameter tuning, or self-translation. 1 Introduct  "
W12-3154 "cotland {bhaddow,pkoehn}@inf.ed.ac.uk Abstract In statistical machine translation (SMT), it is known that performance declines when the training data is in a different domain from the test data. Nevertheless, it is frequently necessary to supplement scarce in-domain training data with out-of-domain data. In this paper, we first try to relate the effect of the outof-domain data on translation performance to measures of corpus similarity, then we separately analyse the effect of adding the outof-domain data at different parts of the training pipeline (alignment, phrase extraction, and phrase scoring). Through experiments in 2 domains and 8 language pairs it is shown that the out-of-domain data improves coverage and translation of rare words, but may degrade the translation quality for more common words. "
W12-3155 "Abstract The new frontier of computer assisted translation technology is the effective integration of statistical MT within the translation workflow. In this respect, the SMT ability of incrementally learning from the translations produced by users plays a central role. A still open problem is the evaluation of SMT systems that evolve over time. In this paper, we propose a new metric for assessing the quality of an adaptive MT component that is derived from the theory of learning curves: the percentage slope. "
W12-3156 "Abstract SMT typically models translation at the sentence level, ignoring wider document context. Does this hurt the consistency of translated documents? Using a phrase-based SMT system in various data conditions, we show that SMT translates documents remarkably consistently, even without document knowledge. Nevertheless, translation inconsistencies often indicate translation errors. However, unlike in human translation, these errors are rarely due to terminology inconsistency. They are more often symptoms of deeper issues with SMT models instead. "
W12-3157 "ermany {wuebker,ney}@cs.rwth-aachen.de Abstract In statistical machine translation, word lattices are used to represent the ambiguities in the preprocessing of the source sentence, such as word segmentation for Chinese or morphological analysis for German. Several approaches have been proposed to define the probability of different paths through the lattice with external tools like word segmenters, or by applying indicator features. We introduce a novel lattice design, which explicitly distinguishes between different preprocessing alternatives for the source sentence. It allows us to make use of specific features for each preprocessing type and to lexicalize the choice of lattice path directly in the phrase translation model. We argue that forced alignment training can be used to learn lattice path and phrase translation model simultaneously. On the newscommentary portion of the GermanEnglish WMT 2011 task we can show moderate improvements of up to 0.6% BLEU over a stateof-the-art baseline system. "
W12-3158 ", WA, USA {mehwang,chrisq}@microsoft.com Abstract Training the phrase table by force-aligning (FA) the training data with the reference translation has been shown to improve the phrasal translation quality while significantly reducing the phrase table size on medium sized tasks. We apply this procedure to several large-scale tasks, with the primary goal of reducing model sizes without sacrificing translation quality. To deal with the noise in the automatically crawled parallel training data, we introduce on-demand word deletions, insertions, and backoffs to achieve over 99% successful alignment rate. We also add heuristics to avoid any increase in OOV rates. We are able to reduce already heavily pruned baseline phrase tables by more than 50% with little to no degradation in quality and occasionally slight improvement, without any increase in OOVs. We further introduce two global scaling factors for re-estimation of the phrase table via posterior phrase alignment probabilities and a modified absolute discounting method that can be applied to fractional counts. Index Terms: phrasal machine translation, phrase training, phrase table pruning 1 Introduction Extracting phrases from large amounts of noisy word-aligned training data for statistical machine translation (SMT) generally has the disadvantage of producing many unnecessary phrases (Johnson et al., 2007). These can include poor quality phrases, composite phrases that are concatenations of shorter ones, or phrases that are assigned very low probabilities, so that they have no realistic chance when competing against higher scoring phrase pairs. The goal of this work is two-fold: (i) investigating forced alignment training as a phrase table pruning method for large-scale commercial SMT systems and (ii) proposing several extensions to the training procedure to deal with practical issues and stimulate further research. Generative phrase translation models have the inherent problem of over-fitting to the training data (Koehn et al., 2003; DeNero et al., 2006). (Wuebker et al., 2010) introduce a leave-one-out procedure which is shown to counteract over-fitting effects. The authors report significant improvements on the German-English Europarl data with the additional benefit of a severely reduced phrase table size. This paper investigates its impact on a number of commercial large-scale systems and presents several extensions. The first extension is to deal with the highly noisy training data, which is automatically crawled and sentence aligned. The noise and the baseline pruning of the phrase table lead to low success rates when aligning the source sentence with the target sentence. We introduce on-demand word deletions, insertions, and backoff phrases to increase the success rate so that we can cover essentially the entire training data. Secondly, phrase table pruning makes out-of-vocabulary (OOV) issues even more pronounced. To avoid an increased OOV rate, we retrieve single-word translations from the baseline phrase table. Lastly, we propose two global scaling 460 factors to allow fine-tuning of the phrase counts in an attempt to re-estimate the translation probabilities and a modification of absolute discounting that can be applied to fractional counts. Our main contribution is applying forcedalignment on the training data to prune the phrase table. The rationale behind this is that by decoding the training data, we can identify the phrases that are actually used by the decoder. Further, we present preliminary experiments on re-estimating the channel models in the phrase table based on counts extracted from the force-aligned data. This work is organized as follows. We discuss related work in Section 2, describe our decoder and training procedure in Section 3 and the experiments in Section 4. A conclusion and discussion of future work is given in Section 5. "
W12-3159 " Minimum error rate training is often the preferred method for optimizing parameters of statistical machine translation systems. MERT minimizes error rate by using a surrogate representation of the search space, such as N best lists or hypergraphs, which only offer an incomplete view of the search space. In our work, we instead minimize error rate directly by integrating the decoder into the minimizer. This approach yields two benefits. First, the function being optimized is the true error rate. Second, it lets us optimize parameters of translations systems other than standard linear model features, such as distortion limit. Since integrating the decoder into the minimizer is often too slow to be practical, we also exploit statistical significance tests to accelerate the search by quickly discarding unpromising models. Experiments with a phrasebased system show that our approach is scalable, and that optimizing the parameters that MERT cannot handle brings improvements to translation results. "
W12-3401 " This paper investigates the impact on French dependency parsing of lexical generalization methods beyond lemmatization and morphological analysis. A distributional thesaurus is created from a large text corpus and used for distributional clustering and WordNet automatic sense ranking. The standard approach for lexical generalization in parsing is to map a word to a single generalized class, either replacing the word with the class or adding a new feature for the class. We use a richer framework that allows for probabilistic generalization, with a word represented as a probability distribution over a space of generalized classes: lemmas, clusters, or synsets. Probabilistic lexical information is introduced into parser feature vectors by modifying the weights of lexical features. We obtain improvements in parsing accuracy with some lexical generalization configurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach. "
W12-3402 "Abstract In the last decade, substantial progress has been made in the induction of semantic relations from raw text, especially of hypernymy and meronymy in the English language and in the classification of noun-noun relations in compounds or other contexts. We investigate the question of learning qualia-like semantic relations that cross part-of-speech boundaries for German, by first introducing a hand-tagged dataset of associated noun-verb pairs for this task, and then provide classification results using a general framework for supervised classification of lexical relations. "
W12-3403 "<NoAbstract>"
W12-3404 "Abstract This paper introduces a novel unsupervised approach to semantic role induction that uses a generative Bayesian model. To the best of our knowledge, it is the first model that jointly clusters syntactic verbs arguments into semantic roles, and also creates verbs classes according to the syntactic frames accepted by the verbs. The model is evaluated on French and English, outperforming, in both cases, a strong baseline. On English, it achieves results comparable to state-of-the-art unsupervised approaches to semantic role induction. "
W12-3405 "Abstract In many morphologically rich languages, conceptually independent morphemes are glued together to form a new word (a compound) with a meaning that is often at least in part predictable from the meanings of the contributing morphemes. Assuming that most compounds express a subconcept of exactly one sense of its nominal head, we use compounds as a higher-quality alternative to simply using general second-order collocate terms in the task of word sense discrimination. We evaluate our approach using lexical entries from the German wordnet GermaNet (Henrich and Hinrichs, 2010). "
W12-3406 "nces Abstract The paper presents a novel approach to extracting dependency information in morphologically rich languages using co-occurrence statistics based not only on lexical forms (as in previously described collocation-based methods), but also on morphosyntactic and wordnet-derived semantic properties of words. Statistics generated from a corpus annotated only at the morphosyntactic level are used as features in a Machine Learning classifier which is able to detect which heads of groups found by a shallow parser are likely to be connected by an edge in the complete parse tree. The approach reaches the precision of 89% and the recall of 65%, with an extra 6% recall, if only words present in the wordnet are considered. "
W12-3407 " This paper presents the results of a set of preliminary experiments combining two knowledge-based partial dependency analyzers with two statistical parsers, applied to the Basque Dependency Treebank. The general idea will be to apply a stacked scheme where the output of the rule-based partial parsers will be given as input to MaltParser and MST, two state of the art statistical parsers. The results show a modest improvement over the baseline, although they also present interesting lines for further research.  Basque Dependency Treebank. The general idea will be to apply a stacked scheme where the output of the rule-based partial parsers will be given as input to MaltParser and MST, two state of the art statistical parsers. The results show a modest improvement over the baseline, although they also present interesting lines for further research. "
W12-3408 " Although parsing performances have greatly improved in the last years, grammar inference from treebanks for morphologically rich languages, especially from small treebanks, is still a challenging task. In this paper we investigate how state-of-the-art parsing performances can be achieved on Spanish, a language with a rich verbal morphology, with a non-lexicalized parser trained on a treebank containing only around 2,800 trees. We rely on accurate part-of-speech tagging and datadriven lemmatization to provide parsing models able to cope lexical data sparseness. Providing state-of-the-art results on Spanish, our methodology is applicable to other languages with high level of inflection. 1 Introductio  "
W12-3409 "Abstract Deep linguistic grammars are able to provide rich and highly complex grammatical representations of sentences, capturing, for instance, long-distance dependencies and returning a semantic representation. These grammars lack robustness in the sense that they do not gracefully handle words missing from their lexicon. Several approaches have been explored to handle this problem, many of which consist in pre-annotating the input to the grammar with shallow processing machine-learning tools. Most of these tools, however, use features based on a fixed window of context, such as n-grams. We investigate whether the use of features that encode discrete structures, namely grammatical dependencies, can improve the performance of a machine learning classifier that assigns deep lexical types. In this paper we report on the design and evaluation of this classifier. "
W12-3410 "public {green,ramasamy,zabokrtsky}@ufal.mff.cuni.cz Abstract Dependency parsing has been shown to improve NLP systems in certain languages and in many cases helps achieve state of the art results in NLP applications, in particular applications for free word order languages. Morphologically rich languages are often short on training data or require much higher amounts of training data due to the increased size of their lexicon. This paper examines a new approach for addressing morphologically rich languages with little training data to start. Using Tamil as our test language, we create 9 dependency parse models with a limited amount of training data. Using these models we train an SVM classifier using only the model agreements as features. We use this SVM classifier on an edge by edge decision to form an ensemble parse tree. Using only model agreements as features allows this method to remain language independent and applicable to a wide range of morphologically rich languages. We show a statistically significant 5.44% improvement over the average dependency model and a statistically significant 0.52% improvement over the best individual system. 1 Introduction Dependency parsing has made many advancements in recent years. A prime reason for the quick advancement has been the CoNLL shared task competitions, which gave the community a common training/testing framework along with many open source systems. These systems have, for certain languages, achieved high accuracy ranging from on average from approximately 60% to 80% (Buchholz and Marsi, 2006). The range of scores are more often language dependent rather than system dependent, as some languages contain more morphological complexities. While some of these languages are morphologically rich, we would like to additionally address dependency parsing methods that may help under-resourced languages as well, which often overlaps with morphologically rich languages. For this reason, we have chosen to do the experiments in this paper using the Tamil Treebank (Ramasamy and  Zabokrtsk  y, 2012). Tamil belongs to Dravidian family of languages and is mainly spoken in southern India and also in parts of Sri Lanka, Malaysia and Singapore. Tamil is agglutinative and has a rich set of morphological suffixes. Tamil has nouns and verbs as two major word classes, and hundreds of word forms can be produced by the application of concatenative and derivational morphology. Tamils rich morphology makes the language free word order except that it is strictly head final. When working with small datasets it is often very difficult to determine which dependency model will best represent your data. One can try to pick the model through empirical means on a tuning set but as the data grows in the future this model may no longer be the best choice. The change in the best model may be due to new vocabulary or through a domain shift. If the wrong single model is chosen early on when training is cheap, when the model is applied in semi supervised or self training it could lead to significantly reduced annotation accuracy. 72 For this reason, we believe ensemble combinations are an appropriate direction for lesser resourced languages, often a large portion of morphologically rich languages. Ensemble methods are robust as data sizes grow, since the classifier can easily be retrained with additional data and the ensemble model chooses the best model on an edge by edge basis. This cost is substantially less than retraining multiple dependency models. "
W12-3411 "Abstract Korean is a morphologically rich language in which grammatical functions are marked by inflections and affixes, and they can indicate grammatical relations such as subject, object, predicate, etc. A Korean sentence could be thought as a sequence of eojeols. An eojeol is a word or its variant word form agglutinated with grammatical affixes, and eojeols are separated by white space as in English written texts. Korean treebanks (Choi et al., 1994; Han et al., 2002; Korean Language Institute, 2012) use eojeol as their fundamental unit of analysis, thus representing an eojeol as a prepreterminal phrase inside the constituent tree. This eojeol-based annotating schema introduces various complexity to train the parser, for example an entity represented by a sequence of nouns will be annotated as two or more different noun phrases, depending on the number of spaces used. In this paper, we propose methods to transform eojeol-based Korean treebanks into entity-based Korean treebanks. The methods are applied to Sejong treebank, which is the largest constituent treebank in Korean, and the transformed treebank is used to train and test various probabilistic CFG parsers. The experimental result shows that the proposed transformation methods reduce ambiguity in the training corpus, increasing the overall F1 score up to about 9 %. "
W12-3501 " This contribution focuses on multimodal interaction techniques for a mobile communication and assistance system on a robot platform. The system comprises of acoustic, visual and haptic input modalities. Feedback is given to the user by a graphical user interface and a speech synthesis system. By this, multimodal and natural communication with the robot system is possible. 1 Introdu  "
W12-3502 " This paper discusses the significance of the multimodal interaction in virtual environments (VE) and the criticalities involved in integration and coordination between modes during interaction. Also, we present an architecture and design of the integration mechanism with respect to information access in second language learning. In this connection, we have conducted an experiential study on speech inputs to understand how far users experience of information can be considered to be supportive to this architecture.  "
W12-3503 "Abstract The VASA project develops a multimodal assistive system mediated by a virtual agent that is intended to foster autonomy of communication and activity management in older people and people with disabilities. Assistive systems intended for these user groups have to take their individual vulnerabilities into account. A variety of psychic, emotional as well as behavioral conditions can manifest at the same time. Systems that fail to take them into account might not only fail at joint tasks, but also risk damage to their interlocutors. We identify important conditions and disorders and analyze their immediate consequences for the design of careful assistive systems. "
W12-3504 " In this paper we describe our recent and future research on multimodal interaction in an Ambient Assisted Living Lab. Our work combines two interaction modes, speech and gesture, for multiple device control in Ambient Assisted Living environments. We conducted a user study concerning multimodal interaction between participants and an intelligent wheelchair in a smart home environment. Important empirical data were collected through the user study, which encouraged further developments on our multimodal interactive system for Ambient Assisted Living environments. 1  "
W12-3505 "1,2 1 Fraunhofer Institute for Digital Media Technology (IDMT), Project group Hearing, Speech and Audio Technology (HSA), 26129 Oldenburg, Germany 2 University of Oldenburg, Signal Processing group, 26129 Oldenburg, Germany {benjamin.cauchi,s.goetze,simon.doclo}@idmt.fraunhofer.de Abstract Due to the demographic changes, support by means of assistive systems will become inevitable for home care and in nursing homes. Robot systems are promising solutions but their value has to be acknowledged by the patients and the care personnel. Natural and intuitive human-machine interfaces are an essential feature to achieve acceptance of the users. Therefore, automatic speech recognition (ASR) is a promising modality for such assistive devices. However, noises produced during movement of robots can degrade the ASR performances. This work focuses on noise reduction by a non-negative matrix factorization (NMF) approach to efficiently suppress non stationary noise produced by the sensors of an assisting robot system. "
W12-3506 "Abstract This paper introduces research within the ALADIN project, which aims to develop an assistive vocal interface for people with a physical impairment. In contrast to existing approaches, the vocal interface is self-learning, which means it can be used with any language, dialect, vocabulary and grammar. This paper describes the overall learning framework, and the two components that will provide vocabulary learning and grammar induction. In addition, the paper describes encouraging results of early implementations of these vocabulary and grammar learning components, applied to recorded sessions of a vocally guided card game, Patience. "
W12-3601 " As NLP confronts the challenge of Big Data for natural language text, the role played by linguistically annotated data in training machine learning algorithms is reaching a critical question. Namely, what role can annotated corpora play for supervised learning algorithms when the datasets become significantly outsized, compared to the gold standards used for training? The use of semi-supervised learning techniques to help solve this problem is a good next step, one that requires not less adherence to annotated data, but an even stricter adherence to linguistic models and the features that are derived from these models for subsequent annotation. 1  "
W12-3602 " We investigate aspects of interoperability between a broad range of common annotation schemes for syntacto-semantic dependencies. With the practical goal of making the LinGO Redwoods Treebank accessible to broader usage, we contrast seven distinct annotation schemes of functorargument structure, both in terms of syntactic and semantic relations. Drawing examples from a multi-annotated gold standard, we show how abstractly similar information can take quite different forms across frameworks. We further seek to shed light on the representational distance between pure bilexical dependencies, on the one hand, and full-blown logical-form propositional semantics, on the other hand. Furthermore, we propose a fully automated conversion procedure from (logical-form) meaning representation to bilexical semantic dependencies.  1 Introduction  "
W12-3603 "Abstract In this paper we describe the Prague Markup Language (PML), a generic and open XMLbased format intended to define format of linguistic resources, mainly annotated corpora. We also provide an overview of existing tools supporting PML, including annotation editors, a corpus query system, software libraries, etc. "
W12-3604 "Abstract This paper brings a contribution to the field of discourse annotation of corpora. Using ANNODIS, a french corpus annotated with discourse relations by naive and expert annotators, we focus on two of them, Elaboration and Entity-Elaboration. These two very frequent relations are (a) often confused by naive annotators (b) difficult to detect automatically as their signalling is poorly studied. We propose to use lexical cohesion to differentiate between them, and show that Elaboration is more cohesive than Entity-Elaboration. We then integrate lexical cohesion cues in a classification experiment, obtaining highly satisfying results. "
W12-3605 " This paper will introduce a procedure that we call pair annotation after pair programming. We describe initial annotation procedure of the TDB, followed by the inception of the pair annotation idea and how it came to be used in the Turkish Discourse Bank. We discuss the observed benefits and issues encountered during the process, and conclude by discussing the major benefit of pair annotation, namely higher inter-annotator agreement values. 1  "
W12-3606 ", {olivier.galibert,juliette.kahn}@lne.fr Abstract This paper compares the reference annotation of structured named entities in two corpora with different origins and properties. It addresses two questions linked to such a comparison. On the one hand, what specific issues were raised by reusing the same annotation scheme on a corpus that differs from the first in terms of media and that predates it by more than a century? On the other hand, what contrasts were observed in the resulting annotations across the two corpora? "
W12-3607 " We present two approaches (rule-based and statistical) for automatically annotating intra-chunk dependencies in Hindi. The intra-chunk dependencies are added to the dependency trees for Hindi which are already annotated with inter-chunk dependencies. Thus, the intra-chunk annotator finally provides a fully parsed dependency tree for a Hindi sentence. In this paper, we first describe the guidelines for marking intra-chunk dependency relations. Although the guidelines are for Hindi, they can easily be extended to other Indian languages. These guidelines are used for framing the rules in the rule-based approach. For the statistical approach, we use MaltParser, a data driven parser. A part of the ICON 2010 tools contest data for Hindi is used for training and testing the MaltParser. The same set is used for testing the rule-based approach.  "
W12-3608 "Abstract This paper describes a comprehensive standard for resource description developed within ISO TC37 SC4). The standard is instantiated in a system of XML headers that accompany data and annotation documents represented using the the Linguistic Annotation Frameworks Graph Annotation Format (GrAF) (Ide and Suderman, 2007; Ide and Suderman, Submitted). It provides mechanisms for describing the organization of the resource, documenting the conventions used in the resource, associating data and annotation documents, and defining and selecting defined portions of the resource and its annotations. It has been designed to accommodate the use of XML technologies for processing, including XPath, XSLT, and, by virtue of the systems linkage strategy, RDF/OWL, and to accommodate linkage to web-based ontologies and data category registries such as the OLiA ontologies (Chiarcos, 2012) and ISOCat (Marc KempsSnijders and Wright, 2008). "
W12-3609 " This paper describes the development of an Indonesian speech recognition web service which complies with two standards: it operates on the Language Grid, ensuring process interoperability, and its output uses the LAF/GrAF format, ensuring data interoperability. It is part of a larger system, currently in development, that aims to collect speech transcriptions via crowdsourcing methods. Its utility is twofold: it exposes a functional speech recognizer to the web, and allows the incremental construction of a large speech corpus.  "
W12-3610 " This paper explores how and why the Linguistic Annotation Framework might be adapted for compatibility with recent more general proposals for the representation of annotations in the Semantic Web, referred to here as the Open Annotation models. We argue that the adapted model, in addition to being interoperable with other annotations and annotation tools, also resolves some representational limitations and semantic ambiguity of the original data model.  "
W12-3611 " In this work, we present the data structures that were developed for the Rhapsodie project, an intonosyntactic annotation project of spoken French. Phoneticians and syntacticians work on different base units: a time aligned sound file for the former, and a partially ordered list of tokens for the latter. The alignment between the sound-file and the tokens is partial and nontrivial. We propose to encode this data with a small set of interconnected structures: lists, constituent trees, and directed acyclic graphs (DAGs). Our query language remains simple, similar to the Annis Query language, as the precedence and including relations are handled in accordance with the requested objects and their type of alignment: The order between prosodic units is timebased, whereas the order between syntactic units is lexeme-based. 1 Introduction   "
W12-3612 "logy Abstract The broad goal of this study is to further the understanding of doctors diagnostic styles and reasoning processes. We analyze and validate methods for annotating verbal diagnostic narratives collected together with eyemovement data. The long-term goal is to understand the cognitive reasoning and decisionmaking processes of medical experts, which could be useful for clinical information systems. The linguistic data set consists of transcribed recordings. Dermatologists were shown images of cutaneous conditions and asked to explain their observations aloud as they proceeded towards a diagnosis. We report on two linked annotation studies. In the first study, a subset of narratives were annotated by experts using a unique annotation scheme developed specifically for capturing decision-making components in the diagnostic process of dermatologists. We analyze annotator agreement as well as compare this annotation scheme to semantic types of the Unified Medical Language System as validation. In the second study, we explore the annotation of diagnostic correctness in the narratives at three relevant diagnostic steps, and we also explore the relationship between the two annotation schemes. "
W12-3613 "Abstract In this paper we present the results of a heuristic usability evaluation of three annotation tools (GATE, MMAX2 and UAM CorpusTool). We describe typical usability problems from two categories: (1) general problems, which arise from a disregard of established best practices and guidelines for user interface (UI) design, and (2) more specific problems, which are closely related to the domain of linguistic annotation. By discussing the domainspecific problems we hope to raise tool developers awareness for potential problem areas. A set of 28 design recommendations, which describe generic solutions for the identified problems, points toward a structured and systematic collection of usability patterns for linguistic annotation tools. "
W12-3614 " We show how the lexicographic task of finding informative and diverse example sentences can be cast as a search result diversification problem, where an objective based on relevance and diversity is maximized. This problem has been studied intensively in the information retrieval community during recent years, and efficient algorithms have been devised. We finally show how the approach has been implemented in a lexicographic project, and describe the relevance and diversity functions used in that context. 1 Introduct  "
W12-3615 "Abstract The paper describes a method for measuring compatibility between two levels of manual corpus annotation: shallow and deep. The proposed measures translate into a procedure for finding annotation errors at either level. "
W12-3616 "Abstract This paper proposes schematic changes to the TempEval framework that target the temporal vagueness problem. Specifically, two elements of vagueness are singled out for special treatment: vague time expressions, and explicit/implicit temporal modification of events. As proof of concept, an annotation experiment on explicit/implicit modification is conducted on Amazons Mechanical Turk. Results show that the quality of a considerable segment of the annotation is comparable to annotation obtained in the traditional doubleblind setting, only with higher coverage. This approach offers additional flexibility in how the temporal annotation data can be used. "
W12-3617 "Abstract We aim to sufficiently define annotation for post-positional particle errors in L2 Korean writing, so that future work on automatic particle error detection can make progress. To achieve this goal, we outline the linguistic properties of Korean particles in learner data. Given the agglutinative nature of Korean and the range of functions of particles, this annotation effort involves issues such as defining the tokens and target forms. 1 Introduction and Motivation One area of analyzing second language learner data is that of detecting errors in function words, e.g. prepositions, articles, and particles (e.g., Tetreault and Chodorow, 2008; De Felice and Pulman, 2008; de Ilarraza et al., 2008; Dickinson et al., 2011; Tetreault et al., 2010; Han et al., 2006), as these tend to be problematic for learners. This work has developed much, but it has mostly been for English. We thus aim to further the development of methods for detecting errors in functional elements across languages, by developing annotation for postpositional particles in Korean, a significant source of error for learners (Ko et al., 2004; Lee et al., 2009) and an area of interest for computer-assisted language learning (CALL) (Dickinson et al., 2008). As there is at present very little work on annotated learner corpora for morphologically-rich languages, this represents a significant step forward. There have been some efforts for annotating particle errors in Korean, but they have not directly linked to automatic error detection. The corpus in Lee et al. (2009) is made up of college student essays; is divided according to student level (beginner, intermediate) and student background (heritage, non-heritage); 1 and is hand-annotated for particle errors. This corpus, however, does not contain gold standard segmentation, requiring users to semiautomatically determine particle boundaries. In addition to segmentation, to make particle error detection a widespread task where real systems are developed, we need to outline the scope of particle errors (e.g., error types, influence of other errors) and incorporate insights into an annotation scheme. Selecting the correct particle in Korean is complicated by many factors. First, particles combine with preceding words in written Korean, as opposed to being set apart by white space, as in English. Thus, segmentation plays an integrated role. Secondly, selecting a particle for annotation is not a simple question, as they are sometimes optional, influenced by surrounding errors, and can be interchangeable. Thirdly, Korean particles have a wide range of functions, including modification and case-marking. Annotation, and by extension the task of particle error detection, must account for these issues. We focus on the utility of annotation in evaluating particle error detection systems, ensuring that it can support the automatic task of predicting the correct particle (or no particle) in a given context. Given that other languages, such as Japanese and Arabic, face some of the same issues (e.g., Hanaoka et al., 2010; Abuhakema et al., 2008), fleshing them out for error annotation and detection is useful beyond this one situation and help in the overall process of developing best practices for annotation and eval 1 Heritage learners have had exposure to Korean at a young age, such as growing up with Korean spoken at home. 129 ation of learner data (Tetreault et al., 2010). 2 Korean particles Korean postpositional particles are morphemes 2 that appear after a nominal to indicate a range of linguistic functions, including grammatical functions, e.g., subject and object; semantic roles; and discourse functions. In (1), for instance, ka marks the subject (function) and agent (semantic role). (1) Sumi-ka Sumi-SBJ John-uy John-GEN cip-eyse house-LOC ku-lul he-OBJ twu two sikan-ul hours-OBJ kitaly-ess-ta. wait-PAST-END Sumi waited for John for (the whole) two hours in his house. Similar to English prepositions, particles can have modifier functions, adding meanings of time, location, instrument, possession, etc., also as in (1). Note here that ul/lul has multiple uses. 3 Particles are one of the most frequent error types for Korean language learners (Ko et al., 2004). 3 Defining particle error annotation "
W12-3618 "EURAC, Italy Abstract Developing content extraction methods for Humanities domains raises a number of challenges, from the abundance of non-standard entity types to their complexity to the scarcity of data. Close collaboration with Humanities scholars is essential to address these challenges. We discuss an annotation schema for Archaeological texts developed in collaboration with domain experts. Its development required a number of iterations to make sure all the most important entity types were included, as well as addressing challenges including a domain-specific handling of temporal expressions, and the existence of many systematic types of ambiguity. "
W12-3619 "IRIT, CNRS and University of Toulouse 118, route de Narbonne 31062 Toulouse, France {cadilhac, asher, benamara}@irit.fr Abstract This paper describes an annotation scheme for expressions of preferences in on-line chats concerning bargaining negotiations in the online version of the competitive game Settlers of Catan. 1 Introduction Information about preferences is an important part of what is communicated in dialogue. A knowledge of ones own preferences and those of other agents are crucial to decision-making (Arora and Allenby, 1999), strategic interactions between agents (Brainov, 2000) (Hausman, 2000) (Meyer and Foo, 2004). Modeling preferences divides into three subtasks (Brafman and Domshlak, 2009): preference acquisition, which extracts preferences from users, preference modeling where a model of users preferences is built using a preference representation language and preference reasoning which aims at computing the set of optimal outcomes. We focus in this paper on a particular instantiation of the first task, extracting preferences from chat turns of actual conversation; and we propose an annotation scheme that is general enough to cover several domains. We extend the annotation scheme of (Cadilhac et al., 2012), which investigates preferences within negotiation dialogues with a common goal like fixing a meeting time (Verbmobil (C V )) or making a hotel or plane reservation (Booking (C B )) to a more complex domain provided by a corpus of on line chats concerning the game Settlers of Catan. In Settlers, players with opposing strategic interests bargain over scarce resources. Our results show that preferences can be easily annotated by humans and that our scheme adapts relatively easily to different domains. 2 Preferences in game theory A preference is traditionally a complete ordering by an agent over outcomes. In traditional game theory (Osborne and Rubinstein, 1994), preferences or utilities over outcomes drive rational, strategic decision. They are the terminal states of the game, the end states of complete strategies, which are functions from the set of players P to the set of actions A; by assigning end states a utility, strategies are thereby also assigned a preference. Game theory postulates that agents calculate their actions based on a common knowledge of all the players preferences. In real life, strategic interactions almost always occur under the handicap of various forms of imperfect information. People dont know what other relevant actors are going to do, because they typically dont know what they believe and what they want. In addition, the underlying game is so large that agents with limited computational power cant hope to compute in analytical fashion the optimal actions they should perform. Because a knowledge of preferences is crucial to informed strategic action, people try to extract information about the preferences of other agents and often provide information about their own preferences when they talk. Almost always this information provides an ordinal definition of preferences, which consists in imposing a ranking over relevant possible outcomes and not a cardinal definition based on 139 numerical values. A preference relation, written , is a reflexive and transitive binary relation over elements of . The preference orderings are not necessarily complete, since some candidates may not be comparable for a given agent. Let o 1 , o 2  , o 1 o 2 means that outcome o 1 is equally or more preferred to the decision maker than o 2 . Strict preference o 1 o 2 holds iff o 1 o 2 and not o 2 o 1 . The associated indifference relation is o 1  o 2 if o 1 o 2 and o 2 o 1 . Among elements of , some outcomes are acceptable for the agent, i.e. the agent is ready to act in such a way as to realize them, and some outcomes are not. Among the acceptable outcomes, the agent will typically prefer some to others. "
W12-3620 " Morphological segmentation data for the METU-Sabanc Turkish Treebank is provided in this paper. The generalized lexical forms of the morphemes which the treebank previously lacked are added to the treebank. This data maybe used to train POS-taggers that use stemmer outputs to map these lexical forms to morphological tags. 1  "
W12-3621 "acquisition Fr  ed  eric Papazian Robert Bossy Math  ematique, Informatique et G  enome, Institut National de la Recherche Agronomique INRA UR1077  F78352 Jouy-en-Josas {forename.lastname}@jouy.inra.fr Claire N  edellec Abstract AlvisAE is a text annotation editor aimed at knowledge acquisition projects. An expressive annotation data model allows AlvisAE to support various knowledge acquisition tasks like construction gold standard corpus, ontology population and assisted reading. Collaboration is achieved through a workflow of tasks that emulates common practices (e.g. automatic pre-annotation, adjudication). It is implemented as a Web application requiring no installation by the end-user, thus facilitating the participation of domain experts. AlvisAE is used in several knowledge acquisition projects in the domains of biology and crop science. "
W12-3622 " This paper presents a community-sourcing annotation framework, which is designed to implement a marketplace model of annotation tasks and annotators, with an emphasis on efficient management of community of potential annotators. As a position paper, it explains the motivation and the design concept of the framework, with a prototype implementation. 1 Int  "
W12-3623 "Abstract In this paper we describe a currently underway treebanking effort for Urdu-a South Asian language. The treebank is built from a newspaper corpus and uses a Karaka based grammatical framework inspired by Paninian grammatical theory. Thus far 3366 sentences (0.1M words) have been annotated with the linguistic information at morpho-syntactic (morphological, part-of-speech and chunk information) and syntactico-semantic (dependency) levels. This work also aims to evaluate the correctness or reliability of this manual annotated dependency treebank. Evaluation is done by measuring the inter-annotator agreement on a manually annotated data set of 196 sentences (5600 words) annotated by two annotators. We present the qualitative analysis of the agreement statistics and identify the possible reasons for the disagreement between the annotators. We also show the syntactic annotation of some constructions specific to Urdu like Ezaf e and discuss the problem of word segmentation (tokenization). "
W12-3624 "Abstract Finding coordinations provides useful information for many NLP endeavors. However, the task has not received much attention in the literature. A major reason for that is that the annotation of major treebanks does not reliably annotate coordination. This makes it virtually impossible to detect coordinations in which two conjuncts are separated by punctuation rather than by a coordinating conjunction. In this paper, we present an annotation scheme for the Penn Treebank which introduces a distinction between coordinating from non-coordinating punctuation. We discuss the general annotation guidelines as well as problematic cases. Eventually, we show that this additional annotation allows the retrieval of a considerable number of coordinate structures beyond the ones having a coordinating conjunction. "
W12-3625 " We present a novel scheme for annotating the realization and ellipsis of Korean particles. Annotated data include 100,128 Ecel (a spacebased word unit) in spoken and written corpora composed of four different genres in order to evaluate how register variation contributes to Korean particle ellipsis. Identifying the grammatical functions of particles and zero particles is critical for deriving a valid linguistic analysis of argument realization, semantic and discourse analysis, and computational processes of parsing. The primary challenge is to design a reliable scheme for classifying particles while making a clear distinction between ellipsis and non-occurrences. We determine in detail issues involving particle annotation and present solutions. In addition to providing a statistical analysis and outcomes, we briefly discuss linguistic factors involving particle ellipsis. 1 Introduct  "
W12-3701 "Multimodal Sentiment Analysis (Abstract of Invited Talk)  Rada Mihalcea  cea Department of Computer Science and Enginee  ing University of North T  "
W12-3702 "(Abstract of Invited Talk) Janyce Wiebe  ebe Department of Computer Sci  nce University of Pittsb urgh Sennott Square Building  "
W12-3703 " This paper presents a novel approach in Sentiment Polarity Detection on Twitter posts, by extracting a vector of weighted nodes from the graph of WordNet. These weights are used on SentiWordNet to compute a final estimation of the polarity. Therefore, the method proposes a non-supervised solution that is domain-independent. The evaluation over a generated corpus of tweets shows that this technique is promising. 1 I  "
W12-3704 "Abstract Twitter is a micro blogging website, where users can post messages in very short text called Tweets. Tweets contain user opinion and sentiment towards an object or person. This sentiment information is very useful in various aspects for business and governments. In this paper, we present a method which performs the task of tweet sentiment identification using a corpus of pre-annotated tweets. We present a sentiment scoring function which uses prior information to classify (binary classification ) and weight various sentiment bearing words/phrases in tweets. Using this scoring function we achieve classification accuracy of 87% on Stanford Dataset and 88% on Mejaj dataset. Using supervised machine learning approach, we achieve classification accuracy of 88% on Stanford dataset. "
W12-3705 "Abstract In this work, we present SAMAR, a system for Subjectivity and Sentiment Analysis (SSA) for Arabic social media genres. We investigate: how to best represent lexical information; whether standard features are useful; how to treat Arabic dialects; and, whether genre specific features have a measurable impact on performance. Our results suggest that we need individualized solutions for each domain and task, but that lemmatization is a feature in all the best approaches. "
W12-3706 "Avenida Universidad, s/n. Edificio Quorum III. 03202 Elche, Alicante (Spain) {boyan,gramirez,sortiz}@prompsit.com Abstract The classification of opinion texts in positive and negative can be tackled by evaluating separate key words but this is a very limited approach. We propose an approach based on the order of the words without using any syntactic and semantic information. It consists of building one probabilistic model for the positive and another one for the negative opinions. Then the test opinions are compared to both models and a decision and confidence measure are calculated. In order to reduce the complexity of the training corpus we first lemmatize the texts and we replace most namedentities with wildcards. We present an accuracy above 81% for Spanish opinions in the financial products domain. "
W12-3707 " Current sentiment analysis systems rely on static (context independent) sentiment lexica with proximity based fixed-point prior polarities. However, sentimentorientation changes with context and these lexical resources give no indication of which value to pick at what context. The general trend is to pick the highest one, but which that is may vary at context. To overcome the problems of the present proximity-based static sentiment lexicon techniques, the paper proposes a new way to represent sentiment knowledge in a Vector Space Model. This model can store dynamic prior polarity with varying contextual information. The representation of the sentiment knowledge in the Conceptual Spaces of distributional Semantics is termed Sentimantics. 1  "
W12-3708 " In the NLP field, there have been a lot of works which focus on the reviewers point of view conducted on sentiment analyses, which ranges from trying to estimate the reviewers score. However the reviews are used by the readers. The reviews that give a big influence to the readers should have the highest value, rather than the reviews to which was assigned the highest score by the writer. In this paper, we conducted the analyses using the readers point of view. We asked 20 subjects to read 500 sentences in the reviews of Rakuten travel and extracted the sentences that gave a big influence to the subjects. We analyze the influential sentences from the following two points of view, 1) targets and evaluations and 2) personal tastes. We found that room, service, meal and scenery are important targets which are items included in the reviews, and that features and human senses are important evaluations which express sentiment or explain targets. Also we showed personal tastes appeared on meal and service.  "
W12-3709 "Abstract The past years have shown a steady growth in interest in the Natural Language Processing task of sentiment analysis. The research community in this field has actively proposed and improved methods to detect and classify the opinions and sentiments expressed in different types of text from traditional press articles, to blogs, reviews, fora or tweets. A less explored aspect has remained, however, the issue of dealing with sentiment expressed in texts in languages other than English. To this aim, the present article deals with the problem of sentiment detection in three different languages French, German and Spanish using three distinct Machine Translation (MT) systems Bing, Google and Moses. Our extensive evaluation scenarios show that SMT systems are mature enough to be reliably employed to obtain training data for languages other than English and that sentiment analysis systems can obtain comparable performances to the one obtained for English. "
W12-3710 "Abstract Online debate forums provide a powerful communication platform for individual users to share information, exchange ideas and express opinions on a variety of topics. Understanding peoples opinions in such forums is an important task as its results can be used in many ways. It is, however, a challenging task because of the informal language use and the dynamic nature of online conversations. In this paper, we propose a new method for identifying participants agreement or disagreement on an issue by exploiting information contained in each of the posts. Our proposed method first regards each post in its local context, then aggregates posts to estimate a participants overall position. We have explored the use of sentiment, emotional and durational features to improve the accuracy of automatic agreement and disagreement classification. Our experimental results have shown that aggregating local positions over posts yields better performance than nonaggregation baselines when identifying users global positions on an issue. "
W12-3711 "Abstract A set of words labelled with their prior emotion is an obvious place to start on the automatic discovery of the emotion of a sentence, but it is clear that context must also be considered. No simple function of the labels on the individual words may capture the overall emotion of the sentence; words are interrelated and they mutually influence their affectrelated interpretation. We present a method which enables us to take the contextual emotion of a word and the syntactic structure of the sentence into account to classify sentences by emotion classes. We show that this promising method outperforms both a method based on a Bag-of-Words representation and a system based only on the prior emotions of words. The goal of this work is to distinguish automatically between prior and contextual emotion, with a focus on exploring features important for this task. "
W12-3712 "<NoAbstract>"
W12-3713 "Abstract This paper presents a corpus targeting evaluative meaning as it pertains to descriptions of events. The corpus, POLITICAL-ADS is drawn from 141 television ads from the 2008 U.S. presidential race and contains 3945 NPs and 1549 VPs annotated for scalar sentiment from three different perspectives: the narrator, the annotator, and general society. We show that annotators can distinguish these perspectives reliably and that correlation between the annotators own perspective and that of a generic individual is higher than those with the narrator. Finally, as a sample application, we demonstrate that a simple compositional model built off of lexical resources outperforms a lexical baseline. "
W12-3714 "Abstract This paper presents our research on automatic annotation of a five-billion-word corpus of Japanese blogs with information on affect and sentiment. We first perform a study in emotion blog corpora to discover that there has been no large scale emotion corpus available for the Japanese language. We choose the largest blog corpus for the language and annotate it with the use of two systems for affect analysis: ML-Ask for wordand sentence-level affect analysis and CAO for detailed analysis of emoticons. The annotated information includes affective features like sentence subjectivity (emotive/non-emotive) or emotion classes (joy, sadness, etc.), useful in affect analysis. The annotations are also generalized on a 2-dimensional model of affect to obtain information on sentence valence/polarity (positive/negative) useful in sentiment analysis. The annotations are evaluated in several ways. Firstly, on a test set of a thousand sentences extracted randomly and evaluated by over forty respondents. Secondly, the statistics of annotations are compared to other existing emotion blog corpora. Finally, the corpus is applied in several tasks, such as generation of emotion object ontology or retrieval of emotional and moral consequences of actions. "
W12-3715 "Abstract Evaluation often denotes a key issue in semanticsor subjectivity-related tasks. Here we discuss the difficulties of evaluating opinionated keyphrase extraction. We present our method to reduce the subjectivity of the task and to alleviate the evaluation process and we also compare the results of human and machine-based evaluation. "
W12-3716 "Abstract Current work on sentiment analysis is characterized by approaches with a pragmatic focus, which use shallow techniques in the interest of robustness but often rely on ad-hoc creation of data sets and methods. We argue that progress towards deep analysis depends on a) enriching shallow representations with linguistically motivated, rich information, and b) focussing different branches of research and combining ressources to create synergies with related work in NLP. In the paper, we propose SentiFrameNet, an extension to FrameNet, as a novel representation for sentiment analysis that is tailored to these aims. "
W12-5101 "<NoAbstract>"
W12-5102 " Lexical networks can be used with benefit for semantic analysis of texts, word sense disambiguation (WSD) and in general for graph-based Natural Language Processing. Usually strong relations between terms (e.g.: cat --> animal) are sufficient to help for the task, but quite often, weak relations (e.g.: cat --> ball of wool) are necessary. Our purpose here is to acquire such relations by means of online serious games as other classical approaches seems impractical. Indeed, it is difficult to ask the users (non experts) to define a proper weighting for the relations they propose, and then we decided to relate weights with the frequency of their propositions. It allows us to acquire first the strongest relations, but also to populate the long tail of an already existing network. Furthermore, trying to get an estimation of our network by the very users thanks to a tip of the tongue (TOT) software, we realized that they rather tend to favor the relations of the long tail and thus promote their emergence. Developing the long tail of a lexical network with standard and non-standard relations of low weight can be of advantage for tasks such that words retrieval from clues or WSD in texts. KEYWORDS : LEXICAL NETWORK, LONG TAIL, GAME WITH A PURPOSE, TIP OF THE TONGUE SOFTWARE, TYPED RELATIONS, WEIGHTED RELATIONS, WSD Introduction Lexical/semantic networks are very precious resources for NLP applications in general and for Word Sense Disambiguation (WSD) in particular. Their construction is delicate as automated approaches from corpora may have various shortcomings (mainly high noise level and/or low recall) and a manual approach may be long, tedious, costly and of unsatisfactory quality or coverage. A way of handling the building of such resources can be direct crowdsourcing (as contributive approaches) or indirect crowdsourcing through for instance serious games. What is a long tail in a lexical network? A lexical/semantic network (thereafter dubbed JDM) for French is under construction with methods based on popular consensus by means of games with a purpose named JeuxDeMots (Lafourcade 2007). Thus, in 5 years, a high number of players lead to the construction a large scale lexical network for the French language (currently more than 240 000 terms with around 1.4 million semantic relations) representing a common general knowledge but also including word senses referred as word usages (Lafourcade and Joubert, 2010). The relations of the lexical networks created this way are directed and typed, with classical ontological relations (like hypernym, hyponyms, part-of, whole, material/substance, ...), lexical relations (synonyms, antonyms, lexical family, ...); 5 semantic roles (agent, patient, instrument, ...) and less standard relations (typical location and time, cause, consequence, ...). Furthermore, relation occurrences are weighted which constitutes a quite original aspect in the lexical network domain exemplified by (for example) WordNet (Miller, 1990). The interpretation of a weight might by difficult but can be related to the strength of the relation as collectively perceived by speakers/players. The weight computation is done by emergence along with the gaming activity. Obviously by intuition, the relation cat--> animal is stronger than cat --> ball of wool, none withstanding their types. The lexical network has been made available (at http://jeuxdemots.org) and free to use by their authors, giving the research community a resource to play with. The question of the evaluation of its quality, usability in WSD and word recollection (Tip of the Tongue problem), and distributional properties are the main subjects of this article. One specific question is whether low weight but still important relations can be captured by some similar approaches and to which extend they are useful. We observed that many (if not most) relations in JDM are frontal/direct/obvious relations (e.g.: chat-->-feline), but some others are more farfetched/indirect. We wish to evaluate but also find practical ways to densify the network increasing the number of indirect relations (e.g.: chat --> allergy) belonging to the long tail. To do so, we use a TOT tool in a taboo mode, that is, refraining from using the strongest relations. In a first section, we will briefly remind to the reader the principles of long tail and the link with the network construction. Then, we introduce our TOT (tip of the tongue) tool, named AKI and we will explain the taboo mode, and show how it leads to densifying the JDM network. An evaluation of the long tailed network obtained is done for AKI and for a simplified WSD task.  http://jeuxdemots.org ) and free to use by their authors, giving the research community a resource to play with. The question of the evaluation of its quality, usability in WSD and word recollection (Tip of the Tongue problem), and distributional properties are the main subjects of this article. One specific question is whether low weight but still important relations can be captured by some similar approaches and to which extend they are useful. We observed that many (if not most) relations in JDM are frontal/direct/obvious relations (e.g.: chat-->-feline), but some others are more farfetched/indirect. We wish to evaluate but also find practical ways to densify the network increasing the number of indirect relations (e.g.: chat --> allergy) belonging to the long tail. To do so, we use a TOT tool in a taboo mode, that is, refraining from using the strongest relations. In a first section, we will briefly remind to the reader the principles of long tail and the link with the network construction. Then, we introduce our TOT (tip of the tongue) tool, named AKI and we will explain the taboo mode, and show how it leads to densifying the JDM network. An evaluation of the long tailed network obtained is done for AKI and for a simplified WSD task. "
W12-5103 "On Discriminating fMRI Representations of Abstract WordNet Taxonomic Categories Andrew James ANDERSON 1 ,Yuan TAO 1 , Brian MURPHY 2 , Massimo POESIO 1,3 (1) Centro Interdipartimentale Mente e Cervello (CIMeC), University of Trento, Italy (2) Machine Learning Department, School of Computer Science, Carnegie Mellon University, USA (3) School of Computer Science and Electronic Engineering, University of Essex, UK andrew.anderson@unitn.it , yuan.tao@unitn.it , brianmurphy@cmu.edu , massimo.poesio@unitn.it  "
W12-5104 "<NoAbstract>"
W12-5105 "<NoAbstract>"
W12-5106 " In the last 20 years dictionaries and lexicographic resources such as WordNet have started to be enriched with multimodal content. Short videos depicting basic actions support the users need (especially in second language acquisition) to fully understand the range of applicability of verbs. The IMAGACT project has among its results a repository of action verbs ontologically organised around prototypical action scenes in the form of both video recordings and 3D animations. The creation of the IMAGACT ontology, which consists in deriving action types from corpus instances of action verbs, intra and cross linguistically validating them and producing the prototypical scenes thereof, is the preliminary step for the creation of a resouce that users can browse by verb, learning how to match different action prototypes with the correct verbs in the target language. The mapping of IMAGACT types onto WordNet synsets allows for a mutual enrichment of both resources. Interpretazione dei verbi per tipi azionali di base: annotazione, induzione di ontologia e creazione di scene prototipiche Negli ultimi venti anni dizionari e risorse lessicografiche come WordNet sono stati arricchiti con contenuto multimediale. Brevi video in grado di rappresentare azioni di base supportano i bisogni degli utenti (in particolar modo per quanto riguarda l' acquisizione della seconda lingua) nel comprendere l' ambito di applicabilita dei verbi. Il progetto IMAGACT ha tra i suoi risultati una base di dati di verbi d'azione ontologicamente organizzati e raffiguranti scene che riproducono azioni prototipiche sottoforma di registrazioni video e animazioni 3D. La creazione dell' ontologia IMAGACT che consiste nella derivazione di tipi azionali da istanze di verbi d'azione estratte da un corpus, nella loro validazione intra e crosslinguisticamente e nella conseguente produzione di scene prototipiche, e il passaggio preliminare per la creazione di una risorsa che gli utenti possono consultare partendo dal verbo, imparando come alllineare differenti prototipi d'azione con il verbo corretto nella lingua da apprendere. Il mapping dei tipi di IMAGACT sui synsets di WordNet consente un arricchimento reciproco di entrambe le risorse. KEYWORDS : ontology of actions, lexical resource, 3D animations KEYWORDS IN ITALIAN : ontologia di azioni, risorse lessicali, animazioni 3D 69 1 Introduction In the last 20 years dictionaries and lexicographic resources such as WordNet have started to be enriched with multimodal content (e.g. pictorial illustrations, animations, videos, audio files). Pictures are effective in conveying the meaning of denotative words such as concrete nouns, while for abstract relations (instantiated by prepositional meanings) schematic illustrations can depict several semantic properties. Conveying the meaning of verbs with static representations is not possible; for such cases the use of animations and videos has been proposed (see Stein 1991 cited in Lew 2010). Short videos depicting basic actions support the users need (especially in second language acquisition) to fully understand the range of applicability of verbs i.e. to start with a mental image of an action and from this image find out the L2 verb(s) that can be used to predicate that action. This process involves semantic and pragmatic comparisons that occur in the mind of the learner, with considerations respecting the type of movement involved, the instrument/tool that can be used, the duration, the strength of the movement etc. In this paper we introduce the IMAGACT project and its results: a repository of action verbs ontologically organised around prototypical action scenes in the form of both video recordings and 3D animations. The focus of IMAGACT is on action verbs, because in all language modalities they bear basic information that should be processed in order to make sense of a sentence. Especially in speech, they are the most frequent structuring elements (Moneglia and Panunzi, 2007), but unfortunately no one-to-one correspondence can be established between an action verb, conceived as a lexical entry, and an action type, conceived as an ontological entity. In order to bridge this gap 500 English and Italian action verbs have been analysed in their different contexts of use in corpora and grouped into action types according to their internal variation. Types representing the same prototypical actions are then gathered together under the same scene and represented in 3D animations, generated ad hoc which thus illustrate the different uses of action verbs across languages (see Figure 1). For instance, the English verb to roll can refer to qualitatively different actions. In some uses the agent changes the form of the object (B and 1), in some other uses the agent moves himself in space (  "
W12-5107 "<NoAbstract>"
W12-5108 "<NoAbstract>"
W12-5109 "<NoAbstract>"
W12-5110 " The present paper continues the successful parsing experiments with the method of Segmentation-Cohesion-Dependency (SCD) configurations, a breadth-first, formal grammar-free, and optimal approach to dictionary entry parsing, proposed in the previous CogALex Workshops and applied to the following five very large thesaurus-dictionaries: DLR (The Romanian Thesaurus  new format), DAR (The Romanian Thesaurus  old format), TLF (Le Tresor de la Langue Francaise), DWB (Deutsches Worterbuch  GRIMM), and GWB (Gothe-Worterbuch). In this work we report new results: (a) The lexicographic modeling and parsing experiments of the sixth large DMLRL (Dictionary of Modern Literary Russian Language); (b) Outlining the Enumeration Closing Condition (ECC) for solving the recursive calls between sense marker classes situated on different nodes of a sense dependency hypergraph (SCD-configuration, i.e. parsing level); (c) The central result we report here is the project of a new, procedural DTD (Document Type Description) for dictionaries, based on the formalization of the SCD parsing method, providing parameterized grammars to describe the dependency hypergraphs that correspond to the main parsing levels in a dictionary entry. Here we give two parameterized grammars for DLR, as a small sample from a larger package of combined grammars for the above mentioned dictionaries. This package is constructed as the least common multiple of the parameterized grammars written for the parsed dictionaries; it represents the DTD description of a general parser for large dictionary entries, and thoroughly extends the current DTD in the XCES TEI P5 standard. KEYWORDS: SCD dictionary parsing method; procedural DTD for dictionary entry parsi  CogALex Workshops and applied to the following five very large thesaurus-dictionaries: DLR (The Romanian Thesaurus  new format), DAR (The Romanian Thesaurus  old format), TLF (Le Tresor de la Langue Francaise), DWB (Deutsches Worterbuch  GRIMM), and GWB (Gothe-Worterbuch). In this work we report new results: (a) The lexicographic modeling and parsing experiments of the sixth large DMLRL (Dictionary of Modern Literary Russian Language); (b) Outlining the Enumeration Closing Condition (ECC) for solving the recursive calls between sense marker classes situated on different nodes of a sense dependency hypergraph (SCD-configuration, i.e. parsing level); (c) The central result we report here is the project of a new, procedural DTD (Document Type Description) for dictionaries, based on the formalization of the SCD parsing method, providing parameterized grammars to describe the dependency hypergraphs that correspond to the main parsing levels in a dictionary entry. Here we give two parameterized grammars for DLR, as a small sample from a larger package of combined grammars for the above mentioned dictionaries. This package is constructed as the least common multiple of the parameterized grammars written for the parsed dictionaries; it represents the DTD description of a general parser for large dictionary entries, and thoroughly extends the current DTD in the XCES TEI P5 standard. KEYWORDS: SCD dictionary parsing method; procedural DTD for dictionary entry parsing. "
W12-5111 " To develop a common language, it is essential to have enough vocabulary to express all the concepts contained in all the world languages. Those vocabularies can only be developed by native speakers and should be defined by formal ways. Considering the situation, at this moment Universal Networking Language (UNL) is the best solution as the common language, and Universal Words (UWs) are the most promising candidates to represent all the world concepts in different languages. However, UWs itself are formal and not always to be understandable for human. To ensure every language speakers can create the correct UWs dictionary entry, we need to provide the explanation of UWs in different natural languages for humans. As there are millions of UWs, it is very expensive to manually build the UWs explanation in all natural languages. To solve this problem, this research proposes the way to auto generate the UWs explanation in UNL, using the property inheritance based on UW System. Using UNL DeConverter from that UNL the system can generate the explanation in more than 40 languages. KEYWORDS : UNL; Ontology; Word Semantics; NLP;  "
W12-5112 " The growing amount of available information and the growing importance given to the access to technical information enhance the potential role of NLP applications in enabling users to deal with information for a variety of knowledge domains. In this process, lexical resources are crucial. Using and comparing already existent wordnets for common and technical lexica, we set up a basis for integrating these resources without losing their specific information and properties. We demonstrate their compatibility and discuss strategies to overcome the issues arrising in their merging, namely aspects concerning conceptual variation, subnet and synset merging, and the incorporation of technical and non-technical information in definitions. As we are using models of the lexicon that mirror the organization of the mental lexicon, the accomplishment of this goal can provide insights on the type of relations holding between common lexical items and terms. Also, the results of integrating such resources can contribute to the better intercommunication between experts and non-experts, and provide a useful resource for NLP, particularly for tools simultaneously serving specialist and non-specialist publics. KEYWORDS : wordnet, technical lexicon, common lexicon, merging.  "
W12-5113 "1 Utpal Saikia 1 (1) Department of Information Technology, Institute of Science & Technology, Gauhati University, Guwahati  14 Assam, India {sks001, dibyasarmah, bswjtbrahma, mayashreemahanta, himadri0001, utpal.sk}@gmail.com Abstract The present paper deals with the design and implementation of multilingual lexical resources of Assamese and Bodo Language with the help of Hindi Wordnet. Here, we present the multilingual dictionaries (for Hindi, Assamese and Bodo), synset based word search for Assamese-Hindi and Bodo-Hindi language. These words, of course, will have to go through some pre-processing before finally being uploaded to a database. The user-interface is being developed for specific language (Assamese, Bodo and Hindi language). KEYWORDS: Lexical Resources, Concept Based Dictionary, Multilingual Dictionary Database, Web-based Interface "
W12-5114 " The Mental Lexicon (ML) refers to the organization of lexical entries of a language in the human mind.A clear knowledge of the structure of ML will help us to understand how the human brain processes language. The knowledge of semantic association among the words in ML is essential to many applications. Although, there are works on the representation of lexical entries based on their semantic association in the form of a lexicon in English and other languages, such works of Bangla is in a nascent stage. In this paper, we have proposed a distinct lexical organization based on semantic association between Bangla words which can be accessed efficiently by different applications. We have developed a novel approach of measuring the semantic similarity between words and verified it against user study. Further, a GUI has been designed for easy and efficient access. KEYWORDS : Bangla Lexicon, Synset, Semantic Similarity, Hierarchical Graph  "
W12-5115 "<NoAbstract>"
W12-5116 "Abstract We present a large-coverage lexical and grammatical resource of Polish economic terminology. It consists of two alternative modules. One is a grammatical lexicon of about 11,000 terminological multi-word units, where inflectional and syntactic variation, as well as nesting of terms, are described via graph-based rules. The other one is a fully lexicalized shallow grammar, obtained by an automatic conversion of the lexicon, and partly manually validated. Both resources have a good coverage, evaluated on a manually annotated corpus, and are freely available under the Creative Commons BY-SA license. Keywords: electronic lexicon, shallow grammar, Polish, economic terminology, language resources and tools. "
W12-5301 " For the past two decades linguists working within the framework of systemic functional linguistics have been developing appraisal theory as a tool for analysing evaluation in discourse. In this talk I will present a brief overview of the current model and then move on to address a number of the challenges that have arisen over the years including distinguishing inscribed from invoked attitude, determining the prosodic domain of attitude selections and the role of attitude in the negotiation of affiliation. Recent work in Maton's Legitimation Code Theory, it sociological perspective on axiologically charged constellations of meaning in particular, will be introduced in relation to these challenges. 1  "
W12-5302 "<NoAbstract>"
W12-5303 " Sentiment analysis is to extract the opinion of the user from of the text documents. Sentiment classification using machine learning methods face problem of handling huge number of unique terms in a feature vector for the classification. Thus it is required to eliminate the irrelevant and noisy terms from the feature vector. Feature selection methods reduce the feature size by selecting prominent features for better classification. In this paper, a new feature selection method namely Probability Proportion Difference (PPD) is proposed which is based on the probability of belongingness of a term to a particular class. It is capable of removing irrelevant terms from the feature vector. Further, a Categorical Probability Proportion Difference (CPPD) feature selection method is proposed based on Probability Proportion Difference (PPD) and Categorical Proportion Difference (CPD). CPPD feature selection method is able to select the features which are relevant and capable of discriminating the class. The performance of the proposed feature selection methods is compared with the CPD method and Information Gain (IG) method which has been identified as one of the best feature selection method for sentiment classification. Experimentation of proposed feature selection methods was performed on two standard datasets viz. movie review dataset and product review (i.e. book) dataset. Experimental results show that proposed CPPD feature selection method outperforms other feature selection method for sentiment classification. KEYWORDS : Feature Selection, Sentiment Classification, Categorical Probability Proportional Difference (CPPD), Probability Proportion Difference (PPD), CPD. 17  "
W12-5304 " With the rapid expansion of Web 2.0, a variety of documents abound online. Thus, it is important to find methods that can annotate and organize documents in meaningful ways to expedite the search process. A considerable amount of research on document classification has been conducted. However, this paper introduces the classification of interviews of cancer patients into several cancer diseases based on the features collected from the corpus. We have developed a corpus of 727 interviews collected from a web archive of medical articles. The TF-IDF features of unigram, bigram, trigram and emotion words as well as the SentiWordNet and Cosine similarity features have been used in training and testing of the classification systems. We have employed three different classifiers like k-NN, Decision Tree and Naive Bayes for classifying the documents into different classes of cancer. The experimental results obtain maximum accuracy of 99.31% tested on 73 documents of the test data. KEYWORDS: TF-IDF, document classification, cancer patients, emotion words and SentiWordNet. 27  "
W12-5306 "<NoAbstract>"
W12-5307 "<NoAbstract>"
W12-5308 " In spontaneous speech, emotion information is embedded at several levels: acoustic, linguistic, gestural (non-verbal), etc. For emotion recognition in speech, there is much attention to acoustic level and some attention at the linguistic level. In this study, we identify paralinguistic markers for emotion in the language. We study two Indian languages belonging to two distinct language families. We consider Marathi from Indo-Aryan and Kannada from Dravidian family. We show that there exist large numbers of specific paralinguistic emotion markers in these languages, referred to as emotiphons. They are inter-twined with prosody and semantics. Preprocessing of speech signal with respect to emotiphons would facilitate emotion recognition in speech for Indian languages. Some of them are common between the two languages, indicating cultural influence in language usage. KEYWORDS : Emotion recognition, emotiphons, emotion markers, Indian languages 73  "
W12-5309 " In this paper, we try to understand how the human cognition identifies various sentiments expressed by different lexical indicators of sentiments in opinion sentences. We use the psychological index, Reaction Time (RT) for the analysis of various lexical indicators required for understanding the sentiment polarity. The test bed was developed using linguistic categories of lexical indicators of sentiments and selected sentences which have various levels of sentiments. Experimental results indicate that variations in syntactic categories of the lexical indicators influence the thought in deciding sentiments at varied levels. The results from this work is to be used for fine tuning machine learning algorithms which are used for sentiment analysis and it can also be used in the development of real time applications such as educational tools to better educate students, particularly those with neurocognitive disorders. KEYWORDS: Cognition, Syntactic Categories, Reaction-Time, Lexical Indicators, Sentiment Analysis 81  "
W12-5310 " Music is a universal language to convey sentiments. Hindustani classical music (HCM) has a long tradition and people from various cultural backgrounds are fascinated by it. Each performance of a given raga in HCM is supposed to create a common mood among most listeners. We have selected solo instrumental clips of bamboo flute for pilot study. We have chosen one instrument in order to eliminate the effect of words in vocal and effect of different timbres. We selected 2 ragas and 3 clips of each raga to understand possible sentiments created. We had total 4 sessions with 20 novice listeners and played 2 clips per session. Listeners have given rating for 13 sentiments on a numeric scale. From the Listeners feedback, we have stated our own observations about the sentiment creation. General sentiments felt by novice Indian listeners were found similar to the expected mood of specific raga. 91  "
W12-5311 "<NoAbstract>"
W12-5801 " Accelerated growth of the World Wide Web has resulted in the evolution of many online collaboration platforms in various domains, including education domain. These platforms, apart from bringing interested stakeholders together, also provide innovative and value added services such as smart search, notifications, suggestions, etc. Techpedia is one of such a platform which facilitates students to submit any original project description and aims to nurture the project idea by mentoring, collaborating and recognizing significant contributions by the system of awards and entrepreneurship. An important aspect of this platform is its ability to suggest a suitable mentor to a students project. We propose an elegant approach to find an appropriate mentor for a given project by analyzing the project abstract. By analyzing past projects guided by various mentors and by engaging Wikipedia knowledge structure during the analysis, we show that our method suggests mentor(s) to a new project with good accuracy. KEYWORDS: mentor suggestion, collaboration, information extraction, wikipedia. 1  "
W12-5802 "<NoAbstract>"
W12-5803 " Computer-aided spoken language learning has been an important area of research. The assessment of a learners pronunciation with respect to native pronunciation lends itself to automation using speech recognition technology. However phone recognition accuracies achievable in state-of-the-art automatic speech recognition systems make their direct application challenging. In this work, linguistic knowledge and the knowledge of speech production are incorporated to obtain a system that discriminates clearly between native and non-native speech. Experimental results on aspirated consonants of Hindi by 10 speakers shows that acousticphonetic features outperform traditional cepstral features in a statistical likelihood based assessment of pronunciation. KEYWORDS : acoustic-phonetic features, language learners, pronunciation, aspirated stops. 17 1  "
W12-5804 " The purpose of this research was to develop an issue-oriented syllabus retrieval system that combined terminological processing, information retrieval, similarity calculation-based document clustering, and visualization. Recently, scientific knowledge has grown explosively because of rapid advancements that have occurred in academia and society. Because of this dramatic expansion of knowledge, learners and educators sometimes struggle to comprehend the overall aspects of syllabi. In addition, learners may find it difficult to discover appropriate courses of study from syllabi because of the increasing growth of interdisciplinary studies programs. We believe that an issueoriented syllabus structure might be more efficient because it provides clear directions for users. In this paper, we introduce an issue-oriented automatic syllabus retrieval system that integrates automatic term recognition as an issue extraction, and similarity calculation as terminology-based document clustering. We use automatically-recognized terms to represent each lecture in clustering and visualization. Retrieved syllabi are automatically classified based on their included terms or issues. The main goal of syllabus retrieval and classification is the development of an issue-oriented syllabus retrieval website that will present users with distilled knowledge in a concise form. In comparison with conventional systems, simple keyword-based syllabus retrieval is based on the assumption that our methods can provide users, and, in particular, novice users (students), with efficient lecture retrieval from an enormous number of syllabi. The system is currently in practical use for issue-oriented syllabus retrieval and clustering for syllabi for the University of Tokyos Open Course Ware and for the School/Department of Engineering. Usability evaluations based on questionnaires used to survey over 100 students revealed that our proposed system is sufficiently efficient at syllabus retrieval. KEYWORDS: Issue oriented, syllabus retrieval, term extraction, knowledge structuring, visualization 25 1  "
W12-5805 "<NoAbstract>"
W12-5807 "<NoAbstract>"
W12-5808 "Abstract Feedback on pronunciation is vital for spoken language teaching. Automatic pronunciation evaluation and feedback can help non-native speakers to identify their errors, learn sounds and vocabulary, and improve their pronunciation performance. These evaluations commonly rely on automatic speech recognition, which could be performed using Sphinx trained on a database of native exemplar pronunciation and non-native examples of frequent mistakes. Adaptation techniques using target users' enrollment data would yield much better recognition of non-native speech. Pronunciation scores can be calculated for each phoneme, word, and phrase by means of Hidden Markov Model alignment with the phonemes of the expected text. In addition to the basic acoustic alignment scores, we have also adopted the edit distance based criterion to compare the scores of the spoken phrase with those of models for various mispronunciations and alternative correct pronunciations. These scores may be augmented with factors such as expected duration and relative pitch to achieve more accurate agreement with expert phoneticians' average manual subjective pronunciation scores. Such a system is built and documented using the CMU Sphinx3 system and an Adobe Flash microphone recording, HTML/JavaScript, and rtmplite/Python user interface. Keywords: Pronunciation Evaluation, Text-independent, forced-alignment, editdistance neighbor phones decoding, CMUSphinx. 61 1 Introduction Pronunciation learning is one of the most important parts of second language acquisition. The aim of this work is to utilize automatic speech recognition technology to facilitate learning spoken language and reading skills. Computer Aided Language Learning (CALL) has received a considerable attention in recent years. Many research eorts have been done for improvement of such systems especially in the eld of second language teaching. Two desirable features of speech enabled computer-based language learning applications are the ability to recognize accented or mispronounced speech produced by language learners, and the ability to provide meaningful feedback on pronunciation quality. The paper is organized into the following sections : Section 2 discusses in detail some of the popular and best performing approaches proposed for pronunciation scoring and computeraided language learning. We present in Section 3 our database preparation for evaluation of the proposed method along with description of TIMIT database used as reference statistics in Text-independent approach and is explained in section 5. Section 4 presents an algorithm to detect mispronunciations based on neighbor phones decoding. Section 5 presents scoring routines for both Text-dependent and Text-independent approaches and nally results are tabulated in section 6 followed by conclusions. "
W12-5809 "<NoAbstract>"
W12-5810 "<NoAbstract>"
W12-5811 " This paper examines the introduction of Easy Japanese by extracting important segments for translation. The need for Japanese language has increased dramatically due to the recent influx of non-Japanese-speaking foreigners. Therefore, in order for non-native speakers of Japanese to successfully adapt to society, the so-called Easy Japanese is being developed to aid them in every aspect from basic conversation to translation of official documents. The materials of our project are the official documents since they are generally distributed in public offices, hospitals, and schools, where they include essential information that should be accessed for all residents. Through an analysis of Japanese language dependency as a pre-experiment, this paper introduces a translation by extracting important segments to facilitate the acquisition of Easy Japanese. Upon effective completion, the project will be introduced for use on the Internet and proposed for use by foreigners living in Japan as well as educators. KEYWORDS : Easy Japanese, Extracting important segments, Translation system, Official documents, Japanese education. 85  "
W12-5901 "ndia {mikhapra,aramana5,v-karthik}@in.ibm.com Abstract Several studies have shown that the task of reordering source sentences to match the target order is crucial to improve the performance of Statistical Machine Translation, especially when the source and target languages have significantly divergent grammatical structures. In fact, it is now become a standard practice to include reordering as a pre-processing step or as an integrated module (within the decoder). However, despite the importance of this sub-task, there is no forum dedicated for its evaluation. The objective of the proposed Shared Task is to provide a common benchmarking platform to evaluate state of the art approaches for reordering. Keywords: Reordering, Machine Translation. 1 "
W12-5902 "ndia {mikhapra,aramana5,v-karthik}@in.ibm.com Abstract Several studies have shown that the task of reordering source sentences to match the target order is crucial to improve the performance of Statistical Machine Translation, especially when the source and target languages have significantly divergent grammatical structures. In fact, it is now become a standard practice to include reordering as a pre-processing step or as an integrated module (within the decoder). However, despite the importance of this sub-task, there is no forum dedicated for its evaluation. The objective of this Shared Task was to provide a common benchmarking platform to evaluate state of the art approaches for reordering. Keywords: Reordering, Machine Translation. 9 "
W12-5903 " For current statistical machine translation system, reordering is still a major problem for language pairs like Chinese-English, where the source and target language have significant word order differences. In this paper we propose a novel tagging-style reordering model. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. For the given source sentence, we assign each source token a label which contains the reordering information for that token. We also design an unaligned word tag so that the unaligned word phenomenon is automatically covered in the proposed model. Our reordering model is conditioned on the whole source sentence. Hence it is able to catch long dependencies in the source sentence. The decoder makes use of the tagging information as soft constraints so that in the test phase (during translation) our model is very efficient. The model training on large scale tasks requests notably amounts of computational resources. We carried out experiments on five Chinese-English NIST tasks trained with BOLT data. Results show that our model improves the baseline system by 0.98 BLEU 1.21 TER on average. KEYWORDS: statistical machine translation, reordering, conditional random fields. 17  "
W12-5904 " This paper describes our submission to the First Workshop on Reordering for Statistical Machine Translation. We have decided to build a reordering system based on tree-tostring model, using only publicly available tools to accomplish this task. With the provided training data we have built a translation model using Moses toolkit, and then we applied a chart decoder, implemented in Moses, to reorder the sentences. Even though our submission only covered English-Farsi language pair, we believe that the approach itself should work regardless of the choice of the languages, so we have also carried out the experiments for English-Italian and English-Urdu. For these language pairs we have noticed a significant improvement over the baseline in BLEU, Kendall-Tau and Hamming metrics. A detailed description is given, so that everyone can reproduce our results. Also, some possible directions for further improvements are discussed. KEYWORDS : reordering with parse, tree-to-string model, Moses toolkit 27 t1 t2 t3 t4 t5 A B t1 t2 t3 t4 t5 A B t1 t2 t3 t4 t5 A B Good Good Bad 1  "
W12-5905 "<NoAbstract>"
W13-0301 " This short paper introduces the first notes about a modality annotation system that is under development for a spontaneous speech Brazilian Portuguese corpus (C-ORALBRASIL). We indicate our methodological decisions, the points which seem to be well resolved and two issues for further discussion and investigation. 1  "
W13-0302 " This paper introduces our methodology for annotating variations in enunciative and modal commitment in a text. We first present the theoretical background of the study which puts the emphasis on the close interaction between time, aspect, modality and evidentiality (TAME) categories (and also markers). We then present our semantic resources which encompass not only lexical items, but also morphological inflections and syntactic constructions. We finally describe the first step of our global natural language processing (NLP) workflow which uses a syntactic analysis parser.  "
W13-0303 " This paper reports on a series of annotation experiments carried out on a number of English adverbials. The experiments, based on occurrences obtained from the British National Corpus, focused on the distinction of epistemic and evidential meanings from other kinds of meanings. The results led to the conclusion that many of the cases of inter-annotator disagreement were due to certain syntactic and semantic factors. Some of these factors will be described in detail, together with the decisions made in each case for prospective annotation.  National Corpus , focused on the distinction of epistemic and evidential meanings from other kinds of meanings. The results led to the conclusion that many of the cases of inter-annotator disagreement were due to certain syntactic and semantic factors. Some of these factors will be described in detail, together with the decisions made in each case for prospective annotation. "
W13-0304 " This paper reports an effort to annotate modality in the Penn Chinese Treebank. We introduce the modals and features that were annotated, and describe the phases of our working process. Along with this, we address the issues in the preparation of annotation guidelines, and present the preliminary results of the first pass. Finally, we analyze the types of disagreement, and propose directions to improve consistency.  "
W13-0305 " This paper investigates the impact of modality markers on the conditional interpretation of the German preposition ohne (without). It tackles the question whether it is the preposition itself that possesses a conditional sense or whether it may be due to a modal context that the interpretation arises. The paper presents an annotation study for modality factors (e.g. mood, modal auxiliary verbs, modal adjectives, modal adverbs, modal infinitives, negation) in the context of these sentences. The statistical analysis of the data has been carried out by means of a correspondence analysis in order to identify the relevant factors for the conditional interpretation. The results suggest that primarily the verb mood has an influence.  "
W13-0306 " We present a linguistically-informed schema for annotating modal expressions and describe its application to a subset of the MPQA corpus of English texts (Wiebe et al. 2005). The annotation is fine-grained in two respects: (i) in the range of expressions that are defined as modal targets and (ii) in the amount of information that is annotated for each target expression. We use inter-annotator reliability results to support a two-way distinction between priority and nonpriority modality types. 1  "
W13-0501 "a Abstract We present an annotation model of modality which is (i) cross-linguistic, relying on a wide, strongly typologically motivated approach, and (ii) hierarchical and layered, accounting for both factuality and speakers attitude, while modelling these two aspects through separate annotation schemes. Modality is defined through cross-linguistic categories, but the classification of actual linguistic expressions is language-specific. This makes our annotation model a powerful tool for investigating linguistic diversity in the field of modality on the basis of real language data, being thus also useful from the perspective of machine translation systems. "
W13-0502 "Abstract Spatial and spatio-temporal information is often carried by non-textual data such as maps, diagrams, tables, or pictures, both still and moving, either embedded in a text or standalone. The annotation of nontextual data raises the following questions: (i) what are the markables and how should they be coded? (ii) how should relevant information be inferred which is implicit in the data? We answer these questions with a multilayered approach. "
W13-0503 "Abstract This paper presents the first description of the motion subcorpus of ISO-SpaceBank (MotionBank) and discusses how motion-events are represented in ISO-Space 1.5, a specification language for the representation of spatial information in language. We present data from this subcorpus with examples from the pilot annotation, focusing specifically on the annotation of motion-events and their various participants. These data inform further discussion of outstanding issues concerning semantic annotation, such as quantification and measurement. We address these questions briefly as they impact the design of ISO-Space. "
W13-0504 "Abstract The Australian National Corpus (AusNC) provides a technical infrastructure for collecting and publishing language resources representing Australian language use. As part of the project we have ingested a wide range of resource types into the system, bringing together the different meta-data and annotations into a single interoperable database. This paper describes the initial collections in AusNC and the procedures used to parse a variety of data types into a single unified annotation store. "
W13-0505 "Abstract In this note, we look at the factors that influence veridicity judgments with factive predicates. We show that more context factors play a role than is generally assumed. We propose to use crowd sourcing techniques to understand these factors better and briefly discuss the consequences for the association of lexical signatures with items in the lexicon. "
W13-0506 "Abstract The computational processing of compound semantics poses several interesting challenges. Up to now, the processing of nominal compounds with non-noun left-hand constituents (henceforth XN compounds) has not received any attention, despite the fact that these also seem to be rather productive in Germanic languages. In our research project, we aim to fill this hiatus by investigating various kinds of compounds in Afrikaans and Dutch, develop annotation protocols and data sets, and model the semantics of such compounds. In this publication we present the alpha version of an annotation protocol that was designed for both descriptive linguistic and computational linguistic purposes. We describe the protocol development and discuss the current version. "
W13-0507 "Department of Spoken Language Systems Saarland University Saarbr  ucken, Germany v.petukhova@lsv. uni-saarland.de Abstract This paper analyzes the issues that arise when trying to add annotations to the dialogues in the Switchboard corpus according to ISO standard 24617-2, exploiting the existing SWBD-DAMSL annotations. These issues relate to differences between the two tag sets; to the highly multidimensional view that underlies the ISO standard; to differences in segmenting the dialogues into functional units; to the use of in-line markups for certain phenomena in Switchboard, and to the use of intra-dialogue dependence relations as defined in the ISO standard. The analysis is supplemented by a discussion of how the existing annotations may be helpful to semi-automatically create a fullyfledged ISO standard annotation alongside the existing SWBD-DAMSL annotation. 1 Introduction In September 2013 the International Organisation for Standardisation ISO published the international standard 24617-2 1 , a comprehensive applicationindependent scheme for dialogue act annotation that is both empirically and theoretically wellfounded, that can deal with typed, spoken, and multimodal dialogue, and that can be used effectively by human annotators and by automatic annotation methods. With the aim of building a large corpus of dialogues, annotated according to this standard, an effort was initiated to create ISO 24617-2 annotations for the dialogues in the Switchboard corpus, which forms a valuable resource for the study of spoken dialogue. 2 In particular, this effort ex1 See the official description of the standard in ISO 246172:2013, and summary descriptions in Bunt et al. (2010; 2012). 2 The Switchboard Dialogue Act Corpus is distributed by LDC. ploits the similarities between the ISO 24617-2 and the SWBD-DAMSL scheme (Jurafsky et al., 1997) by semi-automatically converting SWBDDAMSL annotations into ISO 24617-2 annotations where possible. An additional benefit of this approach is that it allows an in-depth comparison between the two annotation schemes. Fang et al. (2011) have described initial explorations in this project, and Fang et al. (2012) have described the possibilities and limitations of automatically converting SWBD-DAMSL tags to ISO 24617-2 tags. This paper deals with other issues, relating in particular to (1) the highly multidimensional approach to annotation that underlies the ISO standard more clearly than the annotations in the Switchboard corpus; (2) the segmentation of the Switchboard dialogues into slashunits rather than into functional segments, as the ISO standard requires; (3) the use of certain in-line markups and tagging of non-functional phenomena in the Switchboard dialogues; and (4) the annotation of dependence relations between units in a dialogue according to the ISO standard. Example (1), showing a small dialogue fragment (from Switchboard dialogue sw01-0105), as marked up in the Switchboard corpus and as annotated according to ISO 24617-2, illustrates some of the differences between the two approaches. (1) a. (dialogue sw01-0105 lines 0007-0008) A003: qwd {D So } when you say the morning news, or evening news or national news is when? / B004: sd {F Uh, } evening news at six thirty I believe / b. ISO-24617-2 segmentation: fs1 = So fs2 = when you say the morning news, or evening news, or national news, is when? 67 fs3 = Uh fs4 = evening news is at six thirty I believe / c. ISO 24617-2 annotation:       SWBD-DAMSL annotations and ISO 24617-2 annotations clearly use very different representation formats. SWBD-DAMSL makes use of functional tags like qwd (which stands for Declarative WhQuestion) and sd (for Statement non-opinion), in the form of strings attached to stretches of text delineated by /, so-called slash-units (see Section 2.1). Other information is encoded as in-line markups, such as in (1a) a discourse marker by {D So } and a filled pause by {F Uh }; and the identity of the speaker is encoded in line numbers like A003 and B004. ISO standard annotations represent all the information in the form of XML-expressions, making use of the XML-based annotation language DiAML (Dialogue Act Markup Language) which is defined as part of the standard. These annotations are in stand-off form, with an attribute @target whose value identifies the stretch of dialogue that the annotation applies to (a functional segment, see Section 2.1). The annotations in DiAML include not only an identification of the speaker, as in Switchboard, but also of one or more addressees (the attribute @addressee may have multiple values); a specification not only of the communicative function of a dialogue act expressed by the functional segment but also of the communicative dimension that the act belongs to (such as the task that motivates the dialogue, the dimension of turntaking, or the dimension of time management) 3 ; and an indication of relations among dialogue acts, in this example an indication of the question that is answered by an Answer act. An analysis of the similarities and differences between the SWBD-DAMSL and ISO 24617-2 tag sets in Fang et al. (2011; 2012) shows that 14 of the SWBD-DAMSL tags exactly match an ISO 24617-2 communicative function tag, and 27 SWBD-DAMSL tags correspond to 9 ISO standard tags. The latter is due to the fact that SWBDDAMSL sometimes makes distinctions which are not motivated semantically but syntactically or lexically; for example, the tags Yes-answer, Affirmative non-yes answer, No-answer, and Negative non-no answer all correspond to the single ISO tag Answer. In the case of exact matches and many-toone matches, the conversion from SWBD-DAMSL tags to ISO communicative function tags can be done automatically; Fang et al. (2012) report that this can be done for 187,768 of the 223,606 units annotated in the Switchboard corpus, which amounts to 84,0% of the corpus. Replacing SWBD-DAMSL tags by ISO communicative function tags does not create full ISO standard annotations, however, as example (1) showed; not only do we have to replace the tags qwd and sd by the appropriate ISO tags (Set-Question and Inform, respectively) but we also have to consider (1) for each communicative 3 The ISO standard distinguishes nine dimensions: Task, Turn Management, Time Management, Auto-Feedback, AlloFeedback, Own Communication Management, Partner Communication Management, Discourse Structuring, and Social Obligations Management. For definitions see Bunt (2009). 68 function the dimension in which it is used; (2) the addition of communicative functions in those dimensions where SWBD-DAMSL doesnt have any, such as turn management; (3) what to do with the in-line markup of discourse connectives like and filled pauses; (4) how to produce the ISO qualifiers, like certainty=\"uncertain\" and relations between dialogue acts, like functionalDependence=\"#a3\". This paper is structured as follows. Section 2 discusses issues relating to the segmentation of dialogues into meaningful units. Section 3 discusses the annotation of in-line markups. Section 4 discusses the treatment of some phenomena that are not annotated in Switchboard. The concluding Section 6 summarizes the analysis of the main issues involved in adding ISO standard annotations to the Switchboard corpus, and indicates for each of these issues how the additions could be made, exploiting the existing SWBD-DAMSL annotations and the in-line markups of various phenomena. "
W13-0508 "Abstract The last two decades witnessed a great success of revived empiricism in NLP research. However, there are still several NLP tasks that are not successful enough. As one of many directions for going beyond the revived empiricism, this paper introduces a project for annotating annotations with annotators rationales behind them. As a first step of this enterprise, the paper particularly focuses on data collection during the annotation and discusses their potential uses. Finally a preliminary experiment for data collection is described with the data analysis. "
W13-1002 " This paper introduces PersPred, the first manually elaborated syntactic and semantic database for Persian Complex Predicates (CPs). Beside their theoretical interest, Persian CPs constitute an important challenge in Persian lexicography and for NLP. The first delivery, PersPred 1 1 , contains 700 CPs, for which 22 fields of lexical, syntactic and semantic information are encoded. The semantic classification PersPred provides allows to account for the productivity of these combinations in a way which does justice to their compositionality without overlooking their idiomaticity. 1 Introduct  "
W13-1003 " The paper describes a method for identifying and translating multiword expressions using a bi-directional dictionary. While a dictionarybased approach suffers from limited recall, precision is high; hence it is best employed alongside an approach with complementing properties, such as an n-gram language model. We evaluate the method on data from the English-German translation part of the crosslingual word sense disambiguation task in the 2010 semantic evaluation exercise (SemEval). The output of a baseline disambiguation system based on n-grams was substantially improved by matching the target words and their immediate contexts against compound and collocational words in a dictionary. 1 Intro  "
W13-1004 "<NoAbstract>"
W13-1005 "{schulte,scheible}@ims.uni-stuttgart.de Abstract Human ratings are an important source for evaluating computational models that predict compositionality, but like many data sets of human semantic judgements, are often fraught with uncertainty and noise. However, despite their importance, to our knowledge there has been no extensive look at the effects of cleansing methods on human rating data. This paper assesses two standard cleansing approaches on two sets of compositionality ratings for German noun-noun compounds, in their ability to produce compositionality ratings of higher consistency, while reducing data quantity. We find (i) that our ratings are highly robust against aggressive filtering; (ii) Z-score filtering fails to detect unreliable item ratings; and (iii) Minimum Subject Agreement is highly effective at detecting unreliable subjects. "
W13-1006 "Abstract This research focuses on determining semantic compositionality of word expressions using word space models (WSMs). We discuss previous works employing WSMs and present differences in the proposed approaches which include types of WSMs, corpora, preprocessing techniques, methods for determining compositionality, and evaluation testbeds. We also present results of our own approach for determining the semantic compositionality based on comparing distributional vectors of expressions and their components. The vectors were obtained by Latent Semantic Analysis (LSA) applied to the ukWaC corpus. Our results outperform those of all the participants in the Distributional Semantics and Compositionality (DISCO) 2011 shared task. "
W13-1007 "<NoAbstract>"
W13-1008 "Abstract Clich  es, as trite expressions, are predominantly multiword expressions, but not all MWEs are clich  es. We conduct a preliminary examination of the problem of determining how clich  ed a text is, taken as a whole, by comparing it to a reference text with respect to the proportion of more-frequent n-grams, as measured in an external corpus. We find that more-frequent n-grams are over-represented in clich  ed text. We apply this finding to the Eumaeus episode of James Joyces novel Ulysses, which literary scholars believe to be written in a deliberately clich  ed style. "
W13-1009 "tics  {uresova,sindlerova,fucikova,hajic}@ufal.mff.cuni.cz Abstract While working on valency lexicons for Czech and English, it was necessary to define treatment of multiword entities (MWEs) with the verb as the central lexical unit. Morphological, syntactic and semantic properties of such MWEs had to be formally specified in order to create lexicon entries and use them in treebank annotation. Such a formal specification has also been used for automated quality control of the annotation vs. the lexicon entries. We present a corpus-based study, concentrating on multilayer specification of verbal MWEs, their properties in Czech and English, and a comparison between the two languages using the parallel Czech-English Dependency Treebank (PCEDT). This comparison revealed interesting differences in the use of verbal MWEs in translation (discovering that such MWEs are actually rarely translated as MWEs, at least between Czech and English) as well as some inconsistencies in their annotation. Adding MWE-based checks should thus result in better quality control of future treebank/lexicon annotation. Since Czech and English are typologically different languages, we believe that our findings will also contribute to a better understanding of verbal MWEs and possibly their more unified treatment across languages.  This work has been supported by the Grant No. GPP406/13/03351P of the Grant Agency of the Czech Republic. The data used have been provided by the LINDAT/Clarin infrastructural project LM2010013 supported by the MSMT CR http://lindat.cz ).  Authors full address: Institute of Formal and Applied Linguistics, Charles University in Prague, Faculty of Mathematics and Physics, Malostranske nam. 25, 11800 Prague 1, Czech Republic "
W13-1010 " This paper presents a supervised machine learning approach that uses a machine learning algorithm called Random Forest for recognition of Bengali noun-noun compounds as multiword expression (MWE) from Bengali corpus. Our proposed approach to MWE recognition has two steps: (1) extraction of candidate multi-word expressions using Chunk information and various heuristic rules and (2) training the machine learning algorithm to recognize a candidate multi-word expression as Multi-word expression or not. A variety of association measures, syntactic and linguistic clues are used as features for identifying MWEs. The proposed system is tested on a Bengali corpus for identifying noun-noun compound MWEs from the corpus. 1 Intro  "
W13-1011 "E, Russia Abstract This paper presents an algorithm that allows the user to issue a query pattern, collects multi-word expressions (MWEs) that match the pattern, and then ranks them in a uniform fashion. This is achieved by quantifying the strength of all possible relations between the tokens and their features in the MWEs. The algorithm collects the frequency of morphological categories of the given pattern on a unified scale in order to choose the stable categories and their values. For every part of speech, and for all of its categories, we calculate a normalized Kullback-Leibler divergence between the categorys distribution in the pattern and its distribution in the corpus overall. Categories with the largest divergence are considered to be the most significant. The particular values of the categories are sorted according to a frequency ratio. As a result, we obtain morphosyntactic profiles of a given pattern, which includes the most stable category of the pattern, and their values. "
W13-1012 " High frequency can convert a word sequence into a multiword expression (MWE), i.e., a collocation. In this paper, we use collocations as well as syntactically-flexible, lexicalized phrases to analyze job specification documents (a kind of corporate technical document) for subsequent acquisition of automated knowledge elicitation. We propose the definition of structural and functional patterns of specific corporate documents by analyzing the contexts and sections in which the expression occurs. Such patterns and its automated processing are the basis for identifying organizational domain knowledge and business information which is used later for the first instances of requirement elicitation processes in software engineering. 1 Introduct  "
W13-1013 " Based on a lexicon of Portuguese MWE, this presentation focuses on an ongoing work that aims at the creation of a typology that describes these expressions taking into account their semantic, syntactic and pragmatic properties. We also plan to annotate each MWEentry in the mentioned lexicon according to the information obtained from that typology. Our objective is to create a valuable resource, which will allow for the automatic identification MWE in running text and for a deeper understanding of these expressions in their context. 1 Intro  "
W13-1014 "Abstract A challenging topic in Portuguese language processing is the multifunctional and ambiguous use of the clitic pronoun se, which impacts NLP tasks such as syntactic parsing, semantic role labeling and machine translation. Aiming to give a step forward towards the automatic disambiguation of se, our study focuses on the identification of pronominal verbs, which correspond to one of the six uses of se as a clitic pronoun, when se is considered a CONSTITUTIVE PARTICLE of the verb lemma to which it is bound, as a multiword unit. Our strategy to identify such verbs is to analyze the results of a corpus search and to rule out all the other possible uses of se. This process evidenced the features needed in a computational lexicon to automatically perform the disambiguation task. The availability of the resulting lexicon of pronominal verbs on the web enables their inclusion in broader lexical resources, such as the Portuguese versions of Wordnet, Propbank and VerbNet. Moreover, it will allow the revision of parsers and dictionaries already in use. "
W13-1015 "<NoAbstract>"
W13-1016 " We deal with syntactic identification of occurrences of multiword expression (MWE) from an existing dictionary in a text corpus. The MWEs we identify can be of arbitrary length and can be interrupted in the surface sentence. We analyse and compare three approaches based on linguistic analysis at a varying level, ranging from surface word order to deep syntax. The evaluation is conducted using two corpora: the Prague Dependency Treebank and Czech National Corpus. We use the dictionary of multiword expressions SemLex, that was compiled by annotating the Prague Dependency Treebank and includes deep syntactic dependency trees of all MWEs. 1 Introduct  "
W13-1017 "Abstract We present an experimental study of how different features help measuring the idiomaticity of noun+verb (NV) expressions in Basque. After testing several techniques for quantifying the four basic properties of multiword expressions or MWEs (institutionalization, semantic non-compositionality, morphosyntactic fixedness and lexical fixedness), we test different combinations of them for classification into idioms and collocations, using Machine Learning (ML) and feature selection. The results show the major role of distributional similarity, which measures compositionality, in the extraction and classification of MWEs, especially, as expected, in the case of idioms. Even though cooccurrence and some aspects of morphosyntactic flexibility contribute to this task in a more limited measure, ML experiments make benefit of these sources of knowledge, allowing to improve the results obtained using exclusively distributional similarity features. "
W13-1018 "80309 {vaidyaa, mpalmer, narasimb}@colorado.edu Abstract The linguistic annotation of noun-verb complex predicates (also termed as light verb constructions) is challenging as these predicates are highly productive in Hindi. For semantic role labelling, each argument of the noun-verb complex predicate must be given a role label. For complex predicates, frame files need to be created specifying the role labels for each noun-verb complex predicate. The creation of frame files is usually done manually, but we propose an automatic method to expedite this process. We use two resources for this method: Hindi PropBank frame files for simple verbs and the annotated Hindi Treebank. Our method perfectly predicts 65% of the roles in 3015 unique noun-verb combinations, with an additional 22% partial predictions, giving us 87% useful predictions to build our annotation resource. "
W13-1019 " Grading is a primary cognitive operation that has an important expressive function. Information on degree is grammatically relevant and constitutes what Lazard (2006) calls a primary domain of grammaticalization: According to typological studies (Cuzzolin & Lehmann, 2004), many languages of the world have in fact at their disposal multiple grammatical devices to express gradation. In Italian, the class of superlativizing structures alternative to the morphological superlative is very rich and consists, among others, of adverbs of degree, focalizing adverbs and prototypical comparisons. This contribution deals with a particular analytic structure of superlative in Italian that is still neglected in the literature. This is what we will call Constructional Intensifying Adjectives (CIAs), adjectives which modify the intensity of other adjectives on the basis of regular semantic patterns, thus giving rise to multiword superlative constructions of the type: ADJ X +ADJ INTENS . A comparative quantitative corpus analysis demonstrates that this strategy, though paradigmatically limited, is nonetheless widely exploited: From a distributional point of view, some of these CIAs only combine with one or a few adjectives and form MWEs that appear to be completely lexicalized, while some others modify wider classes of adjectives thus displaying a certain degree of productivity. "
W13-1020 "<NoAbstract>"
W13-1601 "<NoAbstract>"
W13-1602 "Abstract We present a bootstrapping algorithm to automatically learn hashtags that convey emotion. Using the bootstrapping framework, we learn lists of emotion hashtags from unlabeled tweets. Our approach starts with a small number of seed hashtags for each emotion, which we use to automatically label tweets as initial training data. We then train emotion classifiers and use them to identify and score candidate emotion hashtags. We select the hashtags with the highest scores, use them to automatically harvest new tweets from Twitter, and repeat the bootstrapping process. We show that the learned hashtag lists help to improve emotion classification performance compared to an N-gram classifier, obtaining 8% microaverage and 9% macro-average improvements in F-measure. "
W13-1603 "b Abstract In this paper, we detail a method for domain specific, multi-category emotion recognition, based on human computation. We create an Amazon Mechanical Turk 1 task that elicits emotion labels and phrase-emotion associations from the participants. Using the proposed method, we create an emotion lexicon, compatible with the 20 emotion categories of the Geneva Emotion Wheel. GEW is the first computational resource that can be used to assign emotion labels with such a high level of granularity. Our emotion annotation method also produced a corpus of emotion labeled sports tweets. We compared the crossvalidated version of the lexicon with existing resources for both the positive/negative and multi-emotion classification problems. We show that the presented domain-targeted lexicon outperforms the existing general purpose ones in both settings. The performance gains are most pronounced for the fine-grained emotion classification, where we achieve an accuracy twice higher than the benchmark. 2 "
W13-1604 "gentina {mamerlin,gravano}@dc.uba.ar Abstract The topic of sentiment analysis in text has been extensively studied in English for the past 30 years. An early, influential work by Cynthia Whissell , the Dictionary of Affect in Language (DAL), allows rating words along three dimensions: pleasantness, activation and imagery. Given the lack of such tools in Spanish, we decided to replicate Whissells work in that language. This paper describes the Spanish DAL, a knowledge base formed by more than 2500 words manually rated by humans along the same three dimensions. We evaluated its usefulness on two sentiment analysis tasks, which showed that the knowledge base managed to capture relevant information regarding the three affective dimensions. "
W13-1605 "Abstract To avoid a sarcastic message being understood in its unintended literal meaning, in microtexts such as messages on Twitter.com sarcasm is often explicitly marked with the hashtag #sarcasm. We collected a training corpus of about 78 thousand Dutch tweets with this hashtag. Assuming that the human labeling is correct (annotation of a sample indicates that about 85% of these tweets are indeed sarcastic), we train a machine learning classifier on the harvested examples, and apply it to a test set of a days stream of 3.3 million Dutch tweets. Of the 35 explicitly marked tweets on this day, we detect 101 (75%) when we remove the hashtag. We annotate the top of the ranked list of tweets most likely to be sarcastic that do not have the explicit hashtag. 30% of the top-250 ranked tweets are indeed sarcastic. Analysis shows that sarcasm is often signalled by hyperbole, using intensifiers and exclamations; in contrast, non-hyperbolic sarcastic messages often receive an explicit marker. We hypothesize that explicit markers such as hashtags are the digital extralinguistic equivalent of nonverbal expressions that people employ in live interaction when conveying sarcasm. "
W13-1606 "Natural Language Engineering Lab., ELiRF. 2 Universitat Polit` ecnica de Val` encia Spain. prosso@dsic.upv.es Abstract Nowadays a large number of opinion reviews are posted on the Web. Such reviews are a very important source of information for customers and companies. The former rely more than ever on online reviews to make their purchase decisions and the latter to respond promptly to their clients expectations. Due to the economic importance of these reviews there is a growing trend to incorporate spam on such sites, and, as a consequence, to develop methods for opinion spam detection. In this paper we focus on the detection of deceptive opinion spam, which consists of fictitious opinions that have been deliberately written to sound authentic, in order to deceive the consumers. In particular we propose a method based on the PU-learning approach which learns only from a few positive examples and a set of unlabeled data. Evaluation results in a corpus of hotel reviews demonstrate the appropriateness of the proposed method for real applications since it reached a f-measure of 0.84 in the detection of deceptive opinions using only 100 positive examples for training. "
W13-1607 "Abstract This paper describes a novel approach for sexual predator detection in chat conversations based on sequences of classifiers. The proposed approach divides documents into three parts, which, we hypothesize, correspond to the different stages that a predator employs when approaching a child. Local classifiers are trained for each part of the documents and their outputs are combined by a chain strategy: predictions of a local classifier are used as extra inputs for the next local classifier. Additionally, we propose a ring-based strategy, in which the chaining process is iterated several times, with the goal of further improving the performance of our method. We report experimental results on the corpus used in the first international competition on sexual predator identification (PAN12). Experimental results show that the proposed method outperforms a standard (global) classification technique for the different settings we consider; besides the proposed method compares favorably with most methods evaluated in the PAN12 competition. "
W13-1608 "Qatar Foundation Doha, Qatar {amourad, kdarwish}@qf.org.qa Abstract Though much research has been conducted on Subjectivity and Sentiment Analysis (SSA) during the last decade, little work has focused on Arabic. In this work, we focus on SSA for both Modern Standard Arabic (MSA) news articles and dialectal Arabic microblogs from Twitter. We showcase some of the challenges associated with SSA on microblogs. We adopted a random graph walk approach to extend the Arabic SSA lexicon using ArabicEnglish phrase tables, leading to improvements for SSA on Arabic microblogs. We used different features for both subjectivity and sentiment classification including stemming, part-of-speech tagging, as well as tweet specific features. Our classification features yield results that surpass Arabic SSA results in the literature. "
W13-1609 "Abstract This article provides an in-depth research of machine learning methods for sentiment analysis of Czech social media. Whereas in English, Chinese, or Spanish this field has a long history and evaluation datasets for various domains are widely available, in case of Czech language there has not yet been any systematical research conducted. We tackle this issue and establish a common ground for further research by providing a large humanannotated Czech social media corpus. Furthermore, we evaluate state-of-the-art supervised machine learning methods for sentiment analysis. We explore different pre-processing techniques and employ various features and classifiers. Moreover, in addition to our newly created social media dataset, we also report results on other widely popular domains, such as movie and product reviews. We believe that this article will not only extend the current sentiment analysis research to another family of languages, but will also encourage competition which potentially leads to the production of high-end commercial solutions. "
W13-1610 "Abstract We discuss a tagging scheme to tag data for training information extraction models which can extract the features of a product/service and opinions about them from textual reviews, and which can be used across different domains with minimal adaptation. A simple tagging scheme results in a large number of domain dependent opinion phrases and impedes the usefulness of the trained models across domains. We show that by using minor modifications to this simple tagging scheme the number of domain dependent opinion phrases are reduced from 36% to 17%, which leads to models more useful across domains. "
W13-1611 " We compare the performance of two lexiconbased sentiment systems  SentiStrength (Thelwall et al., 2012) and SO-CAL (Taboada et al., 2011)  on the two genres of newspaper text and tweets. While SentiStrength has been geared specifically toward short social-media text, SO-CAL was built for general, longer text. After the initial comparison, we successively enrich the SO-CAL-based analysis with tweet-specific mechanisms and observe that in some cases, this improves the performance. A qualitative error analysis then identifies classes of typical problems the two systems have with tweets. 1 Intro  "
W13-1612 "Abstract Up until now most of the methods published for polarity classification are applied to English texts. However, other languages on the Internet are becoming increasingly important. This paper presents a set of experiments on English and Spanish product reviews. Using a comparable corpus, a supervised method and two unsupervised methods have been assessed. Furthermore, a list of Spanish opinion words is presented as a valuable resource. "
W13-1613 "<NoAbstract>"
W13-1614 "Abstract We describe TWITA, the first corpus of Italian tweets, which is created via a completely automatic procedure, portable to any other language. We experiment with sentiment analysis on two datasets from TWITA: a generic collection and a topic-specific collection. The only resource we use is a polarity lexicon, which we obtain by automatically matching three existing resources thereby creating the first polarity database for Italian. We observe that albeit shallow, our simple system captures polarity distinctions matching reasonably well the classification done by human judges, with differences in performance across polarity values and on the two sets. "
W13-1615 " In this work, we attempt to detect sentencelevel subjectivity by means of two supervised machine learning approaches: a Fuzzy Control System and Adaptive Neuro-Fuzzy Inference System. Even though these methods are popular in pattern recognition, they have not been thoroughly investigated for subjectivity analysis. We present a novel Pruned ICF Weighting Coefficient, which improves the accuracy for subjectivity detection. Our feature extraction algorithm calculates a feature vector based on the statistical occurrences of words in a corpus without any lexical knowledge. For this reason, these machine learning models can be applied to any language; i.e., there is no lexical, grammatical, syntactical analysis used in the classification process. 1 Introdu  "
W13-1616 " Sentiment analysis means to extract opinion of users from review documents. Sentiment classification using Machine Learning (ML) methods faces the problem of high dimensionality of feature vector. Therefore, a feature selection method is required to eliminate the irrelevant and noisy features from the feature vector for efficient working of ML algorithms. Rough Set Theory based feature selection method finds the optimal feature subset by eliminating the redundant features. In this paper, Rough Set Theory (RST) based feature selection method is applied for sentiment classification. A Hybrid feature selection method based on RST and Information Gain (IG) is proposed for sentiment classification. Proposed methods are evaluated on four standard datasets viz. Movie review, product (book, DVD and electronics) review dataset. Experimental results show that Hybrid feature selection method outperforms than other feature selection methods for sentiment classification. 1 Introductio  "
W13-1901 "Abstract This research analyzed the clinical notes of epilepsy patients using techniques from corpus linguistics and machine learning and predicted which patients are candidates for neurosurgery, i.e. have intractable epilepsy, and which are not. Information-theoretic and machine learning techniques are used to determine whether and how sets of clinic notes from patients with intractable and nonintractable epilepsy are different. The results show that it is possible to predict from an early stage of treatment which patients will fall into one of these two categories based only on text data. These results have broad implications for developing clinical decision support systems. "
W13-1902 " Identification of complex clinical phenotypes among critically ill patients is a major challenge in clinical research. The overall research goal of our work is to develop automated approaches that accurately identify critical illness phenotypes to prevent the resource intensive manual abstraction approach. In this paper, we describe a text processing method that uses Natural Language Processing (NLP) and supervised text classification methods to identify patients who are positive for Acute Lung Injury (ALI) based on the information available in free-text chest x-ray reports. To increase the classification performance we enhanced the baseline unigram representation with bigram and trigram features, enriched the n-gram features with assertion analysis, and applied statistical feature selection. We used 10-fold cross validation for evaluation and our best performing classifier achieved 81.70% precision (positive predictive value), 75.59% recall (sensitivity), 78.53% f-score, 74.61% negative predictive value, 76.80% specificity in identifying patients with ALI. 1 Introduction  "
W13-1903 " The clinical narrative contains a great deal of valuable information that is only understandable in a temporal context. Events, time expressions, and temporal relations convey information about the time course of a patients clinical record that must be understood for many applications of interest. In this paper, we focus on extracting information about how time expressions and events are related by narrative containers. We use support vector machines with composite kernels, which allows for integrating standard feature kernels with tree kernels for representing structured features such as constituency trees. Our experiments show that using tree kernels in addition to standard feature kernels improves F1 classification for this task. 1 Intro  "
W13-1904 "Abstract In order to integrate heterogeneous clinical information sources, semantically correlating information entities have to be linked. Our discussions with radiologists revealed that anatomical entities with pathological findings are of particular interest when linking radiology text and images. Previous research to identify pathological findings focused on simplistic approaches that recognize diseases or negated findings, but failed to establish a holistic approach. In this paper, we introduce our syntacto-semantic parsing approach to classify sentences in radiology reports as either pathological or non-pathological based on the findings they describe. Although we operate with an incomplete, RadLex-based linguistic resource, the obtained results show the effectiveness of our approach by identifying a recall value of 74.3% for the classification task. 1 Introdu     3 1 Siemens AG, Corporate Technology, 81739 Munich, Germany 2 University Munich, Center for Information and Language Processing, 80538 Munich, Germany 3 University Hospital Erlangen, Department of Radiology, 91054 Erlangen, Germany {claudia.bretschneider.ext,sonja.zillner}@siemens.com, matthias.hammon@uk-erlangen.de Abstract In order to integrate heterogeneous clinical information sources, semantically correlating information entities have to be linked. Our discussions with radiologists revealed that anatomical entities with pathological findings are of particular interest when linking radiology text and images. Previous research to identify pathological findings focused on simplistic approaches that recognize diseases or negated findings, but failed to establish a holistic approach. In this paper, we introduce our syntacto-semantic parsing approach to classify sentences in radiology reports as either pathological or non-pathological based on the findings they describe. Although we operate with an incomplete, RadLex-based linguistic resource, the obtained results show the effectiveness of our approach by identifying a recall value of 74.3% for the classification task. "
W13-1905 "ia San Diego, USA Abstract The various ways in which one can refer to the same clinical concept needs to be accounted for in a semantic resource such as SNOMED CT. Developing terminological resources manually is, however, prohibitively expensive and likely to result in low coverage, especially given the high variability of language use in clinical text. To support this process, distributional methods can be employed in conjunction with a large corpus of electronic health records to extract synonym candidates for clinical terms. In this paper, we exemplify the potential of our proposed method using the Swedish version of SNOMED CT, which currently lacks synonyms. A medical expert inspects two thousand term pairs generated by two semantic spaces  one of which models multiword terms in addition to single words  for one hundred preferred terms of the semantic types disorder and finding. "
W13-1906 "Istituto di Linguistica Computazionale Antonio Zampolli (ILCCNR) Via G. Moruzzi, 1  Pisa (Italy) {felice.dellorletta,giulia.venturi,simonetta.montemagni}@ilc.cnr.it Abstract In this paper, a new selftraining method for domain adaptation is illustrated, where the selection of reliable parses is carried out by an unsupervised linguistically driven algorithm, ULISSE. The method has been tested on biomedical texts with results showing a significant improvement with respect to considered baselines, which demonstrates its ability to capture both reliability of parses and domain specificity of linguistic constructions. "
W13-1907 " While interest in biomedical question answering has been growing, research in consumer health question answering remains relatively sparse. In this paper, we focus on the task of consumer health question understanding. We present a rule-based methodology that relies on lexical and syntactic information as well as anaphora/ellipsis resolution to construct structured representations of questions (frames). Our results indicate the viability of our approach and demonstrate the important role played by anaphora and ellipsis in interpreting consumer health questions. 1 Int  "
W13-1908 " Text mining methods for the biomedical domain have matured substantially and are currently being applied on a large scale to support a variety of applications in systems biology, pathway curation, data integration and gene summarization. Community-wide challenges in the BioNLP research field provide goldstandard datasets and rigorous evaluation criteria, allowing for a meaningful comparison between techniques as well as measuring progress within the field. However, such evaluations are typically conducted on relatively small training and test datasets. On a larger scale, systematic erratic behaviour may occur that severely influences hundreds of thousands of predictions. In this work, we perform a critical assessment of a large-scale text mining resource, identifying systematic errors and determining their underlying causes through semi-automated analyses and manual evaluations 1 . "
W13-1909 "Abstract It has long been realized that sublanguages are relevant to natural language processing and text mining. However, practical methods for recognizing or characterizing them have been lacking. This paper describes a publicly available set of tools for sublanguage recognition. Closure properties are used to assess the goodness of fit of two biomedical corpora to the sublanguage model. Scientific journal articles are compared to general English text, and it is shown that the journal articles fit the sublanguage model, while the general English text does not. A number of examples of implications of the sublanguage characteristics for natural language processing are pointed out. The software is made publicly available at [edited for anonymization]. "
W13-1910 "Interpreting the rapidly increasing amount of experimental data requires the availability and representation of biological knowledge in a computable form. The Biological expression language (BEL) encodes the data in form of causal relationships, which describe the association between biological events. BEL can successfully be applied to large data and support causal reasoning and hypothesis generation. With the rapid growth of biomedical literature, automated methods are a crucial prerequisite for handling and encoding the available knowledge. The BioNLP shared tasks support the development of such tools and provide a linguistically motivated format for the annotation of relations. On the other hand, BEL statements and the corresponding evidence sentences might be a valuable resource for future BioNLP shared task training data generation. In this paper, we briefly introduce BEL and investigate how far BioNLP-shared task annotations could be converted to BEL statements and in such a way directly support BEL statement generation. We present the first results of the automatic BEL statement generation and emphasize the need for more training data that captures the underlying biological meaning. "
W13-1911 "Abstract We present a set of new measures designed to reveal latent information of language use in children at the lexico-syntactic level. We used these metrics to analyze linguistic patterns in spontaneous narratives from children developing typically and children identified as having a language impairment. We observed significant differences in the z-scores of both populations for most of the metrics. These findings suggest we can use these metrics to aid in the task of language assessment in children. "
W13-1912 "Abstract Sentence types typical to Swedish clinical text were extracted by comparing sentence part-of-speech tag sequences in clinical and in standard Swedish text. Parsings by a syntactic dependency parser, trained on standard Swedish, were manually analysed for the 33 sentence types most typical to clinical text. This analysis resulted in the identification of eight error types, and for two of these error types, preprocessing rules were constructed to improve the performance of the parser. For all but one of the ten sentence types affected by these two rules, the parsing was improved by pre-processing. "
W13-1913 "Abstract MEDLINE/PubMed contains structured abstracts that can provide argumentative labels. Selection of abstract sentences based on the argumentative label has shown to improve the performance of information retrieval tasks. These abstracts make up less than one quarter of all the abstracts in MEDLINE/PubMed, so it is worthwhile to learn how to automatically label the non-structured ones. We have compared several machine learning algorithms trained on structured abstracts to identify argumentative labels. We have performed an intrinsic evaluation on predicting argumentative labels for non-structured abstracts and an extrinsic evaluation to predict argumentative labels on abstracts relevant to Gene Reference Into Function (GeneRIF) indexing. Intrinsic evaluation shows that argumentative labels can be assigned effectively to structured abstracts. Algorithms that model the argumentative structure seem to perform better than other algorithms. Extrinsic results show that assigning argumentative labels to non-structured abstracts improves the performance on GeneRIF indexing. On the other hand, the algorithms that model the argumentative structure of the abstracts obtain lower performance in the extrinsic evaluation. "
W13-1914 "Abstract Child language narratives are used for language analysis, measurement of language development, and the detection of language impairment. In this paper, we explore the use of Latent Dirichlet Allocation (LDA) for detecting topics from narratives, and use the topics derived from LDA in two classification tasks: automatic prediction of coherence and language impairment. Our experiments show LDA is useful for detecting the topics that correspond to the narrative structure. We also observed improved performance for the automatic prediction of coherence and language impairment when we use features derived from the topic words provided by LDA. "
W13-1915 "<NoAbstract>"
W13-2001 " The BioNLP Shared Task 2013 is the third edition of the BioNLP Shared Task series that is a community-wide effort to address fine-grained, structural information extraction from biomedical literature. The BioNLP Shared Task 2013 was held from January to April 2013. Six main tasks were proposed. 38 final submissions were received, from 22 teams. The results show advances in the state of the art and demonstrate that extraction methods can be successfully generalized in various aspects. "
W13-2002 " The Genia Event Extraction task is organized for the third time, in BioNLP Shared Task 2013. Toward knowledge based construction, the task is modified in a number of points. As the final results, it received 12 submissions, among which 2 were withdrawn from the final report. This paper presents the task setting, data sets, and the final results with discussion for possible future directions. 1 Intro  "
W13-2003 "<NoAbstract>"
W13-2004 " During the past few years, several novel text mining algorithms have been developed in the context of the BioNLP Shared Tasks on Event Extraction. These algorithms typically aim at extracting biomolecular interactions from text by inspecting only the context of one sentence. However, when humans interpret biomolecular research articles, they usually build upon extensive background knowledge of their favorite genes and pathways. To make such world knowledge available to a text mining algorithm, it could first be applied to all available literature to subsequently make a more informed decision on which predictions are consistent with the current known data. In this paper, we introduce our participation in the latest Shared Task using the largescale text mining resource EVEX which we previously implemented using state-ofthe-art algorithms, and which was applied to the whole of PubMed and PubMed Central. We participated in the Genia Event Extraction (GE) and Gene Regulation Network (GRN) tasks, ranking first in the former and fifth in the latter. "
W13-2005 "Abstract The Genia Event (GE) extraction task of the BioNLP Shared Task addresses the extraction of biomedical events from the natural language text of the published literature. In our submission, we modified an existing system for learning of event patterns via dependency parse subgraphs to utilise a more accurate parser and significantly more, but noisier, training data. We explore the impact of these two aspects of the system and conclude that the change in parser limits recall to an extent that cannot be offset by the large quantities of training data. However, our extensions of the system to extract modification events shows promise. "
W13-2006 "Abstract This paper describes the HDS4NLP entry to the BioNLP 2013 shared task on biomedical event extraction. This system is based on a pairwise model that transforms trigger classification in a simple multi-class problem in place of the usual multi-label problem. This model facilitates inference compared to global models while relying on richer information compared to usual pipeline approaches. The HDS4NLP system ranked 6th on the Genia task (43.03% f-score), and after fixing a bug discovered after the final submission, it outperforms the winner of this task (with a f-score of 51.15%). "
W13-2007 " Semantic querying over the biomedical literature has gained popularity, where a semantic representation of biomedical documents is required. Previous BioNLP Shared Tasks exercised semantic event extraction with a small number of pre-defined event concepts. The GRO task of the BioNLP13-ST imposes the challenge of dealing with over 100 GRO concepts. Its annotated corpus consists of 300 MEDLINE abstracts, and an analysis of interannotator agreement on the annotations by two experts shows Kappa values between 43% and 56%. The results from the only participant are promising with F-scores 22% (events) and 63% (relations), and also lead us to open issues such as the need to consider the ontology structure. 1 Backgroun  "
W13-2008 " We present the design, preparation, results and analysis of the Cancer Genetics (CG) event extraction task, a main task of the BioNLP Shared Task (ST) 2013. The CG task is an information extraction task targeting the recognition of events in text, represented as structured n-ary associations of given physical entities. In addition to addressing the cancer domain, the CG task is differentiated from previous event extraction tasks in the BioNLP ST series in addressing a wide range of pathological processes and multiple levels of biological organization, ranging from the molecular through the cellular and organ levels up to whole organisms. Final test set submissions were accepted from six teams. The highest-performing system achieved an Fscore of 55.4%. This level of performance is broadly comparable with the state of the art for established molecular-level extraction tasks, demonstrating that event extraction resources and methods generalize well to higher levels of biological organization and are applicable to the analysis of scientific texts on cancer. The CG task continues as an open challenge to all interested parties, with tools and resources available from http://2013 . bionlp-st.org/. "
W13-2009 " We present the Pathway Curation (PC) task, a main event extraction task of the BioNLP shared task (ST) 2013. The PC task concerns the automatic extraction of biomolecular reactions from text. The task setting, representation and semantics are defined with respect to pathway model standards and ontologies (SBML, BioPAX, SBO) and documents selected by relevance to specific model reactions. Two BioNLP ST 2013 participants successfully completed the PC task. The highest achieved Fscore, 52.8%, indicates that event extraction is a promising approach to supporting pathway curation efforts. The PC task continues as an open challenge with data, resources and tools available from http://2013.bionlp-st.org/  http://2013.bionlp-st.org/ 1 Introduct  "
W13-2010 "NCBI, Bethesda, MD, USA Abstract We participated in the BioNLP 2013 shared tasks, addressing the GENIA (GE) and the Cancer Genetics (CG) event extraction tasks. Our event extraction is based on the system we recently proposed for mining relations and events involving genes or proteins in the biomedical literature using a novel, approximate subgraph matching-based approach. In addition to handling the GE task involving 13 event types uniformly related to molecular biology, we generalized our system to address the CG task targeting a challenging set of 40 event types related to cancer biology with various arguments involving 18 kinds of biological entities. Moreover, we attempted to integrate a distributional similarity model into our system to extend the graph matching scheme for more events. In addition, we evaluated the impact of using paths of all possible lengths among event participants as key contextual dependencies to extract potential events as compared to using only the shortest paths within the framework of our system. We achieved a 46.38% F-score in the CG task and a 48.93% F-score in the GE task, ranking 3rd and 4th respectively. The consistent performance confirms that our system generalizes well to various event extraction tasks and scales to handle a large number of event and entity types. "
W13-2011 " We tested a linguistically motivated rulebased system in the Cancer Genetics task of the BioNLP13 shared task challenge. The performance of the system was very moderate, ranging from 52% against the development set to 45% against the test set. Interestingly, the performance of the system did not change appreciably when using only entities tagged by the inbuilt tagger as compared to performance using the gold-tagged entities. The lack of an event anaphoric module, as well as problems in reducing events generated by a large trigger class to the task-specific event subset, were likely major contributory factors to the rather moderate performance. 1 Intro  "
W13-2012 " This paper describes NaCTeM entries for the Cancer Genetics (CG) and Pathway Curation (PC) tasks in the BioNLP Shared Task 2013. We have applied a state-ofthe-art event extraction system EventMine to the tasks in two different settings: a single-corpus setting for the CG task and a stacking setting for the PC task. EventMine was applicable to the two tasks with simple task specific configuration, and it produced a reasonably high performance, positioning second in the CG task and first in the PC task. 1 I  "
W13-2013 "{comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov Abstract This paper describes the technical contribution of the supporting resources provided for the BioNLP Shared Task 2013. Following the tradition of the previous two BioNLP Shared Task events, the task organisers and several external groups sought to make system development easier for the task participants by providing automatically generated analyses using a variety of automated tools. Providing analyses created by different tools that address the same task also enables extrinsic evaluation of the tools through the evaluation of their contributions to the event extraction task. Such evaluation can improve understanding of the applicability and benefits of specific tools and representations. The supporting resources described in this paper will continue to be publicly available from the shared task homepage http://2013.bionlp-st.org/  "
W13-2014 " In this paper we present a biomedical event extraction system for the BioNLP 2013 event extraction task. Our system consists of two phases. In the learning phase, a dictionary and patterns are generated automatically from annotated events. In the extraction phase, the dictionary and obtained patterns are applied to extract events from input text. When evaluated on the GENIA event extraction task of the BioNLP 2013 shared task, the system obtained the best results on strict matching and the third best on approximate span and recursive matching, with F-scores of 48.92 and 50.68, respectively. Moreover, it has excellent performance in terms of speed.  "
W13-2015 " We describe a system for extracting biomedical events among genes and proteins from biomedical literature, using the corpus from the BioNLP13 Shared Task on Event Extraction. The proposed system is characterized by a wide array of features based on dependency parse graphs and additional argument information in the second trigger detection. Based on the Uturku system which is the best one in the BioNLP09 Shared Task, we improve the performance of biomedical event extraction by reducing illegal events and false positives in the second trigger detection and the second argument detection. On the development set of BioNLP13, the system achieves an F-score of 50.96% on the primary task. On the test set of BioNLP13, it achieves an F-score of 47.56% on the primary task obtaining the 5th place in task 1, which is 1.78 percentage points higher than the baseline (following the Uturku system), demonstrating that the proposed method is efficient. 1 Introduct  "
W13-2016 "Abstract We describe a biological event detection method implemented for the Genia Event Extraction task of BioNLP 2013. The method relies on syntactic dependency relations provided by a general NLP pipeline, supported by statistics derived from Maximum Entropy models for candidate trigger words, for potential arguments, and for argument frames. "
W13-2017 " In this paper we propose a system which uses hybrid methods that combine both rule-based and machine learning (ML)-based approaches to solve GENIA Event Extraction of BioNLP Shared Task 2013. We apply UIMA 1 Framework to support coding. There are three main stages in model: Pre-processing, trigger detection and biomedical event detection. We use dictionary and support vector machine classifier to detect event triggers. Event detection is applied on syntactic patterns which are combined with features extracted for classification. 1 Intro  "
W13-2018 "Kingdom {R.Roller, M.Stevenson}@dcs.shef.ac.uk Abstract We describe our system to extract genia events that was developed for the BioNLP 2013 Shared Task. Our system uses a supervised information extraction platform based on Support Vector Machines (SVM) and separates the process of event classification into multiple stages. For each event type the SVM parameters are adjusted and feature selection carried out. We find that this optimisation improves the performance of our approach. Overall our system achieved the highest precision score of all systems and was ranked 6th of 10 participating systems on F-measure (strict matching). "
W13-2019 " We describe a high precision system for extracting events of biomedical significance that was developed during the BioNLP shared task 2013 and tested on the Cancer Genetics data set. The system achieved an F-score on the development data of 73.67 but was ranked 5 th out of six with an F-score of 29.94 on the test data. However, precision was the second highest ranked on the task at 62.73. Analysis suggests the need to continue to improve our system for complex events particularly taking into account cross-domain differences in argument distributions. 1 Int  "
W13-2020 "<NoAbstract>"
W13-2021 "Abstract In the perspective of annotating a text with respect to an ontology, we have participated in the subtask 1 of the BB BioNLPST whose aim is to detect, in the text, Bacteria Habitats and associate to them one or several categories from the OntoBiotope ontology provided for the task. We have used a rule-based machine learning algorithm (WHISK) combined with a rule-based automatic ontology projection method and a rote learning technique. The combination of these three sources of rules leads to good results with a SER measure close to the winner and a best F-measure. "
W13-2022 "Abstract In this paper, we present the methods we used to extract bacteria and biotopes names and then to identify the relation between those entities while participating to the BioNLP13 Bacteria and Biotopes Shared Task. We used machine-learning based approaches for this task, namely a CRF to extract bacteria and biotopes names and a simple matching algorithm to predict the relations. We achieved poor results: an SER of 0.66 in sub-task 1, and a 0.06 F-measure in both sub-tasks 2 and 3. "
W13-2023 " The goal of the Genic Regulation Network task (GRN) is to extract a regulation network that links and integrates a variety of molecular interactions between genes and proteins of the well-studied model bacterium Bacillus subtilis. It is an extension of the BI task of BioNLP-ST11. The corpus is composed of sentences selected from publicly available PubMed scientific abstracts. The paper details the corpus specifications, the evaluation metrics, and it summarizes and discusses the participant results.  "
W13-2024 "<NoAbstract>"
W13-2025 "Abstract The absence of a comprehensive database of locations where bacteria live is an important obstacle for biologists to understand and study the interactions between bacteria and their habitats. This paper reports the results to a challenge, set forth by the Bacteria Biotopes Task of the BioNLP Shared Task 2013. Two systems are explained: Sub-task 1 system for identifying habitat mentions in unstructured biomedical text and normalizing them through the OntoBiotope ontology and Sub-task 2 system for extracting localization and partof relations between bacteria and habitats. Both approaches rely on syntactic rules designed by considering the shallow linguistic analysis of the text. Sub-task 2 system also makes use of discourse-based rules. The two systems achieve promising results on the shared task test data set. "
W13-2026 "University of Ljubljana Tr za ska cesta 25 SI-1000 Ljubljana {name.surname}@fri.uni-lj.si  Optilab d.o.o. Dunajska cesta 152 SI-1000 Ljubljana Abstract Published literature in molecular genetics may collectively provide much information on gene regulation networks. Dedicated computational approaches are required to sip through large volumes of text and infer gene interactions. We propose a novel sieve-based relation extraction system that uses linear-chain conditional random fields and rules. Also, we introduce a new skip-mention data representation to enable distant relation extraction using first-order models. To account for a variety of relation types, multiple models are inferred. The system was applied to the BioNLP 2013 Gene Regulation Network Shared Task. Our approach was ranked first of five, with a slot error rate of 0.73. "
W13-2101 "{v.basile,johan.bos}@rug.nl Center for Language and Cognition Groningen (CLCG) University of Groningen, The Netherlands Abstract Statistical natural language generation from abstract meaning representations presupposes large corpora consisting of textmeaning pairs. Even though such corpora exist nowadays, or could be constructed using robust semantic parsing, the simple alignment between text and meaning representation is too coarse for developing robust (statistical) NLG systems. By reformatting semantic representations as graphs, fine-grained alignment can be obtained. Given a precise alignment at the word level, the complete surface form of a meaning representations can be deduced using a simple declarative rule. "
W13-2102 "The increasing amount of machinereadable data available in the context of the Semantic Web creates a need for methods that transform such data into human-comprehensible text. In this paper we develop and evaluate a Natural Language Generation (NLG) system that converts RDF data into natural language text based on an ontology and an associated ontology lexicon. While it follows a classical NLG pipeline, it diverges from most current NLG systems in that it exploits an ontology lexicon in order to capture context-specific lexicalisations of ontology concepts, and combines the use of such a lexicon with the choice of lexical items and syntactic structures based on statistical information extracted from a domain-specific corpus. We apply the developed approach to the cooking domain, providing both an ontology and an ontology lexicon in lemon format. Finally, we evaluate fluency and adequacy of the generated recipes with respect to two target audiences: cooking novices and advanced cooks. "
W13-2103 "Abstract In this paper we describe a natural language generation system which produces complex sentences from a biology knowledge base. The NLG system allows domain experts to discover errors in the knowledge base and generates certain parts of answers in response to users questions in an e-textbook application. The system allows domain experts to customise its lexical resources and to set parameters which influence syntactic constructions in generated sentences. The system is capable of dealing with certain types of incomplete inputs arising from a knowledge base which is constantly edited and includes a referring expression generation module which keeps track of discourse history. Our referring expression module is available for download as the open source Antfarm tool 1 . "
W13-2104 "0, USA {howcroft,cnakatsu,mwhite}@ling.osu.edu Abstract We show that Nakatsu & Whites (2010) proposed enhancements to the SPaRKy Restaurant Corpus (SRC; Walker et al., 2007) for better expressing contrast do indeed make it possible to generate better texts, including ones that make effective and varied use of contrastive connectives and discourse adverbials. After first presenting a validation experiment for naturalness ratings of SRC texts gathered using Amazons Mechanical Turk, we present an initial experiment suggesting that such ratings can be used to train a realization ranker that enables higher-rated texts to be selected when the ranker is trained on a sample of generated restaurant recommendations with the contrast enhancements than without them. We conclude with a discussion of possible ways of improving the ranker in future work. "
W13-2105 " In this paper, we focus on the task of generating elliptic sentences. We extract from the data provided by the Surface Realisation (SR) Task (Belz et al., 2011) 2398 input whose corresponding output sentence contain an ellipsis. We show that 9% of the data contains an ellipsis and that both coverage and BLEU score markedly decrease for elliptic input (from 82.3% coverage for non-elliptic sentences to 65.3% for elliptic sentences and from 0.60 BLEU score to 0.47). We argue that elided material should be represented using phonetically empty nodes and we introduce a set of rewrite rules which permits adding these empty categories to the SR data. Finally, we evaluate an existing surface realiser on the resulting dataset. We show that, after rewriting, the generator achieves a coverage of 76% and a BLEU score of 0.74 on the elliptical data. 1 Introduct  "
W13-2106 "Abstract We present an Integer Linear Programming model of content selection, lexicalization, and aggregation that we developed for a system that generates texts from OWL ontologies. Unlike pipeline architectures, our model jointly considers the available choices in these three text generation stages, to avoid greedy decisions and produce more compact texts. Experiments with two ontologies confirm that it leads to more compact texts, compared to a pipeline with the same components, with no deterioration in the perceived quality of the generated texts. We also present an approximation of our model, which allows longer texts to be generated efficiently. "
W13-2107 "Abstract Planning-based approaches to reference provide a uniform treatment of linguistic decisions, from content selection to lexical choice. In this paper, we show how the issues of lexical ambiguity, vagueness, unspecific descriptions, ellipsis, and the interaction of subsective modifiers can be expressed using a belief-state planner modified to support context-dependent actions. Because the number of distinct denotations it searches grows doublyexponentially with the size of the referential domain, we present representational and search strategies that make generation and interpretation tractable. "
W13-2108 "TiCC University of Tilburg Tilburg, The Netherlands Abstract When they introduced the Graph-Based Algorithm (GBA) for referring expression generation, Krahmer et al. (2003) flaunted the natural way in which it deals with relations between objects; but this feature has never been tested empirically. We fill this gap in this paper, exploring referring expression generation from the perspective of the GBA and focusing in particular on generating human-like expressions in visual scenes with spatial relations. We compare the original GBA against a variant that we introduce to better reflect human reference, and find that although the original GBA performs reasonably well, our new algorithm offers an even better match to human data (77.91% Dice). Further, it can be extended to capture speaker variation, reaching an 82.83% Dice overlap with human-produced expressions. "
W13-2109 "<NoAbstract>"
W13-2110 "<NoAbstract>"
W13-2111 "<NoAbstract>"
W13-2112 "Abstract In this overview paper we present the outcome of the first content selection challenge from open semantic web data, focusing mainly on the preparatory stages for defining the task and annotating the data. The task to perform was described in the challenges call as follows: given a set of RDF triples containing facts about a celebrity, select those triples that are reflected in the target text (i.e., a short biography about that celebrity). From the initial nine expressions of interest, finally two participants submitted their systems for evaluation. "
W13-2113 "<NoAbstract>"
W13-2114 "Abstract When instructors prepare learning materials for students, they frequently develop accompanying questions to guide learning. Natural language processing technology can be used to automatically generate such questions but techniques used have not fully leveraged semantic information contained in the learning materials or the full context in which the question generation task occurs. We introduce a sophisticated template-based approach that incorporates semantic role labels into a system that automatically generates natural language questions to support online learning. While we have not yet incorporated the full learning context into our approach, our preliminary evaluation and evaluation methodology indicate our approach is a promising one for supporting learning. "
W13-2115 "cotland {dg106, h.hastie, sc445, o.lemon} @hw.ac.uk Abstract We describe a statistical Natural Language Generation (NLG) method for summarisation of time-series data in the context of feedback generation for students. In this paper, we initially present a method for collecting time-series data from students (e.g. marks, lectures attended) and use example feedback from lecturers in a datadriven approach to content selection. We show a novel way of constructing a reward function for our Reinforcement Learning agent that is informed by the lecturers method of providing feedback. We evaluate our system with undergraduate students by comparing it to three baseline systems: a rule-based system, lecturerconstructed summaries and a Brute Force system. Our evaluation shows that the feedback generated by our learning agent is viewed by students to be as good as the feedback from the lecturers. Our findings suggest that the learning agent needs to take into account both the student and lecturers preferences. "
W13-2116 "<NoAbstract>"
W13-2117 "Abstractive Meeting Summarization with Entailment and Fusion  Yashar Mehdad  Giuseppe Carenini Frank W. Tompa  Raymond T. NG  Department of Computer Science  University of British Columbia  University of Waterloo {mehdad, carenini, rng}@cs.ubc.ca fwtompa@cs.uwaterloo.ca Abstract We propose a novel end-to-end framework for abstractive meeting summarization. We cluster sentences in the input into communities and build an entailment graph over the sentence communities to identify and select the most relevant sentences. We then aggregate those selected sentences by means of a word graph model. We exploit a ranking strategy to select the best path in the word graph as an abstract sentence. Despite not relying on the syntactic structure, our approach significantly outperforms previous models for meeting summarization in terms of informativeness. Moreover, the longer sentences generated by our method are competitive with shorter sentences generated by the previous word graph model in terms of grammaticality. "
W13-2118 "2 Japan {ryu-i,take}@cl.cs.titech.ac.jp Abstract This paper focuses on a subtask of natural language generation (NLG), voice selection, which decides whether a clause is realised in the active or passive voice according to its contextual information. Automatic voice selection is essential for realising more sophisticated MT and summarisation systems, because it impacts the readability of generated texts. However, to the best of our knowledge, the NLG community has been less concerned with explicit voice selection. In this paper, we propose an automatic voice selection model based on various linguistic information, ranging from lexical to discourse information. Our empirical evaluation using a manually annotated corpus in Japanese demonstrates that the proposed model achieved 0.758 in F-score, outperforming the two baseline models. "
W13-2119 "Abstract The cross-disciplinary MIME project aims to develop a mobile medical monitoring system that improves handover transactions in rural pre-hospital scenarios between the first person on scene and ambulance clinicians. NLG is used to produce a textual handover report at any time, summarising data from novel medical sensors, as well as observations and actions recorded by the carer. We describe the MIME project with a focus on the NLG algorithm and an initial evaluation of the generated reports. "
W13-2120 "Abstract We present the results from an elicitation experiment in which human speakers were asked to produced quantified referring expressions (QREs), as in The crate with 10 apples, The crate with many apples, etc. These results suggest that some subtle contextual factors govern the choice between different types of QREs, and that numerals are highly preferred for subitizable quantities despite the availability of coarser-grained expressions. "
W13-2121 "Abstract In this paper we present the preliminary work of a Basque poetry generation system. Basically, we have extracted the POS-tag sequences from some verse corpora and calculated the probability of each sequence. For the generation process we have defined 3 different experiments: Based on a strophe from the corpora, we (a) replace each word with other according to its POS-tag and suffixes, (b) replace each noun and adjective with another equally inflected word and (c) replace only nouns with semantically related ones (inflected). Finally we evaluate those strategies using a Turing Test-like evaluation. "
W13-2122 " We present first results of our project on the generation of contextually adequate greeting exchanges in video role playing games. To make greeting exchanges computable, an analysis of the factors influencing greeting behavior as well as the factors influencing greeting exchanges is given. Based on the politeness model proposed by Brown & Levinson (1987) we develop a simple algorithm for the generation of greeting exchanges. An evaluation, comparing dialog from the video role playing game Skyrim to dialog determined by our algorithm, shows that our algorithm is able to generate greeting exchanges that are contextually more adequate than those featured by Skyrim. 1 Introdu  "
W13-2123 "Abstract This paper introduces the problem of generating descriptions of n-dimensional spatial data by decomposing it via modelbased clustering. I apply the approach to the error function of supervised classification algorithms, a practical problem that uses Natural Language Generation for understanding the behaviour of a trained classifier. I demonstrate my system on a dataset taken from CoNLL shared tasks. "
W13-2124 "Abstract We introduce GenNext, an NLG system designed specifically to adapt quickly and easily to different domains. Given a domain corpus of historical texts, GenNext allows the user to generate a template bank organized by semantic concept via derived discourse representation structures in conjunction with general and domain-specific entity tags. Based on various features collected from the training corpus, the system statistically learns template representations and document structure and produces wellformed texts (as evaluated by crowdsourced and expert evaluations). In addition to domain adaptation, GenNexts hybrid approach significantly reduces complexity as compared to traditional NLG systems by relying on templates (consolidating micro-planning and surface realization) and minimizing the need for domain experts. In this description, we provide details of GenNexts theoretical perspective, architecture and evaluations of output. "
W13-2125 "<NoAbstract>"
W13-2126 " Paraphrasing is expressing the same semantic content using different linguistic means. Although previous work has addressed linguistic variations at different levels of language, paraphrasing in Turkish has not been yet thoroughly studied. This paper presents the first study towards Turkish paraphrase alignment. We perform an analysis of different types of paraphrases on a modest Turkish paraphrase corpus and present preliminary results on that analysis from different standpoints. We also explore the impact of human interpretation of paraphrasing on the alignment of paraphrase sentence pairs. 1 Introductio  "
W13-2127 ", Mobyen Uddin Ahmed and Amy Loutfi Center for Applied Autonomous Sensor Systems   Orebro University, Sweden {hadi.banaee,mobyen.ahmed,amy.loutfi}@oru.se Abstract This position paper presents an on-going work on a natural language generation framework that is particularly tailored for summary text generation from body area networks. We present an overview of the main challenges when considering this type of sensor devices used for at home monitoring of health parameters. This paper describes the first steps towards the implementation of a system which collects information from heart rate and respiration rate using a wearable sensor. The paper further outlines the direction for future work and in particular the challenges for NLG in this application domain. "
W13-2128 "Abstract We present the first prototype of a handover report generator developed for the MIME (Managing Information in Medical Emergencies) project. NLG applications in the medical domain have been varied but most are deployed in clinical situations. We develop a mobile device for prehospital care which receives streamed sensor data and user input, and converts these into a handover report for paramedics. "
W13-2129 "Abstract This demo showcases Thoughtland, an end-to-end system that takes training data and a selected machine learning model, produces a cloud of points via crossvalidation to approximate its error function, then uses model-based clustering to identify interesting components of the error function and natural language generation to produce an English text summarizing the error function. "
W13-2130 "<NoAbstract>"
W13-2131 "Abstract This abstract describes a contribution to the 2013 KBGen Challenge from CNRS/LORIA and the University of Lorraine. Our contribution focuses on an attempt to automate the extraction of a Feature Based Tree Adjoining Grammar equipped with a unification based compositional semantics which can be used to generate from KBGen data. Introduction Semantic grammars, i.e., grammars which link syntax and semantics, have been shown to be useful for generation and for semantic parsing. This abstract outlines an attempt to automatically extract from the KBGen data, a Feature Based Tree Adjoining Grammar which can be used for generation from the KBGen data. Data The KBGen data consists of sets of triples extracted from the AURA knowledge base which encodes knowledge contained in a college-level biology textbook. Each set of triple was selected to be verbalisable as a simple, possibly complex sentence. For instance, the input shown in Figure 1 can be verbalised as 1 : (1) The function of a gated channel is to release particles from the endoplasmic reticulum Sketch of the Overall Grammar Extraction and Generation Procedure To generate from the KBGen data, we parsed each input sentence using the Stanford parser; we aligned the semantic input with a substring in the input sentence; we extracted a grammar from the parsed sentences provided with the input triples; and we generated using an existing surface realiser. In addition some of the input were preprocessed to produce a semantics more compatible with the assumption underlying the syntax/semantic interace of SemTAG; 1 For space reasons, we slightly simplified the KBGen input and removed type information. :TRIPLES ( (|Release-Of-Calcium646| |object| |Particle-In-Motion64582|) (|Release-Of-Calcium646| |base| |Endoplasmic-Reticulum64603|) (|Gated-Channel64605| |has-function||Release-Of-Calcium646|) (|Release-Of-Calcium646| |agent| |Gated-Channel64605|)) :INSTANCE-TYPES (|Particle-In-Motion64582| |instance-of| |Particle-In-Motion|) (|Endoplasmic-Reticulum64603| |instance-of| |Endoplasmic-Reticulum|) (|Gated-Channel64605| |instance-of| |Gated-Channel|) (|Release-Of-Calcium646| |instance-of| |Release-Of-Calcium|)) and a procedure was used to guess missing lexical entries. Alignment and Index Projection Given a Sentence/Input pair (S, I) provided by the KBGen Challenge, we match each entity and event variable in I to a substring in S. Matching uses the variable name, the name of the unary predicate true of that variable and the word form assigned to that predicte in the KBGen lexicon. Digits occurring in the input are removed and the string in the input sentence which is closest to either of the used units is decorated with that variable. Index variables are then projected up the syntactic trees to reflect headedness. For instance, the variable indexed with a noun is projected to the NP level; and the index projected to the NP of a prepositional phrase is project to the PP level. Grammar Extraction Grammar extraction proceeds in two steps as follows. First, the subtrees whose root node are indexed with an entity variable are extracted. This results in a set of NP and PP trees anchored with entity names and associated with the predication true of the indexing variable. Second, the subtrees capturing relations between variables are extracted. To perform this ex204 traction, each input variable X is associated with a set of dependent variables i.e., the set of variables Y such that X is related to Y (R(X, Y )). The minimal tree containing all and only the dependent variables D(X) of a variable X is then extracted and associated with the set of literals  such that  = {R(Y, Z) | (Y = X  Z  D(X)) (Y, Z  D(X))}. This procedure extracts the subtrees relating the argument variables of a semantics functors such as an event or a role. The extracted grammar is a Feature-Based Tree Adjoining Grammar with a Unification-based compositional semantics as described in (Gardent, 2008). Each entry in the grammar associates a natural language expression with a syntactic tree and a semantic representation thereby allowing both for semantic parsing and for generation. Figure 1 shows the tree extracted for the release predicate in Example 1. Pre-Processing The parse trees produced by the Stanford parser are pre-processed to better match TAG recursive modeling of modification. In particular, the flat structure assigned to relative clauses is modified into a recursive structure. The input semantics provided by the KBGen task is also preprocessed to allow for aggregation and to better match the assumptions underlying the syntax/semantics interface of SemTAG. For aggregation, we use a rewrite rule of the form shown below to support the production of e.g., A cellulose-synthase which contains a polypeptide and two glucose synthesizes cellulose.. R(X, Y 1 ), . . . , R(X, Y n ), P (Y 1), . . . , P (Y n )  R(X, Y ), P (Y ), quantity(Y, n) For relative clauses, we rewrite input of the form plays(X Y),in-event(Y E), P(E), R(E X) to plays(X Y),in-event(Y E), P(E), R(E Y). This captures the fact that in sentences such as A biomembrane is a barrier which blocks the hydrogen ion of a chemical., the entity variable bound by the relative clause is that associated with barrier, not that of the main clause subject biomembrane. Guessing Missing Lexical Entries To handle unseen input, we start by partitioning the input semantics into sub-semantics corresponding to events, entities and role. We then search the lexicon for an entry with a matching or similar semantics. An entry with a similar semantics is an entry with the same number and same type of literals (literals with same arity and with identical relations). Similar entries are then adapted to create lexical entries for unseen data. "
W13-2132 " This document describes the University of Delawares entry into KBGen 2013 Challenge which provided teams with input data representation from the AURA knowledge base (KB), developed in the context of the HALO Project at SRI International, along with a lexicon mapping for concepts present on those input files. Training sentences were also provided. The task was to accurately generate an English sentence depicting the information from a set of triples from the knowledge base. 1  "
W13-2133 "<NoAbstract>"
W13-2201 "<NoAbstract>"
W13-2202 "Abstract This paper presents the results of the WMT13 Metrics Shared Task. We asked participants of this task to score the outputs of the MT systems involved in WMT13 Shared Translation Task. We collected scores of 16 metrics from 8 research groups. In addition to that we computed scores of 5 standard metrics such as BLEU, WER, PER as baselines. Collected scores were evaluated in terms of system level correlation (how well each metrics scores correlate with WMT13 official human scores) and in terms of segment level correlation (how often a metric agrees with humans in comparing two translations of a particular sentence). "
W13-2203 "<NoAbstract>"
W13-2204 "Abstract This paper describes LIMSIs submissions to the shared WMT13 translation task. We report results for French-English, German-English and Spanish-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams, and continuous space models in a post-processing step. The main novelties of this years participation are the following: our first participation to the Spanish-English task; experiments with source pre-ordering; a tighter integration of continuous space language models using artificial text generation (for German); and the use of different tuning sets according to the original language of the text to be translated. "
W13-2205 "Abstract We describe the CMU systems submitted to the 2013 WMT shared task in machine translation. We participated in three language pairs, FrenchEnglish, Russian English, and EnglishRussian. Our particular innovations include: a labelcoarsening scheme for syntactic tree-totree translation and the use of specialized modules to create synthetic translation options that can both generalize beyond what is directly observed in the parallel training data and use rich source language context to decide how a phrase should translate in context. "
W13-2206 "Ireland. ergun.bicici@computing.dcu.ie Abstract We use feature decay algorithms (FDA) for fast deployment of accurate statistical machine translation systems taking only about half a day for each translation direction. We develop parallel FDA for solving computational scalability problems caused by the abundance of training data for SMT models and LM models and still achieve SMT performance that is on par with using all of the training data or better. Parallel FDA runs separate FDA models on randomized subsets of the training data and combines the instance selections later. Parallel FDA can also be used for selecting the LM corpus based on the training set selected by parallel FDA. The high quality of the selected training data allows us to obtain very accurate translation outputs close to the top performing SMT systems. The relevancy of the selected LM corpus can reach up to 86% reduction in the number of OOV tokens and up to 74% reduction in the perplexity. We perform SMT experiments in all language pairs in the WMT13 translation task and obtain SMT performance close to the top systems using significantly less resources for training and development. "
W13-2207 " We describe our experiments with phrase-based machine translation for the WMT 2013 Shared Task. We trained one system for 18 translation directions between English or Czech on one side and English, Czech, German, Spanish, French or Russian on the other side. We describe a set of results with different training data sizes and subsets. For the pairs containing Russian, we describe a set of independent experiments with slightly different translation models. 1 Int  "
W13-2208 "Abstract This paper describes our WMT submissions CU-BOJAR and CU-DEPFIX, the latter dubbed CHIMERA because it combines on three diverse approaches: TectoMT, a system with transfer at the deep syntactic level of representation, factored phrase-based translation using Moses, and finally automatic rule-based correction of frequent grammatical and meaning errors. We do not use any off-the-shelf systemcombination method. "
W13-2209 "Yandex School of Data Analysis 16, Leo Tolstoy street, Moscow, Russia {alborisov,jacob,galinskaya}@yandex-team.ru Abstract This paper describes the English-Russian and Russian-English statistical machine translation (SMT) systems developed at Yandex School of Data Analysis for the shared translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation. We adopted phrase-based SMT approach and evaluated a number of different techniques, including data filtering, spelling correction, alignment of lemmatized word forms and transliteration. Altogether they yielded +2.0 and +1.5 BLEU improvement for ru-en and enru language pairs. We also report on the experiments that did not have any positive effect and provide an analysis of the problems we encountered during the development of our systems. "
W13-2210 "Abstract This paper describes the phrase-based SMT systems developed for our participation in the WMT13 Shared Translation Task . Translations for EnglishGerman and EnglishFrench were generated using a phrase-based translation system which is extended by additional models such as bilingual, fine-grained part-ofspeech (POS) and automatic cluster language models and discriminative word lexica (DWL). In addition, we combined reordering models on different sentence abstraction levels. "
W13-2211 "This paper describes T   UB   ITAK-B   ILGEM statistical machine translation (SMT) systems submitted to the Eighth Workshop on Statistical Machine Translation (WMT) shared translation task for German-English language pair in both directions. We implement phrase-based SMT systems with standard parameters. We present the results of using a big tuning data and the effect of averaging tuning weights of different seeds. Additionally, we performed a linguistically motivated compound splitting in the Germanto-English SMT system. "
W13-2212 "dinburgh Scotland, United Kingdom {dnadir,bhaddow,kheafiel,pkoehn}@inf.ed.ac.uk Abstract We validated various novel and recently proposed methods for statistical machine translation on 10 language pairs, using large data resources. We saw gains from optimizing parameters, training with sparse features, the operation sequence model, and domain adaptation techniques. We also report on utilizing a huge language model trained on 126 billion tokens. The annual machine translation evaluation campaign for European languages organized around the ACL Workshop on Statistical Machine Translation offers the opportunity to test recent advancements in machine translation in large data condition across several diverse language pairs. Building on our own developments and external contributions to the Moses open source toolkit, we carried out extensive experiments that, by early indications, led to a strong showing in the evaluation campaign. We would like to stress especially two contributions: the use of the new operation sequence model (Section 3) within Moses, and  in a separate unconstraint track submission  the use of a huge language model trained on 126 billion tokens with a new training tool (Section 4). 1 Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are:  Moses phrase-based models with mostly default settings  training on all available parallel data, including the large UN parallel data, the FrenchEnglish 10 9 parallel data and the LDC Gigaword data  very large tuning set consisting of the test sets from 2008-2010, with a total of 7,567 sentences per language  GermanEnglish with syntactic prereordering (Collins et al., 2005), compound splitting (Koehn and Knight, 2003) and use of factored representation for a POS target sequence model (Koehn and Hoang, 2007)  EnglishGerman with morphological target sequence model Note that while our final 2012 systems included subsampling of training data with modified Moore-Lewis filtering (Axelrod et al., 2011), we did not use such filtering at the starting point of our development. We will report on such filtering in Section 2. Moreover, our system development initially used the WMT 2012 data condition, since it took place throughout 2012, and we switched to WMT 2013 training data at a later stage. In this section, we report cased BLEU scores (Papineni et al., 2001) on newstest2011. 1.1 Factored Backoff (GermanEnglish) We have consistently used factored models in past WMT systems for the GermanEnglish language pairs to include POS and morphological target sequence models. But we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for GermanEnglish. 1.2 Tuning with k-best MIRA In preparation for training with sparse features, we moved away from MERT which is known to fall 114 apart with many more than a couple of dozen features. Instead, we used k-best MIRA (Cherry and Foster, 2012). For the different language pairs, we saw improvements in BLEU of -.05 to +.39, with an average of +.09. There was only a minimal change in the length ratio (Table 1) MERT k-best MIRA  de-en 22.11 (1.010) 22.10 (1.008) .01 (+.002) fr-en 30.00 (1.023) 30.11 (1.026) +.11 (.003) es-en 30.42 (1.021) 30.63 (1.020) +.21 (.001) cs-en 25.54 (1.022) 25.49 (1.024) .05 (.002) en-de 16.08 (0.995) 16.04 (1.001) .04 (.006) en-fr 29.26 (0.980) 29.65 (0.982) +.39 (.002) en-es 31.92 (0.985) 31.95 (0.985) +.03 (.000) en-cs 17.38 (0.967) 17.42 (0.974) +.04 (.007) avg   +.09 Table 1: Tuning with k-best MIRA instead of MERT (cased BLEU scores with length ratio) 1.3 Translation Table Smoothing with Kneser-Ney Discounting Previously, we smoothed counts for the phrasal conditional probability distributions in the translation model with Good Turing discounting. We explored the use of Kneser-Ney discounting, but results are mixed (no difference on average, see Table 2), so we did not pursue this further. Good Turing Kneser Ney  de-en 22.10 22.15 +.05 fr-en 30.11 30.13 +.02 es-en 30.63 30.64 +.01 cs-en 25.49 25.56 +.07 en-de 16.04 15.93 .11 en-fr 29.65 29.75 +.10 en-es 31.95 31.98 +.03 en-cs 17.42 17.26 .16 avg   .00 Table 2: Translation model smoothing with Kneser-Ney 1.4 Sparse Features A significant extension of the Moses system over the last couple of years was the support for large numbers of sparse features. This year, we tested this capability on our big WMT systems. First, we used features proposed by Chiang et al. (2009):  phrase pair count bin features (bins 1, 2, 3, 45, 69, 10+)  target word insertion features  source word deletion features  word translation features  phrase length feature (source, target, both) The lexical features were restricted to the 50 most frequent words. All these features together only gave minor improvements (Table 3). baseline sparse  de-en 22.10 22.02 .08 fr-en 30.11 30.24 +.13 es-en 30.63 30.61 .02 cs-en 25.49 25.49 .00 en-de 16.04 15.93 .09 en-fr 29.65 29.81 +.16 en-es 31.95 32.02 +.07 en-cs 17.42 17.28 .14 avg   +.04 Table 3: Sparse features We also explored domain features in the sparse feature framework, in three different variations. Assume that we have three domains, and a phrase pair occurs in domain A 15 times, in domain B 5 times, and in domain C never. We compute three types of domain features:  binary indicator, if phrase-pairs occurs in domain (example: indA = 1, indB = 1, indC = 0)  ratio how frequent the phrase pairs occurs in domain (example: ratioA = 15 15+5 = .75, ratioB = 5 15+5 = .25, ratioC = 0)  subset of domains in which phrase pair occurs (example: subsetAB = 1, other subsets 0) We tested all three feature types, and found the biggest gain with the domain indicator feature (+.11, Table 4). Note that we define as domain the different corpora (Europarl, etc.). The number of domains ranges from 2 to 9 (see column #d). 1 #d base. indicator ratio subset de-en 2 22.10 22.14 +.04 22.07 .03 22.12 +.02 fr-en 4 30.11 30.34 +.23 30.29 +.18 30.15 +.04 es-en 3 30.63 30.88 +.25 30.64 +.01 30.82 +.19 cs-en 9 25.49 25.58 +.09 25.58 +.09 25.46 .03 en-de 2 16.12 2 16.14 +.02 15.96 .16 16.01 .11 en-fr 4 29.65 29.75 +.10 29.71 +.05 29.70 +.05 en-es 3 31.95 32.06 +.11 32.13 +.18 32.02 +.07 en-cs 9 17.42 17.45 +.03 17.35 .07 17.44 +.02 avg.  +.11 +.03 +.03 Table 4: Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5). We use the domain indicator feature and the other sparse features in subsequent experiments. 1 In the final experiments on the 2013 data condition, one domain (commoncrawl) was added for all language pairs. 115 baseline indicator ratio subset de-en 22.10 22.18 +.08 22.10 .00 22.16 +.06 fr-en 30.11 30.41 +.30 30.49 +.38 30.36 +.25 es-en 30.63 30.75 +.12 30.56 .07 30.85 +.22 cs-en 25.49 25.56 +.07 25.63 +.14 25.43 .06 en-de 16.12 15.95 .17 15.96 .16 16.05 .07 en-fr 29.65 29.96 +.31 29.88 +.23 29.92 +.27 en-es 31.95 32.12 +.17 32.16 +.21 32.08 +.23 en-cs 17.42 17.38 .04 17.35 .07 17.40 .02 avg.  +.11 +.09 +.11 Table 5: Combining domain and other sparse features 1.5 Tuning Settings Given the opportunity to explore the parameter tuning of models with sparse features across many language pairs, we investigated a number of settings. We expect tuning to work better with more iterations, longer n-best lists and bigger cube pruning pop limits. Our baseline settings are 10 iterations with 100-best lists (accumulating) and a pop limit of 1000 for tuning and 5000 for testing. base 25 it. 25it+1k-best 25it+pop5k de-en 22.18 22.16 .02 22.14 .04 22.17 .01 fr-en 30.41 30.40 .01 30.44 +.03 30.49 +.08 es-en 30.75 30.91 +.16 30.86 +.11 30.81 +.06 cs-en 25.56 25.60 +.04 25.64 +.08 25.56 .00 en-de 15.96 15.99 +.03 16.05 +.09 15.96 .00 en-fr 29.96 29.90 .06 29.95 .01 29.92 .04 en-es 32.12 32.17 +.05 32.11 .01 32.19 +.07 en-cs 17.38 17.43 +.05 17.50 +.12 17.38 .00 avg  +.03 +.05 +.02 Table 6: Tuning settings (number of iterations, size of n-best list, and cube pruning pop limit) Results support running tuning for 25 iterations but we see no gains for 5000 pops. There is evidence that an n-best list size of 1000 is better in tuning but we did not adopt this since these large lists take up a lot of disk space and slow down the MIRA optimization step (Table 6). 1.6 Smaller Phrases Given the very large corpus sizes (up to a billion words of parallel data for FrenchEnglish), the size of translation model and lexicalized reordering model becomes a challenge. Hence, we want to examine if restriction to smaller phrases is feasible without loss in translation quality. Results in Table 7 suggest that a maximum phrase length of 5 gives almost identical results, and only with a phrase length limit of 4 significant losses occur. We adopted the limit of 5. max 7 max 6 max 5 max 4 de-en 22.16 22.03 .13 22.05 .11 22.17 +.01 fr-en 30.40 30.30 .10 30.39 .01 30.23 .17 es-en 30.91 30.80 .09 30.86 .05 30.81 .10 cs-en 25.60 25.55 .05 25.53 .07 25.48 .12 en-de 15.99 15.94 .05 15.97 .02 16.03 +.04 en-fr 29.90 29.97 +.07 29.89 .01 29.77 .13 en-es 32.17 32.13 .04 32.27 +.10 31.93 .24 en-cs 17.43 17.46 +.03 17.41 .02 17.41 .02 avg  .05 .03 .09 Table 7: Maximum phrase length, reduced from baseline 1.7 Unpruned Language Models Previously, we trained 5-gram language models using the default settings of the SRILM toolkit in terms of singleton pruning. Thus, training throws out all singletons n-grams of order 3 and higher. We explored whether unpruned language models could give better performance, even if we are only able to train 4-gram models due to memory constraints. At the time, we were not able to build unpruned 4-gram language models for English, but for the other language pairs we did see improvements of -.07 to +.13 (Table 8). We adopted such models for these language pairs. 5g pruned 4g unpruned  en-fr 29.89 29.83 .07 en-es 32.27 32.34 +.07 en-cs 17.41 17.54 +.13 Table 8: Language models without singleton pruning 1.8 Translations per Input Phrase Finally, we explored one more parameter: the limit on how many translation options are considered per input phrase. The default for this setting is 20. However, our experiments (Table 9) show that we can get better results with a translation table limit of 100, so we adopted this. ttl 20 ttl 30 ttl 50 ttl 100 de-en 21.05 +.06 +.09 +.01 fr-en 30.39 .02 +.05 +.07 es-en 30.86 .00 .03 .07 cs-en 25.53 +.24 +.13 +.20 en-de 15.97 +.03 +.07 +.11 en-fr 29.83 +.14 +.19 +.13 en-es 32.34 +.08 +.10 +.07 en-cs 17.54 .05 .02 +.01 avg  +.06 +.07 +.07 Table 9: Maximal number translations per input phrase 1.9 Other Experiments We explored a number of other settings and features, but did not observe any gains. 116  Using HMM alignment instead of IBM Model 4 leads to losses of .01 to .27.  An earlier check of modified MooreLewis filtering (see also below in Section 3) gave very inconsistent results.  Filtering the phrase table with significance filtering (Johnson et al., 2007) leads to losses of .19 to .63.  Throwing out phrase pairs with direct translation probability (  e|   f ) of less than 10 5 has almost no effect.  Double-checking the contribution of the sparse lexical features in the final setup, we observe an average losses of .07 when dropping these features.  For the GermanEnglish language pairs we saw some benefits to using sparse lexical features over POS tags instead of words, so we used this in the final system. 1.10 Summary We adopted a number of changes that improved our baseline system by an average of +.30, see Table 10 for a breakdown. avg. method +.01 factored backoff +.09 kbest MIRA +.11 sparse features and domain indicator +.03 tuning with 25 iterations .03 maximum phrase length 5 +.02 unpruned 4-gram LM +.07 translation table limit 100 +.30 total Table 10: Summary of impact of changes Minor improvements that we did not adopt was avoiding reducing maximum phrase length to 5 (average +.03) and tuning with 1000-best lists (+.02). The improvements differed significantly by language pair, as detailed in Table 11, with the biggest gains for EnglishFrench (+.70), no gain for EnglishGerman and no gain for English German. 1.11 New Data The final experiment of the initial system development phase was to train the systems on the new data, adding newstest2011 to the tuning set (now 10,068 sentences). Table 12 reports the gains on newstest2012 due to added data, indicating very clearly that valuable new data resources became available this year. baseline improved  de-en 21.99 22.09 +.10 fr-en 30.00 30.46 +.46 es-en 30.42 30.79 +.37 cs-en 25.54 25.73 +.19 en-de 16.08 16.08 .00 en-fr 29.26 29.96 +.70 en-es 31.92 32.41 +.49 en-cs 17.38 17.55 +.17 Table 11: Overall improvements per language pair WMT 2012 WMT 2013  de-en 23.11 24.01 +0.90 fr-en 29.25 30.77 +1.52 es-en 32.80 33.99 +1.19 cs-en 22.53 22.86 +0.33 ru-en  31.67  en-de 16.78 17.95 +1.17 en-fr 27.92 28.76 +0.84 en-es 33.41 34.00 +0.59 en-cs 15.51 15.78 +0.27 en-ru  23.78 Table 12: Training with new data (newstest2012 scores)  "
W13-2213 "Abstract This paper describes Munich-EdinburghStuttgarts submissions to the Eighth Workshop on Statistical Machine Translation. We report results of the translation tasks from German, Spanish, Czech and Russian into English and from English to German, Spanish, Czech, French and Russian. The systems described in this paper use OSM (Operation Sequence Model). We explain different pre-/post-processing steps that we carried out for different language pairs. For German-English we used constituent parsing for reordering and compound splitting as preprocessing steps. For Russian-English we transliterated the unknown words. The transliteration system is learned with the help of an unsupervised transliteration mining algorithm. "
W13-2214 "3 1 Dept. of Computer Science 2 Dept. of Linguistics 3 The iSchool Institute for Advanced Computer Studies University of Maryland {vlad,wuke,fture,resnik,jimmylin}@umiacs.umd.edu Abstract We present the system we developed to provide efficient large-scale feature-rich discriminative training for machine translation. We describe how we integrate with MapReduce using Hadoop streaming to allow arbitrarily scaling the tuning set and utilizing a sparse feature set. We report our findings on German-English and RussianEnglish translation, and discuss benefits, as well as obstacles, to tuning on larger development sets drawn from the parallel training data. "
W13-2215 "adrid, Spain {lluis.formiga,marta.ruiz,jose.marino,jose.fonollosa}@upc.edu {albarron, lluism}@lsi.upc.edu Abstract This paper describes the TALP participation in the WMT13 evaluation campaign. Our participation is based on the combination of several statistical machine translation systems: based on standard phrasebased Moses systems. Variations include techniques such as morphology generation, training sentence filtering, and domain adaptation through unit derivation. The results show a coherent improvement on TER, METEOR, NIST, and BLEU scores when compared to our baseline system. "
W13-2216 ", and Ond rej Bojar Charles University in Prague, Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics Malostranske nam esti 25, Prague, Czech Republic {galuscakova,popel,bojar}@ufal.mff.cuni.cz Abstract We present two English-to-Czech systems that took part in the WMT 2013 shared task: TECTOMT and PHRASEFIX. The former is a deep-syntactic transfer-based system, the latter is a more-or-less standard statistical post-editing (SPE) applied on top of TECTOMT. In a brief survey, we put SPE in context with other system combination techniques and evaluate SPE vs. another simple system combination technique: using synthetic parallel data from TECTOMT to train a statistical MT system (SMT). We confirm that PHRASEFIX (SPE) improves the output of TECTOMT, and we use this to analyze errors in TECTOMT. However, we also show that extending data for SMT is more effective. 1 Introduction This paper describes two submissions to the WMT 2013 shared task: 1 TECTOMT  a deepsyntactic tree-to-tree system and PHRASEFIX  statistical post-editing of TECTOMT using Moses (Koehn et al., 2007). We also report on experiments with another hybrid method where TECTOMT is used to produce additional (so-called synthetic) parallel training data for Moses. This method was used in CU-BOJAR and CU-DEPFIX submissions, see Bojar et al. (2013). 2 Overview of Related Work The number of approaches to system combination is enormous. We very briefly survey those that form the basis of our work reported in this paper. "
W13-2217 " We describe the Stanford University NLP Group submission to the 2013 Workshop on Statistical Machine Translation Shared Task. We demonstrate the effectiveness of a new adaptive, online tuning algorithm that scales to large feature and tuning sets. For both English-French and English-German, the algorithm produces feature-rich models that improve over a dense baseline and compare favorably to models tuned with established methods. 1  "
W13-2218 "Abstract We describe the LIA machine translation systems for the Russian-English and English-Russian translation tasks. Various factored translation systems were built using MOSES to take into account the morphological complexity of Russian and we experimented with the romanization of untranslated Russian words. "
W13-2219 "Science Applications International Corporation (SAIC) 7990 Science Applications Ct. Vienna, VA, USA {evgeny.matusov,gregor.leusch}@saic.com Abstract This paper describes Omnifluent TM Translate  a state-of-the-art hybrid MT system capable of high-quality, high-speed translations of text and speech. The system participated in the English-to-French and Russian-to-English WMT evaluation tasks with competitive results. The features which contributed the most to high translation quality were training data sub-sampling methods, document-specific models, as well as rule-based morphological normalization for Russian. The latter improved the baseline Russian-to-English BLEU score from 30.1 to 31.3% on a heldout test set. "
W13-2220 "Abstract We propose a pre-reordering scheme to improve the quality of machine translation by permuting the words of a source sentence to a target-like order. This is accomplished as a transition-based system that walks on the dependency parse tree of the sentence and emits words in target-like order, driven by a classifier trained on a parallel corpus. Our system is capable of generating arbitrary permutations up to flexible constraints determined by the choice of the classifier algorithm and input features. "
W13-2221 "Abstract We present the syntax-based string-totree statistical machine translation systems built for the WMT 2013 shared translation task. Systems were developed for four language pairs. We report on adapting parameters, targeted reduction of the tuning set, and post-evaluation experiments on rule binarization and preventing dropping of verbs. "
W13-2222 " This paper describes shallow semantically-informed Hierarchical Phrase-based SMT (HPBSMT) and Phrase-Based SMT (PBSMT) systems developed at Dublin City University for participation in the translation task between EN-ES and ES-EN at the Workshop on Statistical Machine Translation (WMT 13). The system uses PBSMT and HPBSMT decoders with multiple LMs, but will run only one decoding path decided before starting translation. Therefore the paper does not present a multi-engine system combination. We investigate three types of shallow semantics: (i) Quality Estimation (QE) score, (ii) genre ID, and (iii) context ID derived from context-dependent language models. Our results show that the improvement is 0.8 points absolute (BLEU) for EN-ES and 0.7 points for ES-EN compared to the standard PBSMT system (single best system). It is important to note that we developed this method when the standard (confusion network-based) system combination is ineffective such as in the case when the input is only two. 1 Introduction This paper describes shallow semanticallyinformed Hierarchical Phrase-based SMT (HPBSMT) and Phrase-Based SMT (PBSMT) systems developed at Dublin City University for participation in the translation task between EN-ES and ES-EN at WMT 13. Our objectives are to incorporate several shallow semantics into SMT systems. The first semantics is the QE score for a given input sentence which can be used to select the decoding path either of HPBSMT or PBSMT. Although we call this a QE score, this score is not quite a standard one which does not have access to translation output information. The second semantics is genre ID which is intended to capture domain adaptation. The third semantics is context ID: this context ID is used to adjust the context for the local words. Context ID is used in a continuous-space LM (Schwenk, 2007), but is implicit since the context does not appear in the construction of a continuous-space LM. Note that our usage of the term semantics refers to meaning constructed by a sentence or words. The QE score works as a senten  "
W13-2223 "Abstract This paper describes the joint submission of the QUAERO project for the GermanEnglish translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation (WMT 2013). The submission was a system combination of the output of four different translation systems provided by RWTH Aachen University, Karlsruhe Institute of Technology (KIT), LIMSI-CNRS and SYSTRAN Software, Inc. The translations were joined using the RWTHs system combination approach. Experimental results show improvements of up to 1.2 points in BLEU and 1.2 points in TER compared to the best single translation. "
W13-2224 "Abstract This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation (WMT 2013). We participated in the evaluation campaign for the French-English and German-English language pairs in both translation directions. Both hierarchical and phrase-based SMT systems are applied. A number of different techniques are evaluated, including hierarchical phrase reordering, translation model interpolation, domain adaptation techniques, weighted phrase extraction, word class language model, continuous space language model and system combination. By application of these methods we achieve considerable improvements over the respective baseline systems. "
W13-2225 "2 1PZ, UK {jmp84,aaw35,tx212,ad465,ff257,wjb31}@eng.cam.ac.uk Abstract This paper describes the University of Cambridge submission to the Eighth Workshop on Statistical Machine Translation. We report results for the RussianEnglish translation task. We use multiple segmentations for the Russian input language. We employ the Hadoop framework to extract rules. The decoder is HiFST, a hierarchical phrase-based decoder implemented using weighted finitestate transducers. Lattices are rescored with a higher order language model and minimum Bayes-risk objective. "
W13-2226 "sylvania Abstract We describe improvements made over the past year to Joshua, an open-source translation system for parsing-based machine translation. The main contributions this past year are significant improvements in both speed and usability of the grammar extraction and decoding steps. We have also rewritten the decoder to use a sparse feature representation, enabling training of large numbers of features with discriminative training methods. "
W13-2227 "Abstract This paper presents the experiments conducted by the Machine Translation group at DCU and Prompsit Language Engineering for the WMT13 translation task. Three language pairs are considered: SpanishEnglish and French-English in both directions and German-English in that direction. For the Spanish-English pair, the use of linguistic information to select parallel data is investigated. For the FrenchEnglish pair, the usefulness of the small indomain parallel corpus is evaluated, compared to an out-of-domain parallel data sub-sampling method. Finally, for the German-English system, we describe our work in addressing the long distance reordering problem and a system combination strategy. "
W13-2228 "Abstract This paper describes QCRI-MESs submission on the English-Russian dataset to the Eighth Workshop on Statistical Machine Translation. We generate improved word alignment of the training data by incorporating an unsupervised transliteration mining module to GIZA++ and build a phrase-based machine translation system. For tuning, we use a variation of PRO which provides better weights by optimizing BLEU+1 at corpus-level. We transliterate out-of-vocabulary words in a postprocessing step by using a transliteration system built on the transliteration pairs extracted using an unsupervised transliteration mining system. For the Russian to English translation direction, we apply linguistically motivated pre-processing on the Russian side of the data. "
W13-2229 "Abstract We describe the Uppsala University system for WMT13, for English-to-German translation. We use the Docent decoder, a local search decoder that translates at the document level. We add tunable distortion limits, that is, soft constraints on the maximum distortion allowed, to Docent. We also investigate cleaning of the noisy Common Crawl corpus. We show that we can use alignment-based filtering for cleaning with good results. Finally we investigate effects of corpus selection for recasing. "
W13-2230 "Abstract We present 5 systems of the MunichEdinburgh-Stuttgart 1 joint submissions to the 2013 SMT Shared Task: FR-EN, ENFR, RU-EN, DE-EN and EN-DE. The first three systems employ inflectional generalization, while the latter two employ parser-based reordering, and DE-EN performs compound splitting. For our experiments, we use standard phrase-based Moses systems and operation sequence models (OSM). "
W13-2231 "Fondazione Bruno Kessler, FBK-irst Trento , Italy {turchi|negri|federico}@fbk.eu Abstract Supervised approaches to NLP tasks rely on high-quality data annotations, which typically result from expensive manual labelling procedures. For some tasks, however, the subjectivity of human judgements might reduce the usefulness of the annotation for real-world applications. In Machine Translation (MT) Quality Estimation (QE), for instance, using humanannotated data to train a binary classifier that discriminates between good (useful for a post-editor) and bad translations is not trivial. Focusing on this binary task, we show that subjective human judgements can be effectively replaced with an automatic annotation procedure. To this aim, we compare binary classifiers trained on different data: the human-annotated dataset from the 7 th Workshop on Statistical Machine Translation (WMT-12), and an automatically labelled version of the same corpus. Our results show that human labels are less suitable for the task. "
W13-2232 "Abstract Many tasks in NLP and IR require efficient document similarity computations. Beyond their common application to exploratory data analysis, latent variable topic models have been used to represent text in a low-dimensional space, independent of vocabulary, where documents may be compared. This paper focuses on the task of searching a large multilingual collection for pairs of documents that are translations of each other. We present (1) efficient, online inference for representing documents in several languages in a common topic space and (2) fast approximations for finding near neighbors in the probability simplex. Empirical evaluations show that these methods are as accurate asand significantly faster than Gibbs sampling and brute-force all-pairs search. "
W13-2233 "Computer and Information Science Dept. University of Pennsylvania Abstract Statistical machine translation (SMT) performance suffers when models are trained on only small amounts of parallel data. The learned models typically have both low accuracy (incorrect translations and feature scores) and low coverage (high out-of-vocabulary rates). In this work, we use an additional data resource, comparable corpora, to improve both. Beginning with a small bitext and corresponding phrase-based SMT model, we improve coverage by using bilingual lexicon induction techniques to learn new translations from comparable corpora. Then, we supplement the models feature space with translation scores estimated over comparable corpora in order to improve accuracy. We observe improvements between 0.5 and 1.7 BLEU translating Tamil, Telugu, Bengali, Malayalam, Hindi, and Urdu into English. "
W13-2234 "213, USA {ytsvetko, cdyer, lsl, archna}@cs.cmu.edu Abstract We propose a technique for improving the quality of phrase-based translation systems by creating synthetic translation optionsphrasal translations that are generated by auxiliary translation and postediting processesto augment the default phrase inventory learned from parallel data. We apply our technique to the problem of producing English determiners when translating from Russian and Czech, languages that lack definiteness morphemes. Our approach augments the English side of the phrase table using a classifier to predict where English articles might plausibly be added or removed, and then we decode as usual. Doing so, we obtain significant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classifier. "
W13-2235 "Abstract Our field has seen significant improvements in the quality of machine translation systems over the past several years. The single biggest factor in this improvement has been the accumulation of ever larger stores of data. However, we now find ourselves the victims of our own success, in that it has become increasingly difficult to train on such large sets of data, due to limitations in memory, processing power, and ultimately, speed (i.e., data to models takes an inordinate amount of time). Some teams have dealt with this by focusing on data cleaning to arrive at smaller data sets (Denkowski et al., 2012a; Rarrick et al., 2011), domain adaptation to arrive at data more suited to the task at hand (Moore and Lewis, 2010; Axelrod et al., 2011), or by specifically focusing on data reduction by keeping only as much data as is needed for building models e.g., (Eck et al., 2005). This paper focuses on techniques related to the latter efforts. We have developed a very simple n-gram counting method that reduces the size of data sets dramatically, as much as 90%, and is applicable independent of specific dev and test data. At the same time it reduces model sizes, improves training times, and, because it attempts to preserve contexts for all n-grams in a corpus, the cost in quality is minimal (as measured by BLEU ). Further, unlike other methods created specifically for data reduction that have similar effects on the data, our method scales to very large data, up to tens to hundreds of millions of parallel sentences. "
W13-2236 "ermany {simianer,riezler}@cl.uni-heidelberg.de Abstract Multi-task learning has been shown to be effective in various applications, including discriminative SMT. We present an experimental evaluation of the question whether multi-task learning depends on a natural division of data into tasks that balance shared and individual knowledge, or whether its inherent regularization makes multi-task learning a broadly applicable remedy against overfitting. To investigate this question, we compare natural tasks defined as sections of the International Patent Classification versus random tasks defined as random shards in the context of patent SMT. We find that both versions of multi-task learning improve equally well over independent and pooled baselines, and gain nearly 2 BLEU points over standard MERT tuning. "
W13-2237 "Prashant Mathur, Mauro Cettolo, Marcello Federico  University of Trento  FBK Fondazione Bruno Kessler Trento, Italy {prashant, cettolo, federico}@fbk.eu Abstract We present a novel online learning approach for statistical machine translation tailored to the computer assisted translation scenario. With the introduction of a simple online feature, we are able to adapt the translation model on the fly to the corrections made by the translators. Additionally, we do online adaption of the feature weights with a large margin algorithm. Our results show that our online adaptation technique outperforms the static phrase based statistical machine translation system by 6 BLEU points absolute, and a standard incremental adaptation approach by 2 BLEU points absolute. "
W13-2238 "ermany {wuebker,ney}@cs.rwth-aachen.de Abstract We present an iterative technique to generate phrase tables for SMT, which is based on force-aligning the training data with a modified translation decoder. Different from previous work, we completely avoid the use of a word alignment or phrase extraction heuristics, moving towards a more principled phrase generation and probability estimation. During training, we allow the decoder to generate new phrases on-the-fly and increment the maximum phrase length in each iteration. Experiments are carried out on the IWSLT 2011 Arabic-English task, where we are able to reach moderate improvements on a state-of-the-art baseline with our training method. The resulting phrase table shows only a small overlap with the heuristically extracted one, which demonstrates the restrictiveness of limiting phrase selection by a word alignment or heuristics. By interpolating the heuristic and the trained phrase table, we can improve over the baseline by 0.5% BLEU and 0.5% TER. "
W13-2239 " We present Positive Diversity Tuning, a new method for tuning machine translation models specifically for improved performance during system combination. System combination gains are often limited by the fact that the translations produced by the different component systems are too similar to each other. We propose a method for reducing excess cross-system similarity by optimizing a joint objective that simultaneously rewards models for producing translations that are similar to reference translations, while also punishing them for translations that are too similar to those produced by other systems. The formulation of the Positive Diversity objective is easy to implement and allows for its quick integration with most machine translation tuning pipelines. We find that individual systems tuned on the same data to Positive Diversity can be even more diverse than systems built using different data sets, while still obtaining good BLEU scores. When these individual systems are used together for system combination, our approach allows for significant gains of 0.8 BLEU even when the combination is performed using a small number of otherwise identical individual systems. 1 Introdu  "
W13-2240 "Abstract This paper describes a set of experiments on two sub-tasks of Quality Estimation of Machine Translation (MT) output. Sentence-level ranking of alternative MT outputs is done with pairwise classifiers using Logistic Regression with blackbox features originating from PCFG Parsing, language models and various counts. Post-editing time prediction uses regression models, additionally fed with new elaborate features from the Statistical MT decoding process. These seem to be better indicators of post-editing time than blackbox features. Prior to training the models, feature scoring with ReliefF and Information Gain is used to choose feature sets of decent size and avoid computational complexity. "
W13-2241 "ingdom {debeck1,kashif.shah,t.cohn,l.specia}@sheffield.ac.uk Abstract We describe the results of our submissions to the WMT13 Shared Task on Quality Estimation (subtasks 1.1 and 1.3). Our submissions use the framework of Gaussian Processes to investigate lightweight approaches for this problem. We focus on two approaches, one based on feature selection and another based on active learning. Using only 25 (out of 160) features, our model resulting from feature selection ranked 1st place in the scoring variant of subtask 1.1 and 3rd place in the ranking variant of the subtask, while the active learning model reached 2nd place in the scoring variant using only 25% of the available instances for training. These results give evidence that Gaussian Processes achieve the state of the art performance as a modelling approach for translation quality estimation, and that carefully selecting features and instances for the problem can further improve or at least maintain the same performance levels while making the problem less resource-intensive. "
W13-2242 "Ireland. ergun.bicici@computing.dcu.ie Abstract We introduce referential translation machines (RTM) for quality estimation of translation outputs. RTMs are a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for estimating the quality of translation outputs, judging the semantic similarity between text, and evaluating the quality of student answers. RTMs achieve top performance in automatic, accurate, and language independent prediction of sentence-level and word-level statistical machine translation (SMT) quality. RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations. We develop novel techniques for solving all subtasks in the WMT13 quality estimation (QE) task (QET 2013) based on individual RTM models. Our results achieve improvements over last years QE task results (QET 2012), as well as our previous results, provide new features and techniques for QE, and rank 1st or 2nd in all of the subtasks. "
W13-2243 "FBK-irst Trento, Italy {turchi,negri}@fbk.eu Abstract In this paper we present the approach and system setup of the joint participation of Fondazione Bruno Kessler and University of Edinburgh in the WMT 2013 Quality Estimation shared-task. Our submissions were focused on tasks whose aim was predicting sentence-level Human-mediated Translation Edit Rate and sentence-level post-editing time (Task 1.1 and 1.3, respectively). We designed features that are built on resources such as automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities. Our models consistently overcome the baselines for both tasks and performed particularly well for Task 1.3, ranking first among seven participants. "
W13-2244 "<NoAbstract>"
W13-2245 " This paper is to introduce our participation in the WMT13 shared tasks on Quality Estimation for machine translation without using reference translations. We submitted the results for Task 1.1 (sentence-level quality estimation), Task 1.2 (system selection) and Task 2 (word-level quality estimation). In Task 1.1, we used an enhanced version of BLEU metric without using reference translations to evaluate the translation quality. In Task 1.2, we utilized a probability model Nai ve Bayes (NB) as a classification algorithm with the features borrowed from the traditional evaluation metrics. In Task 2, to take the contextual information into account, we employed a discriminative undirected probabilistic graphical model Conditional random field (CRF), in addition to the NB algorithm. The training experiments on the past WMT corpora showed that the designed methods of this paper yielded promising results especially the statistical models of CRF and NB. The official results show that our CRF model achieved the highest F-score 0.8297 in binary classification of Task 2. "
W13-2246 "Abstract In this paper we present our entry to the WMT13 shared task: Quality Estimation (QE) for machine translation (MT). We participated in the 1.1, 1.2 and 1.3 sub-tasks with our QE system trained on features from diverse information sources like MT decoder features, n-best lists, monoand bi-lingual corpora and giza training models. Our system shows competitive results in the workshop shared task. "
W13-2247 "Abstract In this paper we present the system we submitted to the WMT13 shared task on Quality Estimation. We participated in the Task 1.1. Each translated sentence is given a score between 0 and 1. The score is obtained by using several numerical or boolean features calculated according to the source and target sentences. We perform a linear regression of the feature space against scores in the range [0..1]. To this end, we use a Support Vector Machine with 66 features. In this paper, we propose to increase the size of the training corpus. For that, we use the post-edited and reference corpora during the training step. We assign a score to each sentence of these corpora. Then, we tune these scores on a development corpus. This leads to an improvement of 10.5% on the development corpus, in terms of Mean Average Error, but achieves only a slight improvement on the test corpus. "
W13-2248 "Abstract This paper presents the LIGs systems submitted for Task 2 of WMT13 Quality Estimation campaign. This is a word confidence estimation (WCE) task where each participant was asked to label each word in a translated text as a binary ( Keep/Change) or multi-class (Keep/Substitute/Delete) category. We integrate a number of features of various types (system-based, lexical, syntactic and semantic) into the conventional feature set, for our baseline classifier training. After the experiments with all features, we deploy a Feature Selection strategy to keep only the best performing ones. Then, a method that combines multiple weak classifiers to build a strong composite classifier by taking advantage of their complementarity is presented and experimented. We then select the best systems for submission and present the official results obtained. "
W13-2249 ", Johann Roturier Rasoul Samad Zadeh Kaljahi  and Fred Hollowood  NCLT, School of Computing, Dublin City University, Ireland  Center for Next Generation Localisation, Dublin, Ireland Symantec Research Labs, Dublin, Ireland  {rrubino, jwagner, jfoster}@computing.dcu.ie {johann roturier, fhollowood}@symantec.com Abstract We describe the two systems submitted by the DCU-Symantec team to Task 1.1. of the WMT 2013 Shared Task on Quality Estimation for Machine Translation. Task 1.1 involve estimating postediting effort for English-Spanish translation pairs in the news domain. The two systems use a wide variety of features, of which the most effective are the word-alignment, n-gram frequency, language model, POS-tag-based and pseudoreferences ones. Both systems perform at a similarly high level in the two tasks of scoring and ranking translations, although there is some evidence that the systems are over-fitting to the training data. "
W13-2250 "Abstract This paper describes the machine learning algorithm and the features used by LIMSI for the Quality Estimation Shared Task. Our submission mainly aims at evaluating the usefulness for quality estimation of ngram posterior probabilities that quantify the probability for a given n-gram to be part of the system output. "
W13-2251 "Abstract We describe TerrorCat, a submission to this years metrics shared task. It is a machine learning-based metric that is trained on manual ranking data from WMT shared tasks 20082012. Input features are generated by applying automatic translation error analysis to the translation hypotheses and calculating the error category frequency differences. We additionally experiment with adding quality estimation features in addition to the error analysis-based ones. When evaluated against WMT2012 rankings, the systemlevel agreement is rather high for several language pairs. "
W13-2252 "Abstract This paper gives a detailed description of the ACT (Accuracy of Connective Translation) metric, a reference-based metric that assesses only connective translations. ACT relies on automatic word-level alignment (using GIZA++) between a source sentence and respectively the reference and candidate translations, along with other heuristics for comparing translations of discourse connectives. Using a dictionary of equivalents, the translations are scored automatically or, for more accuracy, semi-automatically. The accuracy of the ACT metric was assessed by human judges on sample data for English/French, English/Arabic, English/Italian and English/German translations; the ACT scores are within 2-5% of human scores. The actual version of ACT is available only for a limited language pairs. Consequently, we are participating only for the English/French and English/German language pairs. Our hypothesis is that ACT metric scores increase with better translation quality in terms of human evaluation. "
W13-2253 " This paper is to describe our machine translation evaluation systems used for participation in the WMT13 shared Metrics Task. In the Metrics task, we submitted two automatic MT evaluation systems nLEPOR_baseline and LEPOR_v3.1. nLEPOR_baseline is an n-gram based language independent MT evaluation metric employing the factors of modified sentence length penalty, position difference penalty, n-gram precision and n-gram recall. nLEPOR_baseline measures the similarity of the system output translations and the reference translations only on word sequences. LEPOR_v3.1 is a new version of LEPOR metric using the mathematical harmonic mean to group the factors and employing some linguistic features, such as the part-of-speech information. The evaluation results of WMT13 show LEPOR_v3.1 yields the highest averagescore 0.86 with human judgments at systemlevel using Pearson correlation criterion on English-to-other (FR, DE, ES, CS, RU) language pairs. "
W13-2254 " The linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, fully automatic approximation to the human HMEANT MT evaluation metric which measures semantic frame similarity between MT output and reference translations. In this paper, we describe HKUSTs submission to the WMT 2013 metrics evaluation task, MEANT and UMEANT. MEANT is optimized by tuning a small number of weightsone for each semantic role labelso as to maximize correlation with human adequacy judgment on a development set. UMEANT is an unsupervised version where weights for each semantic role label are estimated via an inexpensive unsupervised approach, as opposed to MEANTs supervised method relying on more expensive grid search. In this paper, we present a battery of experiments for optimizing MEANT on different development sets to determine the set of weights that maximize MEANTs accuracy and stability. Evaluated on test sets from the WMT 2012/2011 metrics evaluation, both MEANT and UMEANT achieve competitive correlations with human judgments using nothing more than a monolingual corpus and an automatic shallow semantic parser. "
W13-2255 "Abstract In this paper we describe our participation to the WMT13 Shared Task on Quality Estimation. The main originality of our approach is to include features originally designed to classify text according to some authors style. This implies the use of reference categories, which are meant to represent the quality of the MT output. Preamble This paper describes the approach followed in the two systems that we submitted to subtask 1.3 of the WMT13 Shared Task on Quality Estimation, identified as TCD-DCU-CNGL 1-3 SVM1 and TCD-DCU-CNGL 1-3 SVM2. This approach was also used by the first author in his submissions to subtask 1.1, identified as TCD-CNGL OPEN and TCD-CNGL RESTRICTED 1 . In the remaining of this paper we focus on subtask 1.3, but there is very little difference in the application of the approach to task 1.1. "
W13-2256 "Abstract In this paper, we propose a novel syntactic based MT evaluation metric which only employs the dependency information in the source side. Experimental results show that our method achieves higher correlation with human judgments than BLEU, TER, HWCM and METEOR at both sentence and system level for all of the four language pairs in WMT 2010. "
W13-2257 "Fondazione Bruno Kessler Trento, Italy {bisazza,federico}@fbk.eu Abstract Despite being closely related languages, German and English are characterized by important word order differences. Longrange reordering of verbs, in particular, represents a real challenge for state-of-theart SMT systems and is one of the main reasons why translation quality is often so poor in this language pair. In this work, we review several solutions to improve the accuracy of German-English word reordering while preserving the efficiency of phrase-based decoding. Among these, we consider a novel technique to dynamically shape the reordering search space and effectively capture long-range reordering phenomena. Through an extensive evaluation including diverse translation quality metrics, we show that these solutions can significantly narrow the gap between phrase-based and hierarchical SMT. "
W13-2258 "ermany {huck,wuebker,rietig,ney}@i6.informatik.rwth-aachen.de Abstract We introduce a lexicalized reordering model for hierarchical phrase-based machine translation. The model scores monotone, swap, and discontinuous phrase orientations in the manner of the one presented by Tillmann (2004). While this type of lexicalized reordering model is a valuable and widely-used component of standard phrase-based statistical machine translation systems (Koehn et al., 2007), it is however commonly not employed in hierarchical decoders. We describe how phrase orientation probabilities can be extracted from wordaligned training data for use with hierarchical phrase inventories, and show how orientations can be scored in hierarchical decoding. The model is empirically evaluated on the NIST ChineseEnglish translation task. We achieve a significant improvement of +1.2 %BLEU over a typical hierarchical baseline setup and an improvement of +0.7 %BLEU over a syntax-augmented hierarchical setup. On a FrenchGerman translation task, we obtain a gain of up to +0.4 %BLEU. "
W13-2259 "<NoAbstract>"
W13-2260 ",  {first.last}@xrce.xerox.com Abstract We present a method for inference in hierarchical phrase-based translation, where both optimisation and sampling are performed in a common exact inference framework related to adaptive rejection sampling. We also present a first implementation of that method along with experimental results shedding light on some fundamental issues. In hierarchical translation, inference needs to be performed over a high-complexity distribution defined by the intersection of a translation hypergraph and a target language model. We replace this intractable distribution by a sequence of tractable upper-bounds for which exact optimisers and samplers are easy to obtain. Our experiments show that exact inference is then feasible using only a fraction of the time and space that would be required by the full intersection, without recourse to pruning techniques that only provide approximate solutions. While the current implementation is limited in the size of inputs it can handle in reasonable time, our experiments provide insights towards obtaining future speedups, while staying in the same general framework. "
W13-2261 "Abstract Sentence alignment is an important step in the preparation of parallel data. Most aligners do not perform very well when the input is a noisy, rather than a highlyparallel, document pair. Evaluating aligners under noisy conditions would seem to require creating an evaluation dataset by manually annotating a noisy document for gold-standard alignments. Such a costly process hinders our ability to evaluate an aligner under various types and levels of noise. In this paper, we propose a new evaluation framework for sentence aligners, which is particularly suitable for noisy-data evaluation. Our approach is unique as it requires no manual labeling, instead relying on small parallel datasets (already at the disposal of MT researchers) to generate many evaluation datasets that mimic a variety of noisy conditions. We use our framework to perform a comprehensive comparison of three aligners under noisy conditions. Furthermore, our framework facilitates the fine-tuning of a state-of-the-art sentence aligner, allowing us to substantially increase its recall rates by anywhere from 5% to 14% (absolute) across several language pairs. "
W13-2262 "ha, Qatar Abstract We apply multi-rate HMMs, a tree structured HMM model, to the word-alignment problem. Multi-rate HMMs allow us to model reordering at both the morpheme level and the word level in a hierarchical fashion. This approach leads to better machine translation results than a morphemeaware model that does not explicitly model morpheme reordering. "
W13-2263 "192, Japan {shuhei-k,kevinduh,matsu}@is.naist.jp Abstract We propose a novel unsupervised word alignment model based on the Hidden Markov Tree (HMT) model. Our model assumes that the alignment variables have a tree structure which is isomorphic to the target dependency tree and models the distortion probability based on the source dependency tree, thereby incorporating the syntactic structure from both sides of the parallel sentences. In English-Japanese word alignment experiments, our model outperformed an IBM Model 4 baseline by over 3 points alignment error rate. While our model was sensitive to posterior thresholds, it also showed a performance comparable to that of HMM alignment models. "
W13-2301 "ennsylvania {bies,skulick,maamouri}@ldc.upenn.edu Abstract For languages with complex morphologies, limited resources and tools, and/or lack of standard grammars, developing annotated resources can be a challenging task. Annotated resources developed under time/money constraints for such languages tend to tradeoff depth of representation with degree of noise. We present two methods for automatic correction and extension of morphological annotations, and demonstrate their success on three divergent Egyptian Arabic corpora. "
W13-2302 "Abstract This paper presents a method for part-ofspeech tagging of historical data and evaluates it on texts from different corpora of historical German (15th18th century). Spelling normalization is used to preprocess the texts before applying a POS tagger trained on modern German corpora. Using only 250 manually normalized tokens as training data, the tagging accuracy of a manuscript from the 15th century can be raised from 28.65% to 74.89%. "
W13-2303 "Abstract The recent success of statistical parsing methods has made treebanks become important resources for building good parsers. However, constructing highquality annotated treebanks is a challenging task. We utilized two publicly available parsers, Berkeley and MST parsers, for feedback on improving the quality of part-of-speech tagging for the Vietnamese Treebank. Analysis of the treebank and parsing errors revealed how problems with the Vietnamese Treebank influenced the parsing results and real difficulties of Vietnamese parsing that required further improvements to existing parsing technologies. "
W13-2304 "Abstract When creating a new resource, preprocessing the source texts before annotation is both ubiquitous and obvious. How the preprocessing affects the annotation effort for various tasks is for the most part an open question, however. In this paper, we study the effects of preprocessing on the annotation of dependency corpora and how annotation speed varies as a function of the quality of three different parsers and compare with the speed obtained when starting from a least-processed baseline. We also present preliminary results concerning the effects on agreement based on a small subset of sentences that have been doubly-annotated. 1 1 Introduction It is commonly accepted wisdom in treebanking that it is preferable to preprocess data before PoS and syntax annotation, rather than having annotators work from raw text. However, the impact of preprocessing is not well studied and factors such as the lower bound on performance for preprocessing to be useful and the return on investment of increased performance are largely unknown. Corpora and applications based on dependency syntax have become increasingly popular in recent years, and many new corpora are being created. In this work we investigate the task of syntactic annotation based on dependency grammar, and how annotation speed and inter-annotator agreement are influenced by parser performance. Our study is performed in the context of the annotation effort currently under way at the national library of Norway, tasked with creating a freely available syntactically annotated corpus of Norwegian. It is the first widely available such corpus. 1 Code and data used to obtain these results is available at https://github.com/arnsholt/law7-annotation  https://github.com/arnsholt/law7-annotation  "
W13-2305 " We explore the use of continuous rating scales for human evaluation in the context of machine translation evaluation, comparing two assessor-intrinsic qualitycontrol techniques that do not rely on agreement with expert judgments. Experiments employing Amazons Mechanical Turk service show that quality-control techniques made possible by the use of the continuous scale show dramatic improvements to intra-annotator agreement of up to +0.101 in the kappa coefficient, with inter-annotator agreement increasing by up to +0.144 when additional standardization of scores is applied. 1 Introduct  "
W13-2306 "India {monojitc, kalikab}@microsoft.com Abstract Hierarchical or nested annotation of linguistic data often co-exists with simpler non-hierarchical or flat counterparts, a classic example being that of annotations used for parsing and chunking. In this work, we propose a general strategy for comparing across these two schemes of annotation using the concept of entailment that formalizes a correspondence between them. We use crowdsourcing to obtain query and sentence chunking and show that entailment can not only be used as an effective evaluation metric to assess the quality of annotations, but it can also be employed to filter out noisy annotations. "
W13-2307 "School of Computer Science, Carnegie Mellon University  Department of Linguistics, The University of Texas at Austin Abstract We introduce a framework for lightweight dependency syntax annotation. Our formalism builds upon the typical representation for unlabeled dependencies, permitting a simple notation and annotation workflow. Moreover, the formalism encourages annotators to underspecify parts of the syntax if doing so would streamline the annotation process. We demonstrate the efficacy of this annotation on three languages and develop algorithms to evaluate and compare underspecified annotations. "
W13-2308 "Abstract The paper addresses the challenge of converting MIDT, an existing dependency based Italian treebank resulting from the harmonization and merging of smaller resources, into the Stanford Dependencies annotation formalism, with the final aim of constructing a standardcompliant resource for the Italian language. Achieved results include a methodology for converting treebank annotations belonging to the same dependencybased family, the Italian Stanford Dependency Treebank (ISDT), and an Italian localization of the Stanford Dependency scheme. "
W13-2309 " Discourse relation may entail sentiment information. In this work, we annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annotation, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Finally, we highlight some language phenomena and give some remarks. 1 I  "
W13-2310 "Manchester {mihailac,kontonag,batistar,thompsop, korkonti,ananiads}@cs.man.ac.uk Abstract There exist various different discourse annotation schemes that vary both in the perspectives of discourse structure considered and the granularity of textual units that are annotated. Comparison and integration of multiple schemes have the potential to provide enhanced information. However, the differing formats of corpora and tools that contain or produce such schemes can be a barrier to their integration. U-Compare is a graphical, UIMA-based workflow construction platform for combining interoperable natural language processing (NLP) resources, without the need for programming skills. In this paper, we present an extension of U-Compare that allows the easy comparison, integration and visualisation of resources that contain or output annotations based on multiple discourse annotation schemes. The extension works by allowing the construction of parallel subworkflows for each scheme within a single U-Compare workflow. The different types of discourse annotations produced by each sub-workflow can be either merged or visualised side-by-side for comparison. We demonstrate this new functionality by using it to compare annotations belonging to two different approaches to discourse analysis, namely discourse relations and functional discourse annotations. Integrating these different annotation types within an interoperable environment allows us to study the correlations between different types of discourse and report on the new insights that this allows us to discover.  The authors have contributed equally to the development of this work and production of the manuscript. 1 Introduction Over the past few years, there has been an increasing sophistication in the types of available natural language processing (NLP) tools, with named entity recognisers being complemented by relation and event extraction systems. Such relations and events are not intended to be understood in isolation, but rather they are arranged to form a coherent discourse. In order to carry out complex tasks such as automatic summarisation to a high degree of accuracy, it is important for systems to be able to analyse the discourse structure of texts automatically. To facilitate the development of such systems, various textual corpora containing discourse annotations have been made available to the NLP community. However, there is a large amount of variability in the types of annotations contained within these corpora, since different perspectives on discourse have led to the development of a number of different annotation schemes. Corpora containing discourse-level annotations usually treat the text as a sequence of coherent textual zones (e.g., clauses and sentences). One line of research has been to identify which zones are logically connected to each other, and to characterise these links through the assignment of discourse relations. There are variations in the complexity of the schemes used to annotate these discourse relations. For example, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) defines 23 types of discourse relations that are used to structure the text into complex discourse trees. Whilst this scheme was used to enrich the Penn TreeBank (Carlson et al., 2001), the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) used another scheme to identify discourse relations that hold between pairs of text spans. It categorises the relations into types such as causal, temporal and conditional, which can be either explicit or implicit, depending on whether or 79 not they are represented in text using overt discourse connectives. In the biomedical domain, the Biomedical Discourse Relation Bank (BioDRB) (Prasad et al., 2011) annotates a similar set of relation types, whilst BioCause focusses exclusively on causality (Mih  ail  a et al., 2013). A second line of research does not aim to link textual zones, but rather to classify them according to their specific function in the discourse. Examples of functional discourse annotations include whether a particular zone asserts new information into the discourse or represents a speculation or hypothesis. In scientific texts, knowing the type of information that a zone represents (e.g., background knowledge, hypothesis, experimental observation, conclusion, etc.) allows for automatic isolation of new knowledge claims (S  andor and de Waard, 2012). Several annotation schemes have been developed to classify textual zones according to their rhetorical status or general information content (Teufel et al., 1999; Mizuta et al., 2006; Wilbur et al., 2006; de Waard and Pander Maat, 2009; Liakata et al., 2012a). Related to these studies are efforts to capture information relating to discourse function at the level of events, i.e., structured representations of pieces of knowledge which, when identified, facilitate sophisticated semantic searching (Ananiadou et al., 2010). Since there can be multiple events in a sentence or clause, the identification of discourse information at the event level can allow for a more detailed analysis of discourse elements than is possible when considering larger units of text. Certain event corpora such as ACE 2005 (Walker, 2006) and GENIA-MK (Thompson et al., 2011) have been annotated with various types of functional discourse information. It has previously been shown that considering several functional discourse annotation schemes in parallel can be beneficial (Liakata et al., 2012b), since each scheme offers a different perspective. For a common set of documents, the cited study analysed and compared functional discourse annotations at different levels of textual granularity (i.e., sentences, clauses and events), showing how the different schemes could complement each other in order to lay the foundations for a possible future harmonisation of the schemes. The results of this analysis provide evidence that it would be useful to carry out further such analyses involving other such schemes, including an investigation of how discourse relations and functional discourse annotations could complement each other, e.g., which types of functional annotations occur within the arguments of discourse relations. There are, however, certain barriers to carrying out such an analysis. For example, a comparison of annotation schemes would ideally allow the different types of annotations to be visualised simultaneously or seamlessly merged together. However, the fact that annotations in different corpora are encoded using different formats (e.g., stand-off or in-line) and different encoding schemes means that this can be problematic. A solution to the challenges introduced above is offered by the Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004), which defines a common workflow metadata format facilitating the straightforward combination of NLP resources into a workflow. Based on the interoperability of the UIMA framework, numerous researchers distribute their own tools as UIMA-compliant components (Kano et al., 2011; Baumgartner et al., 2008; Hahn et al., 2008; Savova et al., 2010; Gurevych et al., 2007; Rak et al., 2012b). However, UIMA is only intended to provide an abstract framework for the interoperability of language resources, leaving the actual implementation to third-party developers. Hence, UIMA does not explicitly address interoperability issues of tools and corpora. U-Compare (Kano et al., 2011) is a UIMAbased workflow construction platform that provides a graphical user interface (GUI) via which users can rapidly create NLP pipelines using a drag-and-drop mechanism. Conforming to UIMA standards, U-Compare components and pipelines are compatible with any UIMA application via a common and sharable type system (i.e., a hierarchy of annotation types). In defining this type system, U-Compare promotes interoperability of tools and corpora, by exhaustively modelling a wide range of NLP data types (e.g., sentences, tokens, part-of-speech tags, named entities). This type system was recently extended to include discourse annotations to model three discourse phenomena, namely causality, coreference and metaknowledge (Batista-Navarro et al., 2013). In this paper, we describe our extensions to UCompare, supporting the integration and visualisation of resources annotated according to multiple discourse annotation schemes. Our method 80 decomposes pipelines into parallel sub-workflows, each linked to a different annotation scheme. The resulting annotations produced by each subworkflow can be either merged within a single document or visualised in parallel views. "
W13-2311 "hester {rafal.rak,sophia.ananiadou}@manchester.ac.uk Abstract Unstructured Information Management Architecture (UIMA) has been gaining popularity in annotating text corpora. The architecture defines common data structures and interfaces to support interoperability of individual processing components working together in a UIMA application. The components exchange data by sharing common type systemsschemata of data type structureswhich extend a generic, top-level type system built into UIMA. This flexibility in extending type systems has resulted in the development of repositories of components that share one or several type systems; however, components coming from different repositories, and thus not sharing type systems, remain incompatible. Commonly, this problem has been solved programmatically by implementing UIMA components that perform the alignment of two type systems, an arduous task that is impractical with a growing number of type systems. We alleviate this problem by introducing a conversion mechanism based on SPARQL, a query language for the data retrieval and manipulation of RDF graphs. We provide a UIMA component that serialises data coming from a source component into RDF, executes a user-defined, typeconversion query, and deserialises the updated graph into a target component. The proposed solution encourages ad hoc conversions, enables the usage of heterogeneous components, and facilitates highly customised UIMA applications. "
W13-2312 "Abstract This paper describes the importation of Manually Annotated Sub-Corpus (MASC) data and annotations into the linguistic database ANNIS, which allows users to visualize and query linguistically-annotated corpora. We outline the process of mapping MASCs GrAF representation to ANNISs internal format relANNIS and demonstrate how the system provides access to multiple annotation layers in the corpus. This access provides information about inter-layer relations and dependencies that have been previously difficult to explore, and which are highly valuable for continued development of language processing applications. "
W13-2313 " This paper discusses the problem of annotating coreference relations with generic expressions in a large scale corpus. We present and analyze some existing theories of genericity, compare them to the approaches to generics that are used in the state-of-the-art coreference annotation guidelines and discuss how coreference of generic expressions is processed in the manual annotation of the Prague Dependency Treebank. After analyzing some typical problematic issues we propose some partial solutions that can be used to enhance the quality and consistency of the annotation. 1 Int  "
W13-2314 "Abstract Anaphoric shell nouns such as this issue and this fact conceptually encapsulate complex pieces of information (Schmid, 2000). We examine the feasibility of annotating such anaphoric nouns using crowdsourcing. In particular, we present our methodology for reliably annotating antecedents of such anaphoric nouns and the challenges we faced in doing so. We also evaluated the quality of crowd annotation using experts. The results suggest that most of the crowd annotations were good enough to use as training data for resolving such anaphoric nouns. "
W13-2315 "<NoAbstract>"
W13-2316 "Abstract In this paper, we present an annotation tool developed specifically for manual sentiment analysis of social media posts. The tool provides facilities for general and target based opinion marking on different type of posts (i.e. comparative, ironic, conditional) with a web based UI which supports synchronous annotation. It is also designed as a SaaS (Software as a Service). The tools outstanding features are easy and fast annotation interface, detailed sentiment levels, multi-client support, easy to manage administrative modules and linguistic annotation capabilities. "
W13-2317 "3 ArtisTech, Inc., Fairfax, VA 22030 {stephen.c.tratz.civ,douglas.m.briesch.civ,jamal.laoudi.ctr,clare.r.voss.civ}@mail.mil Abstract This paper presents the DATOOL, a graphical tool for annotating conversations consisting of short messages (i.e., tweets), and the results we obtain in using it to annotate tweets for Darija, an historically unwritten Arabic dialect spoken by millions but not taught in schools and lacking standardization and linguistic resources. With the DATOOL, a native-Darija speaker annotated hundreds of mixedlanguage and mixed-script conversations at approximately 250 tweets per hour. The resulting corpus was used in developing and evaluating Arabic dialect classifiers described briefly herein. The DATOOL supports downstream discourse analysis of tweeted conversations by mapping extracted relations such as, who tweets to whom in which language, into graph markup formats for analysis in network visualization tools. "
W13-2318 " We describe a new annotation scheme for formalizing relation structures in research papers. The scheme has been developed through the investigation of computer science papers. Using the scheme, we are building a Japanese corpus to help develop information extraction systems for digital libraries. We report on the outline of the annotation scheme and on annotation experiments conducted on research abstracts from the IPSJ Journal. 1 Introduction Present day researchers need services for searching research papers. Search engines and publishing companies provide specialized search services, such as Google Scholar, Microsoft Academic Search, and Science Direct. Academic societies provide archives of journal articles and/or conference proceedings such as the ACL Anthology. These services focus on simple keywordbased searches as well as extralinguistic relations among research papers, authors, and research topics. However, because contemporary research is becoming increasingly complicated and interrelated, intelligent content-based search systems are desired (Banchs, 2012). A typical query in computational linguistics could be what tasks have CRFs been used for?, which includes the elements of a typical schema for searching research papers; researchers want to find relationships between a technique and its applications (Gupta and Manning, 2011). Answers to this query can be found in various forms in published papers, for example, (1) CRF-based POS tagging has achieved state-ofthe-art accuracy. (2) CRFs have been successfully applied to sequence labeling problems including POS tagging and named entity recognition. (3) We apply feature reduction to CRFs and show its effectiveness in POS tagging. (4) This study proposes a new method for the efficient training of CRFs. The proposed method is evaluated for POS tagging tasks. Note that the same semantic relation, i.e., the use of CRFs for POS tagging, is expressed by various syntactic constructs: internal structures of the phrase in (1), clause-level structures in (2), interclause structures in (3), and discourse-level structures in (4). This implies that an integrated framework is required to represent semantic relations for phrase-level, clause-level, inter-clause level, and discourse-level structures. Another interesting fact is that we can recognize various fragments of information from single texts. For example, from sentence (1), we can identify CRF is applied to POS tagging, state-of-the-art accuracy is achieved for POS tagging, and CRFs achieve high POS tagging accuracy, all of which is valuable content for different search requests. This indicates that we need a framework that can cover (almost) all content in a text. In this paper we describe a new annotation scheme for formalizing typical schemas for representing relations among concepts in research papers, such as techniques, resources, and effects. Our study aims to establish a framework for representing the semantics of research papers to help construct intelligent search systems. In particular, we focus on the formalization of typical schemas that we believe exemplify common query characteristics. From the above observations, we have developed the following criteria for our proposed framework: use the same scheme for annotating contents in all levels of linguistic structures, annotate (almost) all contents presented in texts, and capture relations necessary for surveying research papers. We investigated 71 computer science abstracts (498 sentences) and defined an annotation 140 scheme comprising 16 types of semantic relations. Computer science is particularly suitable for our purpose because it is primarily concerned with abstract concepts rather than concrete entities, which are typically the primary focus of empirical sciences such as physics and biology. In addition, computer and computational methods can be applied to an extraordinarily wide range of topics; computer science papers might discuss a bus timetable (for automatic optimization), a persons palm (as a device for projecting images), or looking over another person!G s shoulder (to obtain passwords). Therefore, to annotate all computer science papers, we cannot develop predefined entity ontologies, which is the typical approach taken in biomedical text mining (Kim et al., 2011). However, most computer science papers have characteristic schemata: the papers describe a problem, postulate a method, apply the method to the problem using particular data or devices, and perform experiments to evaluate the method. The typical schemata clearly represent the structure of interests in this research field. Therefore, we can focus on typical schemata, such as application of a method to a problem and evaluation of a method for a task. As we will demonstrate in this paper, the proposed annotation scheme can cover almost all content, from phrase levels to discourse levels, in computer science papers. Note that this does not necessarily mean that our framework can only be applied to computer science literature. The characteristics of the schemata described above are universal in contemporary science and engineering, and many other activities in human society. Thus, the framework presented in this study can be viewed as a starting point for research focusing on representative schemata of human activities. "
W13-2319 " Semantically annotated corpora play an important role in natural language processing. This paper presents the results of a pilot study on building a sense-tagged parallel corpus, part of ongoing construction of aligned corpora for four languages (English, Chinese, Japanese, and Indonesian) in four domains (story, essay, news, and tourism) from the NTU-Multilingual Corpus. Each subcorpus is first sensetagged using a wordnet and then these synsets are linked. Upon the completion of this project, all annotated corpora will be made freely available. The multilingual corpora are designed to not only provide data for NLP tasks like machine translation, but also to contribute to the study of translation shift and bilingual lexicography as well as the improvement of monolingual wordnets. 1 Introductio  "
W13-2320 "Abstract In this paper, we discuss our efforts to annotate nominals in the Hindi Treebank with the semantic property of animacy. Although the treebank already encodes lexical information at a number of levels such as morph and part of speech, the addition of animacy information seems promising given its relevance to varied linguistic phenomena. The suggestion is based on the theoretical and computational analysis of the property of animacy in the context of anaphora resolution, syntactic parsing, verb classification and argument differentiation. "
W13-2321 "Abstract Automatic pre-annotation is often used to improve human annotation speed and accuracy. We address here out-of-domain named entity annotation, and examine whether automatic pre-annotation is still beneficial in this setting. Our study design includes two different corpora, three pre-annotation schemes linked to two annotation levels, both expert and novice annotators, a questionnaire-based subjective assessment and a corpus-based quantitative assessment. We observe that preannotation helps in all cases, both for speed and for accuracy, and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome. "
W13-2322 "Abstract Meaning Representation for Sembanking  Laura Banarescu SDL lbanarescu@sdl.com  Claire Bonial Linguistics Dept. Univ. Colorado claire.bonial@colorado.edu  Shu Cai ISI USC shucai@isi.edu  Madalina Georgescu SDL mgeorgescu@sdl.com Kira Griffitt LDC kiragrif@ldc.upenn.edu  Ulf Hermjakob ISI USC ulf@isi.edu  Kevin Knight ISI USC knight@isi.edu  Philipp Koehn School of Informatics Univ. Edinburgh pkoehn@inf.ed.ac.uk  Martha Palmer Linguistics Dept. Univ. Colorado martha.palmer@colorado.edu  Nathan Schneider LTI CMU nschneid@cs.cmu.edu Abstract We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it. "
W13-2323 "Abstract This paper presents a case study of a difficult and important categorical annotation task (word sense) to demonstrate a probabilistic annotation model applied to crowdsourced data. It is argued that standard (chance-adjusted) agreement levels are neither necessary nor sufficient to ensure high quality gold standard labels. Compared to conventional agreement measures, application of an annotation model to instances with crowdsourced labels yields higher quality labels at lower cost. "
W13-2324 " We investigate methods for evaluating agreement among a relatively large group of annotators who have not received extensive training and differ in terms of ability and motivation. We show that it is possible to isolate a reliable subgroup of annotators, so that aspects of the difficulty of the underlying task can be studied. Our task is to annotate the argumentative structure of short texts. 1 Intro  "
W13-2325 "Abstract Crowdsourcing, while ideally reducing both costs and the need for domain experts, is no all-purpose tool. We review how paraphrase recognition has benefited from crowdsourcing in the past and identify two problems in paraphrase acquisition and semantic similarity evaluation that can be solved by employing a smart crowdsourcing strategy. First, we employ the CrowdFlower platform to conduct an experiment on sub-sentential paraphrase acquisition with early exclusion of lowaccuracy crowdworkers. Second, we compare two human intelligence task designs for evaluating phrase pairs on a semantic similarity scale. While the first experiment confirms our strategy successful at tackling the problem of missing gold in paraphrase generation, the results of the second experiment suggest that, for both semantic similarity evaluation on a continuous and a binary scale, querying crowdworkers for a semantic similarity value on a multi-grade scale yields better results than directly asking for a binary classification. "
W13-2326 "nology {ryu-i,mitsudak,take}@cl.cs.titech.ac.jp Abstract This paper presents an analysis of an annotators behaviour during her/his annotation process for eliciting useful information for natural language processing (NLP) tasks. Text annotation is essential for machine learning-based NLP where annotated texts are used for both training and evaluating supervised systems. Since an annotators behaviour during annotation can be seen as reflecting her/his cognitive process during her/his attempt to understand the text for annotation, analysing the process of text annotation has potential to reveal useful information for NLP tasks, in particular semantic and discourse processing that require deeper language understanding. We conducted an experiment for collecting annotator actions and eye gaze during the annotation of predicate-argument relations in Japanese texts. Our analysis of the collected data suggests that obtained insight into human annotation behaviour is useful for exploring effective linguistic features in machine learning-based approaches. "
W13-2327 "Sorbonne delphine.battistelli@parissorbonne.fr Abstract In this paper we present the development of a corpus of French newswire texts annotated with enunciative and modal commitment information. The annotation scheme we propose is based on the detection of predicative cues referring to an enunciative and/or modal variation and their scope at a sentence level. We describe how we have improved our annotation guideline by using the evaluation (in terms of precision, recall and F-Measure) of a first round of annotation produced by two expert annotators and by our automatic annotation system. "
W13-2601 "<NoAbstract>"
W13-2602 "Abstract Computational work in the past decade has produced several models accounting for phonetic category learning from distributional and lexical cues. However, there have been no computational proposals for how people might use another powerful learning mechanism: generalization from learned to analogous distinctions (e.g., from /b//p/ to /g//k/). Here, we present a new simple model of generalization in phonetic category learning, formalized in a hierarchical Bayesian framework. The model captures our proposal that linguistic knowledge includes the possibility that category types in a language (such as voiced and voiceless) can be shared across sound classes (such as labial and velar), thus naturally leading to generalization. We present two sets of simulations that reproduce key features of human performance in behavioral experiments, and we discuss the models implications and directions for future research. "
W13-2603 "Abstract Recent work in computational psycholinguistics shows that morpheme lexica can be acquired in an unsupervised manner from a corpus of words by selecting the lexicon that best balances productivity and reuse (e.g. Goldwater et al. (2009) and others). In this paper, we extend such work to the problem of acquiring non-concatenative morphology, proposing a simple model of morphology that can handle both concatenative and non-concatenative morphology and applying Bayesian inference on two datasets of Arabic and English verbs to acquire lexica. We show that our approach successfully extracts the non-contiguous triliteral root from Arabic verb stems. "
W13-2604 "Abstract We use a set of enriched n-gram models to track grammaticality judgements for different sorts of passive sentences in English. We construct these models by specifying scoring functions to map the log probabilities (logprobs) of an n-gram model for a test set of sentences onto scores which depend on properties of the string related to the parameters of the model. We test our models on classification tasks for different kinds of passive sentences. Our experiments indicate that our n-gram models achieve high accuracy in identifying ill-formed passives in which ill-formedness depends on local relations within the n-gram frame, but they are far less successful in detecting non-local relations that produce unacceptability in other types of passive construction. We take these results to indicate some of the strengths and the limitations of word and lexical class n-gram models as candidate representations of speakers grammatical knowledge. "
W13-2605 "Abstract Reading experiments using naturalistic stimuli have shown unanticipated facilitations for completing center embeddings when frequency effects are factored out. To eliminate possible confounds due to surface structure, this paper introduces a processing model based on deep syntactic dependencies. Results on eye-tracking data indicate that completing deep syntactic embeddings yields significantly more facilitation than completing surface embeddings. "
W13-2606 " There are few computational models of second language acquisition (SLA). At the same time, many questions in the field of SLA remain unanswered. In particular, SLA patterns are difficult to study due to the large amount of variation between human learners. We present a computational model of second language construction learning that allows manipulating specific parameters such as age of onset and amount of exposure. We use the model to study general developmental patterns of SLA and two specific effects sometimes found in empirical studies: construction priming and a facilitatory effect of skewed frequencies in the input. Our simulations replicate the expected SLA patterns as well as the two effects. Our model can be used in further studies of various SLA phenomena. 1 I  "
W13-2607 "<NoAbstract>"
W13-2608 "Abstract This paper summarizes the results of a large-scale evaluation study of bag-ofwords distributional models on behavioral data from three semantic priming experiments. The tasks at issue are (i) identification of consistent primes based on their semantic relatedness to the target and (ii) correlation of semantic relatedness with latency times. We also provide an evaluation of the impact of specific model parameters on the prediction of priming. To the best of our knowledge, this is the first systematic evaluation of a wide range of DSM parameters in all possible combinations. An important result of the study is that neighbor rank performs better than distance measures in predicting semantic priming. "
W13-2609 " An increasing body of empirical evidence suggests that concreteness is a fundamental dimension of semantic representation. By implementing both a vector space model and a Latent Dirichlet Allocation (LDA) Model, we explore the extent to which concreteness is reflected in the distributional patterns in corpora. In one experiment, we show that that vector space models can be tailored to better model semantic domains of particular degrees of concreteness. In a second experiment, we show that the quality of the representations of abstract words in LDA models can be improved by supplementing the training data with information on the physical properties of concrete concepts. We conclude by discussing the implications for computational systems and also for how concrete and abstract concepts are represented in the mind 1 Intro  "
W13-2610 "Discourse connectives play an important role in making a text coherent and helping humans to infer relations between spans of text. Using the Penn Discourse Treebank, we investigate what information relevant to inferring discourse relations is conveyed by discourse connectives, and whether the specificity of discourse relations reflects general cognitive biases for establishing coherence. We also propose an approach to measure the effect of a discourse marker on sense identification according to the different levels of a relation sense hierarchy. This will open a way to the computational modeling of discourse processing. "
W13-3901 " The talk will distil experience and results from several projects, over more than a decade, which have researched and developed the application of speech recognition as an input modality for assistive technology (AT). Current interfaces to AT for people with severe physical disabilities, such as switch-scanning, can be prohibitively slow and tiring to use. Many people with severe physical disabilities also have some speech, though many also have poor control of speech articulators, leading to dysarthria. Nonetheless, recognition of dysarthric speech can give people more control options than using body movement alone. Speech can therefore be an attractive option for AT input. Techniques that have been developed for optimising the recognition of dysarthric speech will be described, resulting in recognition rates of greater than 80% for people with even the most severe dysarthria. Speech recognition has been applied as a means of controlling the home (via an environmental control system) and, probably for the first time, as a means of controlling a communication aid. The development of the Voice Input Voice Output Communication Aid (VICOCA) will be described and some early results of its evaluation presented. The talk will discuss some of the lessons learnt from these projects, such as:  The need to work in interdisciplinary teams including speech technologists, speech and language therapists, health researchers and assistive technologists.  The value of user-centred design, involving users in defining their wants and needs and then working with them, in an iterative manner, to refine the AT such that it becomes usable and acceptable.  The gap that exists between the results that can be achieved in the lab and those achievable in peoples homes under real usage conditions  something that is not often covered in research papers.  The practical approaches that can be applied to optimising recognition for individuals. It is often possible to make significant improvements in recognition rates by altering the configuration of the AT set-up. The talk will conclude by describing some of the future potential applications of speech technology that are being developed, or considered, for people with disabilities as well as for frail older people and people with long-term conditions.  "
W13-3902 "<NoAbstract>"
W13-3903 "<NoAbstract>"
W13-3904 ", Sunil Kumar Kopparapu TCS Innovation Labs Mumbai, Yantra Park, Thane (West), Maharashtra, INDIA {bhat.chitralekha, ahmed.imran, vik.saxena, sunilkumar.kopparapu}@tcs.com Abstract We present a visual aid for the hearing impaired to enable access to internet videos. The visual tool is in the form of a time synchronized lip movement corresponding to the speech in the video which is embedded in the original internet video. Conventionally, access to the audio or speech, in a video, by the hearing impaired is provided by means of either text subtitles or sign language gestures by an interpreter. The proposed tool would be beneficial, especially in situations where such aids are not readily available or generating such aids is difficult. We have conducted a number experiments to determine the feasibility and usefulness of the proposed visual aid. Index Terms: Lip movement synthesis, Phone recognition, resource deficient languages "
W13-3905 " An attractive approach to enable the use of vocal interfaces by impaired users with dysarthric speech is the use of a system which learns from the end-user. To enable such technology, it is imperative that the learning is fast to reduce the time spent training the interface. In this paper we investigate to what extend various machine learning techniques are able to learn from only a single or a few spoken training samples. Additionally, we explore whether these techniques can be combined through boosting to improve the performance. Our evaluations on a small, but highly realistic home automation database reveal that nonnegative matrix factorization seems best suited for fast learning and that some of the boosting approaches can indeed improve performance, especially for small amounts of training data. Index Terms: vocal user interface, self-taught learning, machine learning, boosting 1. Introd  "
W13-3906 "<NoAbstract>"
W13-3907 "<NoAbstract>"
W13-3908 " Automatic sign language recognition (ASLR) is a special case of automatic speech recognition (ASR) and computer vision (CV) and is currently evolving from using artificial labgenerated data to using real-life data. Although ASLR still struggles with feature extraction, it can benefit from techniques developed for ASR. We present a large-vocabulary ASLR system that is able to recognize sentences in continuous sign language and uses features extracted from standard single-view video cameras without using additional equipment. ASR techniques such as the multi-layer-perceptron (MLP) tandem approach, speaker adaptation, pronunciation modelling, and parallel hidden Markov models are investigated. We evaluate the influence of each system component on the recognition performance. On two publicly available large vocabulary databases representing lab-data (25 signer, 455 sign vocabulary, 19k sentence) and unconstrained real-life sign language (1 signer, 266 sign vocabulary, 351 sentences) we can achieve 22.1% respectively 38.6% WER. Index Terms: Continuous Sign Language Recognition, Large Vocabulary, ASR, Computer Vision, Recognition System 1  "
W13-3909 "<NoAbstract>"
W13-3910 "<NoAbstract>"
W13-3911 "<NoAbstract>"
W13-3912 "<NoAbstract>"
W13-3913 " In this work we describe research aimed at developing an assistive vocal interface for users with a speech impairment. In contrast to existing approaches, the vocal interface is self-learning, which means it is maximally adapted to the end-user and can be used with any language, dialect, vocabulary and grammar. The paper describes the overall learning framework and the vocabulary acquisition technique, and proposes a novel grammar induction technique based on weakly supervised hidden Markov model learning. We evaluate early implementations of these vocabulary and grammar learning components on two datasets: recorded sessions of a vocally guided card game by non-impaired speakers and speech-impaired users engaging in a home automation task. Index Terms: vocal user interface, self-taught learning, dysarthric speech, non negative matrix factorization, hidden Markov models 1. In  "
W13-3914 "<NoAbstract>"
W13-3915 " This paper presents a dialogue act classification for a spoken dialogue system that delivers necessary information to elderly subjects with mild dementia. Lexical features have been shown to be effective for classification, but the automatic transcription of spontaneous speech demands expensive language modeling. Therefore, this paper proposes a classifier that does not require language modeling and that uses sub-lexical features instead of lexical features. This classifier operates on sequences of phonemes obtained by a phoneme recognizer and exhaustively analyzes the saliency of all possible sub-sequences using a support vector machine with a string kernel. An empirical study of a dialogue corpus containing elderly speech showed that the sub-lexical classifier was robust against the poor modeling of language and it performed better than a lexical classifier that used hidden Markov models of words. Index Terms: dialogue acts, support vector machines, string kernels, spontaneous speech, elderly speech, dementia 1. Introd  "
W13-3916 "<NoAbstract>"
W13-3917 "<NoAbstract>"
W13-3918 " This work examines the use of a low-power Wireless Acoustic Sensor Network (WASN) for the observation of clinically relevant activities of daily living (ADL) (e.g. eating, personal hygiene, toilet usage, etc.) from elderly. The sensors used in the WASN are both audio and ultrasound receivers. To the best of our knowledge, the combination of audio and ultrasound as a basis for ADL monitoring has not been investigated yet. This paper describes a baseline approach for ADL classification based on Gaussian mixture models. Preliminary results in this work indicate that classification accuracies up to 85.0 %  14.6 for audio and 61.7%  11.3 for ultrasound are already achievable on realistic real-life recorded data. Index Terms: acoustic scene analysis, audio, ultrasound, acoustic scene classification, activities of daily living, automatic monitoring 1  "
W13-3919 "<NoAbstract>"
W13-4101 " Lexicon-based classifier is in the long term one of the main and most effective methods of polarity classification used in sentiment analysis, i.e. computational study of opinions, sentiments and emotions expressed in text (see Liu, 2010). Although it achieves relatively good results also for Czech, the classifier still shows some error rate. This paper provides a detailed analysis of such errors caused both by the system and by human reviewers. The identified errors are representatives of the challenges faced by the entire area of opinion mining. Therefore, the analysis is essential for further research in the field and serves as a basis for meaningful improvements of the system. 1 Introduct  "
W13-4102 "Fuji Xerox Co., Ltd., Japan hiroshi.masuichi @fujixerox.co.jp Abstract This paper proposes a method to extract sentiment topics from a text collection. The method utilizes sentiment clues and a relaxed labeling schema to extract sentiment topics. Experiments with a quantitative and a qualitative evaluations was done to confirm the performance of the method. The quantitative evaluation with a polarity classification marked the accuracy of 0.701 in tweets and 0.691 in newswire texts. These performances are comparable to support vector machine baselines. The qualitative evaluation of polarity topic extraction showed an overall accuracy of 0.729, and a higher accuracy of 0.889 for positive topic extraction. The result indicates the efficacy of our method in extracting sentiment topics. "
W13-4103 "Abstract This paper presents the results of GRID project which aimed at studying the semantics of 24 emotion terms in 23 languages belonging to 8 language families (Indo-European, IndoIranian, Afro-Asiatic, Altaic, Uralic, Japonic, Sino-Tibetan, Niger-Congo, and Unclassified). We limit ourselves in this article only to two Slavic languages  Slovak and Czech and to two emotion terms  love and hatred  and try show how greatly information technologies helped the psychologists first of all to obtain, and then to process large volume of information from a bit less than 5000 people, active project participants, who live in 30 countries. "
W14-0501 "<NoAbstract>"
W14-0502 " The paper presents a system for transcribing and annotating phonological information in Brazilian Portuguese, including syllabification. An application of this system for the assessment of language understanding and production is described, following a child longitudinally, comparing expected production with observed production. 1 Introdu  "
W14-0503 " Statistical learning has been proposed as one of the earliest strategies infants could use to segment words out of their native language because it does not rely on language-specific cues that must be derived from existing knowledge of the words in the language. Statistical word segmentation strategies using Bayesian inference have been shown to be quite successful for English (Goldwater et al. 2009), even when cognitively inspired processing constraints are integrated into the inference process (Pearl et al. 2011, Phillips & Pearl 2012). Here we test this kind of strategy on child-directed speech from seven languages to evaluate its effectiveness cross-linguistically, with the idea that a viable strategy should succeed in each case. We demonstrate that Bayesian inference is indeed a viable cross-linguistic strategy, provided the goal is to identify useful units of the language, which can range from sub-word morphology to whole words to meaningful word combinations.  "
W14-0504 " We perform hyperparameter inference within a model of morphology learning (Goldwater et al., 2011) and find that it affects model behaviour drastically. Changing the model structure successfully avoids the unsegmented solution, but results in oversegmentation instead. 1 Introduction Bayesian models provide a sound statistical framework in which to explore aspects of language acquisition. Explicitly specifying the causal and computational structure of a model enables the investigation of hypotheses such as the feasibility of learning linguistic structure from the available input (Perfors et al., 2011), or the interaction of different linguistic levels (Johnson, 2008a). However, these models can be sensitive to small changes in (hyper-)parameters settings. Robustness in this respect is important, since positing specific parameter values is cognitively implausible. In this paper we revisit a model of morphology learning presented by Goldwater and colleagues in Goldwater et al. (2006) and Goldwater et al. (2011) (henceforth GGJ). This model demonstrated the effectiveness of non-parametric stochastic processes, specifically the Pitman-Yor Process, for interpolating between types and tokens. Language learners are exposed to tokens, but many aspects of linguistic structure are lexical; identifying which tokens belong to the same lexical type is crucial. Surface form is not always sufficient, as in the case of ambiguous words. Moreover, morphology in particular is influenced by vocabulary-level type statistics (Bybee, 1995), so it is important for a model to operate on both levels: token statistics from realistic (child-directed) input, and type-level statistics based on the token analyses. The GGJ model learns successfully given fixed hyperparameter values in the Pitman-Yor Process. However, we show that when these hyperparameters are inferred, it collapses to a token-based model with a trivial morphology. In this paper we discuss the reasons for this problematic behaviour, which are relevant for other models based on Pitman-Yor Processes with discrete base distributions, common in natural language tasks. We investigate some potential solutions, by changing the way morphemes are generated within the model. Our results are mixed; we avoid the hyperparameter problem, but learn overly compact morpheme lexicons. 2 The Pitman-Yor Process The Pitman-Yor P  "
W14-0505 "Abstract This paper presents an unsupervised and incremental model of learning segmentation that combines multiple cues whose use by children and adults were attested by experimental studies. The cues we exploit in this study are predictability statistics, phonotactics, lexical stress and partial lexical information. The performance of the model presented in this paper is competitive with the state-of-the-art segmentation models in the literature, while following the child language acquisition more faithfully. Besides the performance improvements over the similar models in the literature, the cues are combined in an explicit manner, allowing easier interpretation of what the model learns. "
W14-0506 "(Extended Abstract) Alexander Clark  ark Department of Philosophy Kings College, London Strand, London alexander.clark@kcl.ac.uk 1 Abstract In recent years, a theory of distributional learning of phrase structure grammars has been developed starting with the simple algorithm presented in (Clark and Eyraud, 2007). These ideas are based on the classic ideas of American structuralist linguistics (Wells, 1947; Harris, 1954). Since that initial paper, the algorithms have been extended to large classes of grammars, notably to the class of Multiple Context-Free grammars by (Yoshinaka, 2011). In this talk we will sketch a theory of language acquisition based on these techniques, and contrast it with other proposals, such as the semantic bootstrapping and parameter setting models. This proposal is based on three recent results: first, a weak learning result for a class of languages that plausibly includes all natural languages (Clark and Yoshinaka, 2013), secondly, a strong learning result for some context-free grammars, that includes a general strategy for converting weak learners to strong learners (Clark, 2013a), and finally a theoretical result that all minimal grammars for a language will have distributionally definable syntactic categories (Clark, 2013b). We argue that we now have all of the pieces for a complete and explanatory theory of language acquisition based on distributional learning and sketch some of the nontrivial predictions of this theory about the syntax and syntax-semantics interface. "
W14-0507 "Abstract This paper describes the design and acquisition of a German multimodal corpus for the development and evaluation of computational models for (grounded) language acquisition and algorithms enabling corresponding capabilities in robots. The corpus contains parallel data from multiple speakers/actors, including speech, visual data from different perspectives and body posture data. The corpus is designed to support the development and evaluation of models learning rather complex grounded linguistic structures, e.g. syntactic patterns, from sub-symbolic input. It provides moreover a valuable resource for evaluating algorithms addressing several other learning processes, e.g. concept formation or acquisition of manipulation skills. The corpus will be made available to the public. "
W14-0508 "Abstract Languages use different lexical inventories to encode information, ranging from small sets of simplex words to large sets of morphologically complex words. Grammaticalization theories argue that this variation arises as the outcome of diachronic processes whereby co-occurring words merge to one word and build up complex morphology. To model these processes we present a) a quantitative measure of lexical diversity and b) a preliminary computational model of changes in lexical diversity over several generations of merging higly frequent collocates. "
W14-0509 "<NoAbstract>"
W14-0510 "{gamback,larsbun}@idi.ntnu.no Abstract Agent-based models of language evolution have received a lot of attention in the last two decades. Researchers wish to understand the origin of language, and aim to compensate for the lacking empirical evidence by utilizing methods from computer science and artificial life. The paper looks at the main theories of language evolution: biological evolution, learning, and cultural evolution. In particular, the Baldwin effect in a naming game model is elaborated on by describing a set of experimental simulations. This is on-going work and ideas for further investigating the social aspects of language evolution are also discussed. "
W14-0801 "Abstract The automatic extraction of verb-particle constructions (VPCs) is of particular interest to the NLP community. Previous studies have shown that word alignment methods can be used with parallel corpora to successfully extract a range of multi-word expressions (MWEs). In this paper the technique is applied to a new type of corpus, made up of a collection of subtitles of movies and television series, which is parallel in English and Spanish. Building on previous research, it is shown that a precision level of 94  4.7% can be achieved in English VPC extraction. This high level of precision is achieved despite the difficulties of aligning and tagging subtitles data. Moreover, many of the extracted VPCs are not present in online lexical resources, highlighting the benefits of using this unique corpus type, which contains a large number of slang and other informal expressions. An added benefit of using the word alignment process is that translations are also automatically extracted for each VPC. A precision rate of 758.5% is found for the translations of English VPCs into Spanish. This study thus shows that VPCs are a particularly good subset of the MWE spectrum to attack using word alignment methods, and that subtitles data provide a range of interesting expressions that do not exist in other corpus types. "
W14-0802 "Abstract We present a method for extracting Multiword Expressions (MWEs) based on the immediate context they occur in, using a supervised model. We show some of these contextual features can be very discriminant and combining them with MWEspecific features results in a relatively accurate extraction. We define context as a sequential structure and not a bag of words, consequently, it becomes much more informative about MWEs. 1 Introdu   Statistical Context Features Meghdad Farahmand The Computer Science Center University of Geneva Switzerland meghdad.farahmand@unige.ch Ronaldo Martins UNDL Foundation Geneva Switzerland r.martins@undl.ch Abstract We present a method for extracting Multiword Expressions (MWEs) based on the immediate context they occur in, using a supervised model. We show some of these contextual features can be very discriminant and combining them with MWEspecific features results in a relatively accurate extraction. We define context as a sequential structure and not a bag of words, consequently, it becomes much more informative about MWEs. "
W14-0803 "Abstract Verb-particle combinations (VPCs) consist of a verbal and a preposition/particle component, which often have some additional meaning compared to the meaning of their parts. If a data-driven morphological parser or a syntactic parser is trained on a dataset annotated with extra information for VPCs, they will be able to identify VPCs in raw texts. In this paper, we examine how syntactic parsers perform on this task and we introduce VPCTagger, a machine learning-based tool that is able to identify English VPCs in context. Our method consists of two steps: it first selects VPC candidates on the basis of syntactic information and then selects genuine VPCs among them by exploiting new features like semantic and contextual ones. Based on our results, we see that VPCTagger outperforms state-of-the-art methods in the VPC detection task. "
W14-0804 "Abstract Although multiword expressions (MWEs) have received an increasing amount of attention in the NLP community over the last two decades, few papers have been dedicated to the specific problem of the interaction between MWEs and parsing. In this paper, we will discuss how the collocation identification task has been integrated in our rulebased parser and show how collocation knowledge has a positive impact on the parsing process. A manual evaluation has been conducted over a corpus of 4000 sentences, comparing outputs of the parser used with and without the collocation component. Results of the evaluation clearly support our claim. "
W14-0805 " We report on the first, still on-going effort to integrate verb MWEs in an LFG grammar of Modern Greek (MG). Text is lemmatized and tagged with the ILSP FBT Tagger and is fed to a MWE filter that marks Words_With_Spaces in MWEs. The output is then formatted to feed an LFG/XLE grammar that has been developed independently. So far we have identified and classified about 2500 MWEs, and have processed 40% of them by manipulating only the lexicon and not the rules of the grammar. Research on MG MWEs (indicatively, Anastasiadi-Simeonidi, 1986; Fotopoulou, 1993; Mini et al., 2011) has developed collections of MWEs and discussed classification, syntax and semantics issues. To the best of our knowledge, this is the first attempt to obtain deep parses of a wide range of types of MG verb MWEs with rich syntactic structure.  "
W14-0806 "{robert.ross,john.d.kelleher}@dit.ie Abstract We evaluate a substitution based technique for improving Statistical Machine Translation performance on idiomatic multiword expressions. The method operates by performing substitution on the original idiom with its literal meaning before translation, with a second substitution step replacing literal meanings with idioms following translation. We detail our approach, outline our implementation and provide an evaluation of the method for the language pair English/Brazilian-Portuguese. Our results show improvements in translation accuracy on sentences containing either morphosyntactically constrained or unconstrained idioms. We discuss the consequences of our results and outline potential extensions to this process. "
W14-0807 "<NoAbstract>"
W14-0808 " This paper reports different experiments created to study the impact of using linguistics to preprocess German compounds prior to translation in Statistical Machine Translation (SMT). Compounds are a known challenge both in Machine Translation (MT) and Translation in general as well as in other Natural Language Processing (NLP) applications. In the case of SMT, German compounds are split into their constituents to decrease the number of unknown words and improve the results of evaluation measures like the Bleu score. To assess to which extent it is necessary to deal with German compounds as a part of preprocessing in SMT systems, we have tested different compound splitters and strategies, such as adding lists of compounds and their translations to the training set. This paper summarizes the results of our experiments and attempts to yield better translations of German nominal compounds into Spanish and shows how our approach improves by up to 1.4 Bleu points with respect to the baseline. 1 Introduction  "
W14-0809 "Abstract An established method for MWE extraction is the combined use of previously identified POS-patterns and association measures. However, the selection of such POSpatterns is rarely debated. Focusing on Italian MWEs containing at least one adjective, we set out to explore how candidate POS-patterns listed in relevant literature and lexicographic sources compare with POS sequences exhibited by statistically significant n-grams including an adjective position extracted from a large corpus of Italian. All literature-derived patterns are foundand new meaningful candidate patterns emergeamong the top-ranking trigrams for three association measures. We conclude that a final solid set to be used for MWE extraction will have to be further refined through a combination of association measures as well as manual inspection. "
W14-0810 "eBay, Inc. San Jose, CA, USA { prathykumar, vsalaka, tracyking, bjohnson } @ebay.com Abstract We describe a method for detecting phrases in e-commerce queries. The key insight is that previous buyer purchasing behavior as well as the general distribution of phrases in item titles must be used to select phrases. Many multiword expression (mwe) phrases which might be useful in other situations are not suitable for buyer query phrases because relevant items, as measured by purchases, do not contain these terms as phrases. "
W14-0811 "Abstract Constructing a lexical resource for Swedish, where compounding is highly productive, requires a well-structured policy of encoding. This paper presents the treatment and encoding of a certain class of compounds in Swedish FrameNet, and proposes a new approach for the automatic analysis of Swedish compounds, i.e. one that leverages existing FrameNet (Ruppenhofer et al., 2010) and Swedish FrameNet (Borin et al. 2010), as well as proven techniques for automatic semantic role labeling (Johansson et al., 2012). "
W14-0812 "weden Abstract Multiword expressions (MWEs) can be extracted automatically from large corpora using association measures, and tools like mwetoolkit allow researchers to generate training data for MWE extraction given a tagged corpus and a lexicon. We use mwetoolkit on a sample of the French Europarl corpus together with the French lexicon Dela, and use Weka to train classifiers for MWE extraction on the generated training data. A manual evaluation shows that the classifiers achieve 6075% precision and that about half of the MWEs found are novel and not listed in the lexicon. We also investigate the impact of the patterns used to generate the training data and find that this can affect the trade-off between precision and novelty. "
W14-0813 "The subcategorization of multiword expressions (MWEs) is still problematic because of the great variability of their phenomenology. This article presents an attempt to categorize Italian nominal MWEs on the basis of their syntactic and semantic behaviour by considering features that can be tested on corpora. Our analysis shows how these features can lead to a differentiation of the expressions in two groups which correspond to the intuitive notions of multiword units and lexical collocations. "
W14-0814 "Abstract This contribution presents the newest version of our Wortverbindungsfelder (fields of multi-word expressions), an experimental lexicographic resource that focusses on aspects of MWEs that are rarely addressed in traditional descriptions: Contexts, patterns and interrelations. The MWE fields use data from a very large corpus of written German (over 6 billion word forms) and are created in a strictly corpus-based way. In addition to traditional lexicographic descriptions, they include quantitative corpus data which is structured in new ways in order to show the usage specifics. This way of looking at MWEs gives insight in the structure of language and is especially interesting for foreign language learners. "
W14-0815 " This work looks at a temporal aspect of multiword expressions (MWEs), namely that the behaviour of a given n-gram and its status as a MWE change over time. We propose a model in which context words have particular probabilities given a usage choice for an n-gram, and those usage choices have time dependent probabilities, and we put forward an expectationmaximisation technique for estimating the parameters from data with no annotation of usage choice. For a range of MWE usages of recent coinage, we evaluate whether the technique is able to detect the emerging usage. 1 Intro  "
W14-0816 "Department of Linguistics, University of Colorado at Boulder  Institute of Cognitive Science, University of Colorado at Boulder {Claire.Bonial,Laura.Green,Jenette.Preciado,Martha.Palmer}@colorado.edu Abstract This research discusses preliminary efforts to expand the coverage of the PropBank lexicon to multi-word and idiomatic expressions, such as take one for the team. Given overwhelming numbers of such expressions, an efficient way for increasing coverage is needed. This research discusses an approach to adding multiword expressions to the PropBank lexicon in an effective yet semantically rich fashion. The pilot discussed here uses double annotation of take multi-word expressions, where annotations provide information on the best strategy for adding the multi-word expression to the lexicon. This work represents an important step for enriching the semantic information included in the PropBank corpus, which is a valuable and comprehensive resource for the field of Natural Language Processing. "
W14-0817 "Abstract This paper examines the effect of paraphrasing noun-noun compounds in statistical machine translation from Swedish to English. The paraphrases are meant to elicit the underlying relationship that holds between the compounding nouns, with the use of prepositional and verb phrases. Though some types of noun-noun compounds are too lexicalized, or have some other qualities that make them unsuitable for paraphrasing, a set of roughly two hundred noun-noun compounds are identified, split and paraphrased to be used in experiments on statistical machine translation. The results indicate a slight improvement in translation of the paraphrased compound nouns, with a minor loss in overall BLEU score. "
W14-0818 "Abstract This paper presents a new data collection of feature norms for 572 German nounnoun compounds. The feature norms complement existing data sets for the same targets, including compositionality ratings, association norms, and images. We demonstrate that the feature norms are potentially useful for research on the nounnoun compounds and their semantic transparency: The feature overlap of the compounds and their constituents correlates with human ratings on the compound constituent degrees of compositionality,  = 0.46. "
W14-0819 "Abstract We introduce a simple and effective crosslingual approach to identifying collocations. This approach is based on the observation that true collocations, which cannot be translated word for word, will exhibit very different association scores before and after literal translation. Our experiments in Japanese demonstrate that our cross-lingual association measure can successfully exploit the combination of bilingual dictionary and large monolingual corpora, outperforming monolingual association measures. "
W14-0820 " We present an unsupervised approach to build a lexicon of Arabic Modal Multiword Expressions (AM-MWEs) and a repository of their variation patterns. These novel resources are likely to boost the automatic identification and extraction of AM-MWEs 1 .  "
W14-1901 "Abstract Augmentative Alternative Communication (AAC) policy suffers from a lack of large scale quantitative evidence on the demographics of users and diversity of devices. The 2013 Domesday Dataset was created to aid formation of AAC policy at the national level. The dataset records purchases of AAC technology by the UKs National Health Service between 2006 and 2012; giving information for each item on: make, model, price, year of purchase, and geographic area of purchase. The dataset was designed to help answer open questions about the provision of AAC services in the UK; and the level of detail of the dataset is such that it can be used at the research level to provide context for researchers and to help validate (or not) assumptions about everyday AAC use. This paper examine three different ways of using the Domesday Dataset to provide verified evidence to support, or refute, assumptions, uncover important research problems, and to properly map the technological distinctiveness of a user community. "
W14-1902 " The requirements of user interface for dyslexics have not been yet properly explored. Accessibility to any kind of information or just to entertainment web pages is a key factor to equality of rights, moreover it breaks down social barriers. Considering that study materials are nowadays very much accessible through internet, by accommodating web content to anyhow disabled users must be seen as natural thing. Dyslexia is considered as an cognitive impairment arising from visual similarity of letters, therefore we focus on Czech language which uses special characters. The aim of our research is to introduce an application that allows dyslexics to decode text easier and understand it properly.  "
W14-1903 "Abstract We aim to build dialogue agents that optimize the dialogue strategy, specifically through learning the dialogue model components from dialogue data. In this paper, we describe our current research on automatically learning dialogue strategies in the healthcare domain. We go through our systematic approach of learning dialogue model components from data, specifically user intents and the user model, as well as the agent reward function. We demonstrate our experiments on healthcare data from which we learned the dialogue model components. We conclude by describing our current research for automatically learning dialogue features that can be used in representing dialogue states and learning the reward function. "
W14-1904 "Abstract To help individuals with Alzheimers disease live at home for longer, we are developing a mobile robotic platform, called ED, intended to be used as a personal caregiver to help with the performance of activities of daily living. In a series of experiments, we study speech-based interactions between each of 10 older adults with Alzheimers disease and ED as the former makes tea in a simulated home environment. Analysis reveals that speech recognition remains a challenge for this recording environment, with word-level accuracies between 5.8% and 19.2% during household tasks with individuals with Alzheimers disease. This work provides a baseline assessment for the types of technical and communicative challenges that will need to be overcome in human-robot interaction for this population. "
W14-1905 "Abstract We present in this paper a voice conversion (VC) method for a person with an articulation disorder resulting from athetoid cerebral palsy. The movements of such speakers are limited by their athetoid symptoms, and their consonants are often unstable or unclear, which makes it difficult for them to communicate. In this paper, exemplar-based spectral conversion using Non-negative Matrix Factorization (NMF) is applied to a voice with an articulation disorder. In order to preserve the speakers individuality, we use a combined dictionary that was constructed from the source speakers vowels and target speakers consonants. However, this exemplar-based approach needs to hold all the training exemplars (frames), and it may cause mismatching of phonemes between input signals and selected exemplars. In this paper, in order to reduce the mismatching of phoneme alignment, we propose a phoneme-categorized sub-dictionary and a dictionary selection method using NMF. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based and conventional NMFbased method. "
W14-2001 "Abstract Eye-movements in reading exhibit frequency spillover effects: fixation durations on a word are affected by the frequency of the previous word. We explore the idea that this effect may be an emergent property of a computationally rational eyemovement strategy that is navigating a tradeoff between processing immediate perceptual input, and continued processing of past input based on memory. We present an adaptive eye-movement control model with a minimal capacity for such processing, based on a composition of thresholded sequential samplers that integrate information from noisy perception and noisy memory. The model is applied to the List Lexical Decision Task and shown to yield frequency spillovera robust property of human eye-movements in this task, even with parafoveal masking. We show that spillover in the model emerges in approximately optimal control policies that sometimes process memory rather than perception. We compare this model with one that is able to give priority to perception over memory, and show that the perception-priority policies in such a model do not perform as well in a range of plausible noise settings. We explain how the frequency spillover arises from a counter-intuitive but fundamental property of sequenced thresholded samplers. "
W14-2002 "Abstract We outline four ways in which uncertainty might affect comprehension difficulty in human sentence processing. These four hypotheses motivate a self-paced reading experiment, in which we used verb subcategorization distributions to manipulate the uncertainty over the next step in the syntactic derivation (single step entropy) and the surprisal of the verbs complement. We additionally estimate wordby-word surprisal and total entropy over parses of the sentence using a probabilistic context-free grammar (PCFG). Surprisal and total entropy, but not single step entropy, were significant predictors of reading times in different parts of the sentence. This suggests that a complete model of sentence processing should incorporate both entropy and surprisal. "
W14-2003 "Abstract This paper presents a vectorial incremental parsing model defined using independently posited operations over activationbased working memory and weight-based episodic memory. This model has the attractive property that it hypothesizes only one unary preterminal rule application and only one binary branching rule application per time step, which allows it to be smoothly integrated into a vector-based recurrence that propagates structural ambiguity from one time step to the next. Predictions of this model are calculated on a center-embedded sentence processing task and shown to exhibit decreased processing accuracy in center-embedded constructions. "
W14-2004 "Abstract In response to Kobele et al. (2012), we evaluate four ways of linking the processing difficulty of sentences to the behavior of the top-down parser for Minimalist grammars developed in Stabler (2012). We investigate the predictions these four metrics make for a number of relative clause constructions, and we conclude that at this point, none of them capture the full range of attested patterns. "
W14-2005 "Canada {libbyb,afsaneh,suzanne}@cs.toronto.edu Abstract The ability of children to generalize over the linguistic input they receive is key to acquiring productive knowledge of verbs. Such generalizations help children extend their learned knowledge of constructions to a novel verb, and use it appropriately in syntactic patterns previously unobserved for that verba key factor in language productivity. Computational models can help shed light on the gradual development of more abstract knowledge during verb acquisition. We present an incremental Bayesian model that simultaneously and incrementally learns argument structure constructions and verb classes given naturalistic language input. We show how the distributional properties in the input language influence the formation of generalizations over the constructions and classes. "
W14-2006 "Abstract The representations and processes yielding the limited length and telegraphic style of language production early on in acquisition have received little attention in acquisitional modeling. In this paper, we present a model, starting with minimal linguistic representations, that incrementally builds up an inventory of increasingly long and abstract grammatical representations (form+meaning pairings), in line with the usage-based conception of language acquisition. We explore its performance on a comprehension and a generation task, showing that, over time, the model better understands the processed utterances, generates longer utterances, and better expresses the situation these utterances intend to refer to. "
W14-2007 "Abstract Previous studies of alignment have focused on two-party conversations. We study multi-party conversation in online health communities, which have shown benefits for their members from forum conversations. So far, our understanding of the relationship between alignment in such multi-party conversations and its possible connection to member benefits has been limited. This paper quantifies linguistic alignment in the oldest and the largest cancer online forum. Alignment at lexical and syntactic levels, as well as decay of alignment was observed in forum threads, although the decay was slower than commonly found in psycholinguistic studies. The different pattern of adaptation to the initial post on a thread suggests that specific roles in the online forum (e.g., seeking support from the community) can potentially be revealed through alignment theory and its extensions. "
W14-2401 "<NoAbstract>"
W14-2402 "Abstract We propose a new approach to semantic parsing that is not constrained by a fixed formal ontology and purely logical inference. Instead, we use distributional semantics to generate only the relevant part of an on-the-fly ontology. Sentences and the on-the-fly ontology are represented in probabilistic logic. For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) and Probabilistic Soft Logic (PSL). This semantic parsing approach is evaluated on two tasks, Textual Entitlement (RTE) and Textual Similarity (STS), both accomplished using inference in probabilistic logic. Experiments show the potential of the approach. "
W14-2403 "Abstract In present CCG-based semantic parsing systems, the extraction of a semantic grammar from sentence-meaning examples poses a computational challenge. An important factor is the decomposition of the sentence meaning into smaller parts, each corresponding to the meaning of a word or phrase. This has so far limited supervised semantic parsing to small, specialised corpora. We propose a set of heuristics that render the splitting of meaning representations feasible on a largescale corpus, and present a method for grammar induction capable of extracting a semantic CCG from the Groningen Meaning Bank. "
W14-2404 "<NoAbstract>"
W14-2405 "ord, UK {edwgre, pblunsom, nando, karher}@cs.ox.ac.uk Abstract Many successful approaches to semantic parsing build on top of the syntactic analysis of text, and make use of distributional representations or statistical models to match parses to ontology-specific queries. This paper presents a novel deep learning architecture which provides a semantic parsing system through the union of two neural models of language semantics. It allows for the generation of ontology-specific queries from natural language statements and questions without the need for parsing, which makes it especially suitable to grammatically malformed or syntactically atypical text, such as tweets, as well as permitting the development of semantic parsers for resourcepoor languages. "
W14-2406 "Abstract We outline a vision for computational semantics in which formal compositional semantics is combined with a powerful, structured lexical semantics derived from distributional statistics. We consider how existing work (Lewis and Steedman, 2013) could be extended with a much richer lexical semantics using recent techniques for modelling processes (Scaria et al., 2013)for example, learning that visiting events start with arriving and end with leaving. We show how to closely integrate this information with theories of formal semantics, allowing complex compositional inferences such as is visitinghas arrived in but will leave, which requires interpreting both the function and content words. This will allow machine reading systems to understand not just what has happened, but when. "
W14-2407 "Abstract We are interested in the automatic interpretation of how-to instructions, such as cooking recipes, into semantic representations that can facilitate sophisticated question answering. Recent work has shown impressive results on semantic parsing of instructions with minimal supervision, but such techniques cannot handle much of the situated and ambiguous language used in instructions found on the web. In this paper, we suggest how to extend such methods using a model of pragmatics, based on a rich representation of world state. "
W14-2408 " This paper offers an Embodied Construction Grammar (Feldman et al. 2010) representation of caused motion, thereby also providing (a sample of) the computational infrastructure for implementing the information that FrameNet has characterized as Caused_motion 1 (Ruppenhofer et al. 2010). This work specifies the semantic structure of caused motion in natural language, using an Embodied Construction Grammar analyzer that includes the semantic parsing of linguistically instantiated constructions. Results from this type of analysis can serve as the input to NLP applications that require rich semantic representations. 1 Introduction  "
W14-2409 "Abstract Many machine reading approaches, from shallow information extraction to deep semantic parsing, map natural language to symbolic representations of meaning. Representations such as first-order logic capture the richness of natural language and support complex reasoning, but often fail in practice due to their reliance on logical background knowledge and the difficulty of scaling up inference. In contrast, low-dimensional embeddings (i.e. distributional representations) are efficient and enable generalization, but it is unclear how reasoning with embeddings could support the full power of symbolic representations such as first-order logic. In this proof-ofconcept paper we address this by learning embeddings that simulate the behavior of first-order logic. "
W14-2410 "Abstract Software requirements are commonly written in natural language, making them prone to ambiguity, incompleteness and inconsistency. By converting requirements to formal semantic representations, emerging problems can be detected at an early stage of the development process, thus reducing the number of ensuing errors and the development costs. In this paper, we treat the mapping from requirements to formal representations as a semantic parsing task. We describe a novel data set for this task that involves two contributions: first, we establish an ontology for formally representing requirements; and second, we introduce an iterative annotation scheme, in which formal representations are derived through step-wise refinements. "
W14-2411 "<NoAbstract>"
W14-2412 "Abstract Consideration of the decoding problem in semantic parsing as finding a maximum spanning DAG of a weighted directed graph carries many complexities that havent been fully addressed in the literature to date, among which are its actual appropriateness for the decoding task in semantic parsing, not to mention an explicit proof of its complexity (and its approximability). In this paper, we consider the objective function for the maximum spanning DAG problem, and what it means in terms of decoding for semantic parsing. In doing so, we give anecdotal evidence against its use in this task. In addition, we consider the only graph-based maximum spanning DAG approximation algorithm presented in the literature (without any approximation guarantee) to date and finally provide an approximation guarantee for it, showing that it is an O( 1 n ) factor approximation algorithm, where n is the size of the digraphs vertex set. "
W14-2413 "Abstract We propose an intermediary-level semantic representation, providing a higher level of abstraction than syntactic parse trees, while not committing to decisions in cases such as quantification, grounding or verbspecific roles assignments. The proposal is centered around the proposition structure of the text, and includes also implicit propositions which can be inferred from the syntax but are not transparent in parse trees, such as copular relations introduced by appositive constructions. Other benefits over dependency-trees are explicit marking of logical relations between propositions, explicit marking of multiword predicate such as light-verbs, and a consistent representation for syntacticallydifferent but semantically-similar structures. The representation is meant to serve as a useful input layer for semanticoriented applications, as well as to provide a better starting point for further levels of semantic analysis such as semantic-rolelabeling and semantic-parsing. "
W14-2414 "Japan {tianran,yusuke,takuya-matsuzaki}@nii.ac.jp Abstract Dependency-based Compositional Semantics (DCS) provides a precise and expressive way to model semantics of natural language queries on relational databases, by simple dependency-like trees. Recently abstract denotation is proposed to enable generic logical inference on DCS. In this paper, we discuss some other possibilities to equip DCS with logical inference, and we discuss further on how logical inference can help textual entailment recognition, or other semantic precessing tasks. "
W14-2415 "Abstract This abstract describes README-EVAL, a novel measure for semantic parsing evaluation of interpreters for instructions in computer program README files. That is enabled by leveraging the tens of thousands of Open Source Software programs that have been annotated by package maintainers of GNU/Linux operating systems. We plan to make available a public shared implementation of this evaluation. "
W14-2601 "<NoAbstract>"
W14-2602 "nhagen zmk867@hum.ku.dk,{dirk,bplank}@cst.dk Abstract While various approaches to domain adaptation exist, the majority of them requires knowledge of the target domain, and additional data, preferably labeled. For a language like English, it is often feasible to match most of those conditions, but in low-resource languages, it presents a problem. We explore the situation when neither data nor other information about the target domain is available. We use two samples of Danish, a low-resource language, from the consumer review domain (film vs. company reviews) in a sentiment analysis task. We observe dramatic performance drops when moving from one domain to the other. We then introduce a simple offline method that makes models more robust towards unseen domains, and observe relative improvements of more than 50%. "
W14-2603 "Abstract Implicit opinions are commonly seen in opinion-oriented documents, such as political editorials. Previous work have utilized opinion inference rules to detect implicit opinions evoked by events that positively/negatively affect entities (goodFor/badFor) to improve sentiment analysis for English text. Since people in different languages may express implicit opinions in different ways, in this work we investigate implicit opinions expressed via goodFor/badFor events in Chinese. The positive results have provided evidences that such implicit opinions and inference rules are similar in Chinese and in English. Moreover, we have observed cases where the inferences are blocked. "
W14-2604 "Abstract In this paper, we discuss how domainspecific noun polarity lexicons can be induced. We focus on the generation of good candidates and compare two machine learning scenarios in order to establish an approach that produces high precision. Candidates are generated on the basis of polarity preferences of adjectives derived from a large domain-independent corpus. The polarity preference of a word, here an adjective, reflects the distribution of positive, negative and neutral arguments the word takes (here: its nominal head). Given a noun modified by some adjectives, a vote among the polarity preferences of these adjectives establishes a good indicator of the polarity of the noun. In our experiments with five domains, we achieved f-measure of 59% up to 88% on the basis of two machine learning approaches carried out on top of the preference votes. "
W14-2605 "Abstract This paper presents a pioneering research on aspect-level sentiment analysis in Czech. The main contribution of the paper is the newly created Czech aspectlevel sentiment corpus, based on data from restaurant reviews. We annotated the corpus with two variants of aspect-level sentiment  aspect terms and aspect categories. The corpus consists of 1,244 sentences and 1,824 annotated aspects and is freely available to the research community. Furthermore, we propose a baseline system based on supervised machine learning. Our system detects the aspect terms with Fmeasure 68.65% and their polarities with accuracy 66.27%. The categories are recognized with F-measure 74.02% and their polarities with accuracy 66.61%. "
W14-2606 "<NoAbstract>"
W14-2607 "da K1A 0R6 {saif.mohammad,xiaodan.zhu,joel.martin}@nrc-cnrc.gc.ca Abstract Past work on emotion processing has focused solely on detecting emotions, and ignored questions such as who is feeling the emotion (the experiencer)? and towards whom is the emotion directed (the stimulus)?. We automatically compile a large dataset of tweets pertaining to the 2012 US presidential elections, and annotate it not only for emotion but also for the experiencer and the stimulus. We then develop a classifier for detecting emotion that obtains an accuracy of 56.84 on an eight-way classification task. Finally, we show how the stimulus identification task can also be framed as a classification task, obtaining an F-score of 58.30. "
W14-2608 "{rklinger,cimiano}@cit-ec.uni-bielefeld.de Abstract Irony is an important device in human communication, both in everyday spoken conversations as well as in written texts including books, websites, chats, reviews, and Twitter messages among others. Specific cases of irony and sarcasm have been studied in different contexts but, to the best of our knowledge, only recently the first publicly available corpus including annotations about whether a text is ironic or not has been published by Filatova (2012). However, no baseline for classification of ironic or sarcastic reviews has been provided. With this paper, we aim at closing this gap. We formulate the problem as a supervised classification task and evaluate different classifiers, reaching an F 1 -measure of up to 74 % using logistic regression. We analyze the impact of a number of features which have been proposed in previous research as well as combinations of them. "
W14-2609 "Abstract Automatic detection of figurative language is a challenging task in computational linguistics. Recognising both literal and figurative meaning is not trivial for a machine and in some cases it is hard even for humans. For this reason novel and accurate systems able to recognise figurative languages are necessary. We present in this paper a novel computational model capable to detect sarcasm in the social network Twitter (a popular microblogging service which allows users to post short messages). Our model is easy to implement and, unlike previous systems, it does not include patterns of words as features. Our seven sets of lexical features aim to detect sarcasm by its inner structure (for example unexpectedness, intensity of the terms or imbalance between registers), abstracting from the use of specific terms. "
W14-2610 "niversity {rzepka,araki}@ ist.hokudai.ac.jp Abstract In this research we focus on discriminating between emotive (emotionally loaded) and non-emotive sentences. We define the problem from a linguistic point of view assuming that emotive sentences stand out both lexically and grammatically. We verify this assumption experimentally by comparing two sets of such sentences in Japanese. The comparison is based on words, longer n-grams as well as more sophisticated patterns. In the classification we use a novel unsupervised learning algorithm based on the idea of language combinatorics. The method reached results comparable to the state of the art, while the fact that it is fully automatic makes it more efficient and language independent. "
W14-2611 "<NoAbstract>"
W14-2612 "Abstract In this study we explore a novel technique for creation of polarity lexicons from the Twitter streams in Russian and English. With this aim we make preliminary filtering of subjective tweets using general domain-independent lexicons in each language. Then the subjective tweets are used for extraction of domain-specific sentiment words. Relying on co-occurrence statistics of extracted words in a large unlabeled Twitter collections we utilize the Markov random field framework for the word polarity classification. To evaluate the quality of the obtained sentiment lexicons they are used for tweet sentiment classification and outperformed previous results. "
W14-2613 "Abstract Determining relevant content automatically is a challenging task for any aggregation system. In the business intelligence domain, particularly in the application area of Online Reputation Management, it may be desirable to label tweets as either customer comments which deserve rapid attention or tweets from industry experts or sources regarding the higher-level operations of a particular entity. We present an approach using a combination of linguistic and Twitter-specific features to represent tweets and examine the efficacy of these in distinguishing between tweets which have been labelled using Amazons Mechanical Turk crowdsourcing platform. Features such as partof-speech tags and function words prove highly effective at discriminating between the two categories of tweet related to several distinct entity types, with Twitterrelated metrics such as the presence of hashtags, retweets and user mentions also adding to classification accuracy. Accuracy of 86% is reported using an SVM classifier and a mixed set of the aforementioned features on a corpus of tweets related to seven business entities. "
W14-2614 "Abstract We provide a simple but novel supervised weighting scheme for adjusting term frequency in tf-idf for sentiment analysis and text classification. We compare our method to baseline weighting schemes and find that it outperforms them on multiple benchmarks. The method is robust and works well on both snippets and longer documents. "
W14-2616 "Abstract Online political discussions have received a lot of attention over the past years. In this paper we compare two sentiment lexicon approaches to classify the sentiment of sentences from political discussions. The first approach is based on applying the number of words between the target and the sentiment words to weight the sentence sentiment score. The second approach is based on using the shortest paths between target and sentiment words in a dependency graph and linguistically motivated syntactic patterns expressed as dependency paths. The methods are tested on a corpus of sentences from online Norwegian political discussions. The results show that the method based on dependency graphs performs significantly better than the word-based approach. "
W14-2617 "<NoAbstract>"
W14-2618 "Abstract Opinion inference arises when opinions are expressed toward states and events which positive or negatively affect entities, i.e., benefactive and malefactive events. This paper addresses creating a lexicon of such events, which would be helpful to infer opinions. Verbs may be ambiguous, in that some meanings may be benefactive and others may be malefactive or neither. Thus, we use WordNet to create a sense-level lexicon. We begin with seed senses culled from FrameNet and expand the lexicon using WordNet relationships. The evaluations show that the accuracy of the approach is well above baseline accuracy. "
W14-2619 "Abstract This paper illustrates the use of deep semantic processing for sentiment analysis. Existing methods for sentiment analysis use supervised approaches which take into account all the subjective words and or phrases. Due to this, the fact that not all of these words and phrases actually contribute to the overall sentiment of the text is ignored. We propose an unsupervised rule-based approach using deep semantic processing to identify only relevant subjective terms. We generate a UNL (Universal Networking Language) graph for the input text. Rules are applied on the graph to extract relevant terms. The sentiment expressed in these terms is used to figure out the overall sentiment of the text. Results on binary sentiment classification have shown promising results. "
W14-2620 "Abstract There are numerous studies suggesting that published news stories have an important effect on the direction of the stock market, its volatility, the volume of trades, and the value of individual stocks mentioned in the news. There is even some published research suggesting that automated sentiment analysis of news documents, quarterly reports, blogs and/or Twitter data can be productively used as part of a trading strategy. This paper presents just such a family of trading strategies, and then uses this application to re-examine some of the tacit assumptions behind how sentiment analyzers are generally evaluated, in spite of the contexts of their application. This discrepancy comes at a cost. "
W14-2621 " We present a new feature type named rating-based feature and evaluate the contribution of this feature to the task of document-level sentiment analysis. We achieve state-of-the-art results on two publicly available standard polarity movie datasets: on the dataset consisting of 2000 reviews produced by Pang and Lee (2004) we obtain an accuracy of 91.6% while it is 89.87% evaluated on the dataset of 50000 reviews created by Maas et al. (2011). We also get a performance at 93.24% on our own dataset consisting of 233600 movie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task. 1 Introductio  "
W14-2622 "Abstract In this study, we aim to test our hypothesis that confidence scores of sentiment values of tweets aid in classification of sentiment. We used several feature sets consisting of lexical features, emoticons, features based on sentiment scores and combination of lexical and sentiment features. Since our dataset includes confidence scores of real numbers in [0-1] range, we employ regression analysis on each class of sentiments. We determine the class label of a tweet by looking at the maximum of the confidence scores assigned to it by these regressors. We test the results against classification results obtained by converting the confidence scores into discrete labels. Thus, the strength of sentiment is ignored. Our expectation was that taking the strength of sentiment into consideration would improve the classification results. Contrary to our expectations, our results indicate that using classification on discrete class labels and ignoring sentiment strength perform similar to using regression on continuous confidence scores. "
W14-2623 "demy, India {abhijitmishra, adityaj, pb}@cse.iitb.ac.in Abstract Existing sentiment analysers are weak AI systems: they try to capture the functionality of human sentiment detection faculty, without worrying about how such faculty is realized in the hardware of the human. These analysers are agnostic of the actual cognitive processes involved. This, however, does not deliver when applications demand order of magnitude facelift in accuracy, as well as insight into characteristics of sentiment detection process. In this paper, we present a cognitive study of sentiment detection from the perspective of strong AI. We study the sentiment detection process of a set of human sentiment readers. Using eye-tracking, we show that on the way to sentiment detection, humans first extract subjectivity. They focus attention on a subset of sentences before arriving at the overall sentiment. This they do either through anticipation where sentences are skipped during the first pass of reading, or through homing where a subset of the sentences are read over multiple passes, or through both. Homing behaviour is also observed at the sub-sentence level in complex sentiment phenomena like sarcasm. "
W14-2624 " To overcome the increasingly time consuming and potentially challenging identification of key points and the associated rationales in large-scale online deliberations, we propose a computational linguistics method that has the potential of facilitating this process of reading and evaluating the text. Our approach is novel in how we determine the sentiment of a rationale at the sentence level and in that it includes a text similarity measure and sentence-level sentiment analysis to achieve this goal. 1 Intro  "
W14-3301 " A main output of the annual Workshop on Statistical Machine Translation (WMT) is a ranking of the systems that participated in its shared translation tasks, produced by aggregating pairwise sentencelevel comparisons collected from human judges. Over the past few years, there have been a number of tweaks to the aggregation formula in attempts to address issues arising from the inherent ambiguity and subjectivity of the task, as well as weaknesses in the proposed models and the manner of model selection. We continue this line of work by adapting the TrueSkill TM algorithm  an online approach for modeling the relative skills of players in ongoing competitions, such as Microsofts Xbox Live  to the human evaluation of machine translation output. Our experimental results show that TrueSkill outperforms other recently proposed models on accuracy, and also can significantly reduce the number of pairwise annotations that need to be collected by sampling non-uniformly from the space of system competitions. 1 Introduction The Workshop on Statistical Machine Translation (WMT) has long been a central event in the machine translation (MT) community for the evaluation of MT output. It hosts an annual set of shared translation tasks focused mostly on the translation of western European languages. One of its main functions is to publish a ranking of the systems for each task, which are produced by aggregating a large number of human judgments of sentencelevel pairwise rankings of system outputs. While the performance on many automatic metrics is also # score range system 1 0.638 1 UEDIN-HEAFIELD 2 0.604 2-3 UEDIN 0.591 2-3 ONLINE-B 4 0.571 4-5 LIMSI-SOUL 0.562 4-5 KIT 0.541 5-6 ONLINE-A 7 0.512 7 MES-SIMPLIFIED 8 0.486 8 DCU 9 0.439 9-10 RWTH 0.429 9-11 CMU-T2T 0.420 10-11 CU-ZEMAN 12 0.389 12 JHU 13 0.322 13 SHEF-WPROA Table 1: System rankings presented as clusters (WMT13 French-English competition). The score column is the percentage of time each system was judged better across its comparisons (2.1). reported (e.g., BLEU (Papineni et al., 2002)), the human evaluation is considered primary, and is in fact used as the gold standard for its metrics task, where evaluation metrics are evaluated. In machine translation, the longstanding disagreements about evaluation measures do not go away when moving from automatic metrics to human judges. This is due in no small part to the inherent ambiguity and subjectivity of the task, but also arises from the particular way that the WMT organizers produce the rankings. The systemlevel rankings are produced by collecting pairwise sentence-level comparisons between system outputs. These are then aggregated to produce a complete ordering of all systems, or, more recently, a partial ordering (Koehn, 2012), with systems clustered where they cannot be distinguished in a statistically significant way (Table 1, taken from Bojar et al. (2013)). A number of problems have been noted with this approach. The first has to do with the nature of ranking itself. Over the past few years, the WMT organizers have introduced a number of minor tweaks to the ranking algorithm (2) in reaction to largely intuitive arguments that have been 1 raised about how the evaluation is conducted (Bojar et al., 2011; Lopez, 2012). While these tweaks have been sensible (and later corroborated), Hopkins and May (2013) point out that this is essentially a model selection task, and should properly be driven by empirical performance on heldout data according to some metric. Instead of intuition, they suggest perplexity, and show that a novel graphical model outperforms existing approaches on that metric, with less amount of data. A second problem is the deficiency of the models used to produce the ranking, which work by computing simple ratios of wins (and, optionally, ties) to losses. Such approaches do not consider the relative difficulty of system matchups, and thus leave open the possibility that a system is ranked highly from the luck of comparisons against poorer opponents. Third, a large number of judgments need to be collected in order to separate the systems into clusters to produce a partial ranking. The sheer size of the space of possible comparisons (all pairs of systems times the number of segments in the test set) requires sampling from this space and distributing the annotations across a number of judges. Even still, the number of judgments needed to produce statistically significant rankings like those in Table 1 grows quadratically in the number of participating systems (Koehn, 2012), often forcing the use of paid, lower-quality annotators hired on Amazons Mechanical Turk. Part of the problem is that the sampling strategy collects data uniformly across system pairings. Intuitively, we should need many fewer annotations between systems with divergent base performance levels, instead focusing the collection effort on system pairs whose performance is more matched, in order to tease out the gaps between similarly-performing systems. Why spend precious human time on redundantly affirming predictable outcomes? To address these issues, we developed a variation of the TrueSkill model (Herbrich et al., 2006), an adaptative model of competitions originally developed for the Xbox Live online gaming community. It assumes that each players skill level follows a Gaussian distribution N (,  2 ), in which  represents a players mean performance, and  2 the systems uncertainty about its current estimate of this mean. These values are updated after each game (in our case, the value of a ternary judgment) in proportion to how surprising the outcome is. TrueSkill has been adapted to a number of areas, including chess, advertising, and academic conference management. The rest of this paper provides an empirical comparison of a number of models of human evaluation (2). We evaluate on perplexity and also on accuracy, showing that the two are not always correlated, and arguing for the primacy of the latter (3). We find that TrueSkill outperforms other models (4). Moreover, TrueSkill also allows us to drastically reduce the amount of data that needs to be collected by sampling non-uniformly from the space of all competitions (5), which also allows for greater separation of the systems into ranked clusters (6). "
W14-3302 "field Ale s Tamchyna Charles University in Prague Abstract This paper presents the results of the WMT14 shared tasks, which included a standard news translation task, a separate medical translation task, a task for run-time estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. "
W14-3303 "Abstract We use parallel FDA5, an efficiently parameterized and optimized parallel implementation of feature decay algorithms for fast deployment of accurate statistical machine translation systems, taking only about half a day for each translation direction. We build Parallel FDA5 Moses SMT systems for all language pairs in the WMT14 translation task and obtain SMT performance close to the top Moses systems with an average of 3.49 BLEU points difference using significantly less resources for training and development. "
W14-3304 "Yandex School of Data Analysis 16, Leo Tolstoy street, Moscow, Russia {alborisov, galinskaya}@yandex-team.ru Abstract This paper describes the Yandex School of Data Analysis Russian-English system submitted to the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task. We start with the system that we developed last year and investigate a few methods that were successful at the previous translation task including unpruned language model, operation sequence model and the new reparameterization of IBM Model 2. Next we propose a {simple yet practical} algorithm to transform Russian sentence into a more easily translatable form before decoding. The algorithm is based on the linguistic intuition of native Russian speakers, also fluent in English. "
W14-3305 " We present the CimS submissions to the 2014 Shared Task for the language pair ENDE. We address the major problems that arise when translating into German: complex nominal and verbal morphology, productive compounding and flexible word ordering. Our morphologyaware translation systems handle word formation issues on different levels of morpho-syntactic modeling. 1 Int  "
W14-3306 "Abstract This paper describes the IPN-UPV participation on the English-to-Hindi translation task from WMT 2014 International Evaluation Campaign. The system presented is based on Moses and enhanced with deep learning by means of a source-context feature function. This feature depends on the input sentence to translate, which makes it more challenging to adapt it into the Moses framework. This work reports the experimental details of the system putting special emphasis on: how the feature function is integrated in Moses and how the deep learning representations are trained and used. "
W14-3307 " This paper describes the joined submission of LIMSI and KIT to the Shared Translation Task for the German-toEnglish direction. The system consists of a phrase-based translation system using a pre-reordering approach. The baseline system already includes several models like conventional language models on different word factors and a discriminative word lexicon. This system is used to generate a k-best list. In a second step, the list is reranked using SOUL language and translation models (Le et al., 2011). Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score. "
W14-3308 "ogy, Bombay {piyushdd,rajen,abhijitmishra,anoopk,ritesh,pb}@cse.iitb.ac.in Abstract In this paper, we describe our EnglishHindi and Hindi-English statistical systems submitted to the WMT14 shared task. The core components of our translation systems are phrase based (Hindi-English) and factored (English-Hindi) SMT systems. We show that the use of number, case and Tree Adjoining Grammar information as factors helps to improve English-Hindi translation, primarily by generating morphological inflections correctly. We show improvements to the translation systems using pre-procesing and post-processing components. To overcome the structural divergence between English and Hindi, we preorder the source side sentence to conform to the target language word order. Since parallel corpus is limited, many words are not translated. We translate out-of-vocabulary words and transliterate named entities in a post-processing stage. We also investigate ranking of translations from multiple systems to select the best translation. "
W14-3309 "Abstract This paper describes the University of Edinburghs (UEDIN) phrase-based submissions to the translation and medical translation shared tasks of the 2014 Workshop on Statistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus. "
W14-3310 "maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk  {teresa.herrmann,eunah.cho,alex.waibel}@kit.edu Abstract This paper describes one of the collaborative efforts within EU-BRIDGE to further advance the state of the art in machine translation between two European language pairs, GermanEnglish and EnglishGerman. Three research institutes involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014). We combined up to nine different machine translation engines via system combination. RWTH Aachen University, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield empirical gains of up to 1.6 points in BLEU and 1.0 points in TER on the WMT newstest2013 test set compared to the best single systems. "
W14-3311 " We present a new version of Phrasal, an open-source toolkit for statistical phrasebased machine translation. This revision includes features that support emerging research trends such as (a) tuning with large feature sets, (b) tuning on large datasets like the bitext, and (c) web-based interactive machine translation. A direct comparison with Moses shows favorable results in terms of decoding speed and tuning time. 1 Int  "
W14-3312 "Abstract We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for EnglishFrench. Furthermore, we investigate post-ordering and tunable POS distortion models for English German. 1 Introduction In this paper we describe the Uppsala University systems for WMT14. We present three different systems. Two of them are based on the documentlevel decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). In our EnglishFrench system we extend Docent to handle pronoun anaphora, and in our EnglishGerman system we add partof-speech phrase-distortion models to Docent. For GermanEnglish we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using preand postreordering. "
W14-3313 "Abstract In this paper, we present the KIT systems participating in the Shared Translation Task translating between EnglishGerman and EnglishFrench. All translations are generated using phrase-based translation systems, using different kinds of word-based, part-ofspeech-based and cluster-based language models trained on the provided data. Additional models include bilingual language models, reordering models based on part-of-speech tags and syntactic parse trees, as well as a lexicalized reordering model. In order to make use of noisy web-crawled data, we apply filtering and data selection methods for language modeling. A discriminative word lexicon using source context information proved beneficial for all translation directions. "
W14-3314 " This paper describes the DCU submission to WMT 2014 on German-English translation task. Our system uses phrasebased translation model with several popular techniques, including Lexicalized Reordering Model, Operation Sequence Model and Language Model interpolation. Our final submission is the result of system combination on several systems which have different pre-processing and alignments. 1 Introdu  "
W14-3315 "Abstract We describe the CMU systems submitted to the 2014 WMT shared translation task. We participated in two language pairs, GermanEnglish and HindiEnglish. Our innovations include: a label coarsening scheme for syntactic tree-to-tree translation, a host of new discriminative features, several modules to create synthetic translation options that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. "
W14-3316 " We describe Stanfords participation in the French-English and English-German tracks of the 2014 Workshop on Statistical Machine Translation (WMT). Our systems used large feature sets, word classes, and an optional unconstrained language model. Among constrained systems, ours performed the best according to uncased BLEU: 36.0% for French-English and 20.9% for English-German. 1 I  "
W14-3317 "Abstract This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the GermanEnglish translation task of the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014). Both hierarchical and phrase-based SMT systems are applied employing hierarchical phrase reordering and word class language models. For the phrase-based system, we run discriminative phrase training. In addition, we describe our preprocessing pipeline for GermanEnglish. "
W14-3318 "Abstract We present the IMS-TTT submission to WMT14, an experimental statistical treeto-tree machine translation system based on the multi-bottom up tree transducer including rule extraction, tuning and decoding. Thanks to input parse forests and a no pruning strategy during decoding, the obtained translations are competitive. The drawbacks are a restricted coverage of 70% on test data, in part due to exact input parse tree matching, and a relatively high runtime. Advantages include easy redecoding with a different weight vector, since the full translation forests can be stored after the first decoding pass. "
W14-3319 "Abstract This paper presents the machine translation systems submitted by the AbuMaTran project to the WMT 2014 translation task. The language pair concerned is EnglishFrench with a focus on French as the target language. The French to English translation direction is also considered, based on the word alignment computed in the other direction. Large language and translation models are built using all the datasets provided by the shared task organisers, as well as the monolingual data from LDC. To build the translation models, we apply a two-step data selection method based on bilingual crossentropy difference and vocabulary saturation, considering each parallel corpus individually. Synthetic translation rules are extracted from the development sets and used to train another translation model. We then interpolate the translation models, minimising the perplexity on the development sets, to obtain our final SMT system. Our submission for the English to French translation task was ranked second amongst nine teams and a total of twenty submissions. "
W14-3320 "V  ctor M. S  anchez-Cartagena,   Juan Antonio P  erez-Ortiz,  Felipe S  anchez-Mart  nez   Dep. de Llenguatges i Sistemes Inform` atics, Universitat dAlacant, E-03071, Alacant, Spain  Prompsit Language Engineering, Av. Universitat, s/n. Edifici Quorum III, E-03202, Elx, Spain {vmsanchez,japerez,fsanchez}@dlsi.ua.es Abstract This paper describes the system jointly developed by members of the Departament de Llenguatges i Sistemes Inform` atics at Universitat dAlacant and the Prompsit Language Engineering company for the shared translation task of the 2014 Workshop on Statistical Machine Translation. We present a phrase-based statistical machine translation system whose phrase table is enriched with information obtained from dictionaries and shallowtransfer rules like those used in rule-based machine translation. The novelty of our approach lies in the fact that the transfer rules used were not written by humans, but automatically inferred from a parallel corpus. "
W14-3321 " This paper describes the AFRL statistical MT system and the improvements that were developed during the WMT14 evaluation campaign. As part of these efforts we experimented with a number of extensions to the standard phrase-based model that improve performance on Russian to English and Hindi to English translation tasks. In addition, we describe our efforts to make use of monolingual English speakers to correct the output of machine translation, and present the results of monolingual postediting of the entire 3003 sentences of the WMT14 Russian-English test set. 1 Introdu  "
W14-3322 " We present our EnglishCzech and EnglishHindi submissions for this years WMT translation task. For EnglishCzech, we build upon last years CHIMERA and evaluate several setups. EnglishHindi is a new language pair for this year. We experimented with reverse self-training to acquire more (synthetic) parallel data and with modeling target-side morphology.  "
W14-3323 "Abstract We describe the Manawi 1 () system submitted to the 2014 WMT translation shared task. We participated in the English-Hindi (EN-HI) and Hindi-English (HI-EN) language pair and achieved 0.792 for the Translation Error Rate (TER) score 2 for EN-HI, the lowest among the competing systems. Our main innovations are (i) the usage of outputs from NLP tools, viz. billingual multi-word expression extractor and named-entity recognizer to improve SMT quality and (ii) the introduction of a novel filter method based on sentence-alignment features. The Manawi system showed the potential of improving translation quality by incorporating multiple NLP tools within the MT pipeline. "
W14-3324 "ns University Abstract This paper describes the string-to-tree systems built at the University of Edinburgh for the WMT 2014 shared translation task. We developed systems for English-German, Czech-English, FrenchEnglish, German-English, Hindi-English, and Russian-English. This year we improved our English-German system through target-side compound splitting, morphosyntactic constraints, and refinements to parse tree annotation; we addressed the out-of-vocabulary problem using transliteration for Hindi and Russian and using morphological reduction for Russian; we improved our GermanEnglish system through tree binarization; and we reduced system development time by filtering the tuning sets. "
W14-3325 "Abstract This paper describes the DCU-Lingo24 submission to WMT 2014 for the HindiEnglish translation task. We exploit miscellaneous methods in our system, including: Context-Informed PB-SMT, OOV Word Conversion (OWC), MultiAlignment Combination (MAC), Operation Sequence Model (OSM), Stemming Align and Normal Phrase Extraction (SANPE), and Language Model Interpolation (LMI). We also describe various preprocessing steps we tried for Hindi in this task. "
W14-3326 "Charles University in Prague, Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics Malostransk  e n  am est   25, 11800 Prague, Czech Republic {odusek,hajic,hlavacova,mnovak,pecina,rosa,tamchyna,uresova,zeman}@ufal.mff.cuni.cz Abstract This paper presents the participation of the Charles University team in the WMT 2014 Medical Translation Task. Our systems are developed within the Khresmoi project, a large integrated project aiming to deliver a multi-lingual multi-modal search and access system for biomedical information and documents. Being involved in the organization of the Medical Translation Task, our primary goal is to set up a baseline for both its subtasks (summary translation and query translation) and for all translation directions. Our systems are based on the phrasebased Moses system and standard methods for domain adaptation. The constrained/unconstrained systems differ in the training data only. "
W14-3327 " This short paper presents a system description for intrinsic evaluation of the WMT 14s medical text translation task. Our systems consist of phrase-based statistical machine translation system and query translation system between German-English language pairs. Our work focuses on the query translation task and we achieved the highest BLEU score among the all submitted systems for the English-German intrinsic query translation evaluation. 1 O  "
W14-3328 " This paper describes adapting statistical machine translation (SMT) systems to medical domain using in-domain and general-domain data as well as webcrawled in-domain resources. In order to complement the limited in-domain corpora, we apply domain focused webcrawling approaches to acquire indomain monolingual data and bilingual lexicon from the Internet. The collected data is used for adapting the language model and translation model to boost the overall translation quality. Besides, we propose an alternative filtering approach to clean the crawled data and to further optimize the domain-specific SMT system. We attend the medical summary sentence unconstrained translation task of the Ninth Workshop on Statistical Machine Translation (WMT2014). Our systems achieve the second best BLEU scores for Czech-English, fourth for French-English, English-French language pairs and the third best results for reminding pairs. 1 Introduction  "
W14-3329 " This paper describes the Dublin City University terminology translation system used for our participation in the query translation subtask in the medical translation task in the Workshop on Statistical Machine Translation (WMT14). We deployed six different kinds of terminology extraction methods, and participated in three different tasks: FREN and EN FR query tasks, and the CLIR task. We obtained 36.2 BLEU points absolute for FREN and 28.8 BLEU points absolute for ENFR tasks where we obtained the first place in both tasks. We obtained 51.8 BLEU points absolute for the CLIR task. 1 Int  "
W14-3330 " This paper describes LIMSIs submission to the first medical translation task at WMT14. We report results for EnglishFrench on the subtask of sentence translation from summaries of medical articles. Our main submission uses a combination of NCODE (n-gram-based) and MOSES (phrase-based) output and continuous-space language models used in a post-processing step for each system. Other characteristics of our submission include: the use of sampling for building MOSES phrase table; the implementation of the vector space model proposed by Chen et al. (2013); adaptation of the POStagger used by NCODE to the medical domain; and a report of error analysis based on the typology of Vilar et al. (2006). 1 Introduction This paper describes LIMSIs submission to the first medical translation task at  "
W14-3331 " This paper explores a number of simple and effective techniques to adapt statistical machine translation (SMT) systems in the medical domain. Comparative experiments are conducted on large corpora for six language pairs. We not only compare each adapted system with the baseline, but also combine them to further improve the domain-specific systems. Finally, we attend the WMT2014 medical summary sentence translation constrained task and our systems achieve the best BLEU scores for Czech-English, EnglishGerman, French-English language pairs and the second best BLEU scores for reminding pairs. 1. Intr  "
W14-3332 "The CNGL Centre for Global Intelligent Content School of Computing Dublin City University, Ireland {zhangj,xiaofengwu, icalixto,avahid,xzhang, away,qliu}@computing.dcu.ie Abstract This paper describes Dublin City Universitys (DCU) submission to the WMT 2014 Medical Summary task. We report our results on the test data set in the French to English translation direction. We also report statistics collected from the corpora used to train our translation system. We conducted our experiment on the Moses 1.0 phrase-based translation system framework. We performed a variety of experiments on translation models, reordering models, operation sequence model and language model. We also experimented with data selection and removal the length constraint for phrase-pair extraction. "
W14-3333 "Abstract Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scale human evaluation of shared task systems for two language pairs to provide a gold standard for tests. Results show very little difference in accuracy across the three methods of significance testing. Notably, accuracy of all test/metric combinations for evaluation of English-to-Spanish are so low that there is not enough evidence to conclude they are any better than a random coin toss. "
W14-3334 "Abstract Previous studies of the effect of word alignment on translation quality in SMT generally explore link level metrics only and mostly do not show any clear connections between alignment and SMT quality. In this paper, we specifically investigate the impact of word alignment on two pre-reordering tasks in translation, using a wider range of quality indicators than previously done. Experiments on GermanEnglish translation show that reordering may require alignment models different from those used by the core translation system. Sparse alignments with high precision on the link level, for translation units, and on the subset of crossing links, like intersected HMM models, are preferred. Unlike SMT performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks. Moreover, we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on SMT reordering tasks. "
W14-3335 "Abstract Scrambling is acceptable reordering of verb arguments in languages such as Japanese and German. In automatic evaluation of translation quality, BLEU is the de facto standard method, but BLEU has only very weak correlation with human judgements in case of Japanese-toEnglish/English-to-Japanese translations. Therefore, alternative methods, IMPACT and RIBES, were proposed and they have shown much stronger correlation than BLEU. Now, RIBES is widely used in recent papers on Japanese-related translations. RIBES compares word order of MT output with manually translated reference sentences but it does not regard scrambling at all. In this paper, we present a method to enumerate scrambled sentences from dependency trees of reference sentences. Our experiments based on NTCIR Patent MT data show that the method improves sentence-level correlation between RIBES and human-judged adequacy. "
W14-3336 "Abstract This paper presents the results of the WMT14 Metrics Shared Task. We asked participants of this task to score the outputs of the MT systems involved in WMT14 Shared Translation Task. We collected scores of 23 metrics from 12 research groups. In addition to that we computed scores of 6 standard metrics (BLEU, NIST, WER, PER, TER and CDER) as baselines. The collected scores were evaluated in terms of system level correlation (how well each metrics scores correlate with WMT14 official manual ranking of systems) and in terms of segment level correlation (how often a metric agrees with humans in comparing two translations of a particular sentence). "
W14-3337 "Abstract In this paper we describe experiments on predicting HTER, as part of our submission in the Shared Task on Quality Estimation, in the frame of the 9th Workshop on Statistical Machine Translation. In our experiment we check whether it is possible to achieve better HTER prediction by training four individual regression models for each one of the edit types (deletions, insertions, substitutions, shifts), however no improvements were yielded. We also had no improvements when investigating the possibility of adding more data from other non-minimally post-edited and freely translated datasets. Best HTER prediction was achieved by adding deduplicated WMT13 data and additional features such as (a) rule-based language corrections (language tool) (b) PCFG parsing statistics and count of tree labels (c) position statistics of parsing labels (d) position statistics of tri-grams with low probability. "
W14-3338 " We describe our systems for the WMT14 Shared Task on Quality Estimation (subtasks 1.1, 1.2 and 1.3). Our submissions use the framework of Multi-task Gaussian Processes, where we combine multiple datasets in a multi-task setting. Due to the large size of our datasets we also experiment with Sparse Gaussian Processes , which aim to speed up training and prediction by providing sensible sparse approximations. 1 Introduction The purpose of machine translation (MT) quality estimation (QE) is to provide a quality prediction for new, unseen machine translated texts, without relying on reference translations (Blatz et al., 2004; Specia et al., 2009; Bojar et al., 2013). A common use of quality predictions is the decision between post-editing a given machine translated sentence and translating its source from scratch, based on whether its post-editing effort is estimated to be lower than the effort of translating the source sentence. The WMT 2014 QE shared task defined a group of tasks related to QE. In this paper, we describe our submissions for subtasks 1.1, 1.2 and 1.3. Our models are based on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a non-parametric kernelised probabilistic framework. We propose to combine multiple datasets to improve our QE models by applying GPs in a multi-task setting. Our hypothesis is that using sensible multi-task learning settings gives improvements over simply pooling all datasets together. Task 1.1 focuses on predicting post-editing effort for four language pairs: English-Spanish (en-es), Spanish-English (es-en), English-German (en-de), and German-English (de-en). Each contains a different number of source sentences and their human translations, as well as 2-3 versions of machine translations: by a statistical (SMT) system, a rule-based system (RBMT) system and, for en-es/de only, a hybrid system. Source sentences were extracted from tests sets of WMT13 and WMT12, and the translations were produced by top MT systems of each type and a human translator. Labels range from 1 to 3, with 1 indicating a perfect translation and 3, a low quality translation. The purpose of task 1.2 is to predict HTER scores (Human Translation Error Rate) (Snover et al., 2006) using a dataset composed of 896 English-Spanish sentences translated by a MT system and post-edited by a professional translator. Finally, task 1.3 aims at predicting post-editing time, using a subset of 650 sentences from the Task 1.2 dataset. For each task, participants can submit two types of results: scoring and ranking. For scoring, evaluation is made in terms of Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). For ranking, DeltaAvg and Spearmans rank correlation were used as evaluation metrics. "
W14-3339 "Centre for Next Generation Localisation School of Computing Dublin City University, Dublin, Ireland. away@computing.dcu.ie Abstract We use referential translation machines (RTM) for quality estimation of translation outputs. RTMs are a computational model for identifying the translation acts between any two data sets with respect to interpretants selected in the same domain, which are effective when making monolingual and bilingual similarity judgments. RTMs achieve top performance in automatic, accurate, and language independent prediction of sentence-level and word-level statistical machine translation (SMT) quality. RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations and achieve the top performance in WMT13 quality estimation task (QET13). We improve our RTM models with the Parallel FDA5 instance selection model, with additional features for predicting the translation performance, and with improved learning models. We develop RTM models for each WMT14 QET (QET14) subtask, obtain improvements over QET13 results, and rank 1st in all of the tasks and subtasks of QET14. "
W14-3340 "Abstract This paper describes the joint submission of Fondazione Bruno Kessler, Universitat Polit` ecnica de Val` encia and University of Edinburgh to the Quality Estimation tasks of the Workshop on Statistical Machine Translation 2014. We present our submissions for Task 1.2, 1.3 and 2. Our systems ranked first for Task 1.2 and for the Binary and Level1 settings in Task 2. "
W14-3341 "reland {chokamp|icalixto|jwagner|zhangj}@computing.dcu.ie Abstract We describe the DCU-MIXED and DCUSVR submissions to the WMT-14 Quality Estimation task 1.1, predicting sentencelevel perceived post-editing effort. Feature design focuses on target-side features as we hypothesise that the source side has little effect on the quality of human translations, which are included in task 1.1 of this years WMT Quality Estimation shared task. We experiment with features of the QuEst framework, features of our past work, and three novel feature sets. Despite these efforts, our two systems perform poorly in the competition. Follow up experiments indicate that the poor performance is due to improperly optimised parameters. "
W14-3342 "Abstract This paper describes our Word-level QE system for WMT 2014 shared task on Spanish English pair. Compared to WMT 2013, this years task is different due to the lack of SMT setting information and additional resources. We report how we overcome this challenge to retain most of the important features which performed well last year in our system. Novel features related to the availability of multiple systems output (new point of this year) are also proposed and experimented along with baseline set. The system is optimized by several ways: tuning the classification threshold, combining with WMT 2013 data, and refining using Feature Selection strategy on our development set, before dealing with the test set for submission. "
W14-3343 "S1 4DP, UK {c.scarton,l.specia}@sheffield.ac.uk Abstract This paper presents the use of consensus among Machine Translation (MT) systems for the WMT14 Quality Estimation shared task. Consensus is explored here by comparing the MT system output against several alternative machine translations using standard evaluation metrics. Figures extracted from such metrics are used as features to complement baseline prediction models. The hypothesis is that knowing whether the translation of interest is similar or dissimilar to translations from multiple different MT systems can provide useful information regarding the quality of such a translation. "
W14-3344 "and Franc  ois Yvon Universit  e Paris Sud and LIMSI-CNRS 91 403 ORSAY CEDEX, France {wisniews, pecheux, allauzen, yvon}@limsi.fr Abstract This paper describes LIMSI participation to the WMT14 Shared Task on Quality Estimation; we took part to the wordlevel quality estimation task for English to Spanish translations. Our system relies on a random forest classifier, an ensemble method that has been shown to be very competitive for this kind of task, when only a few dense and continuous features are used. Notably, only 16 features are used in our experiments. These features describe, on the one hand, the quality of the association between the source sentence and each target word and, on the other hand, the fluency of the hypothesis. Since the evaluation criterion is the f 1 measure, a specific tuning strategy is proposed to select the optimal values for the hyper-parameters. Overall, our system achieves a 0.67 f 1 score on a randomly extracted test set. "
W14-3345 "<NoAbstract>"
W14-3346 "Abstract BLEU is the de facto standard machine translation (MT) evaluation metric. However, because BLEU computes a geometric mean of n-gram precisions, it often correlates poorly with human judgment on the sentence-level. Therefore, several smoothing techniques have been proposed. This paper systematically compares 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques. Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine translation tuning. "
W14-3347 " In this paper we present VERTa, a linguistically-motivated metric that combines linguistic features at different levels. We provide the linguistic motivation on which the metric is based, as well as describe the different modules in VERTa and how they are combined. Finally, we describe the two versions of VERTa, VERTa-EQ and VERTa-W, sent to WMT14 and report results obtained in the experiments conducted with the WMT12 and WMT13 data into English. 1 Int  "
W14-3348 "13 USA {mdenkows,alavie}@cs.cmu.edu Abstract This paper describes Meteor Universal, released for the 2014 ACL Workshop on Statistical Machine Translation. Meteor Universal brings language specific evaluation to previously unsupported target languages by (1) automatically extracting linguistic resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). "
W14-3349 "Abstract As described in this paper, we propose a new automatic evaluation metric for machine translation. Our metric is based on chunking between the reference and candidate translation. Moreover, we apply a prize based on sentence-length to the metric, dissimilar from penalties in BLEU or NIST. We designate this metric as Automatic Evaluation of Machine Translation in which the Prize is Applied to a Chunkbased metric (APAC). Through metaevaluation experiments and comparison with several metrics, we confirmed that our metric shows stable correlation with human judgment. "
W14-3350 "Abstract This paper describes the LAYERED metric which is used for the shared WMT14 metrics task. Various metrics exist for MT evaluation: BLEU (Papineni, 2002), METEOR (Alon Lavie, 2007), TER (Snover, 2006) etc., but are found inadequate in quite a few language settings like, for example, in case of free word order languages. In this paper, we propose an MT evaluation scheme that is based on the NLP layers: lexical, syntactic and semantic. We contend that higher layer metrics are after all needed. Results are presented on the corpora of ACL-WMT, 2013 and 2014. We end with a metric which is composed of weighted metrics at individual layers, which correlates very well with human judgment. "
W14-3351 "Abstract This paper describes the UPC submissions to the WMT14 Metrics Shared Task: UPCIPA and UPC-STOUT. These metrics use a collection of evaluation measures integrated in ASIYA, a toolkit for machine translation evaluation. In addition to some standard metrics, the two submissions take advantage of novel metrics that consider linguistic structures, lexical relationships, and semantics to compare both source and reference translation against the candidate translation. The new metrics are available for several target languages other than English. In the the official WMT14 evaluation, UPC-IPA and UPC-STOUT scored above the average in 7 out of 9 language pairs at the system level and 8 out of 9 at the segment level. "
W14-3352 "ALT Research Group Qatar Computing Research Institute  Qatar Foundation {sjoty,fguzman,lmarquez,pnakov}@qf.org.qa Abstract We present novel automatic metrics for machine translation evaluation that use discourse structure and convolution kernels to compare the discourse tree of an automatic translation with that of the human reference. We experiment with five transformations and augmentations of a base discourse tree representation based on the rhetorical structure theory, and we combine the kernel scores for each of them into a single score. Finally, we add other metrics from the ASIYA MT evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level. "
W14-3353 "Charles University in Prague, Faculty of Mathematics and Physics Institute of Formal ad Applied Linguistics {libovicky, pecina}@ufal.mff.cuni.cz Abstract This paper describes a machine translation metric submitted to the WMT14 Metrics Task. It is a simple modification of the standard BLEU metric using a monolingual alignment of reference and test sentences. The alignment is computed as a minimum weighted maximum bipartite matching of the translated and the reference sentence words with respect to the relative edit distance of the word prefixes and suffixes. The aligned words are included in the n-gram precision computation with a penalty proportional to the matching distance. The proposed tBLEU metric is designed to be more tolerant to errors in inflection, which usually does not effect the understandability of a sentence, and therefore be more suitable for measuring quality of translation into morphologically richer languages. "
W14-3354 "Abstract We present the UvA-ILLC submission of the BEER metric to WMT 14 metrics task. BEER is a sentence level metric that can incorporate a large number of features combined in a linear model. Novel contributions are (1) efficient tuning of a large number of features for maximizing correlation with human system ranking, and (2) novel features that give smoother sentence level scores. "
W14-3355 "Abstract Based on the last years DCU-CASIST participation on WMT metrics task, we further improve our model in the following ways: 1) parameter tuning 2) support languages other than English. We tuned our system on all the data of WMT 2010, 2012 and 2013. The tuning results as well as the WMT 2014 test results are reported. "
W14-3356 "Abstract High-quality parallel data is crucial for a range of multilingual applications, from tuning and evaluating machine translation systems to cross-lingual annotation projection. Unfortunately, automatically obtained parallel data (which is available in relative abundance) tends to be quite noisy. To obtain high-quality parallel data, we introduce a crowdsourcing paradigm in which workers with only basic bilingual proficiency identify translations from an automatically extracted corpus of parallel microblog messages. For less than $350, we obtained over 5000 parallel segments in five language pairs. Evaluated against expert annotations, the quality of the crowdsourced corpus is significantly better than existing automatic methods: it obtains an performance comparable to expert annotations when used in MERT tuning of a microblog MT system; and training a parallel sentence classifier with it leads also to improved results. The crowdsourced corpora will be made available in http://www.cs.cmu.edu/ ~lingwang/microtopia/. "
W14-3357 "Computer and Information Science Dept. University of Pennsylvania Abstract In previous work we showed that when using an SMT model trained on old-domain data to translate text in a new-domain, most errors are due to unseen source words, unseen target translations, and inaccurate translation model scores (Irvine et al., 2013a). In this work, we target errors due to inaccurate translation model scores using new-domain comparable corpora, which we mine from Wikipedia. We assume that we have access to a large olddomain parallel training corpus but only enough new-domain parallel data to tune model parameters and do evaluation. We use the new-domain comparable corpora to estimate additional feature scores over the phrase pairs in our baseline models. Augmenting models with the new features improves the quality of machine translations in the medical and science domains by up to 1.3 BLEU points over very strong baselines trained on the 150 million word Canadian Hansard dataset. "
W14-3358 ", {bhaddow,pkoehn}@inf.ed.ac.uk Abstract Despite its potential to improve lexical selection, most state-of-the-art machine translation systems take only minimal contextual information into account. We capture context with a topic model over distributional profiles built from the context words of each translation unit. Topic distributions are inferred for each translation unit and used to adapt the translation model dynamically to a given test context by measuring their similarity. We show that combining information from both local and global test contexts helps to improve lexical selection and outperforms a baseline system by up to 1.15 BLEU. We test our topic-adapted model on a diverse data set containing documents from three different domains and achieve competitive performance in comparison with two supervised domain-adapted systems. "
W14-3359 "ermany {mansour,ney}@cs.rwth-aachen.de Abstract In this work, we tackle the problem of language and translation models domainadaptation without explicit bilingual indomain training data. In such a scenario, the only information about the domain can be induced from the source-language test corpus. We explore unsupervised adaptation, where the source-language test corpus is combined with the corresponding hypotheses generated by the translation system to perform adaptation. We compare unsupervised adaptation to supervised and pseudo supervised adaptation. Our results show that the choice of the adaptation (target) set is crucial for successful application of adaptation methods. Evaluation is conducted over the German-to-English WMT newswire translation task. The experiments show that the unsupervised adaptation method generates the best translation quality as well as generalizes well to unseen test sets. "
W14-3360 " Scalable discriminative training methods are now broadly available for estimating phrase-based, feature-rich translation models. However, the sparse feature sets typically appearing in research evaluations are less attractive than standard dense features such as language and translation model probabilities: they often overfit, do not generalize, or require complex and slow feature extractors. This paper introduces extended features, which are more specific than dense features yet more general than lexicalized sparse fe tures. Large-scale experiments show that extended features yield robust BLEU gains for both Arabic-English (+1.05) and Chinese-English (+0.67) relative to a strong feature-rich baseline. We also specialize the feature set to specific data domains, identify an objective function that is less prone to overfitting, and release fast, scalable, and language-independent tools for implementing the features. 1 Introductio  "
W14-3361 "In phrase-based statistical machine translation systems, variation in grammatical structures between source and target languages can cause large movements of phrases. Modeling such movements is crucial in achieving translations of long sentences that appear natural in the target language. We explore generative learning approach to phrase reordering in Arabic to English. Formulating the reordering problem as a classification problem and using naive Bayes with feature selection, we achieve an improvement in the BLEU score over a lexicalized reordering model. The proposed model is compact, fast and scalable to a large corpus. "
W14-3362 " We present an effective technique to easily augment GHKM-style syntax-based machine translation systems (Galley et al., 2006) with phrase pairs that do not comply with any syntactic well-formedness constraints. Non-syntactic phrase pairs are distinguished from syntactic ones in order to avoid harming effects. We apply our technique in state-of-the-art string-totree and tree-to-string setups. For tree-tostring translation, we furthermore investigate novel approaches for translating with source-syntax GHKM rules in association with input tree constraints and input tree features. 1 Introduct  "
W14-3402 "Abstract In this paper, we present a system for recognizing temporal expressions related to cell cycle phase (CCP) concepts in biomedical literature. We identified 11 classes of cell cycle related temporal expressions, for which we made extensions to TIMEX3, arranging them in an ontology derived from the Gene Ontology. We annotated 310 abstracts from PubMed. Annotation guidelines were developed, consistent with existing time-related annotation guidelines for TimeML. Two annotators participated in the annotation. We achieved an inter-annotator agreement of 0.79 for an exact span match and 0.82 for relaxed constraints. Our approach is a hybrid of machine learning to recognize temporal expressions and a rule-based approach to map them to the ontology. We trained a named entity recognizer using Conditional Random Fields (CRF) models. An off-the-shelf implementation of the linear chain CRF model was used. We obtained an F-score of 0.77 for temporal expression recognition. We achieved 0.79 macro-averagee F-score and 0.78 microaveraged F-score for mapping to the ontology. "
W14-3403 " Publication bias refers to the phenomenon that statistically significant, positive results are more likely to be published than non-significant, negative results. Currently, researchers have to manually identify negative results in a large number of publications in order to examine publication biases. This paper proposes an NLP approach for automatically classifying negated sentences in biomedical abstracts as either reporting negative findings or not. Using multinomial naive Bayes algorithm and bag-ofwords features enriched by parts-ofspeeches and constituents, we built a classifier that reached 84% accuracy based on 5-fold cross validation on a balanced data set. 1 Introduction Pu  "
W14-3404 " While machine learning methods for named entity recognition (mention-level detection) have become common, machine learning methods have rarely been applied to normalization (concept-level identification). Recent research introduced a machine learning method for normalization based on pairwise learning to rank. This method, DNorm, uses a linear model to score the similarity between mentions and concept names, and has several desirable properties, including learning term variation directly from training data. In this manuscript we employ a dimensionality reduction technique based on low-rank matrix approximation, similar to latent semantic indexing. We compare the performance of the low rank method to previous work, using disease name normalization in the NCBI Disease Corpus as the test case, and demonstrate increased performance as the matrix rank increases. We further demonstrate a significant reduction in the number of parameters to be learned and discuss the implications of this result in the context of algorithm scalability. 1 Introductio  "
W14-3405 " This paper presents a method for decomposing long, complex consumer health questions. Our approach largely decomposes questions using their syntactic structure, recognizing independent questions embedded in clauses, as well as coordinations and exemplifying phrases. Additionally, we identify elements specific to disease-related consumer health questions, such as the focus disease and background information. To achieve this, our approach combines rank-and-filter machine learning methods with rule-based methods. Our results demonstrate significant improvements over the heuristic methods typically employed for question decomposition that rely only on the syntactic parse tree. 1 Introduct  "
W14-3406 "Abstract We apply semi-supervised topic modeling techniques to detect health-related discussions in everyday telephone conversations, which has applications in large-scale epidemiological studies and for clinical interventions for older adults. The privacy requirements associated with utilizing everyday telephone conversations preclude manual annotations; hence, we explore semi-supervised methods in this task. We adopt a semi-supervised version of Latent Dirichlet Allocation (LDA) to guide the learning process. Within this framework, we investigate a strategy to discard irrelevant words in the topic distribution and demonstrate that this strategy improves the average F-score on the in-domain task and an out-of-domain task (Fisher corpus). Our results show that the increase in discussion of health related conversations is statistically associated with actual medical events obtained through weekly selfreports. "
W14-3407 "20894 {kilicogluh,ddemner}@mail.nih.gov Abstract FDA drug package inserts provide comprehensive and authoritative information about drugs. DailyMed database is a repository of structured product labels extracted from these package inserts. Most salient information about drugs remains in free text portions of these labels. Extracting information from these portions can improve the safety and quality of drug prescription. In this paper, we present a study that focuses on resolution of coreferential information from drug labels contained in DailyMed. We generalized and expanded an existing rule-based coreference resolution module for this purpose. Enhancements include resolution of set/instance anaphora, recognition of appositive constructions and wider use of UMLS semantic knowledge. We obtained an improvement of 40% over the baseline with unweighted average F 1 -measure using B-CUBED, MUC, and CEAF metrics. The results underscore the importance of set/instance anaphora and appositive constructions in this type of text and point out the shortcomings in coreference annotation in the dataset. "
W14-3408 "Abstract An up-to-date problem list is useful for assessing a patients current clinical status. Natural language processing can help maintain an accurate problem list. For instance, a patient problem list from a clinical document can be derived from individual problem mentions within the clinical document once these mentions are mapped to a standard vocabulary. In order to develop and evaluate accurate document-level inference engines for this task, a patient problem list could be generated using a standard vocabulary. Adequate coverage by standard vocabularies is important for supporting a clear representation of the patient problem concepts described in the texts and for interoperability between clinical systems within and outside the care facilities. In this pilot study, we report the reliability of domain expert generation of a patient problem list from a variety of clinical texts and evaluate the coverage of annotated patient problems against SNOMED CT and SNOMED Clinical Observation Recording and Encoding (CORE) Problem List. Across report types, we learned that patient problems can be annotated with agreement ranging from 77.1% to 89.6% F1-score and mapped to the CORE with moderate coverage ranging from 45%-67% of patient problems. "
W14-3409 "Abstract Medical coding is a process of classifying health records according to standard code sets representing procedures and diagnoses. It is an integral part of health care in the U.S., and the high costs it incurs have prompted adoption of natural language processing techniques for automatic generation of these codes from the clinical narrative contained in electronic health records. The need for effective auto-coding methods becomes even greater with the impending adoption of ICD-10, a code inventory of greater complexity than the currently used code sets. This paper presents a system that predicts ICD-10 procedure codes from the clinical narrative using several levels of abstraction. First, partial hierarchical classification is used to identify potentially relevant concepts and codes. Then, for each of these concepts we estimate the confidence that it appears in a procedure code for that document. Finally, confidence values for the candidate codes are estimated using features derived from concept confidence scores. The concept models can be trained on data with ICD-9 codes to supplement sparse ICD-10 training resources. Evaluation on held-out data shows promising results. "
W14-3410 "Abstract We present an active learning method for placing the event mentions in an operative note into a pre-specified event structure. Event mentions are first classified into action, peripheral action, observation, and report events. The actions are further classified into their appropriate location within the event structure. We examine how utilizing active learning significantly reduces the time needed to completely annotate a corpus of 2,820 appendectomy notes. "
W14-3411 "Abstract Free text notes typed by primary care physicians during patient consultations typically contain highly non-canonical language. Shallow syntactic analysis of free text notes can help to reveal valuable information for the study of disease and treatment. We present an exploratory study into chunking such text using offthe-shelf language processing tools and pre-trained statistical models. We evaluate chunking accuracy with respect to partof-speech tagging quality, choice of chunk representation, and breadth of context features. Our results indicate that narrow context feature windows give the best results, but that chunk representation and minor differences in tagging quality do not have a significant impact on chunking accuracy. "
W14-3412 "Abstract The dual process model (Evans, 2008) posits two types of decision-making, which may be ordered on a continuum from intuitive to analytical (Hammond, 1981). This work uses a dataset of narrated image-based clinical reasoning, collected from physicians as they diagnosed dermatological cases presented as images. Two annotators with training in cognitive psychology assigned each narrative a rating on a four-point decision scale, from intuitive to analytical. This work discusses the annotation study, and makes contributions for resource creation methodology and analysis in the clinical domain. "
W14-3413 "Abstract One of the most important features of health care is to be able to follow a patients progress over time and identify events in a temporal order. We describe initial steps in creating resources for automatic temporal reasoning of Swedish medical text. As a first step, we focus on the identification of temporal expressions by exploiting existing resources and systems available for English. We adapt the HeidelTime system and manually evaluate its performance on a small subset of Swedish intensive care unit documents. On this subset, the adapted version of HeidelTime achieves a precision of 92% and a recall of 66%. We also extract the most frequent temporal expressions from a separate, larger subset, and note that most expressions concern parts of days or specific times. We intend to further develop resources for temporal reasoning of Swedish medical text by creating a gold standard corpus also annotated with events and temporal links, in addition to temporal expressions and their normalised values. "
W14-3414 "Abstract The MIMIC II database contains 1,237,686 clinical documents of various kinds. A common task for researchers working with this database is to run MetaMap, which uses the UMLS Metathesaurus, on those documents to identify specific semantic types of entities mentioned in them. However, this task is computationally expensive and time-consuming. Research in many groups could be accelerated if there were a community-accessible set of outputs from running MetaMap on this document collection, cached and available on the MIMIC-II website. This paper describes a repository of all MetaMap output from the MIMIC II database, publicly available, assuming compliance with usage agreements required by UMLS and MIMIC-II. Additionally, software for manipulating MetaMap output, available on SourceForge with a liberal Open Source license, is described. "
W14-3415 ", Spain {isegura|spena|pmf}@inf.uc3m.es Abstract In this paper, we present preliminary results obtained using a system based on cooccurrence of drug-effect pairs as a first step in the study of detecting adverse drug reactions and drug indications from social media texts. To the best of our knowledge, this is the first work that extracts this kind of relationships from user messages that were collected from an online Spanish health-forum. In addition, we also describe the automatic construction of the first Spanish database for drug indications and adverse drug reactions. "
W14-3416 "sity thierry.charnois @lipn.univ-paris13.fr Abstract This work focuses on signs and symptoms recognition in biomedical texts abstracts. First, this specific task is described from a linguistic point of view. Then a methodology combining pattern mining and language processing is proposed. In the absence of an authoritative annotated corpus, our approach has the advantage of being weakly-supervised. Preliminary experimental results are discussed and reveal promising avenues. "
W14-3417 "Abstract The continuously increasing number of publications within the biomedical domain has fuelled the creation of literature based discovery (LBD) systems which identify unconnected pieces of knowledge appearing in separate literatures which can be combined to make new discoveries. Without filtering, the amount of hidden knowledge found is vast due to noise, making it impractical for a researcher to examine, or clinically evaluate, the potential discoveries. We present a number of filtering techniques, including two which exploit the LBD system itself rather than being based on a statistical or manual examination of document collections, and we demonstrate usefulness via replication of known discoveries. "
W14-3418 "65 88397 Biberach, Germany matthias.zwick @boehringer-ingelheim.com Abstract Retrieving information about highly ambiguous gene/protein homonyms is a challenge, in particular where their non-protein meanings are more frequent than their protein meaning (e. g., SAH or HF). Due to their limited coverage in common benchmarking data sets, the performance of existing gene/protein recognition tools on these problematic cases is hard to assess. We uniformly sample a corpus of eight ambiguous gene/protein abbreviations from MEDLINE and provide manual annotations for each mention of these abbreviations. 1 Based on this resource, we show that available gene recognition tools such as conditional random fields (CRF) trained on BioCreative 2 NER data or GNAT tend to underperform on this phenomenon. We propose to extend existing gene recognition approaches by combining a CRF and a support vector machine. In a crossentity evaluation and without taking any entity-specific information into account, our model achieves a gain of 6 points F 1 -Measure over our best baseline which checks for the occurrence of a long form of the abbreviation and more than 9 points over all existing tools investigated. "
W14-3419 ",gangopad@umbc.edu Abstract The vast array of medical text data represents a valuable resource that can be analyzed to advance the state of the art in medicine. Currently, text mining methods are being used to analyze medical research and clinical text data. Some of the main challenges in text analysis are high dimensionality and noisy data. There is a need to develop novel feature transformation methods that help reduce the dimensionality of data and improve the performance of machine learning algorithms. In this paper we present a feature transformation method named FFTM. We illustrate the efficacy of our method using local term weighting, global term weighting, and Fuzzy clustering methods and show that the quality of text analysis in medical text documents can be improved. We compare FFTM with Latent Dirichlet Allocation (LDA) by using two different datasets and statistical tests show that FFTM outperforms LDA. "
W14-4301 "<NoAbstract>"
W14-4302 " We present a technique for crowdsourcing street-level geographic information using spoken natural language. In particular, we are interested in obtaining first-person-view information about what can be seen from different positions in the city. This information can then for example be used for pedestrian routing services. The approach has been tested in the lab using a fully implemented spoken dialogue system, and has shown promising results. 1 Int  "
W14-4303 "Abstract Mobile Internet access via smartphones puts demands on in-car infotainment systems, as more and more drivers like to access the Internet while driving. Spoken dialog systems (SDS) distract drivers less than visual/haptic-based dialog systems. However, in conversational SDSs drivers might speak utterances which are not in the domain of the SDS and thus cannot be understood. In a Wizard of Oz study, we evaluate the effects of out-of-domain utterances on cognitive load, driving performance, and usability. The results show that an SDS which reacts as expected by the driver, is a good approach to control incar infotainment systems, whereas unexpected SDS reactions might cause severe accidents. We evaluate how a dialog initiative switch, which guides the user and enables him to reach his task goal, performs. "
W14-4304 "CA 93085 Abstract In this paper, we address issues in situated language understanding in a rapidly changing environment  a moving car. Specifically, we propose methods for understanding user queries about specific target buildings in their surroundings. Unlike previous studies on physically situated interactions such as interaction with mobile robots, the task is very sensitive to timing because the spatial relation between the car and the target is changing while the user is speaking. We collected situated utterances from drivers using our research system, Townsurfer, which is embedded in a real vehicle. Based on this data, we analyze the timing of user queries, spatial relationships between the car and targets, head pose of the user, and linguistic cues. Optimized on the data, our algorithms improved the target identification rate by 24.1% absolute. "
W14-4305 "Abstract We present a spoken dialogue system for navigating information (such as news articles), and which can engage in small talk. At the core is a partially observable Markov decision process (POMDP), which tracks users state and focus of attention. The input to the POMDP is provided by a spoken language understanding (SLU) component implemented with logistic regression (LR) and conditional random fields (CRFs). The POMDP selects one of six action classes; each action class is implemented with its own module. "
W14-4306 "sity Raleigh, North Carolina, USA {akvail, keboyer}@ncsu.edu Abstract This paper explores dialogue adaptation over repeated interactions within a taskoriented human tutorial dialogue corpus. We hypothesize that over the course of four tutorial dialogue sessions, tutors adapt their strategies based on the personality of the student, and in particular to student introversion or extraversion. We model changes in strategy over time and use them to predict how effectively the tutorial interactions support student learning. The results suggest that students leaning toward introversion learn more effectively with a minimal amount of interruption during task activity, but occasionally require a tutor prompt before voicing uncertainty; on the other hand, students tending toward extraversion benefit significantly from increased interaction, particularly through tutor prompts for reflection on task activity. This line of investigation will inform the development of future user-adaptive dialogue systems. "
W14-4307 "Abstract Human-computer trust has shown to be a critical factor in influencing the complexity and frequency of interaction in technical systems. Particularly incomprehensible situations in human-computer interaction may lead to a reduced users trust in the system and by that influence the style of interaction. Analogous to human-human interaction, explaining these situations can help to remedy negative effects. In this paper we present our approach of augmenting task-oriented dialogs with selected explanation dialogs to foster the humancomputer trust relationship in those kinds of situations. We have conducted a webbased study testing the effects of different goals of explanations on the components of human-computer trust. Subsequently, we show how these results can be used in our probabilistic trust handling architecture to augment pre-defined task-oriented dialogs. "
W14-4308 "Abstract Non-cooperative dialogue behaviour has been identified as important in a variety of application areas, including education, military operations, video games and healthcare. However, it has not been addressed using statistical approaches to dialogue management, which have always been trained for co-operative dialogue. We develop and evaluate a statistical dialogue agent which learns to perform noncooperative dialogue moves in order to complete its own objectives in a stochastic trading game. We show that, when given the ability to perform both cooperative and non-cooperative dialogue moves, such an agent can learn to bluff and to lie so as to win games more often  against a variety of adversaries, and under various conditions such as risking penalties for being caught in deception. For example, we show that a non-cooperative dialogue agent can learn to win an additional 15.47% of games against a strong rulebased adversary, when compared to an optimised agent which cannot perform noncooperative moves. This work is the first to show how an agent can learn to use noncooperative dialogue to effectively meet its own goals. "
W14-4309 "4, USA {morbini,forbell,sagae}@ict.usc.edu Abstract Although data-driven techniques are commonly used for Natural Language Understanding in dialogue systems, their efficacy is often hampered by the lack of appropriate annotated training data in sufficient amounts. We present an approach for rapid and cost-effective annotation of training data for classification-based language understanding in conversational dialogue systems. Experiments using a webaccessible conversational character that interacts with a varied user population show that a dramatic improvement in natural language understanding and a substantial reduction in expert annotation effort can be achieved by leveraging non-expert annotation. "
W14-4310 "gy, Japan Abstract When using spoken dialog systems in actual environments, users sometimes abandon the dialog without making any input utterance. To help these users before they give up, the system should know why they could not make an utterance. Thus, we have examined a method to estimate the state of a dialog user by capturing the users non-verbal behavior even when the users utterance is not observed. The proposed method is based on vector quantization of multi-modal features such as non-verbal speech, feature points of the face, and gaze. The histogram of the VQ code is used as a feature for determining the state. We call this feature the Bagof-Behaviors. According to the experimental results, we prove that the proposed method surpassed the results of conventional approaches and discriminated the target users states with an accuracy of more than 70%. "
W14-4311 " When deploying a spoken dialogue system in a new domain, one faces a situation where little to no data is available to train domain-specific statistical models. We describe our experience with bootstrapping a dialogue system for public transit and weather information in real-word deployment under public use. We proceeded incrementally, starting from a minimal system put on a toll-free telephone number to collect speech data. We were able to incorporate statistical modules trained on collected data  in-domain speech recognition language models and spoken language understanding  while simultaneously extending the domain, making use of automatically generated semantic annotation. Our approach shows that a successful system can be built with minimal effort and no in-domain data at hand. "
W14-4312 "Dialogue Systems Group Bielefeld University david.schlangen 2 Abstract In order to process incremental situated dialogue, it is necessary to accept information from various sensors, each tracking, in real-time, different aspects of the physical situation. We present extensions of the incremental processing toolkit INPROTK which make it possible to plug in such multimodal sensors and to achieve situated, real-time dialogue. We also describe a new module which enables the use in INPROTK of the Google Web Speech API, which offers speech recognition with a very large vocabulary and a wide choice of languages. We illustrate the use of these extensions with a description of two systems handling different situated settings. "
W14-4313 ", MI 48824, USA {chengyu9, jiayunyi, xin}@egr.msu.edu Abstract This paper describes an approach for a robotic arm to learn new actions through dialogue in a simplified blocks world. In particular, we have developed a threetier action knowledge representation that on one hand, supports the connection between symbolic representations of language and continuous sensorimotor representations of the robot; and on the other hand, supports the application of existing planning algorithms to address novel situations. Our empirical studies have shown that, based on this representation the robot was able to learn and execute basic actions in the blocks world. When a human is engaged in a dialogue to teach the robot new actions, step-by-step instructions lead to better learning performance compared to one-shot instructions. "
W14-4314 "Abstract Incrementality as a way of managing the interactions between a dialogue system and its users has been shown to have concrete advantages over the traditional turn-taking frame. Incremental systems are more reactive, more human-like, offer a better user experience and allow the user to correct errors faster, hence avoiding desynchronisations. Several incremental models have been proposed, however, their core underlying architecture is different from the classical dialogue systems. As a result, they have to be implemented from scratch. In this paper, we propose a method to transform traditional dialogue systems into incremental ones. A new module, called the Scheduler is inserted between the client and the service so that from the clients point of view, the system behaves incrementally, even though the service does not. "
W14-4315 " This paper presents an extension of the Kaldi automatic speech recognition toolkit to support on-line recognition. The resulting recogniser supports acoustic models trained using state-of-theart acoustic modelling techniques. As the recogniser produces word posterior lattices, it is particularly useful in statistical dialogue systems, which try to exploit uncertainty in the recognisers output. Our experiments show that the online recogniser performs significantly better in terms of latency when compared to a cloud-based recogniser. 1 Introduction  "
W14-4316 "Abstract Unsupervised machine learning approaches hold great promise for recognizing dialogue acts, but the performance of these models tends to be much lower than the accuracies reached by supervised models. However, some dialogues, such as task-oriented dialogues with parallel task streams, hold rich information that has not yet been leveraged within unsupervised dialogue act models. This paper investigates incorporating task features into an unsupervised dialogue act model trained on a corpus of human tutoring in introductory computer science. Experimental results show that incorporating task features and dialogue history features significantly improve unsupervised dialogue act classification, particularly within a hierarchical framework that gives prominence to dialogue history. This work constitutes a step toward building high-performing unsupervised dialogue act models that will be used in the next generation of task-oriented dialogue systems. "
W14-4317 "Abstract Speech-enabled dialogue systems have the potential to enhance the ease with which blind individuals can interact with the Web beyond what is possible with screen readers the currently available assistive technology which narrates the textual content on the screen and provides shortcuts to navigate the content. In this paper, we present a dialogue act model towards developing a speech enabled browsing system. The model is based on the corpus data that was collected in a wizard-of-oz study with 24 blind individuals who were assigned a gamut of browsing tasks. The development of the model included extensive experiments with assorted feature sets and classifiers; the outcomes of the experiments and the analysis of the results are presented. "
W14-4318 " In this paper, we present a novel supervised approach to the problem of summarizing email conversations and modeling dialogue acts. We assume that there is a relationship between dialogue acts and important sentences. Based on this assumption, we introduce a sequential graphical model approach which simultaneously summarizes email conversation and models dialogue acts. We compare our model with sequential and non-sequential models, which independently conduct the tasks of extractive summarization and dialogue act modeling. An empirical evaluation shows that our approach significantly outperforms all baselines in classifying correct summary sentences without losing performance on dialogue act modeling task. 1 Introduct  "
W14-4320 "Abstract In this paper we address the problem of skewed class distribution in implicit discourse relation recognition. We examine the performance of classifiers for both binary classification predicting if a particular relation holds or not and for multi-class prediction. We review prior work to point out that the problem has been addressed differently for the binary and multi-class problems. We demonstrate that adopting a unified approach can significantly improve the performance of multi-class prediction. We also propose an approach that makes better use of the full annotations in the training set when downsampling is used. We report significant absolute improvements in performance in multi-class prediction, as well as significant improvement of binary classifiers for detecting the presence of implicit Temporal, Comparison and Contingency relations. "
W14-4321 "Abstract We study the role that logical polarity plays in determining the rejection or acceptance function of an utterance in dialogue. We develop a model inspired by recent work on the semantics of negation and polarity particles and test it on annotated data from two spoken dialogue corpora: the Switchboard Corpus and the AMI Meeting Corpus. Our experiments show that taking into account the relative polarity of a proposal under discussion and of its response greatly helps to distinguish rejections from acceptances in both corpora. "
W14-4322 "1, USA {mriaz2,girju}@illinois.edu Abstract Recognition of causality is important to achieve natural language discourse understanding. Previous approaches rely on shallow linguistic features. In this work, we propose to identify causality in verbnoun pairs by exploiting deeper semantics of nouns and verbs. Particularly, we acquire and employ three novel types of knowledge: (1) semantic classes of nouns with a high and low tendency to encode causality along with information regarding metonymies, (2) data-driven semantic classes of verbal events with the least tendency to encode causality, and (3) tendencies of verb frames to encode causality. Using these knowledge sources, we achieve around 15% improvement in Fscore over a supervised classifier trained using linguistic features. "
W14-4323 "Abstract This paper describes work on automatically identifying categories of narrative clauses in personal stories written by ordinary people about their daily lives and experiences. We base our approach on Labov & Waletzkys theory of oral narrative which categorizes narrative clauses into subtypes, such as ORIENTATION, ACTION and EVALUATION. We describe an experiment where we annotate 50 personal narratives from weblogs and experiment with methods for achieving higher annotation reliability. We use the resulting annotated corpus to train a classifier to automatically identify narrative categories, achieving a best average F-score of .658, which rises to an F-score of .767 on the cases with the highest annotator agreement. We believe the identified narrative structure will enable new types of computational analysis of narrative discourse. "
W14-4324 "Abstract We present an evaluation of a spoken dialogue system that detects and adapts to user disengagement and uncertainty in real-time. We compare this version of our system to a version that adapts to only user disengagement, and to a version that ignores user disengagement and uncertainty entirely. We find a significant increase in task success when comparing both affectadaptive versions of our system to our nonadaptive baseline, but only for male users. "
W14-4325 "Abstract We examine the relationship between initiative behavior in negotiation dialogues and the goals and outcomes of the negotiation. We propose a novel annotation scheme for dialogue initiative, including four labels for initiative and response behavior in a dialogue turn. We annotate an existing human-human negotiation dataset, and use initiative-based features to try to predict both negotiation goal and outcome, comparing our results to prior work using other (non-initiative) features sets. Results show that combining initiative features with other features leads to improvements over either set and a majority class baseline. "
W14-4326 "sity {aasish, air}@cs.cmu.edu Abstract Many goal-oriented dialog agents are expected to identify slot-value pairs in a spoken query, then perform lookup in a knowledge base to complete the task. When the agent encounters unknown slotvalues, it may ask the user to repeat or reformulate the query. But a robust agent can proactively seek new knowledge from a user, to help reduce subsequent task failures. In this paper, we propose knowledge acquisition strategies for a dialog agent and show their effectiveness. The acquired knowledge can be shown to subsequently contribute to task completion. "
W14-4327 "Abstract The earliest work on automatic detection of implicit discourse relations relied on lexical features. More recently, researchers have demonstrated that syntactic features are superior to lexical features for the task. In this paper we re-examine the two classes of state of the art representations: syntactic production rules and word pair features. In particular, we focus on the need to reduce sparsity in instance representation, demonstrating that different representation choices even for the same class of features may exacerbate sparsity issues and reduce performance. We present results that clearly reveal that lexicalization of the syntactic features is necessary for good performance. We introduce a novel, less sparse, syntactic representation which leads to improvement in discourse relation recognition. Finally, we demonstrate that classifiers trained on different representations, especially lexical ones, behave rather differently and thus could likely be combined in future systems. "
W14-4328 "Abstract Research trends on SDS evaluation are recently focusing on objective assessment methods. Most existing methods, which derive quality for each systemuser-exchange, do not consider temporal dependencies on the quality of previous exchanges. In this work, we investigate an approach for determining Interaction Quality for human-machine dialogue based on methods modeling the sequential characteristics using HMM modeling. Our approach significantly outperforms conventional approaches by up to 4.5% relative improvement based on Unweighted Average Recall metrics. "
W14-4329 "lla, CA, USA {muralib, elkan}@cs.ucsd.edu Abstract We study the design of an information retrieval (IR) system that assists customer service agents while they interact with end-users. The type of IR needed is difficult because of the large lexical gap between problems as described by customers, and solutions. We describe an approach that bridges this lexical gap by learning semantic relatedness using tensor representations. Queries that are short and vague, which are common in practice, result in a large number of documents being retrieved, and a high cognitive load for customer service agents. We show how to reduce this burden by providing suggestions that are selected based on the learned measures of semantic relatedness. Experiments show that the approach offers substantial benefit compared to the use of standard lexical similarity. "
W14-4330 "rlin, OH, USA Abstract Segmentation of spoken discourse into distinct conversational activities has been applied to broadcast news, meetings, monologs, and two-party dialogs. This paper considers the aspectual properties of discourse segments, meaning how they transpire in time. Classifiers were constructed to distinguish between segment boundaries and non-boundaries, where the sizes of utterance spans to represent data instances were varied, and the locations of segment boundaries relative to these instances. Classifier performance was better for representations that included the end of one discourse segment combined with the beginning of the next. In addition, classification accuracy was better for segments in which speakers accomplish goals with distinctive start and end points. "
W14-4331 " Spoken Dialogue Systems ask for clarification when they think they have misunderstood users. Such requests may differ depending on the information the system believes it needs to clarify. However, when the error type or location is misidentified, clarification requests appear confusing or inappropriate. We describe a classifier that identifies inappropriate requests, trained on features extracted from user responses in laboratory studies. This classifier achieves 88.5% accuracy and .885 Fmeasure in detecting such requests. "
W14-4332 "Abstract When humans speak they often use grammatically incorrect sentences, which is a problem for grammar-based language processing methods, since they expect input that is valid for the grammar. We present two methods to transform spoken language into grammatically correct sentences. The first is an algorithm for automatic ellipsis detection, which finds ellipses in spoken sentences and searches in a combinatory categorial grammar for suitable words to fill the ellipses. The second method is an algorithm that computes the semantic similarity of two words using WordNet, which we use to find alternatives to words that are unknown to the grammar. In an evaluation, we show that the usage of these two methods leads to an increase of 38.64% more parseable sentences on a test set of spoken sentences that were collected during a human-robot interaction experiment. "
W14-4333 "Abstract We present a tool that allows human wizards to select appropriate response utterances for a given dialogue context from a set of utterances observed in a dialogue corpus. Such a tool can be used in Wizard-of-Oz studies and for collecting data which can be used for training and/or evaluating automatic dialogue models. We also propose to incorporate such automatic dialogue models back into the tool as an aid in selecting utterances from a large dialogue corpus. The tool allows a user to rank candidate utterances for selection according to these automatic models. "
W14-4334 ", Playa Vista, CA 90094 {morbini,devault,kgeorgila,artstein,traum,morency}@ict.usc.edu Abstract This demonstration highlights the dialogue processing in SimSensei Kiosk, a virtual human dialogue system that conducts interviews related to psychological distress conditions such as depression, anxiety, and post-traumatic stress disorder (PTSD). The dialogue processing in SimSensei Kiosk allows the system to conduct coherent spoken interviews of human users that are 15-25 minutes in length, and in which users feel comfortable talking and openly sharing information. We present the design of the individual dialogue components, and show examples of natural conversation flow between the system and users, including expressions of empathy, follow-up responses and continuation prompts, and turn-taking. "
W14-4335 "1 AT&T Labs Research 1 , AT&T 2 {johnston,jchen,ehlen,hjung,jlieske,aarthi, ethan,sveta,vasilieff,jgw}@research.att.com Abstract The Multimodal Virtual Assistant (MVA) is an application that enables users to plan an outing through an interactive multimodal dialog with a mobile device. MVA demonstrates how a cloud-based multimodal language processing infrastructure can support mobile multimodal interaction. This demonstration will highlight incremental recognition, multimodal speech and gesture input, contextually-aware language understanding, and the targeted clarification of potentially incorrect segments within user input. "
W14-4336 "for full list of affiliations Abstract We demonstrate a mobile application in English and Mandarin to test and evaluate components of the Parlance dialogue system for interactive search under real-world conditions. 1 Introduction With the advent of evaluations in the wild, emphasis is being put on converting research prototypes into mobile applications that can be used for evaluation and data collection by real users downloading the application from the market place. This is the motivation behind the work demonstrated here where we present a modular framework whereby research components from the Parlance project (Hastie et al., 2013) can be plugged in, tested and evaluated in a mobile environment. The goal of Parlance is to perform interactive search through speech in multiple languages. The domain for the demonstration system is interactive search for restaurants in Cambridge, UK for Mandarin and San Francisco, USA for English. The scenario is that Mandarin speaking tourists would be able to download the application and use it to learn about restaurants in English speaking towns and cities. "
W14-4337 "Abstract A spoken dialog system, while communicating with a user, must keep track of what the user wants from the system at each step. This process, termed dialog state tracking, is essential for a successful dialog system as it directly informs the systems actions. The first Dialog State Tracking Challenge allowed for evaluation of different dialog state tracking techniques, providing common testbeds and evaluation suites. This paper presents a second challenge, which continues this tradition and introduces some additional features  a new domain, changing user goals and a richer dialog state. The challenge received 31 entries from 9 research groups. The results suggest that while large improvements on a competitive baseline are possible, trackers are still prone to degradation in mismatched conditions. An investigation into ensemble learning demonstrates the most accurate tracking can be achieved by combining multiple trackers. "
W14-4338 "e, KAIST 2 LG Electronics Abstract For robust spoken dialog management, various dialog state tracking methods have been proposed. Although discriminative models are gaining popularity due to their superior performance, generative models based on the Partially Observable Markov Decision Process model still remain attractive since they provide an integrated framework for dialog state tracking and dialog policy optimization. Although a straightforward way to fit a generative model is to independently train the component probability models, we present a gradient descent algorithm that simultaneously train all the component models. We show that the resulting tracker performs competitively with other top-performing trackers that participated in DSTC2. "
W14-4339 "Abstract In spoken dialog systems, statistical state tracking aims to improve robustness to speech recognition errors by tracking a posterior distribution over hidden dialog states. This paper introduces two novel methods for this task. First, we explain how state tracking is structurally similar to web-style ranking, enabling mature, powerful ranking algorithms to be applied. Second, we show how to use multiple spoken language understanding engines (SLUs) in state tracking  multiple SLUs can expand the set of dialog states being tracked, and give more information about each, thereby increasing both recall and precision of state tracking. We evaluate on the second Dialog State Tracking Challenge; together these two techniques yield highest accuracy in 2 of 3 tasks, including the most difficult and general task. "
W14-4340 "ge, U.K. {mh521, brmt2, sjy}@eng.cam.ac.uk Abstract Recently discriminative methods for tracking the state of a spoken dialog have been shown to outperform traditional generative models. This paper presents a new wordbased tracking method which maps directly from the speech recognition results to the dialog state without using an explicit semantic decoder. The method is based on a recurrent neural network structure which is capable of generalising to unseen dialog state hypotheses, and which requires very little feature engineering. The method is evaluated on the second Dialog State Tracking Challenge (DSTC2) corpus and the results demonstrate consistently high performance across all of the metrics. "
W14-4341 "Abstract A primary motivation of the Dialog State Tracking Challenge (DSTC) is to allow for direct comparisons between alternative approaches to dialog state tracking. While results from DSTC 1 mention performance limitations, an examination of the errors made by dialog state trackers was not discussed in depth. For the new challenge, DSTC 2, this paper describes several techniques for examining the errors made by the dialog state trackers in order to refine our understanding of the limitations of various approaches to the tracking process. The results indicate that no one approach is universally superior, and that different approaches yield different error type distributions. Furthermore, the results show that a pairwise comparative analysis of tracker performance is a useful tool for identifying dialogs where differential behavior is observed. These dialogs can provide a data source for a more careful analysis of the source of errors. "
W14-4342 " During the recent Dialog State Tracking Challenge (DSTC), a fundamental question was raised: Would better performance in dialog state tracking translate to better performance of the optimized policy by reinforcement learning? Also, during the challenge system evaluation, another nontrivial question arose: Which evaluation metric and schedule would best predict improvement in overall dialog performance? This paper aims to answer these questions by applying an off-policy reinforcement learning method to the output of each challenge system. The results give a positive answer to the first question. Thus the effort to separately improve the performance of dialog state tracking as carried out in the DSTC may be justified. The answer to the second question also draws several insightful conclusions on the characteristics of different evaluation metrics and schedules. 1  "
W14-4343 "i, China {accreator, chenlusz, paul2204, kai.yu}@sjtu.edu.cn Abstract Dialog state tracking challenge provides a common testbed for state tracking algorithms. This paper describes the SJTU system submitted to the second Dialogue State Tracking Challenge in detail. In the system, a statistical semantic parser is used to generate refined semantic hypotheses. A large number of features are then derived based on the semantic hypotheses and the dialogue log information. The final tracker is a combination of a rulebased model, a maximum entropy and a deep neural network model. The SJTU system significantly outperformed all the baselines and showed competitive performance in DSTC 2. "
W14-4344 "ng, China, 100190 {renhang, xuweiqun, yanyonghong}@hccl.ioa.ac.cn Abstract Discriminative dialog state tracking has become a hot topic in dialog research community recently. Compared to generative approach, it has the advantage of being able to handle arbitrary dependent features, which is very appealing. In this paper, we present our approach to the DSTC2 challenge. We propose to use discriminative Markovian models as a natural enhancement to the stationary discriminative models. The Markovian structure allows the incorporation of transitional features, which can lead to more efficiency and flexibility in tracking user goal changes. Results on the DSTC2 dataset show considerable improvements over the baseline, and the effects of the Markovian dependency is tested empirically. "
W14-4401 " In the highly competitive weather industry, demand for timely, accurate and personalized weather reports is always on the rise. In this paper we present a case study where Arria NLG and the UK national weather agency, the Met Office came together to test the hypothesis that NLG can meet the quality and quantity demands of a real-world use case. 1 Int  "
W14-4402 " PatientNarr summarizes information taken from textual discharge notes written by physicians, and structured nursing documentation. It builds a graph that highlights the relationships between the two types of documentation; and extracts information from the graph for content planning. SimpleNLG is used for surface realization. 1 Introduction Every year, 7.9% of the US population is hospitalized (CDC, 2011). Patients need to understand what happened to them in the hospital, and what they should do after discharge. PatientNarr will ultimately be able to generate concise, laylanguage summaries of hospital stays. We hypothesize that such summaries will help patients take care of themselves after they are discharged, and supplement current approaches to patient education, which is not always effective (Olson and Windish, 2010). PatientNarr needs to summarize documentation that is currently segregated by profession; as a minimum, as physician discharge notes and as nursing plans-of-care. We contend that both sources are necessary to provide the patient with full understanding, also because much of the direct care provided by nurses will need to be continued following discharge (Cain et al., 2012). In our case, PatientNarr summarizes data that is heterogeneous (textual for physician discharge notes, structured for nursing documentation). This paper describes the steps we have undetaken so far: (a) To demonstrate that physician and nurse documentations diverge, we map both to a graph, and study the relationships therein. This graph supports content planning. (b) We have developed the pipeline that extracts the information to be communicated, and renders it in English via SimpleNLG (Gatt and Reiter, 2009). Related work. NLG and Summarization in the biomedical domain have been pursued for a few years (Di Eugenio and Green, 2010), but most work addresses health care personnel: to navigate cancer patients medical histories (Hallett, 2008; Scott et al., 2013); to generate textual summaries describing a hospitalized infant for nurses (Portet et al., 2009); to generates reports of care for hand-off between emergency workers (Schneider et al., 2013). Most applications of NLG that target patients focus on behavioral changes (Reiter et al., 2003), or patient counseling (Green et al., 2011). Only few NLG systems attempt at generating personalized medical information from medical records or data (Williams et al., 2007; Mahamood and Reiter, 2011). 2 A motivating example So far, we have gained access to 28 de-identified discharge notes of cardiology patients from the University of Illinois Hospital and Health Science System (UIHHSS). Figure 1 shows about 20% of the physician discharge notes for Patient 9. It is difficult to understand, not only because of jargon, but also because of ignorance of relevant domain relationships. Importantly, these notes do not talk about issues that are potentially important for the patient, like his state of mind, which are more often addressed by nurses. In our case, the nursing documentation is not textual, but entered via the HANDS tool, and stored in a relational database (Keenan et al., 2002). A tiny portion of the initial plan-of-care (POC) for Patient 9 is shown in Figure 2 (this nursing data is reconstructed, see Section 3). One POC is documented at every formal handoff (admission, shift change, or discharge). HANDS employs the NANDA-I taxon6 Patient was admitted to Cardiology for new onset a fib in RVR. Was given an additional dose of diltazem 30mg po when first seen. Patient was started on a heparin drip for possible TEE and cardioversion. Overnight his HR was in the 100-110s; however did increase to 160s for which patient was given 120mg er of diltazem. HR improved; however, in the morning while awake and moving around, HR did increase to the 130-140s. Patient was given another dose of IV dilt 20mg. [...] Upon discharge was given two prescriptions for BP, HCTZ and losartan given LVH seen on echo. Patient was counseled on the risks of stroke and different options for anticoagulation. [...] Figure 1: Excerpt from physician discharge notes (Patient 9) omy of nursing diagnoses, represented by squares in Figure 2; the Nursing Outcomes Classification (NOC)  circles; and the Nursing Interventions Classification (NIC)  triangles (NNN, 2014). In Figure 2, Acute Pain is a diagnosis, and Anxiety Level and Pain Level (some of) its associated outcomes. Anxiety Reduction is an intervention associated with Anxiety Level; Pain Management and Analgesic Administration are interventions associated with Pain Level. A scale from 1 to 5 indicates the initial value associated with an outcome (i.e., the state the patient was in when s/he was admitted), the expected rating, and the actual rating at discharge. In Figure 2, the current level for Pain Level and Anxiety Level is 2 each, with an expected level of 5 at discharge, i.e., no pain/anxiety. Figure 2: Excerpt from nursing documentation Figures 1 and 2 suggest that physician and nursing documentations provide different perspectives on patients: e.g., Anxiety is not even mentioned in the discharge notes. One of the authors (a nursing student) wrote summaries for five of the 28 discharge summaries and their corresponding HANDS POCs  Figure 3 shows the summary for Patient 9. This initial round of human authoring was meant to provide some preliminary guidelines to generate automatic summaries. Please see Section 5 for our plans on obtaining a much larger quantity of more informed human-authored summaries. 3 Extracting relationships between physician notes and nursing data To extract and relate information from our two sources, we rely on UMLS, MedLEE and HANDS. UMLS, the Unified Medical Language System (NLM, 2009), includes 2.6 million concepts (identified by Concept Unique Identifiers or CUIs) organized in a network. Importantly, many different medical and nursing terminologies have been incorporated into UMLS, including those used by HANDS (NANDA-I, NIC and NOC). UMLS provides mapping between their concepts and CUIs, via 8.6 million concept names and relationships between terminologies. Some relationships are of a hierarchical nature, where one concept is narrower than the other (e.g., Chest X-ray and Diagnostic radiologic examination). MedLEE is a medical information extraction system (Friedman et al., 2004). In its semistructured output, recognized entities are mapped to the corresponding CUI in UMLS. HANDS has not been adopted at UIHHSS yet. Hence, we reconstructed HANDS POCs for those 28 patients on the basis of 40,661 cases collected at four hospitals where HANDS is in use. For each of the 28 patients, the same nursing student who authored the five summaries, selected similar cases, and used them to produce high-quality records consistent with actual nursing practice. To relate physician and nursing documentations, we seed a graph with two sets of CUIs: those returned by MedLEE as a result of processing the physician discharge notes; and the CUIs corresponding to all the NANDA-I, NIC and NOC terms from the HANDS POCs. We then grow the graph by querying UMLS for the set of concepts related to each of the concepts in our set; the concepts that were not already part of the graph are then used to begin a new round of growth (we stop at round 2, to keep the time used by UMLS to answer, reasonable). From this graph, we keep the concepts that either belong to one of the 7 You were admitted with new onset of atrial fibrillation. You reported feeling weakness, chest pressure and increased shortness of breath. You reported acute pain and you were anxious. During your hospitalization you were treated with analgesics for your pain and pain management was performed by the nursing team. Your shortness of breath improved. Your decreased cardiac output was treated with medication administration and knowledge about cardiac precautions, hypertension management, your treatment regimen and the prescribed medication were taught to you by the nurses. A Transophageal Echocardiography was performed. You met the expected outcomes for your condition and you were discharged under the condition improved for your home. You have an appointment scheduled at Union Medical Center on [DATE] with Physician [DR.]. The list of your medications is attached to this discharge. Figure 3: Human-authored summary (Patient 9) source lists, or that are required to form a connection between a doctor-originated concept and a nurse-originated concept that would otherwise remain unconnected. All other concepts are removed. The result is a graph with several separate connected components, which correspond to clusters of related concepts occurring in the discharge notes or in the plans of care, or forming connections between the two sources. We count distances in terms of relationships traversed, starting from the nursing concepts since they are fewer, and since path traversal is reversible. 1 Concepts can overlap; or be directly connected (distance one); or be directly connected through an intermediate concept (distance two). We do not consider distances beyond two. Table 1 shows results for our specific example, Patient 9, and average results across our 28 test cases. As we can see, there are very few concepts in common, or even at distance 1. Our results provide quantitative evidence for the hypothesis that physicians and nurses talk differently, not just as far as terminology is concerned, but as regards aspects of patient care. This provides strong evidence for our hypothesis that a hospitalization summary should include both perspectives. "
W14-4403 "and Amy Loutfi Center for Applied Autonomous Sensor Systems   Orebro University   Orebro, Sweden {hadi.banaee, amy.loutfi}@oru.se Abstract This position paper introduces the utility of the conceptual spaces theory to conceptualise the acquired knowledge in data-totext systems. A use case of the proposed method is presented for text generation systems dealing with sensor data. Modelling information in a conceptual space exploits a spatial representation of domain knowledge in order to perceive unexpected observations. This ongoing work aims to apply conceptual spaces in NLG for grounding numeric information into the symbolic representation and confronting the important step of acquiring adequate knowledge in data-to-text systems. "
W14-4404 " We present an approach to text simplification based on synchronous dependency grammars. Our main contributions in this work are (a) a study of how automatically derived lexical simplification rules can be generalised to enable their application in new contexts without introducing errors, and (b) an evaluation of our hybrid system that combines a large set of automatically acquired rules with a small set of hand-crafted rules for common syntactic simplification. Our evaluation shows significant improvements over the state of the art, with scores comparable to human simplifications. 1 Introdu  "
W14-4405 "Abstract With the rise of the Semantic Web more and more data become available encoded using the Semantic Web standard RDF. RDF is faced towards machines: designed to be easily processable by machines it is difficult to be understood by casual users. Transforming RDF data into human-comprehensible text would facilitate non-experts to assess this information. In this paper we present a languageindependent method for extracting RDF verbalization templates from a parallel corpus of text and data. Our method is based on distant-supervised simultaneous multi-relation learning and frequent maximal subgraph pattern mining. We demonstrate the feasibility of our method on a parallel corpus of Wikipedia articles and DBpedia data for English and German. "
W14-4406 "Abstract This paper presents an encoding of Generation-TAG (G-TAG) within Abstract Categorial Grammars (ACG). We show how the key notions of G-TAG have a natural interpretation in ACG, allowing us to use its reversibility property for text generation. It also offers solutions to several limitations of G-TAG. "
W14-4407 "ive Meeting Summarization: Leveraging Summary and Source Text Relationships Tatsuro Oya , Yashar Mehdad , Giuseppe Carenini , Raymond Ng  ond Ng Department of Computer Science University of British Columbia, Vancouver, Canada {toya, mehdad, carenini, rng}@cs.ubc.ca   Summary and Source Text Relationships Tatsuro Oya, Yashar Mehdad, Giuseppe Carenini, Raymond Ng Department of Computer cience University of British Columbia, Vancouve r, Canada {toya, mehdad, carenini, rng}@cs.ubc.ca Abstract In this paper, we present an automatic abstractive summarization system of meeting conversations. Our system extends a novel multi-sentence fusion algorithm in order to generate abstract templates. It also leverages the relationship between summaries and their source meeting transcripts to select the best templates for generating abstractive summaries of meetings. Our manual and automatic evaluation results demonstrate the success of our system in achieving higher scores both in readability and informativeness. "
W14-4408 "Abstract We present a hybrid method to generate summaries of product and services reviews by combining natural language generation and salient sentence selection techniques. Our system, STARLET-H, receives as input textual reviews with associated rated topics, and produces as output a natural language document summarizing the opinions expressed in the reviews. STARLET-H operates as a hybrid abstractive/extractive summarizer: using extractive summarization techniques, it selects salient quotes from the input reviews and embeds them into an automatically generated abstractive summary to provide evidence for, exemplify or justify positive or negative opinions. We demonstrate that, compared to extractive methods, summaries generated with abstractive and hybrid summarization approaches are more readable and compact. "
W14-4409 " Deciding on the complexity of a generated text in NLG systems is a contentious task. Some systems propose the generation of simple text for low-skilled readers; some choose what they anticipate to be a good measure of complexity by balancing sentence length and number of sentences (using scales such as the D-level sentence complexity) for the text; while others target high-skilled readers. In this work, we discuss an approach that aims to leverage the experience of the reader when reading generated text by matching the syntactic complexity of the generated text to the reading level of the surrounding text. We propose an approach for sentence aggregation and lexical choice that allows generated summaries of line graphs in multimodal articles available online to match the reading level of the text of the article in which the graphs appear. The technique is developed in the context of the SIGHT (Summarizing Information Graphics Textually) system. This paper tackles the micro planning phase of sentence generation discussing additionally the steps of lexical choice, and pronominalization. 1 Introduct  "
W14-4410 "Abstract We use efficient screening experiments to investigate and improve topic analysis based multi-document extractive summarization. In our summarization process, topic analysis determines the weighted topic content vectors that characterize the corpora, and then Jensen-Shannon divergence extracts sentences that best match the weighted content vectors to assemble the summaries. We use screening experiments to investigate several control parameters in this process, gaining better understanding of and improving the topic analysis based summarization process. 1 Introduction We use efficient experimental design to investigate and improve topic analysis based multiple document extractive summarization. Our process proceeds in two steps: Latent Dirichlet Analysis (LDA) topic analysis determines the topics that characterize the multi-document corpus, and Jensen-Shannon divergence selects sentences from the corpus. This process offers many potential control settings for understanding and improving the summarization process. Figure 1 shows topic analysis with corpus input, control settings, and product outputs of topics and probability estimates of topic compositions and document mixtures. There are controls for document preparation (headlines) and analysis (number of topics, initial  and , number of iterations, and whether to optimize  and  in process). Figure 2 shows summarization with corpus and topic inputs, control settings, and the text summarization product. There are controls for extraction of sentences (Extract  and JSD Divisor) and for composing the summary (Order policy). Topic analysis has become a popular choice for text summarization as seen in Text Analysis ConCorpus Analyze Topics # Iterations! Optimize , # Topics! Initial , Topics Headlines Figure 1: Topic Analysis Corpus Analyze Topics Order policy Extract ! JSD divisor Summary Topics Figure 2: Text Summarization ferences (TAC, 2010; TAC, 2011) with individual team reports (Delort and Alfonseca, 2011; Lui et al., 2011; Mason and Charniak, 2011). Nenkova and McKeown (2012; 2011) included topic analysis among standard methods in their surveys of text summarization methodologies. Haghighi and Vanderwende (2009) explored extensions of LDA topic analysis for use in multiple document summarization tasks. Yet there are many control settings that can affect summarization that have not been explicitly studied or documented, and that are important for reproducing research results. In this text summarization pilot study, we experiment with several control settings. As in Mason and Charniak (2011) we do a general rather than guided summarization. Our primary contribution is illustrating the use of efficient experimental design on control settings to help understand and improve the text summarization process. We enjoy some success in this endeavor even as we are surprised by some of our results. 74 2 Technical Background "
W14-4411 "<NoAbstract>"
W14-4412 "Abstract This paper describes the ongoing implementation and the current coverage of SimpleNLG-BP, an adaptation of SimpleNLG-EnFr (Vaudry and Lapalme, 2013) for Brazilian Portuguese. 1 Introduction Realisation is the last step in natural language generation (NLG) systems, so the goal of a realisation engine is to output text. SimpleNLG is a Java library that employs morphological, syntactic and orthographical operations on non-linguistic input to output well-formed sentences in English. SimpleNLG-EnFr (Vaudry and Lapalme, 2013) is an adaptation of SimpleNLG for French. This paper describes the current state of SimpleNLG-BP 1 , an adaptation of SimpleNLG-EnFr for realisation in Brazilian Portuguese. "
W14-4413 " This demo presents a Natural Language Generation (NLG) system that generates summaries of informational graphics, specifically simple line graphs, present in popular media. The system is intended to capture the high-level knowledge conveyed by the graphic and its outstanding visual features. It comprises a content selection phase that extracts the most important content of the graphic, an organization phase, which orders the propositions in a coherent manner, and a realization phase that uses the text surrounding the article to make decisions on the choice of lexical items and amount of aggregation applied to the propositions to generate the summary of the graphic. 1 Introdu  "
W14-4414 ", Pittsburgh, PA 15213-3891, USA {yvchen, air}@cs.cmu.edu Abstract This paper presents the design and implementation details of an email synthesizer using two-stage stochastic natural language generation, where the first stage structures the emails according to sender style and topic structure, and the second stage synthesizes text content based on the particulars of an email structure element and the goals of a given communication for surface realization. The synthesized emails reflect sender style and the intent of communication, which can be further used as synthetic evidence for developing other applications. "
W14-4415 " In this paper, we describe a dialogue system framework for a companionable robot, which aims to guide patients towards health behavior changes via natural language analysis and generation. The framework involves three broad stages, rapport building and health topic identification, assess patients opinion of change, and designing plan and closing session. The framework uses concepts from psychology, computational linguistics, and machine learning and builds on them. One of the goals of the framework is to ensure that the Companionbot builds and maintains rapport with patients. 1 Introdu  "
W14-4416 "Abstract State-of-the-art statistical sentence generators deal with isomorphic structures only. Therefore, given that semantic and syntactic structures tend to differ in their topology and number of nodes, i.e., are not isomorphic, statistical generation saw so far itself confined to shallow, syntactic generation. In this paper, we present a series of fine-grained classifiers that are essential for data-driven deep sentence generation in that they handle the problem of the projection of non-isomorphic structures. "
W14-4417 " If an NLG system needs to be put in place as soon as possible it is not always possible to know in advance who the users of a system are or what kind of information will interest them. This paper describes the development of a system and contextualized text for unknown users. We describe the development, design and initial findings with a system for unknown users that allows the users to design their own contextualised text. 1 Introdu  "
W14-4418 "Abstract In this paper we describe a method for generating a procedural text given its flow graph representation. Our main idea is to automatically collect sentence skeletons from real texts by replacing the important word sequences with their type labels to form a skeleton pool. The experimental results showed that our method is feasible and has a potential to generate natural sentences. "
W14-4419 " The Arria NLG Engine has been extended to generate annotated graphs: data graphs that contain computer-generated textual annotations to explain phenomena in those graphs. These graphs are generated alongside text-only data summaries. 1 Introduction A  Arria NLG Engine has been extended to generate annotated graphs: data graphs that contain computer-generated textual annotations to explain phenomena in those graphs. These graphs are generated alongside text-only data summaries. 1 Introduction Arria NLG 1 develops NLG solutions, primarily in the data-to-text area. These solutions are NLG systems, which generate textual summaries of large numeric data sets. Arrias core product is the Arria NLG Engine, 2 which is configured and customised for the needs of different clients. Recently Arria has extended this core engine so that it can automatically produce annotated graphs, that is, data graphs that have textual annotations explaining phenomena in those graphs (see example in Figure 1). This was developed after listening to one of our customers, whose staff manually created annotated graphs and found this process to be very time-consuming. The annotated graph generation process is integrated into the NLG pipeline, and is carried out in conjunction with the generation of a textual summary of a data set. In this short paper we summarise the relevant research background, and briefly describe what we have achieved in this area. "
W14-4420 "Abstract Valence shifting is the task of rewriting a text towards more/less positively or negatively slanted versions. This paper presents a rule-based approach to producing Turkish sentences with varying sentiment. The approach utilizes semantic relations in the Turkish and English WordNets to determine word polarities and involves the use of lexical substitution and adverbial rules to alter the sentiment of a text in the intended direction. In a user study, the effectiveness of the generation approach is evaluated on real product reviews. "
W14-4421 " This paper explores Natural Language Generation techniques for online river information tailoring. To solve the problem of unknown users, we propose latent models, which relate typical visitors to river web pages, river data types, and river related activities. A hierarchy is used to integrate domain knowledge and latent user knowledge, and serves as the search space for content selection, which triggers user-oriented selection rules when they visit a page. Initial feedback received from user groups indicates that the latent models deserve further research efforts. 1 Int  "
W14-4422 "dinburgh {dg106, h.hastie, o.lemon}@hw.ac.uk Abstract We present FeedbackGen, a system that uses a multi-adaptive approach to Natural Language Generation. With the term multi-adaptive, we refer to a system that is able to adapt its content to different user groups simultaneously, in our case adapting to both lecturers and students. We present a novel approach to student feedback generation, which simultaneously takes into account the preferences of lecturers and students when determining the content to be conveyed in a feedback summary. In this framework, we utilise knowledge derived from ratings on feedback summaries by extracting the most relevant features using Principal Component Regression (PCR) analysis. We then model a reward function that is used for training a Reinforcement Learning agent. Our results with students suggest that, from the students perspective, such an approach can generate more preferable summaries than a purely lecturer-adapted approach. "
W14-4423 "Abstract The TBI-Doc prototype demonstrates the feasibility of automatically producing draft case reports for a new brain imaging technology, High Definition Fiber Tracking (HDFT). Here we describe the ontology for the HDFT domain, the system architecture and our goals for future research and development. "
W14-4424 "Abstract We present a novel algorithm for inducing Combinatory Categorial Grammars from dependency treebanks, along with initial experiments showing that it can be used to achieve competitive realization results using an enhanced version of the surface realization shared task data. "
W14-4701 " The shared task of the 4th Workshop on Cognitive Aspects of the Lexicon (CogALexIV) was devoted to a subtask of the lexical access problem, namely multi-stimulus association. In this task, participants were supposed to determine automatically an expected response based on a number of received stimulus words. We describe here the task definition, the theoretical background, the training and test data sets, and the evaluation procedure used for ranking the participating systems. We also summarize the approaches used and present the results of the evaluation. In conclusion, the outcome of the competition are a number of systems which provide very good solutions to the problem. 1 Intro  "
W14-4702 "Abstract This paper describes the system submitted by the IIIT-H team for the CogALex-2014 shared task on multiword association. The task involves generating a ranked list of responses to a set of stimulus words. The two-stage approach combines the strength of neural network based word embeddings and frequency based association measures. The system achieves an accuracy of 34.9% over the test set. "
W14-4703 "Abstract This paper explores advanced learning mechanisms  neural networks trained by the Word2Vec method  for predicting word associations. We discuss how the approach can be built into dictionary interfaces to help tip-of-the-tongue searches. We also describe our contribution to the CogALex 2014 shared task. We argue that the reverse response-stimulus word associations chosen for the shared task are only mildly related to the motivation idea of the lexical access support system. The methods employed in our contribution are briefly introduced. We present results of experiments with various parameter settings and show what improvement can be expected if more than one answer is allowed. The paper concludes with a proposal for a new collective effort to assemble real tip-of-the-tongue situation records for future, more-realistic evaluations. "
W14-4704 "Abstract In our participation on the task we wanted to test three different kinds of relatedness algorithms: one based on embeddings induced from corpora, another based on random walks on WordNet and a last one based on random walks based on Wikipedia. All three of them perform similarly in noun relatedness datasets like WordSim353, close to the highest reported values. Although the task definition gave examples of nouns, the train and test data were based on the Edinburgh Association Thesaurus, and around 50% of the target words were not nouns. The corpus-based algorithm performed much better than the other methods in the training dataset, and was thus submitted for the test. "
W14-4705 " We present an automated system that computes multi-cue associations and generates associated-word suggestions, using lexical co-occurrence data from a large corpus of English texts. The system performs expansion of cue words to their inflectional variants, retrieves candidate words from corpus data, finds maximal associations between candidates and cues, computes an aggregate score for each candidate, and outputs an n-best list of candidates. We present experiments using several measures of statistical association, two methods of score aggregation, ablation of resources and applying additional filters on retrieved candidates. The system achieves 18.6% precision on the COGALEX-4 shared task data. Results with additional evaluation methods are presented. We also describe an annotation experiment which suggests that the shared task may underestimate the appropriateness of candidate words produced by the corpus-based system.  "
W14-4706 "Natural Language Processing Group, University of Leipzig, Germany teckart, dgoldhahn, quasthoff @informatik.uni-leipzig.de Abstract One way to analyse word relations is to examine their co-occurrence in the same context. This allows for the identification of potential semantic or lexical relationships between words. As previous studies showed word co-occurrences often reflect human stimuli-response pairs. In this paper significant sentence co-occurrences on word level were used to identify potential responses for word stimuli based on three automatically generated text corpora of the Leipzig Corpora Collection. "
W14-4707 "Abstract This paper describes NaDiR (Naive DIstributional Response generation), a corpus-based system that, from a set of word stimuli as an input, generates a response word relying on association strength and distributional similarity. NaDiR participated in the CogALex 2014 shared task on multiword associations (restricted systems track), operationalizing the task as a ranking problem: candidate words from a large vocabulary are ranked by their average association or similarity to a given set of stimuli. We also report on a number of experiments conducted on the shared task data, comparing first-order models (based on co-occurrence and statistical association) to second-order models (based on distributional similarity). "
W14-4708 " The paper explains the procedure to obtain word associations starting from a graph that has not been specifically built for that purpose. Our goal is being able to simulate human word associations by using the simplest possible methods, including the basic tools of a co-occurrence network from a non-annotated corpus, and a very simple search algorithm based on neighborhood. The method has been tested in the Cogalex shared task, revealing the difficulty of achieving word associations without semantic annotation. 1 I  "
W14-4709 "Abstract This paper presents our system to address the CogALex-IV 2014 shared task of identifying a single word most semantically related to a group of 5 words (queries). Our system uses an implementation of a neural language model and identifies the answer word by finding the most semantically similar word representation to the sum of the query representations. It is a fully unsupervised system which learns on around 20% of the UkWaC corpus. It correctly identifies 85 exact correct targets out of 2,000 queries, 285 approximate targets in lists of 5 suggestions. "
W14-4710 "Abstract This paper presents our relations-oriented approach to the shared task on lexical access in language production, as well as the results we obtained. We relied mainly on the semantic and lexical relations between words as they are recorded in the Princeton WordNet , although also considering co-occurrence in the Google n-gram corpus. After the end of the shared task we continued working on the system and the further adjustments (involving part of speech information and position of the candidate in the synset) and those results are presented as well. "
W14-4711 "Abstract Multilinguality is a key feature of todays Web, and it is this feature that we leverage and exploit in our research work at the Sapienza University of Romes Linguistic Computing Laboratory, which I am going to overview and showcase in this talk. I will start by presenting BabelNet 2.5 (Navigli and Ponzetto, 2012), available at http://babelnet.org , a very large multilingual encyclopedic dictionary and semantic network, which covers 50 languages and provides both lexicographic and encyclopedic knowledge for all the open-class parts of speech, thanks to the seamless integration of WordNet, Wikipedia, Wiktionary, OmegaWiki, Wikidata and the Open Multilingual WordNet. In order to construct the BabelNet network, we extract at different stages: from WordNet, all available word senses (as concepts) and all the lexical and semantic pointers between synsets (as relations); from Wikipedia, all the Wikipages (i.e., Wikipages, as concepts) and semantically unspecified relations from their hyperlinks. WordNet and Wikipedia overlap both in terms of concepts and relations: this overlap makes the merging between the two resources possible, enabling the creation of a unified knowledge resource. In order to enable multilinguality, we collect the lexical realizations of the available concepts in different languages. Finally, we connect the multilingual Babel synsets by establishing semantic relations between them. Next, I will present Babelfy (Moro et al., 2014), available at http://babelfy.org , a unified approach that leverages BabelNet to perform Word Sense Disambiguation (WSD) and Entity Linking in arbitrary languages, with performance on both tasks on a par with, or surpassing, those of task-specific state-of-the-art supervised systems. Babelfy works in three steps: first, given a lexicalized semantic network, we associate with each vertex, i.e., either concept or named entity, a semantic signature, that is, a set of related vertices. This is a preliminary step which needs to be performed only once, independently of the input text. Second, given a text, we extract all the linkable fragments from this text and, for each of them, list the possible meanings according to the semantic network. Third, we create a graph-based semantic interpretation of the whole text by linking the candidate meanings of the extracted fragments using the previously-computed semantic signatures. We then extract a dense subgraph of this representation and select the best candidate meaning for each fragment. Our experiments show state-of-the-art performances on both WSD and EL on 6 different datasets, including a multilingual setting. In the third part of the talk I will present two novel approaches to large-scale knowledge acquisition and validation developed in my lab. I will first introduce video games with a purpose (Vannella et al., 2014), a novel, powerful paradigm for the large scale acquisition and validation of knowledge and data http://knowledgeforge.org ). We demonstrate that converting games with a purpose into more traditional video games provides a fun component that motivates players to annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing. Moreover, we show that video games with a purpose produce higher-quality annotations than crowdsourcing. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 75 Then I will introduce the Wikipedia Bitaxonomy (Flati et al., 2014, WiBi), available at http://wibitaxonomy.org and now integrated into BabelNet. WiBi is the largest and most accurate currently available taxonomy of Wikipedia pages and taxonomy of categories, aligned to each other. WiBi is created in three steps: we first create a taxonomy for the Wikipedia pages by parsing textual definitions, extracting the hypernym(s) and disambiguating them according to the page inventory; next, we leverage the hypernyms in the page taxonomy, together with their links to the corresponding categories, so as to induce a taxonomy over Wikipedia categories while at the same time improving the page taxonomy in an iterative way; finally we employ structural heuristics to overcome inherent problems affecting categories. The output of our threephase approach is a bitaxonomy of millions of pages and hundreds of thousands of categories for the English Wikipedia. "
W14-4712 "Abstract Two types of semantic similarity are usually distinguished: attributional and relational similarities. These similarities measure the degree between words or word pairs. Attributional similarities are bidrectional, while relational similarities are one-directional. It is possible to compute such similarities based on the occurrences of words in actual sentences. Inside sentences, syntagmatic associations and paradigmatic associations can be used to characterize the relations between words or word pairs. In this paper, we propose a vector space model built from syntagmatic and paradigmatic associations to measure relational similarity between word pairs from the sentences contained in a small corpus. We conduct two experiments with different datasets: SemEval-2012 task 2, and 400 word analogy quizzes. The experimental results show that our proposed method is effective when using a small corpus. "
W14-4713 "{ying.zhang, mathieu.mangeot, valerie.bellynck, christian.boitet}@imag.fr Abstract Between simple electronic dictionaries such as the TLFi (computerized French Language Treasure) 1 and lexical networks like WordNet 2 (Diller et al., 1990; Vossen, 1998), the lexical databases are growing at high speed. Our work is about the addition of rich links to lexical databases, in the context of the parallel development of lexical networks. Current research on management tools for lexical databases is strongly influenced by the field of massive data (\"big data\") and by the Web of data (\"linked data\"). In lexical networks, one can build and use arbitrary links, but possible queries cannot model all the usual interactions with lexicographers-developers and users, that are needed, and derive from the paper world. Our work aims to find a solution that allows for the main advantages of lexical networks, while providing the equivalent of paper dictionaries by doing the lexicographic work in lexical DBs. "
W14-4714 " One of the main approaches to extract multi-word units is the frequency threshold approach, but the way this approach considers dispersion data still leaves a lot to be desired. This study adopts Griess (2008) dispersion measure to extract trigrams from a Chinese corpus, and the results are compared with those of the frequency threshold approach. It is found that the overlap between the two approaches is not very large. This demonstrates the necessity of taking dispersion data more seriously and the dynamic nature of lexical representations. Moreover, the trigrams extracted in the present study can be used in a wide range of language resources in Chinese.  "
W14-4715 " Mental lexicon plays a central role in human language competence and inspires the creation of new lexical resources. The traditional linguistic experiment method which is used to explore mental lexicon has some disadvantages. Crowdsourcing has become a promising method to conduct linguistic experiments which enables us to explore mental lexicon in an efficient and economic way. We focus on the feasibility and quality control issues of conducting Chinese linguistic experiments to collect Chinese word segmentation and semantic transparency data on the international crowdsourcing platforms Amazon Mechanical Turk and Crowdflower. Through this work, a framework for crowdsourcing linguistic experiments is proposed. 1 Int  "
W14-4716 " While humans are capable of building connections between words and sensorial modalities by using commonsense knowledge, it is not straightforward for machines to interpret sensorial information. To this end, a lexicon associating words with human senses, namely sight, hearing, taste, smell and touch, would be crucial. Nonetheless, to the best of our knowledge, there is no systematic attempt in the literature to build such a resource. In this paper, we propose a computational method based on bootstrapping and corpus statistics to automatically associate English words with senses. To evaluate the quality of the resulting lexicon, we create a gold standard via crowdsourcing and show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task, both at word and sentence level. The results confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications. 1 Int  "
W14-4717 "Abstract Chinese noun classifiers are an indispensible part of the Chinese language, but are difficult for non-native speakers to use correctly. Chinese language teachers often face challenges in finding an effective way to teach classifiers, as the rules for defining which nouns can be associated with which classifiers are not straightforward. Many theoretical studies have explored the nature of Chinese classifiers, but few studies take an empirical approach to the investigation of effective teaching and learning methods of classifiers. Learners often find that existing dictionaries either do not have classifiers as lexical entries, or give very brief explanations that are hardly helpful. This paper presents the progress of an ongoing project on the construction of an e-dictionary of Chinese classifiers. The objective of the project is to provide a platform for Chinese language learners to explore and learn classifier uses in a bottom-up fashion. The current work is on the design of an e-learning tool database and its connection to the e-dictionary database. Descriptions of the design and the functions of the e-learning tool are provided in the paper. "
W14-4718 "Abstract The following paper presents a further extension of the Suggested Upper Merged Ontology (SUMO), i. e. the development of default physical measurements for most of its classes (Artifacts, Devices, Objects) and respective children. The extension represents an arbitrary, computable and reproducible approximation of defaults for upper and middle-level concepts. The paper illustrates advantages of such extension, challenges encountered during the compilation, related work and future research. "
W14-4719 "<NoAbstract>"
W14-4720 "Abstract The French Lexical Network (fr-LN) is a global model of the French lexicon presently under construction. The fr-LN accounts for lexical knowledge as a lexical network structured by paradigmatic and syntagmatic relations holding between lexical units. This paper describes how morphological knowledge is presently being introduced into the fr-LN through the implementation and lexicographic exploitation of a dynamic morphological model. Section 1 presents theoretical and practical justifications for the approach which we believe allows for a cognitively sound description of morphological data within semantically-oriented lexical databases. Section 2 gives an overview of the structure of the dynamic morphological model, which is constructed through two complementary processes: a Morphological Processsection 3and a Lexicographic Processsection 4. "
W14-4721 "Abstract Recent work suggests that concreteness and imageability play an important role in the meanings of figurative expressions. We investigate this idea in several ways. First, we try to define more precisely the context within which a figurative expression may occur, by parsing a corpus annotated for metaphor. Next, we add both concreteness and imageability as features to the parsed metaphor corpus, by marking up words in this corpus using a psycholinguistic database of scores for concreteness and imageability. Finally, we carry out detailed statistical analyses of the augmented version of the original metaphor corpus, cross-matching the features of concreteness and imageability with others in the corpus such as parts of speech and dependency relations, in order to investigate in detail the use of such features in predicting whether a given expression is metaphorical or not. "
W14-4722 " Regarding the construction of an ontology of Japanese lexical properties (JLP-O) as fundamental in terms of establishing a conceptual framework to guide and facilitate the construction of a large-scale lexical resource (LR) database of the Japanese lexicon, this paper primarily focuses on two major concerns for the construction of the JLP-O. The first is to map out and appropriately structure the numerous lexical and psycholinguistic properties, or variables, associated with the Japanese lexicon. The second concern is to specify an appropriate range of lexical entries classes within the JLP-O. Both concerns have far-reaching implications for effectively capturing the rich patterns of interconnections among lexical entries and lexical properties and thus for realizing a multifunctional LR. After discussing the solutions integrated into the current Resource Description Framework (RDF) representation of the JLP-O, the paper also briefly describes the extraction of a corpus-based lexicon from the recently released Balanced Corpus of Contemporary Written Japanese (BCCWJ; Maekawa et al., 2013), an authoritative sampling of the contemporary Japanese lexicon. Categorized according to the JLP-Os range of lexical entry classes, and supplemented with orthographic variant and decomposition information, the BCCWJ-based lexicon represents a key reference LR for constructing the large-scale LR. 1 I  "
W14-4723 " Terminological resources have traditionally focused on terms referring to entities, thereby ignoring other important concepts (processes, events and properties) in specialized fields of knowledge. Consequently, large parts of the conceptual structure of these fields are not taken into consideration nor represented. In this article, we show how terms that refer to processes and events (and, to a lesser extent, properties) can be characterized using Frame Semantics (Fillmore, 1982) and the methodology developed within the FrameNet project (Ruppenhofer et al., 2010). More specifically, we applied the framework to a subset of terms in the field of the environment. Frames are unveiled first by comparing similarities between the argument structures of terms already recorded in a terminological database and the relationships they share with other terms. A comparison is also carried out with the lexical units recorded in FrameNet. Then, relations between frames are defined that allow us to build small conceptual scenarios that are specific to the field of the environment. These relations are determined on the basis of the set of relations listed in the FrameNet project. This article reports on the methodology, the frames defined up to now and two specific conceptual scenarios (Risk_scenario and Managing_waste).  "
W14-4724 "Abstract The modelling of the semantics of adjectives is notoriously challenging. We consider this problem in the context of the so called ontology-lexicon interface, which attempts to capture the semantics of words by reference to an ontology in description logics or some other, typically first-order, logical formalism. The use of first order logic (hence also description logics), while effective for nouns and verbs, breaks down in the case of adjectives. We argue that this is primarily due to a lack of logical expressivity in the underlying ontology languages. In particular, beyond the straightforward intersective adjectives, there exist gradable adjectives, requiring fuzzy or non-monotonic semantics, as well as operator adjectives, requiring second-order logic for modelling. We consider how we can extend the ontology-lexicon interface as realized by extant models such as lemon in the face of the issues mentioned above, in particular those arising in the context of modelling the ontological semantics of adjectives. We show how more complex logical formalisms that are required to capture the ontological semantics of adjectives can be backward engineered into OWL-based modelling by means of pseudo-classes. We discuss the implications of this modelling in the context of application to ontology-based question answering. "
W14-4725 "<NoAbstract>"
W14-4901 " Part-of-speech tagging (POS-tagging) of spoken data requires different means of annotation than POS-tagging of written and edited texts. In order to capture the features of German spoken language, a distinct tagset is needed to respond to the kinds of elements which only occur in speech. In order to create such a coherent tagset the most prominent phenomena of spoken language need to be analyzed, especially with respect to how they differ from written language. First evaluations have shown that the most prominent cause (over 50%) of errors in the existing automatized POS-tagging of transcripts of spoken German with the Stuttgart Tubingen Tagset (STTS) and the treetagger was the inaccurate interpretation of speech particles. One reason for this is that this class of words is virtually absent from the current STTS. This paper proposes a recategorization of the STTS in the field of speech particles based on distributional factors rather than semantics. The ultimate aim is to create a comprehensive reference corpus of spoken German data for the global research community. It is imperative that all phenomena are reliably recorded in future part-of-speech tag labels. 1 I  "
W14-4902 "<NoAbstract>"
W14-4903 "Abstract Recent work on error detection has shown that the quality of manually annotated corpora can be substantially improved by applying consistency checks to the data and automatically identifying incorrectly labelled instances. These methods, however, can not be used for automatically annotated corpora where errors are systematic and cannot easily be identified by looking at the variance in the data. This paper targets the detection of POS errors in automatically annotated corpora, so-called silver standards, showing that by combining different measures sensitive to annotation quality we can identify a large part of the errors and obtain a substantial increase in accuracy. "
W14-4904 " We investigate the feasibility of aligning Chinese and English parse trees by examining cases of incompatibility between Chinese-English parallel parse trees. This work is done in the context of an annotation project where we construct a parallel treebank by doing word and phrase alignments simultaneously. We discuss the most common incompatibility patterns identified within VPs and NPs and show that most cases of incompatibility are caused by divergent syntactic annotation standards rather than inherent cross-linguistic differences in language itself. This suggests that in principle it is feasible to align the parallel parse trees with some modification of existing syntactic annotation guidelines. We believe this has implications for the use of parallel parse trees as an important resource for Machine Translation models.  "
W14-4905 " The purpose of our work is to explore the possibility of using sentence diagrams produced by schoolchildren as training data for automatic syntactic analysis. We have implemented a sentence diagram editor that schoolchildren can use to practice morphology and syntax. We collect their diagrams, combine them into a single diagram for each sentence and transform them into a form suitable for training a particular syntactic parser. In this study, the object language is Czech, where sentence diagrams are part of elementary school curriculum, and the target format is the annotation scheme of the Prague Dependency Treebank. We mainly focus on the evaluation of individual diagrams and on their combination into a merged better version.  "
W14-4906 " An experimental annotation method is described, showing promise for a subjective labeling task  discourse coherence quality of essays. Annotators developed personal protocols, reducing front-end resources: protocol development and annotator training. Substantial inter-annotator agreement was achieved for a 4-point scale. Correlational analyses revealed how unique linguistic phenomena were considered in annotation. Systems trained with the annotator data demonstrated utility of the data . "
W14-4907 "Abstract Creating high-quality manual annotations on text corpus is time-consuming and often requires the work of experts. In order to explore methods for optimizing annotation efforts, we study three key time burdens of the annotation process: (i) multiple annotations, (ii) consensus annotations, and (iii) careful annotations. Through a series of experiments using a corpus of clinical documents annotated for personally identifiable information written in French, we address each of these aspects and draw conclusions on how to make the most of an annotation effort. "
W14-4908 "<NoAbstract>"
W14-4909 "{kata.simko,viktor.varga.1991}@gmail.com Abstract Uncertainty detection has been a popular topic in natural language processing, which manifested in the creation of several corpora for English. Here we show how the annotation guidelines originally developed for English standard texts can be adapted to Hungarian webtext. We annotated a small corpus of Facebook posts for uncertainty phenomena and we illustrate the main characteristics of such texts, with special regard to uncertainty annotation. Our results may be exploited in adapting the guidelines to other languages or domains and later on, in the construction of automatic uncertainty detectors. "
W14-4910 "junta-m@nict.go.jp {meihe, okazaki, inui}@ecei.tohoku.ac.jp Abstract  junta-m@nict.go.jp {meihe, okazaki, inui}@ecei.tohoku.ac.jp Abstract Microblogs are a popular way for users to communicate and have recently caught the attention of researchers in the natural language processing (NLP) field. However, regardless of their rising popularity, little attention has been given towards determining the properties of discourse relations for the rapid, large-scale microblog data. Therefore, given their importance for various NLP tasks, we begin a study of discourse relations on microblogs by focusing on evidence relations. As no annotated corpora for evidence relations on microblogs exist, we conduct a corpus study to identify such relations on Twitter, a popular microblogging service. We create annotation guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence relations. Finally, we report our observations, annotation difficulties, and data statistics. "
W14-4911 "Abstract This paper presents the semi-semantic part of speech annotation and its evaluation via Krippendorffs  for the URDU.KON-TB treebank developed for the South Asian language Urdu. The part of speech annotation with the additional subcategories of morphology and semantics provides a treebank with sufficient encoded information. The corpus used is collected from the Urdu Wikipedia and news papers. The sentences were annotated manually to ensure a high annotational quality. The inter-annotator agreement obtained after evaluation is 0.964, which lies in the range of perfect agreement on a scale. Urdu is comparatively an under-resourced language and the development of the treebank with rich part of speech annotation will have significant impact on the state-of-the-art for Urdu language processing. "
W14-4912 "Abstract This paper describes a methodology for supporting the task of annotating sentiment in natural language by detecting borderline cases and inconsistencies. Inspired by the co-training strategy, a number of machine learning models are trained on different views of the same data. The predictions obtained by these models are then automatically compared in order to bring to light highly uncertain annotations and systematic mistakes. We tested the methodology against an English corpus annotated according to a fine-grained sentiment analysis annotation schema (SentiML). We detected that 153 instances (35%) classified differently from the gold standard were acceptable and further 69 instances (16%) suggested that the gold standard should have been improved. "
W14-4913 " [We report of the procedures of developing a large representative corpus of 50,000 sentences taken from clinical notes. Previous reports of annotated corpus of clinical notes have been small and they do not represent the whole domain of clinical notes. The sentences included in this corpus have been selected from a very large raw corpus of ten thousand documents. These ten thousand documents are sampled from an internal repository of more than 700,000 documents taken from multiple health care providers. Each of the documents is de-identified to remove any PHI data. Using the Penn Treebank tagging guidelines with a bit of modifications, we annotate this corpus manually with an average inter-annotator agreement of more than 98%. The goal is to create a parts of speech annotated corpus in the clinical domain that is comparable to the Penn Treebank and also represents the totality of the contemporary text as used in the clinical domain. We also report the output of the TnT tagger trained on the initial 21,000 annotated sentences reaching a preliminary accuracy of above 96%.] 1 I  "
W14-4914 "Abstract This project aims to develop linguistic resources to support computational NLP research on the Igbo language. The starting point for this project is the development of a new part-of-speech tagging scheme based on the EAGLES tagset guidelines, adapted to incorporate additional language internal features. The tags are currently being used in a part-of-speech annotation task for the development of POS tagged Igbo corpus. The proposed tagset has 59 tags. "
W14-4915 "<NoAbstract>"
W14-4916 " In an attempt to extend Penn Discourse Tree Bank (PDTB) / Turkish Discourse Bank (TDB) style annotations to spoken Turkish, this paper presents the first attempt at annotating the explicit discourse connectives in the Spoken Turkish Corpus (STC) demo version. We present the data and the method for the annotation. Then we reflect on the issues and challenges of transitioning from written to spoken language. We present the preliminary findings suggesting that the distribution of the search tokens and their use as discourse connectives are similar in the TDB and the STC demo. 1  "
W14-4917 "Abstract In the context of multi-domain and multimodal online asynchronous discussion analysis, we propose an innovative strategy for manual annotation of dialog act (DA) segments. The process aims at supporting the analysis of messages in terms of DA. Our objective is to train a sequence labelling system to detect the segment boundaries. The originality of the proposed approach is to avoid manually annotating the training data and instead exploit the human computational efforts dedicated to message reply formatting when the writer replies to a message by inserting his response just after the quoted text appropriate to his intervention. We describe the approach, propose a new electronic mail corpus and report the evaluation of segmentation models we built. "
W14-4918 "Abstract To computationally model discourse phenomena such as argumentation we need corpora with reliable annotation of the phenomena under study. Annotating complex discourse phenomena poses two challenges: fuzziness of unit boundaries and the need for multiple annotators. We show that current metrics for inter-annotator agreement (IAA) such as P/R/F1 and Krippendorffs  provide inconsistent results for the same text. In addition, IAA metrics do not tell us what parts of a text are easier or harder for human judges to annotate and so do not provide sufficiently specific information for evaluating systems that automatically identify discourse units. We propose a hierarchical clustering approach that aggregates overlapping text segments of text identified by multiple annotators; the more annotators who identify a text segment, the easier we assume that the text segment is to annotate. The clusters make it possible to quantify the extent of agreement judges show about text segments; this information can be used to assess the output of systems that automatically identify discourse units. "
W14-4919 "Abstract Clinical decision-making has high-stakes outcomes for both physicians and patients, yet little research has attempted to model and automatically annotate such decision-making. The dual process model (Evans, 2008) posits two types of decision-making, which may be ordered on a continuum from intuitive to analytical (Hammond, 1981). Training clinicians to recognize decision-making style and select the most appropriate mode of reasoning for a particular context may help reduce diagnostic error (Norman, 2009). This study makes preliminary steps towards detection of decision style, based on an annotated dataset of image-based clinical reasoning in which speech data were collected from physicians as they inspected images of dermatological cases and moved towards diagnosis (Hochberg et al., 2014). A classifier was developed based on lexical, speech, disfluency, physician demographic, cognitive, and diagnostic difficulty features. Using random forests for binary classification of intuitive vs. analytical decision style in physicians diagnostic descriptions, the model improved on the baseline by over 30%. The introduced computational model provides construct validity for decision styles, as well as insights into the linguistic expression of decision-making. Eventually, such modeling may be incorporated into instructional systems that teach clinicians to become more effective decision makers. "
W14-4920 "Department of Linguistics and Beckman Institute  School of Library and Information Science University of Illinois at Urbana-Champaign, USA {alsabba1, girju, jdiesner} @illinois.edu Abstract We present an interactive procedure to annotate a large-scale corpus of Modern Standard and Egyptian Arabic tweets for event modality that comprises obligation, permission, commitment, ability, and volition. The procedure splits up the annotation process into a series of simplified questions, dispenses with the requirement of expert linguistic knowledge, and captures nested modality triggers and their attributes semi-automatically. "
W14-4921 "Department of Computational Linguistics Saarland University, Saarbr  ucken, Germany {afried,apalmer}@coli.uni-saarland.de Abstract This paper presents an annotation scheme for a new semantic annotation task with relevance for analysis and computation at both the clause level and the discourse level. More specifically, we label the finite clauses of texts with the type of situation entity (e.g., eventualities, statements about kinds, or statements of belief) they introduce to the discourse, following and extending work by Smith (2003). We take a feature-driven approach to annotation, with the result that each clause is also annotated with fundamental aspectual class, whether the main NP referent is specific or generic, and whether the situation evoked is episodic or habitual. This annotation is performed (so far) on three sections of the MASC corpus, with each clause labeled by at least two annotators. In this paper we present the annotation scheme, statistics of the corpus in its current version, and analyses of both inter-annotator agreement and intra-annotator consistency. "
W90-0101 "re Newark, DE 19716 Abstract In this paper we investigate the incorporation of Tree Adjoining Grammars (TAG) into the systemic framework. We show that while systemic grammars have many desirable characteristics as a generation paradigm, they appear to have problems in generating certain kinds of sentences (e.g., those containing discontinuity or long-distance dependencies). We argue that these problems can be overcome with an appropriate choice of structural units of realization. We show that TAG provides appropriate units of structural realization because they localize all dependencies and allow the realization of two independent subpieces to be interspersed with each other. We go on to show how TAG can be incorporated without affecting the basic tenants of systemic grammar. Finally, we indicate how the incorporation of TAG yields several benefits to the systemic framework. Introduction As pointed out by many researchers (e.g., [Davey 1978; Mann 1983; Matthiessen & Kasper 1985; Patten 1988; Bateman & Paris 1989]), systemic linguistics offers many advantages to a sentence generation component of a text generation system. Perhaps the strongest asset of systemics is its view of the generation process as a goal directed enterprise. Its emphasis is on function rather than form [Halliday 1985; Fawcett 1980; Hudson 1971], where the functional distinctions that are required in the grammar manifest themselves in the eventual output form. While systemic linguists have remained agnostic with respect to certain processing decisions, a computer implementation of a systemic grammar requires that explicit decisions be made concerning realization operators and the structures available for manipulation at each point in the processing. The explicit decisions that were made in previous implementations of systemic *This work is supported in part by Grant #H133ES0015 from the National Institute on Disability and Rehabilitation Research. Support was also provided by the Nemours Foundation. grammar (e.g., [Mann 1983; Mann & Matthiessen 1985; Matthiessen & Kasper 1985]) have proven to be problematic in some respects. In particular, the current implementations have difficulty in generating certain sentences which exhibit discontinuities or long distance dependencies. To date, these can only be handled in a limited fashion, and the solutions provided are not very satisfying. We argue that Tree Adjoining Grammar (TAG) provides a structural unit that is precisely appropriate for the implementation of a systemic grammar for the generation task. Moreover, we believe our use of TAG for this purpose is completely consistent with the systemic paradigm and helps to overcome the above difficulties. In this paper we first introduce the notion of a systemic grammar and the processing paradigm it espouses. We indicate problems with current implementations of this paradigm. Next, we introduce the notion of lexicalized Tree Adjoining Grammars, emphasizing their structural domains of locality, and justify that the basic structures of TAG are appropriate structures to be used in an implementation of a systemic grammar. Following this we indicate how a tree adjoining grammar can be used as the basis for an implementation of systemic grammar indicating the differences between the approach of current implementations of systemic grammar and that which would result from the incorporation of TAG. Finally, we indicate potential gains resulting from the incorporation of TAGs and the scope of current work. Generating in Systemic Paradigm: Problems? Systemic linguistics deals with the meaning and function of an utterance, it is a semantics driven approach rather than a syntax driven approach. In systemics, form follows function. The grammar itself is factored into three metafunctional domains (each of which affect the final text): ideational (concerning the characteristics of the conceptual situation to be presented), interpersonal (concerning the interaction between speaker and hearer) and textual (concerning the coherence of 1 the text as a whole). A systemic functional grammar consists of networks of grammatical choice alternatives, where individual networks are concerned with one of the metafunctional domains. In generation, these networks are traversed and fragments of grammatical structure are built up and manipulated using a set of realization operators which are associated with the various choice alternatives. The correct choice is made by consulting the information concerning the planned utterances. As the choices are made, the associated realization statements in the network are evaluated in order to realize the final structure. For instance, figure 1 contains a small fragment of a systemic grammar for clauses. The network indicates that a clause may either be simple or complex. If it is simple and full, then the grammatical function process is inserted (indicated by +process). The realization operation '% subject process\" indicates that the subject should be ordered before the process in the final realization. Systemics deals with communicative function and its eventual surface manifestation at many levels. The basic processing starts with a semantically meaningful piece of representation which is decomposed into its component pieces via network traversal. The component pieces may then be further specified by re-entering the network. Given this rank-based decomposition (or stepwise decomposition), it is not unreasonable to assume that 1) decisions at a higher rank are made prior to decisions at a lower rank that is, the decomposition of a particular semantic unit may not be influenced by the eventual decomposition of its component pieces, and 2) the structural realizations of the component pieces of a decomposed unit must be handled independently. These criteria, which we call the independence criterion, are implicitly followed by current computer implementations of systemic grammar. Implementing a systemic grammar on computer has forced researchers to be very explicit about certain representational issues. Such explicitness (coupled with an implicit following of the independence criterion), has enabled the uncovering of certain constructions which appear to be problematic for the systemic framework. For instance, Matthiessen points out that: \"There are various structural relationships (e.g., discontinuity...) that do not pose a problem for the informal box diagram representation [used by systemic linguists] but prove to be a problem for explicit realization statements [necessary for computer implementations].\"[Matthiessen & Kasper 1985, p. 6]. We believe that the same applies to (so called) long distance dependencies. It can be argued that the problems uncovered by Matthiessen are not due to explicit realization statements, but rather result from using explicit realization statements coupled with the implicit assumption that grammatical functions are realized as atomic strings. We further argue that if the independence criterion is to be followed, the choice of realization operators, the scope of the realization operators, and the choice of appropriate units of realization must be considered together. Consider, for example, the problems which arise in generating a sentence containing a long distance dependency when atomic strings are taken as the structural unit of realization. Consider the sentence: \"It was Mary that John thought Bill liked\". A natural decomposition would result in the semantic subpieces which correspond to \"john thought\" and the dependent clause \"bill liked mary\". The extraction of \"mary\" will be done subsequent to this decomposition; this extraction should influence the realization of the structural unit for the dependent clause (and should not affect the functional decomposition). But notice, if we assume the structural units of realization are atomic strings, we can get \"John thought it was Mary that Bill liked\" but not our desired utterance \"It was Mary that John thought Bill liked\" because to do so would necessitate inserting one atomic string (\"John thought\") within another (\"It was Mary that Bill liked\"). Two possible ways to get around this problem are: (1) to say that the intended sentence is made up of several independent but smaller functions (e.g., realized as \"john thought\", \"bill\", \"liked\", \"mary\"). But this solution goes against the step-wise decomposition into semantically meaningful units. Moreover, this method is not sufficient because we can always consider a sentence that requires one more level of embedding which would necessitate a revision of units. (2) not to break up the sentence into the two functional constituents mentioned, but use some other decomposition. But this is not a correct solution because then one functional decision at a lower level (extraction of \"mary\") influences another which logically precedes the first (decomposition in the clause complex network). In order to follow the independence criterion (i.e., realization of different independent functions do not influence each other), the structural units being built as the realization of a grammatical function must be capable of localizing all structural dependencies because they must embody all constraints specified with that function. In addition, the chosen structural units must be composable in such a way as to allow the surface string of one unit to be interspersed with the surface string for another (as was required in our example). If this is the case, then it makes sense that these structural units should be taken as the bounding domain for the realization operators. The structures used by TAGs have precisely these qualities and are thus an appropriate choice for a structural unit of realization in an implementation of systemic grammar. The structures used in a TAG have an enlarged domain of locality that factor all dependencies (e.g., agreement constraints, predicate-argument structure, filler-gap depen2 Transitive Tree Group S S NPO ~[+wh] S NPO~ VP NP0 VP V NP1 ~ [ V NP1 ~ S NP1 ~ S NPO ~ VP V NP1 I means that at that node substitution must occur. Figure 3: A Tree Group Selected by Like from one another by syntactic variations (which we shall call transformations). For example, the verb like, which takes a nominal subject and a nominal object, selects the transitive tree group. Some of the members of this tree group axe shown in Figure 3. The figure contains three initial trees, the first corresponds to a declarative sentence, the second to a wh-question on the subject, and the third to an it-cleft construction on the object. S-TAGs The processing within a Systemic Tree Adjoining Grammar (S-TAG) is similar to that in systemic grammar (e.g., the networks axe traversed and the realization operators associated with the choices taken are evaluated). In S-TAG we have already stated that the individual grammatical functions are realized structurally by elementary trees and that elementary trees provide the bounding scope for application of realization operators. Thus, the \"functions\" which are inserted into the systemic structure will be associated with elementary trees in the TAG formalism. While the types of realization operators required by S-TAG will be the same as for general systemic grammars, the individual operators will be tailored to the TAG formalism. Regions in S-TAG The basic processing within a systemic grammar must take into account two dimensions of processing decisions: 1. Metafunctional domains. Structures are built in the three metafunctional domains (ideational, interpersonal, and textual) simultaneously. Certain realization operators are used to \"conflate\" the independently built structures. 2. Processing from one \"rank\" to another. It is through changes in rank that semantic structures are eventually realized as surface form. The general methodology is to insert functional units into a structure. Following this, these functional units are refined by re-entering the network at a lower rank. This process continues until a surface structure has been fleshed out. While the processing in the S-TAG grammar follows the same principles, we differ in some implementation issues to accommodate TAG. One of the major contributions of this work is in the processing from one rank to another. In particular, this work makes explicit the bounding domains for the realization operators which are responsible for realizing a given grammatical function. Thus it becomes clear what is available for manipulation when a network is entered (and re-entered for specifying a function inserted during the initial network traversals). We employ the notion of a region for this purpose. In general a region is created to expand a grammatical function. Since we have said that elementary trees are appropriate structural units for realizing the functions and for the bounding domains for the realization operators, we state that an elementary tree will eventually be associated with every region. The appropriate elementary tree will be chosen after a decision has been made to insert a lexical item. Informally, this lexical item will be the lexical anchor of the elementary tree that will be chosen in the region. For this reason we will call this lexical item the lexical anchor of the region also) The region serves as the bounding domain on the realization operations. All realization operations 1This approach has interesting consequences, such as adopting a head driven generation strategy within the sys4 used within a region are applicable on the features of the lexical anchor of the region or the tree selected by this anchor. In the section on \"Lexicon and Tree Groups\" we will discuss how the features of the anchor, the tree groups selected, and trees selected will be maintained m a region. Once a lexical anchor is picked, the tree groups associated with that anchor will be considered. The choices in the network will cause realization operators to be evaluated which will narrow this set of trees to one. This single tree is then said to be associated with the region and will be the structural.realization of the grammatical function being expanded (whose expansion was the reason for the creation of the region). This filtering will be done by using realization operations that select between tree groups and those that select tree members within tree groups. Such realization operations will be discussed in the section on \"Realization Operators\". Based on the characteristics of the tree associated with a region and directives from the networks being traversed, decisions to expand certain grammatical functions (previously inserted in the region) will be made. Sub-regions will be created to expand these functions and the network will be re-entered to determine the expansion. Notice that this will cause an elementary tree to be associated with each sub-region. These subregions must eventually be combined with the superregions which spawned them using realization operations for adjoining and substitution. This view of the process is potentially complicated since some of the realization operations to be evaluated in the region may not be applied before the set of trees being considered is narrowed down sufficiently. For example, an operator which selects a particular tree need not be applied until the tree groups have been narrowed to one. If at the point an operator which selects a tree is called for, if the number of tree groups has not been reduced to one, it serves no purpose to apply the operation on each tree group. Hence, in such cases, within a region we will maintain a record of realization operations that have to be completed later. These operations will be applied at the appropriate time. Lexicon and Tree Groups In the lexicon, a lexical item will be associated with a set of tree groups, each of which contains a set of elementary trees. We choose to represent a tree group in the lexicon as a feature-structure/tree-group-name pair. The feature-structure includes all of the common features of the trees within the tree groups, the features of the lexical item itself, and its lexical idiosyncrasies, if any. The tree group name can be thought of as a pointer to the actual tree group kept in a separate area (allowing for sharing of tree groups by lexical items). temics framework. It will also have implications on the design of the network. For example, with the lexical item, walk, we will associate the pairs (ft,.a,~s,trans), (fiotra,~,,intrans), (f, ou,~, noun), trans, for instance, is the name of the tree group for transitive verbs. All trees in this group share the information that the lexical anchor is a transitive verb. Thus, this information is stored in ftrans along with any other features that are common to the trees in the group. The other two pairs represent the fact that walk can be used as a intransitive verb as well as a noun. The trees that constitute a tree group are kept together. Some realization operators which will be evaluated in a region will refer to certain grammatical functions that are represented as nodes in the tree associated with that region. Hence we will use a mapping table that maps abstract positions (grammatical functions) to actual nodes in an elementary tree. Realization Operators Having set up the notion of a region (and its associated elementary tree) as the bounding domain over which the realization operators can function, we are now in a position to discuss some of the realization operators that will be used in S-TAG. These operators parallel those found in other systemic grammar implementations, although they are particular to the use of TAG. According to [Matthiessen & Kasper 1985] the realization operators used in a systemic network can be viewed along three dimensions: (1) Structuring (which defines the structure and its organization within one rank and within one functional domain), (2) Rank (which \"organizes the grammar into a scale of units: clausegroup/phrase word morpheme\" [Matthiessen & Kasper 1985, p. 25]), and (3) Metafunctional layering (which integrates the structures developed within the various metafunctional domains (e.g., interpersonal, ideational, and textual)). We concentrate on the rank and structuring operators because they appear to be most affected by the addition of TAG. Aside from the nature of the actual structural units, a major difference between S-TAG and previous implementations of systemics is that previous implementations have built up structures of minimal import: upon proper evidence a functional unit is added to the current structure, ordered with respect to the other elements, accumulates features, and is then expanded so as to satisfy those features. There appears to be no automatic mechanism for carrying out syntactic implications of decisions that have been made. In S-TAG we take an opposite approach. In the TAG we have precompiled minimally complete packages of syntactic structure. Rather than building up structure only when we have enough evidence to know that it is correct (as has previously been done), our operation can be characterized as deciding between the syntactic possibilities that are consistent with what is known at the given point in the network. 2 As a result many of the structuring operators we introduce are designed to narrow down the trees that could possibly realize a particular function. Introducing the Lexical Anchor: Insert(Lexitem) When the lexical anchor is identified in the network, this operation will be used. The purpose of this operation is not only to introduce the lexical anchor into the region but also to bring the associated set of feature-structure/tree-group-name pairs. Thus the tree group itself is not brought in but is indirectly accessible. A tree is brought into the region only after the narrowing process is completed. The anchor is then inserted into the tree. Filtering Tree Groups in a Region We will use one realization operation to choose among the the tree groups being considered in a region. This choice is made on the basis of some features that become known during the traversal of the network and is basically a decision about the functional units the realization must represent. Thus, in some sense it is analogous to the \"insert\" operator in Nigel. For example, the insertion of a particular lexical item, say walk, will bring into consideration all possible tree groups it can participate in. If it becomes known (in the transitivity network) that the recipient function will have to be realized, then among the various tree groups of the lexical anchor of the region, only the appropriate tree groups (such as those corresponding to transitive verb form) will have to be considered. For current purposes, the realization operation that filters the tree groups will be called Select-Group which takes a feature as an argument. In the above example, the network may cause the operation: Select-Group(transitive) to be evaluated. Recall that the three tree-groups referenced for this lexical item are represented by the pairs: (ftrans,trans), (fintran,,intrans), and (fnoun,noun). Since the feature-structures ftran,, fintrans, fnoun are kept in the lexicon itself rather than with the tree group, these tupies will be brought into the region on lexical insertion. If the realization operation Select-Group(transitive) (which is analogous to insert process and recipient in  the Nigel grammar) is evaluated in the region, the feature transitive is unified with the three featurestructures ftrans, fintrans, f.o~,n. Since this feature is only consistent with the features in ftrans, only the pair (ftrans, trans) will remain in the region. Selecting Trees from a Tree Group The realization operation used to narrow down the choice of elementary trees within a tree group considered in a region 2Note it is not necessary to bring all of the syntactic structures into the region, rather much of this processing can be done based on the features stored with the lexical anchor. is called Select-Tree. We had described a tree group to correspond to a specific semantic entity with all of its relevant semantic features inserted. The group itself represents all syntactic realizations of this entity. Therefore the purpose of this operation is to choose among different syntactic forms possible. Its effect is somewhat analogous to that of the \"order\" operators in Nigel. For example, if during the traversal of the network it is realized that the object is to be topicalized then the Select-Tree operation will be evaluated. Among the various syntactic variations possible, the tree(s) which realize this thematization will thus be identified. Composing Trees Recall that sub-regions are created to expand grammatical functions. The elementary trees associated with the sub-regions are to be composed with the tree associated with the super-region either by substitution or by adjunction. Expansion of a grammatical function is done, in the Nigel grammar, when a function is preselected with a set of features. The preselected features determine where to re-enter the network in order to expand the given function. The resulting realization will replace the original function in the eventual realization of the input. In S-TAG this is accomplished by using the realization operation Expand(function, features). This will cause the creation of a sub-region (which is named by the function). The realization of the function will occur in this sub-region by re-entering the network at a point determined by the preselected feature (as in Nigel). The tree which eventually realizes the function must be composed (by substitution or adjoining) with the tree in the super-region at the node corresponding to the function (as given by the mapping table). The decision to adjoin or substitute is made based on the types of the trees that are picked in the suband superregions. "
W90-0102 "a Philadelphia, PA 19104 Abstract Tree-adjoining grammars (TAG) have been proposed as a formalism for generation based on the intuition that the extended domain of syntactic locality that TAGs provide should aid in localizing semantic dependencies as well, in turn serving as an aid to generation from semantic representations. We demonstrate that this intuition can be made concrete by using the formalism of synchronous tree-adjoining grammars. The use of synchronous TAGs for generation provides solutions to several problems with previous approaches to TAG generation. Furthermore, the semantic monotonicity requirement previously advocated for generation grammars as a computational aid is seen to be an inherent property of synchronous TAGs. Introduction The recent history of grammar reversing can be viewed as an effort to recover some notion of semantic locality on which to base a generation process. For instance, Wedekind (1988) requires a property of a grammar that he refers to as connectedness, which specifies that complements be semantically connected to their head. Shieber (1988) defines a notion of semantic monoLonicity, a kind of compositionality property that guarantees that it can be locally determined whether phrases can contribute to forming an expression with a given meaning. Generation schemes that reorder top-down generation (Dymetman and Isabelle, 1988; Strzalkowski, 1989) so as to make available information that well-founds the top-down recursion also fall into the mold of localizing semantic information. Semantichead-driven generation (Shieber et al., forthcoming; Calder et al., 1989) uses semantic heads and their complements as a locus of semantic locality. Joshi (1987) points out that tree-adjoining grammars may be an especially appropriate formalism for generation because of their syntactic locality properties, which, intuitively at least, ought to correlate with some notion of semantic locality. The same observation runs as an undercurrent in the work of McDonald and Pustejovsky (1985), who apply TAGs to the task of generation. As these researchers note, the properties of TAGs for describing the syntactic structuring of a natural language mesh quite naturally with the requirements of natural-language generation. Nonetheless, generation is not, as typically viewed, a problem in natural-language syntax. Any system that attempts to use the TAG formalism as a substrate upon which to build a generation component must devise some mechanism by which a TAG can articulate appropriately with semantic information. In this paper, we discuss one such mechanism, synchronous TAGs, which we have previously proposed in the arena of semantic interpretation and automatic translation, and examine how it might underly a generation system of the sort proposed by Joshi and McDonald and Pustejovsky. In particular, synchronous TAGs allow for a precise notion of semantic locality corresponding to the syntactic locality of pure TAGs. Scope of the Paper The portion of the full-blown generation problem that we address here is what might be referred to as the tactical as opposed to the strategic generation problem. That is, we are concerned only with how to compute instances of a well-defined relation between strings and canonical logical forms 1 in the direction from logical forms to strings, a problem that is sometimes referred to as \"reversing\" a grammar. This aspect of the generation problem, which ignores the crucial issues in determining what content to communicate, what predicates to use in the communication, and so forth, can be seen as the reverse of the problem of parsing natural language to derive a semantic representation. The citations in the first paragraph can serve to place the issue in its historical research context. The other truly difficult issues of general natural-language production are well beyond the scope of this paper. 1This issue of canonicality of logical forms is discussed by Shieber (1988). 9 Semantics in Generation Although Joshi discusses at length the properties of TAGs advantageous to the generation task (1987), he does not address the issue of characterizing a semantic representation off of which generation can proceed. McDonald and Pustejovsky do mention this issue. Because TAGs break up complex syntactic structures into elementary structures in a particular way, their semantic representation follows this structuring by breaking up the logical form into corresponding parts. McDonald and Pustejovsky consider the sentence (1)How many ships did Reuters report that Iraq had said it attacked? Its semantic representation follows the decomposition of the sentence into its elementary TAG trees-corresponding (roughly) to \"How many ships ...it attacked\", \"did Reuters report that ...\", \"Iraq had said ...\". McDonald and Pustejovsky describe their semantic representation: \"The representation we use ... amounts to breaking up the logical expression into individual units and allowing them to include references to each other.\" The units for the example at hand would be: Ux = ~(quantity-of-ships). attack( Iraq, quantity-of-ships) U2 = say( Iraq, UI) U3 = report(Renters, Us) By composing the units using substitution of equals for equals, a more conventional logical form representation is revealed: report( Renters, say( Iraq, )t( quantity-of-ships). attack( lraq, quantity-of-ships))) Three problems present themselves. First, the particular decomposition of the full semantic form must be explicitly specified as part of the input to the generation system. Second, the basic operation that is used (implicitly) to compose the individual parts, namely substitution does not parallel the primitive operation that TAGs make available, namely adjunction. In the particular example, this latter problem is revealed in the scope of the quantity quantifier being inside the say predicate. The more standard representation of scoping would be akin to )t( quantity-of-ships). (2) report( Renters, say( Iraq, attack( Iraq, quantity-of-ships))) but this requires one of the elementary semantic units to be \"broken up\". Consequently, McDonald and Pustejovsky note that they cannot have the logical form (2) as the source of the example sentence (1). Third, the grammatical information alone does not determine where adjunctions should occur. McDonald and Pustejovsky allude to this problem when they note that \"the [generator] must have some principle by which to judge where to start.\" In their own example, they say that \"the two pending units, U2 and U3, are then attached to this matrix ... into complement positions,\" but do not specify how the particular attachment positions within the elementary trees are chosen (which of course has an impact on the semantics). The relationship between syntax and semantics that they propose links elementary trees with units of the realization specification. Apparently, a more finely structured representation is needed. "
W90-0103 "<NoAbstract>"
W90-0104 "<NoAbstract>"
W90-0105 "Abstract In this paper we address the organization and use of the lexicon giving special consideration to how the salience of certain aspects of abstract semantic structure may be expressed. We propose an organization of the lexicon and its interaction with grammar and knowledge that makes extensive use of lexical functions from the Meaning-Text-Theory of Mel'~uk. We integrate this approach with the architecture of the PENMAN text generation system, showing some areas where that architecture is insufficient, and illustrating how the lexicon can provide functionally oriented guidance for the generation process. Introduction In natural language generation, the lexicon can be viewed, generally, as containing information for the verbalization of meanings. This information ranges over both the static organization of vocabulary -including lexical knowledge, often handled separately as \"lexical semantics\" (see, e.g., [Pustejovsky, 1988; Nirenburg and Raskin, 1987]) or \"structural semantics\" -and the process of lexical choice. To make allowance for this latter dynamic aspect of the lexical organization we will henceforth use the term lexis common in systemic linguistics [Hasan, 1987; Matthiessen, 1988] instead of \"lexicon\". Lexis thus represents lexical information at various different levels of abstraction (strata) and mapping structures that provide for the conversion between those levels. In this paper we address the organization of lexis giving special consideration to the expresssion and choice of appropriate expressions as a function of the desired salience or prominence of semantic elements. The choice set of possible configurations of prominence we call the perspectives of the semantic structure. These have been addressed rarely in approaches in generation so far: for example, [Jacobs, 1985] discusses the verbs give and take as two different expressions of the same event; [Nirenburg and Nirenburg, 1988] suggest an approach to open-class lexical item selection for realization of conceptual input; and [Iordanskaja et al. John A. Bateman USC/Information S ciences Institute 4676 AdmirMty Way Marina del Rey CA 90292-6695, U.S.A. e-mail: bateman@isi.edu 1988] propose an approach to linguistic paraphrasing by adapting the Meaning-Text-Theory (MTT) [Mel'~uk and Zholkovsky, 1970] and its paraphrasing rules. Here, we make more extensive use of the MTT in order to provide a richer organization of lexis and its interaction with grammar and knowledge than has been proposed previously. Moreover, we develop this approach in the context of a concrete generation environment, the PENMAN system [Mann and Matthiessen, 1985], showing some areas where the existing architecture is insufficient and how the richer organization of lexis we propose can help. The following set of examples gives an impression of the variety of linguistic phenomena that we include under the term perspective. 1 All the sentences can be interpreted as verbalizations of a single abstract semantic structure with differing aspects of that structfire being given emphasis in each case. For example, in (4), the reader is made salient as a participant of the proposition; in (5), the 'manner of achievement' of the 'indication' is made salient; in (7), a particular temporal aspect of the process, namely the beginning, is made prominentput; and in (8), the intended purpose of the agent is made salient as a 'making clear'. While the variation that can be seen between (1), (2), and (3) can already be treated in, for example, the current PENMAN system by exercising meaning options available in the grammar (i.e., (2) exhibits passivization and (3) nominalization of 'use'), the variation shown in the remaining examples cannot be functionally motivated as possible alternate grammatical realizations of the base semantic form. 1. We use the adjective \"electronic\" to indicate that the dictionaries are deeply dedicated to computers. ~. The adjective \"electronic\" is used to indicate that the dietionarses are deeply dedicated to computers. 3. The use of the adjective \"electronic\" indicates that the dictionaries are deeply dedicated to computers. "
W90-0106 " As text generation systems get more sophisticated and capable of producing a wider syntactic and lexical range, the issue of how to choose among available grammatical options (preferably in a human-like way) becomes more pressing. Thus, devisers of text generation systems are frequently called upon to provide their own analyses of the discourse function of various kinds of alternations. In this paper, I describe a proposed research tool which is designed to help a researcher explore and analyze a natural-language \"target text\" in order to determine the contextual factors that predict the choice of one or another lexical item or grammatical feature. The project described in this paper is still at the drawing-board stage; I welcome suggestions about ways it could be changed or expanded to fulfill particular analytic needs. Theoretical preliminaries While some aspects of a natural-language text are determined by the nature of the information a speaker wishes to convey to a hearer, there are many more aspects that seem to be determined by certain cognitive needs that the hearer has. Speakers tailor their output to the informational resources and deficiencies of the hearer in several ways: by adjusting the amount of information they make explicit, by arranging new information relative to old information in a maximally helpful way, and by giving special marking to information that may be difficult for the hearer to access for a variety of reasons. It is these strategies that give rise to the wide variety of syntactic and lexical resources of any natural language for saying the \"same thing\" in different ways. We can call the relation between lexicogrammatical features and the speaker's communicative goals in choosing those features the \"discourse functions\" of the features. For any particular alternation, then, the best predictor of the speaker's choice should be a model of the cognitive state of the hearer. Unfortunately, neither human speakers nor computer systems have direct access to the hearer's mind. But linguists have long realized that we do have access to a fair approximation of an important subset of the information the hearer possesses at a given point in a discourse: namely the text which has been produced up to that point. ([Chafe 1987] and [Giv6n 1983] are two contemporary expressions of that principle.) And in fact we can make fairly good predictions of lexico-grammatical choices based on inferences that come from the nature of the preceding text. For instance, a referent that has been referred to in the previous clause is likely to receive minimal coding (a pronoun or zero, depending on syntactic considerations). But this principle can be overridden by the presence of other factors that interfere with the accessibility of the referent -e.g. a paragraph break or another competing referent -resulting in the use of a full noun phrase. Or, to give another example, a speaker is likely to use special syntax (such as the \"presentative\" or \"there is...\" construction) to introduce a referent that will be prominent in the following discourse; so a hearer is likely to have easier access to a referent introduced in that way than one that has been introduced \"casually\", e.g. in a prepositional phrase. Therefore, subsequent references to a referent that has been introduced with a presentative are more likely to be pronouns than noun phrases. These are all factors that can be discerned in the preceding text and are taken into account by speakers as affecting the nature of the bearer's expectations. Therefore under these perturbing circumstances the speaker can decide to use a fuller reference, e.g. a proper name or noun phrase. Figure 1 illustrates the relation between the discourse produced by the speaker, the hearer's mental state, and the speaker's model of the hearer. In real face-to-face interaction, the hearer can influence the speaker's model of her or him in more direct ways -e.g. by verbal and nonverbal indications of agreement, understanding, protest, or confusion. But this two-way channel is available neither to human writers nor to computer text generators. 39 Speaker's eommunieaKve Bearer's mental sure, goa.L~ / including rel~resentatian Speaker's modet / of speech oj' heater's state // Figure 1: A model of communication. The fact that we can draw inferences from preceding text about the hearer's slate justifies the methodology common in functional linguistics whereby the Enguist investigates the function of a particular morpho-syntactic alternation by attempting to discover correlations in natural texts between that alternation and various features of the context. This method is also likely to be the one which will lead to the best results for text generation specialists, since prior text is one information source any computational system has easy access to as a guide in making future decisions. There are, of course, currently available several largescale databases of English texts of various kinds, some of which have been provided with various amounts and kinds of coding (from word-class lagging to syntactic analysis). However, for many kinds of discourse work these databases have not been useful. On the one hand, such databases are often not available in languages other than English; on-the other, the coding that is provided assumes an analysis which may not meet the needs of a particular user. In fact it is often the case that aspects of the syntactic analysis depend on a functional analysis having already been done; so a pre-coded database may contain assumptions about the answers to the same qtfestions it is supposed to be helping to solve. The tool described here is designed for the user who is satisfied with a relatively small amount of data, but wants to have total control over the analysis of the corpus. Currently, functional linguists often enter their text data into commercial relational database tools, such as DBase and Paradox. However, such tools have been designed with other kinds of applications in mind, and therefore there are many properties of natural language texts which these tools can only capture by awkward or indirect means. Specifically, natural language texts, unlike e.g. client or sales lists, are simultaneously linear (order matters) and recursively hierarchical. Thus, the query languages available with such database products are generally good at extracting the kind of information from texts which is least useful to linguists, but require considerable ad hoc programming to extract the kind of information most relevant to our needs. The tool proposed here, conversely, should answer most easily the questions a discourse analyst wants to ask most frequently. These problems in representing certain crucial aspects of natural language texts computationally using off-theshelf tools have sent many linguists hack to marking up xeroxed texts by hand with colored pencils. The approach outlined here is intended to combine the advantages of the colored-pencil technique (spontaneity, flexibility, and direct involvement with the whole text) with the advantages of computational techniques (quick and painless identification, compilation, and comparison of various features of text units). Description of the tool The tool proposed in this paper will aid in the generation of hypotheses concerning the discourse function of a lexico-grammatical feature (or combination of features) by allowing the researcher to isolate instances of that feature and view them in relation to various aspects of its discourse context. When the researcher has arrived at and stated a working hypothesis about which features of the context are most relevant to predicting the targeted feature, the system will be able to test the hypothesis against the data and provide feedback by displaying both predicted and non-predicted occurrences of the target feature. Further refinements can then be made until the fit between the researcher's hypothesis and the actual text is as good as possible. The hypothesis can then be integrated into a text generation system with some assurance that the lexicogrammatical choices made by the system will approximate those made by a human with a similar text-production task. In order for the tool to be able to recognize and compare various text features -including relatively covert information such as the reference of noun phrases and the mood of a sentence as well as relatively overt cues such as lexical items and structure -the user must first annotate the text, indicating a) its hierarchical structure, and b) a set of features (attribute/value pairs) associated with each constituent. The annotation will be largely manual (though aspects of it can be automated, as will be explained), because we want the tool itself to be neutral as 40 to the theoretical assumptions inherent in any particular analysis. Thus, it will be possible to use this tool to test among other things the comparative predictive usefulness of various possible analyses of a text. Using the tool will have the following phases: . Annotation: mark constituents, and for each constituent indicate its category membership and any other desired features. The amount and type of annotation is entirely up to the user; it will never be necessary to include any more detail than is relevant for the current analysis. Constituents may be anything from words to paragraphs or episodes. 2. Hypothesis formation: view the target feature and contex  Figure 1: A model of communication. The fact that we can draw inferences from preceding text about the hearer's slate justifies the methodology common in functional linguistics whereby the Enguist investigates the function of a particular morpho-syntactic alternation by attempting to discover correlations in natural texts between that alternation and various features of the context. This method is also likely to be the one which will lead to the best results for text generation specialists, since prior text is one information source any computational system has easy access to as a guide in making future decisions. There are, of course, currently available several largescale databases of English texts of various kinds, some of which have been provided with various amounts and kinds of coding (from word-class lagging to syntactic analysis). However, for many kinds of discourse work these databases have not been useful. On the one hand, such databases are often not available in languages other than English; on-the other, the coding that is provided assumes an analysis which may not meet the needs of a particular user. In fact it is often the case that aspects of the syntactic analysis depend on a functional analysis having already been done; so a pre-coded database may contain assumptions about the answers to the same qtfestions it is supposed to be helping to solve. The tool described here is designed for the user who is satisfied with a relatively small amount of data, but wants to have total control over the analysis of the corpus. Currently, functional linguists often enter their text data into commercial relational database tools, such as DBase and Paradox. However, such tools have been designed with other kinds of applications in mind, and therefore there are many properties of natural language texts which these tools can only capture by awkward or indirect means. Specifically, natural language texts, unlike e.g. client or sales lists, are simultaneously linear (order matters) and recursively hierarchical. Thus, the query languages available with such database products are generally good at extracting the kind of information from texts which is least useful to linguists, but require considerable ad hoc programming to extract the kind of information most relevant to our needs. The tool proposed here, conversely, should answer most easily the questions a discourse analyst wants to ask most frequently. These problems in representing certain crucial aspects of natural language texts computationally using off-theshelf tools have sent many linguists hack to marking up xeroxed texts by hand with colored pencils. The approach outlined here is intended to combine the advantages of the colored-pencil technique (spontaneity, flexibility, and direct involvement with the whole text) with the advantages of computational techniques (quick and painless identification, compilation, and comparison of various features of text units). Description of the tool The tool proposed in this paper will aid in the generation of hypotheses concerning the discourse function of a lexico-grammatical feature (or combination of features) by allowing the researcher to isolate instances of that feature and view them in relation to various aspects of its discourse context. When the researcher has arrived at and stated a working hypothesis about which features of the context are most relevant to predicting the targeted feature, the system will be able to test the hypothesis against the data and provide feedback by displaying both predicted and non-predicted occurrences of the target feature. Further refinements can then be made until the fit between the researcher's hypothesis and the actual text is as good as possible. The hypothesis can then be integrated into a text generation system with some assurance that the lexicogrammatical choices made by the system will approximate those made by a human with a similar text-production task. In order for the tool to be able to recognize and compare various text features -including relatively covert information such as the reference of noun phrases and the mood of a sentence as well as relatively overt cues such as lexical items and structure -the user must first annotate the text, indicating a) its hierarchical structure, and b) a set of features (attribute/value pairs) associated with each constituent. The annotation will be largely manual (though aspects of it can be automated, as will be explained), because we want the tool itself to be neutral as 40 to the theoretical assumptions inherent in any particular analysis. Thus, it will be possible to use this tool to test among other things the comparative predictive usefulness of various possible analyses of a text. Using the tool will have the following phases: . Annotation: mark constituents, and for each constituent indicate its category membership and any other desired features. The amount and type of annotation is entirely up to the user; it will never be necessary to include any more detail than is relevant for the current analysis. Constituents may be anything from words to paragraphs or episodes. 2. Hypothesis formation: view the target feature and context in various arrangements. . Hypothesis testing: state a hypothesis in terms of a pattern in the context that predicts the occurrence of the target feature. . Hypothesis refinement: make changes in hypothesis, annotation, or both and retest against the same data; extend analysis by tesdng against different data. l. Annotation An example text showing segmentation and sample feature specification associated with three of the constituents is given in figure 2. [[We] [wentl [from [Kobell [to [Shanghail],] [[from [Shanghai]l [we] [went] [to [Singapore]l,l [[and] [there] [l] [still remember] calegon/ = np [[[and] [the [natives] [in [sampans]I] ]~= plt~r*l I rlfllctnI = SHIP-FOLK [would see] [it] [[0l [comiae].]] l e,hn = Oeo,ae :::::::::. tlm. role = sub|ec! [[0] [dive].] [[and] i::i[0]:i~:: [come up]] I (*'~ = v4) . ': \"\":':':':':'::':': \":':':':':\":':+:i +:':':':':':+:'::':'':::'::'':'\" \" \" :': :: :': ~ I~lt~lCy' = llp I id = npI5 I ~ = p~o I number = plural  referent = NATIVES i=.-. j ----slm, role = ~bied sere. role = ~ctor\" Im~|. = CtlOI IveVo = V71 class = human Figure 2: Annotated text. Annotation will have the following steps: segmentation, category specification, further feature specification, and dependency specification. Furthermore, \"pseudoconstituents\" may be added and specified at this stage, as explained below. . Segmentation: Mark off a stretch of text to be considered a unit. Initially, allowable units must be continuous and properly contained inside other units; but eventually it would be nice to allow two kinds of violations to this principle, namely discontinuous constituents and overlapping constituents. . Category specification: Specify the category of the constituent. The category will be stored as an attribute-value pair (with the attribute \"category\") like other features. At the time of category specification, the constituent will be assigned a unique ID, also an attribute-value pair (with the attribute \"id\"). . Further feature specification: Specify further attribute-value pairs. In order to ensure consistency the user will be prompted with a list of a) attributes that have previously been used with that category, and b) values that have previously ~n associated with that at~bute. The user may select from these lists or enter a new aUribute or value. . Dependency relations: Some kinds of relations between constituents are best treated by specifying dependency relations between the two items, i.e. by providing a labelled pointer linking the two. Syntactic analyses differ from each other in what kinds of relations are treated as constituency relations (e.g. the relationship of sister nodes in a tree) and what kinds are treated as pure dependency relations. . \"Pseudo-constituent\" insertion: For some purposes, it may be desirable to allow the insertion of \"pseudoconstituents\" in the text, i.e. phonologically null \"elements\" that can be inferred from the surface text, or boundaries which aren't marked by any explicit linguistic element (such as \"episode boundaries\" or possibly the beginning and end of the text). Examples of null elements which can be tracked in discourse as if they were overt include \"zero pronouns\", inferrable propositions, and elements of frames or schemas. Pseudo-constituents can be inserted directly into the text like actual constituents, but they should have distinctive marking in their feature specification to indicate their abstract status; in the examples I have distinguished them by the feature \"type = pseudo\". 41 Figure 3 illustrates a possible implementation (using pop-up menus) of the process of feature specification. category = [] category values: noun [ verb | preposition / adjective / adverb k._._ complomenlizer noun phrase verb group prepositional phrase clause sonlance paragraph ~cate~ory = noun phrase~ numoer = II | number values: ) singular t ~legory = noun phrase 1 f \" noun phrase ath'ibutos: / number / referent / status |syn. role . ._~ sem. role Figure 3: Feature specification. Queries Having annotated a text to the desired degree, it is then possible to start fishing around for an analysis of the distributional characteristics of the feature or combination of features of interest to the analyst (henceforth \"target pattern\"). Experience has shown that the most useful ways to do this is to get an idea of the overall frequency of the phenomenon, and then to \"eyeball\" the target pattern in various kinds of context. Thus, the tool should have the following capabilities: 1. Counting: Count the number of instances of the target pattern. "
W90-0107 "Austin, Texas 78759 U.S.A. Abstract This paper describes the use of a system of semantic rules to generate noun compounds, vague or polysemous words, and cases of metonymy. The rules are bidirectional and are used by the understanding system to interpret the same constructions. Introduction In generation systems that are paired with understanding systems, bidirectionality is desirable for reasons that are both theoretical (a single model of linguistic behaviour) and practical (shorter development time, greater consistency, etc.) 1. Recently, [Shieber et al. 89] and [Calder at al. 89] have presented generation algorithms that share both semantics and syntax with the understanding system. This paper presents an extension of these algorithms to deal with phenomena that have often been lumped together under 'pragmatics', namely noun compounding, metonymy (the use of a word to refer to a related concept), and vague or polysemous words like \"have.\" The difficulty with these constructions is that they are productive, and cannot be handled easily by simply listing meanings in a lexicon. Taking noun compounding as an example, we have \"corn oil\" and \"olive oil\" referring to oil made from corn or olives. We could add a lexical sense for \"corn\" meaning \"made from corn,\" but then we face an explosion in the size of the lexicon, and an inability to understand or generate novel compounds: if we acquire \"safflower\" as the name of a plant, we would like the system to be able to handle \"safflower oil\" immediately, but this won't be possible if we need a separate lexical sense to handle compounding. The system will be more robust (and the lexicon more compact) if we can derive the desired sense of \"safflower\" from the basic noun sense when we need it. We have therefore developed a system of bidirectional semantic rules to handle these phenomena at the appropriate level of generality. IF or more detailed arguments along these lines, see [Appelt 87], [Shieber 88], [Jacobs 88a]. We have implemented these rules in Common Lisp as part of the KBNL system [Barnett et M. 90] at MCC, but nothing depends on the idiosyncracies of our formalisms or implementation, so the technique is compatible with a wide variety of theories of the kinds of relations that are likely to occur in these constructions, as in, e.g., [Finin 80] for noun compounds and [Nunberg 78] for oblique reference. The Framework The algorithms for recognition and generation use an agenda-based blackboard for communication and control [Cohen et al. 89]. Our syntax component uses an extension of Categorial Unification Grammar [Wittenburg 86] as the phrase-structure component of an LFG-style functional representation (f-struCture), and the semantic component maps from this representation to sets of assertions in the interface language of the CYC knowledge base [Lenat et al. 90]. Semantic rules map partial semantic interpretations onto other partial interpretations. They consist of a left-hand side and a right-hand side, each consisting of one or more templates, plus a mechanism for mapping an instantiation of either set of templates onto an instantiation of the other set. The intuitive semantics of these rules is that any interpretation that matches the left-hand side licenses a second interpretation matching the right-hand side. For example, we can use the name of an author to refer to his works (\"I read Shakespeare\"), and the corresponding semantic rule states that the existence of an NP denoting an artist licences the use of the same NP to refer to his works. The generation system applies the rules in a backward-chaining direction, while the understanding system runs them forward. A later section contains a fuller discussion of the implementation of the rules, while the next sections discuss their use at runtime. Generation The generator is divided into strategic and tactical components. The former takes a frame as input and creates a description of it based on a set of discriminative 47 properties which are recorded in the KB and indicate which aspects of a frame are likely to be salient. If a comparison class is available, the resulting description uniquely identifies the frame with respect to that class, otherwise it contains default 'interesting' properties. Once it has generated this set of assertions, the strategic component calls the tactical component with a goal Semantics : Syntax, where Semantics consists of the assertions plus the distinguished variable that the utterance is 'about', and Syntax is an f-structure (which may specify no more than the category.) Given this input, the tactical component uses a variant of the semantic-head driven algorithms described by [Calder at al. 89] and [Shieber et al. 89] to generate a phrase whose syntax and semantics match the goal. Before examining this algorithm, we note that in categorial grammars, most of the syntactic information is contained in the lexical definitions of words. For example, the lexical entry for a transitive verb like \"read\" specifies that it takes an object NP to its right and then a subject NP to its left. Any such constituent that takes at least one argument is called a funclor, while a constituent with no arguments is called atomic. Functors and their arguments are combined by a small number of binary rules, and there is also a set of unary rules, which can change the category of a constituent (forming passive verbs out of actives, for example.) Next we define two relationships between constituents and goals: first, a constituent matches a goal if its semantics subsumes the goal's semantics and its syntactic category is the same as the goal's, with possible extra arguments. Thus the transitive verb \"eat\", with category S\\NP/NP, is a syntactic match for the goal category S because it will he an S once it gets its arguments. Second, a constituent satisfies a goal if it has identical semantics and its f-structure is a supergraph of the goal's f-structure. Given this syntactic framework, the algorithm works by peeling off lexical functors and recursing on their arguments until it bottoms out in an atomic constituent. Given a goal, the first step consists of lexical look-up to find an item that matches the goal. Once this item, called the semantic head, is found, the algorithm proceeds both top-down and bottom up. If the semantic head is a functor, it proceeds top-down trying to solve the sub-goal for its argument. Once this sub-goal is satisfied, the algorithm works bottom-up by applying unary grammar rules to to the argument constituent alone, or binary rules to combine it with the functor. When a complete constituent is found which satisfies the goal, we are done. Extension: Goal Revision The algorithm described above assumes a fixed set of choices in the lexicon. It can generate metonymic expressions and noun compounds, but only at the cost of massive lexical ambiguity. We therefore extend it by considering the possibility of goal revision as an alternative to the lexical look-up step s. By running a semantic rule backward, we can map the current goal onto one or more new goals to which the algorithm recursively applies. Satisfying the new goals will generate an expression with the desired meaning and thus indirectly satisfy the original goal. Revision using noun compounding rules leads to a binary decomposition of the original goal, as shown in Figure 2, while metonymy rules result in a unary decomposition, as shown in Figure 3. From this perspective, we note that the lexical look-up of a functor can be viewed as a kind of guided binary decomposition (Figure 1), splitting the original goal into two sub-goals with the knowledge that one of them will be satisfied immediately. [ :o.L I (X HRSCOLOR Y) [ (x Isg BOOK) I (Y EOURLS RED) | (X HRSCRER/OR Z) I (Z EOURLS MRO) ] .t. I (U HRSCOLOR V)l |(X ISR BOOK) | [(X HRSCRERTOR Z)[ l(V E%URLS RED)] [(Z EOURLS .BO) I Figure 1: Lexical Lookup as Decomposition Our extension to the algorithms of [Calder at al. 89] and [Shieber et al. 89] thus amounts to the decision to allow top-down decomposition to be guided by rules as well as lexical items. As we would expect, this is the mirror image of the situation during understanding, where semantic rules are used as an extension to the lexicon in the process of merging translations bottomup. The extended algorithm is shown in the Appendix. Controlling Rule Application The strategic component can control the choice among alternatives through its specification of the goal's syntax. For example, the strategic component can force the use of a compound by providing an appropriately detailed f-structure (i.e., one that specifies the presence of a modifier of category N.) If it does so, no matter whether we are in best-first or all-paths mode, only the compounding alternative will succeed and satisfy the syntactic goal. On the other hand, if the syntactic goal is underspecified, the output (in best-first mode) will aThe notion of goal revision in generation dates back to [Appelt 83] where various conditions could lead to replanning of the input; for recent work incorporating goal revision see [Vaughan et al. 86]. 48 depend on the tactical component's heuristic ordering. In this case, given the default ordering which prefers lexical look-up to noun compounding to metonymy, the tactical component will use a noun compound when lexical look-up fails (i.e., there is no corresponding adjective or preposition). Another result of this default ordering is that metonymy will never fire in the absence of a syntactic specification since there is always another way (unless the lexicon is incomplete) of saying the same thing using words that are in the lexicon. However, the literal alternative is usually more verbose than the metonymous expression, ,so the strategic component can force the use of metonymy by specifying a limit on the number of words the tactical component is allowed to use. Given a limit of 3 words, descriptive phrases like \"a book by Joyce\" will fail, and only the metonymous expression \"Joyce\" will succeed. In best-first mode, substantial improvements in efficiency are possible by re-ordering the alternatives based on the syntactic properties of the goal. For example, it makes sense to try metonymy first if the desired length is significantly less than the number of assertions in the goal's semantics, since each lexical item normally covers only a few assertions. Generating Noun Compounds Suppose we have a Software-Machine rule, stating that if \"y\" denotes any kind of Software and \"x\" a computer, \"a y x\" means a Computer x that CanRunLanguage y. Now consider a goal with semantics (W ISA Computer)(Z Equals Lisp)(W CanRunLanguage Z), distinguished variable W, and syntax NP. There is no lexical item covering all these assertions, or any lexical functor covering part of them (i.e., \"Lisp\" is not in the lexicon as an adjective.) Thus, even in best-first mode, we will end up applying the Software-Machine rule to this goal, resulting in the decomposition shown in Figure 2. GOAL rtP (Q CRMRUMLPJIGURGE Z) (W ISR COMPUTER) (Z EOURLS LIGP) I (Z EOURLS LISP) I I(W ISR CO.PUIER) I isfy the other. Combining the sub-goal solutions yields \"Lisp machine\" as a solution to the original goal. Multiple compounds are handled by repeated invocations of the rules. Suppose we have a MechanismMaintenance rule, stating that if \"x\" denotes a Machine and \"y\" denotes any kind of MaintenanceOperalion, \"x y\" denotes a MaintenanceOperation y with y Maintains x. Given input semantics (Y ISA RepairOperation)(Y Maintains X)(X ISA Computer)(X CanRunLanguage Z)(Z Equals Lisp), with distinguished referent Y, the maintenance rule will eventually fire, generating patterns for a head (Y ISA RepairOperation) and a modifier (X ISA Computer)(X CanRunLanguage Z)(Z Equals Lisp). The head's goal will be satisfied by the entry for \"repair\", but processing of the modifier will invoke the Software-Machine rule, just as in the example above. The output will be \"Lisp machine repair\", with the left-branching structure [[Lisp machine] repair]. For an example of a right-branching compound, suppose we have a Product-Manufacturer rule stating that if \"x\" is the name of a Product and \"y\" is the name of a Company, then \"a y x\" is a Product x that is ManuffacluredBy company y. Given the input (X ISA Computer)(X UanufacturedBy Y)(X CanRunianguage Z)(Y Equals Symbolics)(Z Equals Lisp), the product rule will fire, producing a modifier sub-goal for (Y Equals Symbolics) and a head sub-goal for (X ISA Computer)(X CanRunLanguage Z)(Z Equals Lisp). This time the Software-Machlne rule will be invoked on the head sub-goal, while lexical item \"Symbolics\" will satisfy the modifier sub-goal, and the output will be [Symbolics [Lisp machine]]. Generating Metonymic "
W90-0108 " A general, reusable computational resource has been developed within the Penman text generation project for organizing domain knowledge appropriately for linguistic realization. This resource, called the upper model, provides a domainand task-independent classification system' that supports sophisticated natural language processing while significantly simplifying the interface between domain-specific knowledge and general linguistic resources. This paper presents the results of our experiences in designing and using the upper model in a variety of applications over the past 5 years. In particular, we present our conclusions concerning the appropriate organization of an upper model, its domainindependence, and the types of interrelationships that need to be supported between upper model and grammar and semantics. Introduction: interfacing with a text generation system Consider the task of interfacing a domain-independent, reusable, general text generation system with a particular application domain, in order to allow that application to express system-internal information in one or more natural languages. Internal information needs to be related to strategies for expressing it. This could be done in a domain-specific way by coding how the application domain requires its information to appear. This is clearly problematic, however: it requires detailed knowledge on the part of the system builder both of how the generator controls its output forms and the kinds of information that the application domain contains. A more general solution to the interfacing problem is thus desirable. We have found that the definition of a mapping between knowledge and its linguistic expression is facilitated if it is possible to classify any particular instances of facts, states of affairs, situations, etc. that occur in terms of a set of general objects and relations of specified types that behave systematically with respect to their possible linguistic realizations. This approach has been followed within the PENMAN text generation system [Mann and Matthiessen, 1985; The Penman Project, 1989] where, over the past 5 years, we have been developing and using an extensive, domainand task-independent organization of knowledge that supports natural language generation: this level of organization is called the upper model [Bateman et aL, 1990; Mann, 1985; Moore and Arens, 1985]. The majority of natural language processing systems currently planned or under development are now recognizing the necessity of some level of abstract 'semantic' organization similar to the upper model that classifies knowledge so that it may be more readily expressed linguisticaUy. 1 However, they mostly suffer from either a lack of theoretical constraint concerning their internal contents and organization and the necessary mappings between them and surface realization, or a lack of abstraction which binds them too closely with linguistic form. It is important both that the contents of such a level of abstraction be motivated on good theoretical grounds and that the mapping between that level and linguistic form is specifiable. Our extensive experiences with the implementation and use of a level of semantic organization of this kind within the PENMAN system now permit us to state some clear design criteria and a well-developed set of necessary functionalities. The Upper Model's Contribution to the Solution to the Interface Problem: Domain independence and reusability The upper model decomposes the mapping problem by establishing a level of linguistically motivated knowledge organization specifically constructed as a reponse XIncluding, for example: the Functional Sentence Structure of XTRA: [Allgayer et al., 1989]; [Chen and Cha, 1988]; [Dahlgren et al., 1989]; POLYGLOSS: [Emele et ai., 1990]; certain of the Domain and Text Structure Objects of SPOKESMAN: [Meteer, 1989]; TRANSLATOR: [Nixenberg et aL, 1987]; the Semantic Relations of ~UROTa^-D: [Steiner et al., 1987]; JANUS: [Weischedel, 1989]. Space naturally precludes detailed comparisons here: see [Bateman, 1990] for further discussion. 54 to the task of constraining linguistic realizations2; generally we refer to this level of organization as meaning rather than as knowledge in order to distinguish it from language-independent knowledge and to emphasize its tight connection with linguistic forms (cf. [Matthiessen, 1987:259-260]). While it may not be reasonable to insist that application domains organize their knowledge in terms that respect linguistic realizations -as this may not provide suitable orgunizations for, e.g., domain-internal reasoning -we have found that it is reasonable, indeed essential, that domain knowledge be so organized if it is also to support expression in natural language relying on general natural language processing capabilities. The general types constructed within the upper model necessarily respect generalizations concerning how distinct semantic types can be realized. We then achieve the necessary link between particular domain knowledge and the upper model by having an application classify its knowledge organization in terms of the general semantic categories that the upper model provides. This does not require any expertise in grammar or in the mapping between upper model and grammar. An application needs only to concern itself with the 'meaning' of its own knowledge, and not with fine details of linguistic form. This classification functions solely as an interface between domain knowledge and upper model; it does not interfere with domain-internal organization. The text generation system is then responsible for realizing the semantic types of the level of meaning with appropriate grammatical forms, s Further, when this classification has been established for a given application, application concepts can be used freely in input specifications since their possiblities for linguistic realization are then known. This supports two significant functionalities:  interfacing with a natural language system is radically simplified since much of the information specific to language processing is factored out of the input specifications required and into the relationship between upper model and linguistic resources;  the need for domain-specific linguistic processing rules is greatly reduced since the upper model provides a domain-independent, general and reusable conceptual organization that may be used to classify all domain-specific knowledge when linguistic processing is to be performed. ~Although my discussion here is oriented towards text generation, our current research aims at fully bi-directional linguistic resources [Kasper, 1988; Kasper, 1989]; the mapping is therefore to be understood as a bi.directional mapping throughout. 3This is handled in the PeNM*N system by the grammar's inquiry semantics, which has been described and illustrated extensively elsewhere (e.g., [Bateman, 1988; Mann, 1983; Matthiessen, 1988]). An example of the simplification that use of the upper model offers for a text generation system interface language can be seen by contrasting the input specification required for a generator such as MUMBLE-86 [Meteer el al., 1987] -which employs realization classes considerably less abstract than those provided by the upper model -with the input required for Penman. 4 Figure 1 shows corresponding inputs for the generation of the simple clause: Fluffy is chasing little mice. The appropriate classification of domain knowledge concepts such as chase, cat, mouse, and little in terms of the general semantic types of the upper model (in this case, directed-action, object, object, and size respectively -for definitions see: [Bateman et al., 1990]) automatically provides information about syntactic realization that needs to be explicitly stated in the MUMBLE-86 input (e.g., S-V-O_two-explicit-args, rip-common-noun, restrictive-modifier, adjective). Thus, for example, the classification of a concept mouse as an object in the upper model is sufficient for the grammar to consider a realization such as, in MUMBLE-86 terms, a general-np with a particular np-common-noun and accessories of gender neuter. Similarly, the classification of chase as a directed-action opens up linguistic realization possibilities including clauses with a certain class of transitive verbs and characteristic possibilities for participants, corresponding nominalizations, etc. Such low-level syntactic information is redundent for the PENMAN input. The further domain-independence of the upper model is shown in the following example of text generation control. Consider two rather different domains: a navy database of ships and an expert system for digital circuit diagnosis. 5 The navy data base contains information concerning ships, submarines, ports, geographical regions, etc. and the kinds of activities that ships, submarines, etc. can take part in. The digital circuit diagnosis expert system contains information about subcomponents of digital circuits, the kinds of connections between those subcomponents, their possible functions, etc. A typical sentence from each domain might be: circuit domain: The faulty system is connected to the input navy domain: The ship which was inoperative is sailing to Sasebo The input specifications for both of these sentences are shown in Figure 2. These specifications freely intermix upper model roles and concepts (e.g., domain, 'Note that this is not intended to single out MUMBL~-88: the problem is quite general; cf. unification-based fframeworks such as [McKeown and Paris, 1987], or the Lexical Functional Grammar (LFG)-based approach of [Momma and DSrre, 1987]. As mentioned above, the current developments within most such approaches are now considering extensions similar to that covered by the upper model. SThese are, in fact, two domains with which we have had experience generating texts using the upper model. 55 (general-clause :head (CHASFES/S-V-0_two-explicit -args (genereL1-np :head (rip-proper-name 'Fluffy\") : accessories ( : number singular : gender masculine :person third : determiner-policy no-determiner) ) (general-np :head (np-common-noun \"mouse\") : accessories ( : number plural : gender neuter : person third : detei~miner-policy init iall y-inde f init e) : further-specifications ( ( : attachment-function restrictive-mod/fier : specification (predication-to-be *self* (adjective \"little\"))) )) ) :accessories (:tense-modal present :progressive :unmarked) ) .Input to MUMSLE-86 for the clause: Fluffy is chasing little mice from: Meteer, McDonald, Anderson, Forster, Gay, Huettner, and Sibun (1987) (e / chase :actor (e / cat :name Fluffy) :actee (m / mouse :size-ascription (s / little) :lultiplicity-q multiple : singulaxit y-q nonsinbmlar) : tense present-progressive) Corresponding input to PENMAN Figure 1: Comparison of input requirements for MUMBLE-86 and PENMAN range, property-ascription) and the respective domain roles and concepts (e.g., system, faulty, input, destination, sail, ship, inoperative). Both forms are rendered interpretable by the subordination of the domain concepts to the single generalized hierarchy of the upper model. This is illustrated graphically in Figure 3. Here we see the single hierarchy of the upper model being used to subordinate concepts from the two domains. The domain concept system, for example, is subordinated to the upper model concept object, domain concept inoperative to upper model concept quality, etc. By virtue of these subordinations, the grammar and semantics of the generator can interpret the input specifications in order to produce appropriate linguistic realizations: the upper model concept object licenses a particular set of realizations, as do the concepts quality, material-process, etc. Our present upper model contains approximately 200 (el / connects :domain (v2 / system : relations (v3 / property-ascription : domain v2 :range (v4 / faulty))) :range (v5 / input) : tense present) Input for digital circuit example sentence: The faulty system is connected to the input (el / sail :actor (v2 / ship : relat ions (v3 / property-ascription : domain v2 :range (v4 / inoperative) : tense past) :destination (sasebo / port) :tense present-progressive) Input for navy example sentence: The ship which was inoperative is sailing to Sasebo Figure 2: Input specifications from navy and digital circuit domains such categories, as motivated by the requirements of the grammar, and is organized as a structured inheritance lattice represented in the LOOM knowledge representation language [MacGregor and Bates, 1987]. Generally, the upper model represents the speaker's experience in terms of generalized linguistically-motivated 'ontological' categories. More specifically, the following information is required (with example categories drawn from the current PENMAN upper model):  abstract specifications of process-type/relations and configurations of participants and circumstances (e.g., NO NDIRECTEDACTION, ADDRESSEE-ORIENTED-VERBAL-PROCESS, ACTOR, SENSER, RECIPIENT, SPATIO-TEMPORAL, CAUSAL-RELATION, GENERALIZED-MEANS),  abstract specifications of object types, for, e.g., semantic selection restrictions (e.g., DECOMPOSABLE-OBJECT, ABSTRACTION, PERSON, SPATIAL-TEMPORAL),  abstract specifications of quality types, and the types of entities which they may relate (e.g., BEHAVIORALQUALITY, SENSE-AND-MEASURE'QUALITY, STATUSQUALITY),  abstract specifications of combinations of events (e.g., DISJUNCTION, EXEMPLIFICATION, RESTATEMENT). These are described in full in [Bateman et al., 1990]. Appropriate linguistic realizations are not in a oneto-one correspondence with upper model concepts, however. The relationship needs to be rather more complex and so the question of justification of upper model concepts and organization becomes crucial. 56 I Figure 3: Upper model organization reuse with differing domains Degree of Abstraction vs. Linguistic Responsibility The general semantic types defined by a level of meaning such as the upper model need to be 'linguistically responsible', in that mappings between them and linguistic form may be constructed. In addition, to be usable by an application, they must also be sufficiently operationalizable so as to support consistent coding of application knowledge. Both of these requirements have tended to push the level of organization defined closer towards linguistic form. However, it is also crucial for this organization to be su~ciently abstract, i.e., removed from linguistic form, so that it is possible for an application to achieve its classification purely on grounds of meaning. It is thus inadequate to rely on form-oriented criteria for upper model construction because grammatical classifications are often non-isomorphic to semantic classifications: they therefore need to deviate from semantic organization in order to respect the syntactic criteria that define them. Reliance on details of linguistic realization also compromises the design aim that the applications should not be burdened with grammatical knowledge, e eThis is also resonant with the design aim in text generation that higher level processes -e.g., text planners should not need direct access to low level information such as the grammar [Hovy et al., 1988]. For descriptions of all these Thus, the level of abstraction of an upper model must be sufficiently high that it generalizes across syntactic alternations, without being so high that the mapping between it and surface form is impossible to state. This tension between the requirements of abstractness and linguistic responsibility presents perhaps the major point of general theoretical difficulty and interest for future developments of upper model-like levels of meaning. Without a resolution, substantive progress that goes beyond revisions of what the PENMAN upper model already contains is unlikely to be achieved. It is essential for constraints to be found for what an upper model should contain and how it should be orga. nized so that an appropriate level of abstraction may be constructed. Constraining the Organization of an Upper Model Figure 4 sets several methodologies have been pursued for uncovering the organization\" and contents of a level of meaning such as an upper model, with examples of approaches that have adopted them, along the continuum of abstraction from linguistic form to abstract ontology. While the problem of being too bound to linguistic form has been mentioned, there are also severe problems with attempts to construct an upper model independent of form and motivated by other criteria, e.g., a logical theory of the organization of knowledge per se. Without a strong theoretical connection to the linguistic system the criteria for organizing an abstraction hierarchy remain ill-specified; there is very little guarantee that such systems will organize themselves in a way appropriate for interfacing well with the linguistic system. 7 An alternative route is offered by the approaches in the middle of the continuum, i.e., those which abstract beyond linguistic form but which still maintain a commitment to language as a motivating force. This is further strengthened by the notion, now resurgent within current linguistics, that the organization of language informs us about the organization of 'knowledge' (e.g., [HaUiday, 1978; Jackendoff, 1983; Langacker, 1987; Matthiessen, 1987; Talmy, 1987]): that is, the relation between grammar and semantics/meaning is not arbitrary. Detailed theories of grammar can then be expected to provide us with insights concerning the organization that is required for the level of meaning. We have found that the range of meanings required to support one particular generalized functional region of distinctions in detail, see the PENMAN documentation [The Penman Project, 1989]. 7Furthermore, the experience of the JANUS project (e.g., [Weischedel, 1989]) has been that the cost of using a sufficiently rich logic to permit axiomatization of the complex phenomenon required is very high, motivating augmentation by an abstraction hierarchy very similar to that of the upper model and facing the same problem of definitional criteria. 57 nonlinguistic linguistic reahty knowledge meaning form cognitive'psychological' situational -'socio/psycho-logical' grammatical semantics inquiry semantics clause-based lexicai semantics word senses word-based syntactic realization classes syntax Weischedel (1989) Langacker (1987) Steiner (fc) Halllday & Matthiessen (fc) PENMAN UPPER MODEL Jackendoff (1983), LVG Mel'euk & ~holkovskij (1970) Steiner et al. (1987) LFG Figure 4: Sources of motivations for upper model development the grammar developed within the PENMAN system provides a powerful set of organizing constraints concerning what an upper model should contain. It provides for the representation of'conceptual' meanings at a high level of abstraction while still maintaining a mapping to linguistic form. This functional region corresponds with the Systemic Functional Linguistic notion of the experiential metafunction [Matthiessen, 19877], one of four generalized meaning types which are simultanously and necessarily made whenever language is used. Any sentence must contain contributions to its function from all four 'metafunctions' R each metafunction providing a distinct type of constraint. The value of this factorization of distinct meaning types as far as the design of an upper model is concerned can best be seen by examining briefly what it ezcludes from consideration for inclusion within an upper model: i.e., all information that is controlled by the remaining three metafunctions should not be represented. The logical metaf~nction is responsible for the construction of composite semantic entities using the resources of interdependency; it is manifested in grammar by dependency relationships such as those that hold between the head of a phrase and its dependents and the association of concepts to be expressed with particular heads in the sentence structure. The removal of this kind of information permits upper model specifications to be independent of grammatical constituents and grammatical dominance relations. This relaxes, for example, the mapping between objects and processes at the upper model level and nominals and verbals at the grammatical level, enabling generalizations to be captured concerning the existence of verbal participants in nominalizations, and permits the largely textual variations shown in (1) and (2) 8 to be removed from the upper model coding. (1) It will probably rain tomorrow It is fikely that it will rain tomorrow SExample taken from [Meteer, 1988]. There is a high probability that it will rain tomorrow (2) independently in a way that is independent No change in upper model representation or classification is required to represent these variations. This can be seen more specifically by considering the following PENMAN input specification that uses only upper model terms: ((cO / came-effect : domain discharge : range breakdown) (discharge / directed-action :actee (electricity / substance)) (breakdoen / nondirected-action :actor (system / object))) This states that there are two configurations of processes and participants -one classified as an upper model directed-action, the other as a nondirected-action -which are related by the upper model relationship cause-effect. Now, the assignment of concepts to differently 'ranked' heads in the grammar governs realization variants including the following: Electricity being discharged resulted in the system breaking down. Because electricity was discharged, the system broke down. Because of electricity being discharged the system broke down. ... the breakdown of the system due to an electrical discharge... Electricity was discharged causing the system to break down. ... an electrical discharge causing the breakdown of the system... etc. Many such 'paraphrase' issues are currently of concern within the text generation community (e.g., [Meteer, 1988; Iordanskaja et al., 1988; Bateman and Paris, 1989; Bateman, 1989]). The textual metafunction is responsible for the creation and presentation of text in context, i.e., for estab58 lishing textual cohesion, thematic development, rhetorical organization, information salience, etc. The removal of this kind of information allows upper model specifications to be invariant with respect to their particular occasions of use in texts and the adoption of textually motivated perspectives, such as, e.g., theme/rheme selections, definiteness, anaphora, etc. Thus, with the same input specification as above, the following variations are supported by varying the textual constraints: It was the electricity being discharged that resulted in the system breaking down. The discharge of electricity resulted in the system breaking down. The system breaking down -the electricity being discharged did it! etc. These textual variations are controlled during the construction of text (cf. [Matthiessen, 1987; Dale, 1989; Hovy and McCoy, 1989; Meteer, 1989; Bateman and Matthiessen, 1990]) and, again, are factored out of the upper model. The interpersonal metafunction is responsible for the speaker's interaction with the listener, for the speech act type of an utterance, the force with which it is expressed, etc. Thus, again with the same input specification, the following variants are possible: Did electricity being discharged result in the system breaking down? Electricity being discharged resulted surprisingly in the whole damn thing breaking down. 1 rather suspect that electricity being discharged may have resulted in the system breaking down. etc. The metafunctional factorization thus permits the upper model to specify experiential meanings that are invariant with respect to the linguistic alternations driven by the other metafunetions. That is, a specification in upper model terms is consistent with a set of linguistic realizations that may be regarded as 'experiential paraphrases': the specification expresses the 'semantic' content that is shared across those paraphrases and often provides just the level of linguistically decommitted representation required for nonlinguistically oriented applications. Generation of any unique surface realization is achieved by additionally respecting the functional constraints that the other metafunctions bring to bear; particular surface forms are only specifiable when a complete set of constraints from each of the four metafunctions are combined. The application of these constraints is directly represented in the PENMAN grammar, which provides for the perspicuous and modular integration of many disparate sources of information. The interdependencies between these constraints and their conditions of applicability are also directly represented in the grammar. This organization of the grammar allows us to construct a rather abstract upper model while still preserving the necessary mapping to linguistic form. The value of achieving the abstract specification of meaning supported by the upper model is then that it permits a genuinely form-independent, but nevertheless form-constraining, 'conceptual' representation that can be used both as a statement of the semantic contents of an utterance and as an abstract specification of content for application domains that require linguistic output. Summary and Conclusions A computational resource has been developed within the PENMAN text generation project that significantly simplifies control of a text generator. This resource, called the upper model, is a hierarchy of concepts that captures semantic distinctions necessary for generating natural language. Although similar levels of abstract semantic organization are now being sought in many natural language systems, they are often built anew for each project, are to an unnecessary extent domain or theory specific, are required to fulfill an ill-determined set of functionalities, and lack criteria for their design. This paper has presented the results of our experiences in designing and using the upper model in a variety of applications; in particular, it presented our conclusions concerning the appropriate source of constraints concerning the organization of an upper model. We have found that restricting the information contained in an upper model to experiential meaning has significantly improved our understanding of how a semantic hierarchy should be organized and how it needs to relate to the rest of the linguistic system. We strongly feel, therefore, that subsequently constructed semantic organizations should follow the guidelines set out by the metafunctional hypothesis; the factorization that it provides concerning what should, and should not, be represented in an 'abstract semantic knowledge' hierarchy supports functionalities well beyond those envisioned in current text generation/understanding systems. "
W90-0109 "Abstract Linguistic Resources for Text Planning Marie W. Meteer BBN Systems & Technologies Corporation 10 Moulton Street Cambridge, Massachusetts 02138 MMETEER@BBN.COM   Linguistic Resources for Text Planning Marie W. Meteer BBN Systems & Technologies Corporation 10 Moulton Street Cambridge, Massachusetts 02138 MMETEER@BBN.COM Abstract In this paper, I define the notion of an abstract linguistic resource which reifies as a term for use by the text planner just those combinations of concrete linguistic resources (the words, morphological markings, syntactic structures, etc. that actually appear in a stream of text) that are expressible. I present a representational level, the Text Structure, which is defined in these abstract linguistic terms and which mediates and constrains the commitments of a text planner to ensure that the utterance being planned will be expressible in language. "
W90-0110 " This paper claims that reliance on discourse focus to guide the production of rhetorically structured texts is insufficient over lengthier stretches of prose. Instead, this paper argues that at least three distinct attentional constraints are required: discourse focus [Sidner, 1979, 1983; Grosz and Sidner, 1986], temporal focus [Webber, 1988], and a novel notion of spatial focus. The paper illustrates the operation of this tripartite theory of focus in a computational system (TEXPLAN) that plans multisentential text. Introduction Effective generation of prose demands not only knowledge of rhetorical structure but also rich models of entities, events and states, knowledge of tense and aspect, and mechanisms to track focus of attention with respect to discourse, time, and space. McKeown [1982] used discourse focus (DF) [Sidner, 1979, 1983] to guide the selection, order, and realization of rhetorical schema-based descriptions of database contents. McKeown suggested the following focus shift preferences to mediate among competing propositional content: 1. shift DF to an entity mentioned in the previous proposition 2. maintain current DF 3. resume a past DF 4. shift DF to an entity most related to the current DF The three global registers (past, current, and potential focus) tracked DF and were updated by examining the content of a rhetorical proposition (instantiated with information from a knowledge base) guided by the type of the rhetorical predicate (e.g., identification, attributive). In contrast to schema-based systems, recent work based on Rhetorical Structure Theory (RST) [Mann and Thompson, 1987] attempts to produce effective text using plan-based strategies. Only Hovy's [1988] implementation of RST has examined the task of conveying events and states. Hovy's [1988] \"structurer\" uses his sequence RST operator to produce the following narration of events in a naval domain (where C4 indicates a condition or level of operational readiness): Knox, which is C4, is en route to Sasebo. Knox, which is at 18N 79E, heads SSW. It arrives on 4/24. It loads for 4 days. To produce this text, Hovy's sequence operator is given a beginning \"action.\" The nucleus of the sequence operator allows the text to \"grow\" and indicate the circumstances, attributes, and/or purpose of this action. Similarly, the satellite of the sequence operator allows the text to indicate the attributes and/or details of the next contiguous action in some sequence. The satellite also includes a recursive call to the sequence operator for the next action. Unfortunately, Hovy's operators, like text schema, fail to indicate what effects these orderings or the addition of information at growth points have on the hearer. Therefore, they fail to characterize the motivation for selecting among the different arrangements that narrative employs to achieve specific effects on the hearer (e.g., creating interest, suspense, or mystery). In addition, the plan operator does not consider states as first order objects in some causal chain (the fact \"Knox is C4\" is just an attribute extending off the \"en route\" event). This is important because states have complex relations (e.g., enablement, causation) to other states and events in the world. Finally, sequences are assumed to be contiguous and yet events are often simultaneous or overlapping in time [Allen, 1984]. This purely RST-based approach was improved upon by Hovy and McCoy [1989] by incorporating Focus Trees [McCoy and Cheng, 1988] to guide the ordering and interrelationships of sentence topics. This combined approach produced: With readiness C4, Knox is en route to Sasebo. It is at 79N 18E heading SSW. It will arrive 4/24 and will load for four days. Text coherence is improved not only by regrouping content (a result of restrictions on the traversal of the Focus Tree) but also by using tensed verbs (e.g., future tense of \"arrive\" in the last utterance) to explicitly indicate 70 the temporal relations among events. Unfortunately, no details of how this tense is generated are provided. Furthermore, examination of human generated prose indicates that not only DF but additional constraints on temporal focus and spatial focus are necessary to produce lengthier prose. Therefore, the remainder of this paper first details a tripartite theory of focus and proposes focus shift rules. Next an ontology of events and states is introduced that serves as the basis for a model of tense and aspect which, guided by temporal focus, is used to verbalize events and states. The temporal organization and realization of events is exemplified in the context of report generation. Finally, an example from a route planner is given to illustrate spatial organization and the use of spatial focus. Discourse Focus, Temporal Focus, and Spatial Focus Like Hovy [1988], TEXPLAN uses a hierarchical planner to select, structure, and order propositional content using a library of plan operators (detailed in a subsequent section). To achieve a given discourse goal (e.g., get the reader to know about an event), the planner selects among competing plan operators using general plan operator selection heuristics [Moore, 1989] such as prefer plan operators that meet all preconditions, that have fewer subplans, that have fewer new variables and so on. The leaf nodes of the resulting text plan are speech acts with associated propositional content in the form of rhetorical predicates [c.f. McKeown, 1982]. As in McKeown's TEXT, TEXPLAN tracks past, current, and potential discourse focus (DF) in global registers. When the text planner selects a particular rhetorical proposition, the attentional mechanism extracts the default discourse focus (a position associated with each rhetorical predicate) and updates the global registers. This focus information is then used to guide surface choice. In contrast to DF, Webber [1988] proposed Temporal Focus (TF) as the event currently being focused on temporally and suggests that TF is used to integrate events into some evolving spatio-temporal event/situation structure. TF can shift depending on the relations that hold between events and their times of occurrence. Webber [1988] suggests three TF shifts: maintenance, forward, and backward. Nakhimovsky [1988] classifies local TF shifts as: forward, sideways, and backward \"micromoves\". Forward and backward shifts correspond to introducing the consequence or preparatory phases of events [Moens and Steedman, 1988]. Backward shifts start a new discourse segment. In TEXPLAN, TF indicates the Reichenbachian [1947] reference time. TF shifts (local or micromoves) are implemented via the plan operators and are ordered as follows: 1. Maintain current 'IF (maintenance) 2. TF progresses \"naturally\" forward (progression) 3. Shift TF to a simultaneous event/state (lateral shift) In addition two other long distance temporal shifts are possible but are not addressed in the current implementation: 4. Shift TF to a prior event/state. (flashback) 5. Shift \"IF to a distant future event/state. (flashforward) Temporal shifts are conveyed to the reader in part by verb tense and aspect as in the use of future tense in \"John just arrived. He was in an accident yesterday and ...\" Temporal shifts are also indicated by adverbials (e.g., \"five minutes later\"), explicit references to time (\"at seven p.m.\"), and cue words (e.g., \"simultaneously\"). TEXPLAN tracks TF by recording pointers to events that appear in the propositional content selected by the text planner just as it records DF from selected propositional content following McKeown (1982). As with DF, past, current, and potential temporal focus registers are updated after each utterance. Just as discourse can be topically and temporally organized, psychologists have observed that humans utilize spatial organizations, for example when when people describe their apartments [Linde and Labov, 1975]. Shifts analogous to those of DF and TF can occur along the dimension not of discourse or time but rather space. I define spatial focus (SF) as the current entity or group of entities (and its/their associated spatial location) that the reader is attending to in space. The notion of spatial focus is related but distinct from Conklin's (1983) notion of visual saliency. Visual saliency is the noteworthiness (from one perspective) of an entity in relation to a set of static objects. Spatial focus, in contrast, refers to a currently focused entity (a \"moving target\") that is spatially related to the other entities currently in the background (static entities) or foreground (dynamic entities). Just as DF and TF follow regular shifts, the following ordered legal shifts appear to govern SF: 1. Maintain the current SF 2. Shift SF to an entity spatially related to the current SF 3. Shift SF to some distant point or region. Shifts in rule 2 can be relational (e.g., behind, in-front-of, left-of, right-of, above, below, on-top-of, etc.) or in terms of distance (e.g., \"five miles away\"). Shifts in rule 3 signal a new discourse segment. Just as TF can refer to points or intervals of time, SF can refer to either a point in space (\"At 23* latitude 5  longitude\"), a region (e.g., \"In Chesterville today .... \") or a set of points or regions (analogous to discourse focus spaces [Grosz, 1977]). After each utterance, by examining the underlying propositional content TEXPLAN updates global registers that encode the past, current, and potential spatial foci. In the current implementation, the system prefers topical over causal over temporal over spatial orderings. The next sections illustrate the input to the generator, and how TEXPLAN uses the notions of Reichenbachian time and temporal focus to narrate events and states. By 71 tracking TF and exploiting the temporal information in the underlying event/state model, TEXPLAN is able to select, order, and linguistically realize events and states. The realization component of the system selects proper verb tense and aspect and indicates shifts in TF, for example, through the use of adverbials. A final section illustrates the use of SF in locative instruction (i.e., route plans). Event and State Ontology As Hovy and McCoy's example in the introduction illustrates, more sophisticated representations of verb tense and aspect is key to generating coherent narrative text. This demands a more sophisticated representation of events, states, and their relationship to tense and aspect. Representing and linguistically realizing events concerns issues of temporality, causality, and enablement as well as verb tense and aspect. Discussion of noninstantaneous events dates at least to Aristotle's distinction between process (energia) and state (stasis) and these issues have been the focus of attention in philosophy, linguistics, and computational linguistics [c.f. Allen, 1988]. While an ontology of events and states is beyond the scope of this paper, it is necessary as a starting point for generation to indicate the nature of the underlying propositional content and so we make a few intuitive distinctions. Events are physical, linguistic, or psychological happenings at some time and place. States, in contrast, refer to perpetual or temporally unbounded conditions such as the physical, psychological, or emotional state of an agent or entity. States include relations that hold between agents or entities (e.g., possession, ownership) [Nakhimovsky, 1988]. This classification is but one (conceptual) classification of events and states. Ehrich [1986], for example, uses the features of duration, resultativity, and intentionality to produce an orthogonal categorization. Processes, in contrast to states, involve changes or transformations over the interval for which they hold and often have some associated rate of progress toward a goal or a rate of consumption of resources [Nakhimovsky, 1988]. Nakhimovsky makes a key distinction between events, processes and states: For a linguist, the distinction between event-process is one of aspecmal perspective: \"The term 'process' means a dynamic situation viewed imperfectively, and the term 'event' means a dynamic situation viewed perfectively\" (Comrie, 1976: 51). The distinction process-state is one of aspectual class. In this paper the term event is used to refer both to an instantaneous event (e.g., snap, click, wink) as well as events with a duration which can be viewed perfectively (event) or imperfectively (process). A collection of related events and states constitutes an event/state network analogous to Webber's [1987] event/situation structure. This network of events and states serves as the basis for generation in TEXPLAN. In the input to the text generator, each event or state is represented in a frame-like structure. Events and states have associated attributes, roles, and relationships. The term attributes refers to characteristics local to the event or state such as its time of occurrence (a point or interval), its type (e.g., physical, linguistic), and any constituents (i.e,, subevents or substates). Roles refer to the semantic role an entity plays in the event or state (e.g., agent, patient). Finally, relations refer to the associated enablement(s), cause(s), and effect(s) of an event or state. Tense and Aspect The rich notions of time associated with events and states are conveyed in part through verb tense. English verb tense (e.g., simple past, present, and future; and past, present, and future perfect) relies on a tripartite notion of time which includes: the point or time at which the utterance is spoken (S), the point at which the event happens (E), and the point of reference (R) [Reichenbach, 1947]. R is the time \"talked about\" or \"focused on\" and in TEXPLAN corresponds to the above notion of TF. Because the absolute time of the event (E) appears in the event structure, the linguistic realization component can select the appropriate verb tense by reasoning about the time the speaker is narrating (S) (e.g., \"now\") and the time the overall narration focuses on (R). This contrasts with verb choice based solely on the underlying event structure [e.g., Kalita, 1989, p. 410]. Table 1 relates E, R, and S to tense where \" . Time E=R=S E=R  Tense Example simple present \"John eats.\" simple past \"John ate the beans.\" simple future \"John will eat the beans.\" present perfect \"John has eaten.\" past perfect \"John had eaten the beans.\" future perfect \"John will have eaten.\" Table 1 This point-based time representation could be extended to consider time intervals [Allen, 1984]. TEXPLAN's sentence generator uses an admittedly simplified 1 prototypical verb sequence following Winograd [1983] (e.g., Modal + Have + Bel + Be2 + Main-verb). Individual verbs include both modals such as \"will\", \"can\", \"could\" (which have only one form), and ordinary verbs which have five basic forms in third person, singular: infinitive (e.g., \"to walk\"), simple present (\"walks\"), simple past (\"walked\"), present participle (\"walking\"), and past participle (\"walked\"). Future tense does not have its own syntactic form and is implemented by the modals \"will\" or \"shall\". In contrast to tense, ~pect is a grammatical category of the verb implemented by affixes, auxiliaries, and so on lMatthiesen (1984) has discussed more general tense assignment. 72 [Nakhimovsky, 1988]. This arises from the temporal characteristics of the underlying event (i.e., point versus interval), the relationship of the reference time (R) to event time (E) (e.g., E < R indicates perfective; E = R imperfective) and the lexical aspect of the verb (e.g., the progressive \"I am eating\" versus the culmination \"I am finishing\"). The current implementation addresses only the first two cases. Perfective sequences use Have (e.g., has taken), progressive use Bel (e.g., was taking), and passive use Be2 (e.g., was taken).  "
W90-0111 "<NoAbstract>"
W90-0112 "Philadelphia, PA 19104 Abstract This paper advances the hypothesis that any text planning task relies, explicitly or implicitly, on domainspecific text planning knowledge. This knowledge, \"domain communication knowledge\", is different from both domain knowledge and general knowledge about communication. The paper presents the text generation system Joyce, which represents such knowledge explicitly. The Joyce Text Generation System The Joyce text generation system is a part of the software design environment Ulysses (Korelsky and Ulysses Staff, 1988; Rosenthal el al., 1988). Ulysses is a graphical environment for the design of secure, distributed software systems. The design proceeds hierarchically by top-down refinement. A formal specification interface and a theorem prover allow the user to formally verify the flow security of the designed system. Joyce is part of the user interface. Joyce generates different types of texts about software designs:  It generates annotations of the design which are intended to serve as system documentation during and after the design process.  It is used to explain the result of a heuristic security design tool, the \"flow analyzer\". The texts Joyce generates are specifically conceived of as written texts: there is no feature for interactive natural-language explanation. The texts may be several paragraphs long. The text in figure 1 is an annotation of the component \"Host\"; the graphical representation of the first level of the design of the Host is shown in figure 2. (This picture corresponds to the first of the two paragraphs of the text.) The text annotates the software design by describing its structure and interpreting it in terms of its security characteristics. *The research reported in this paper was carried out while the author was at Odyssey Research Associates, Ithaca, NY. It was supported by the Air Force Systems Command at Rome Air Development Center under Contract No. F30602-85-C-0098 Structure of Joyce Joyce consists of three separate modules, which perform distinct tasks and access their own knowledge bases. "
W90-0113 " I'his paper describes GENIE, an object-oriented architecture that generates text with the intent of extending user expertise in interactive environments. Such environments present three interesting goals. First, to provide information within the task at hand. Second to both respond to a user's task related question and simultaneously extend their knowledge. Third, to do this in a manner that is concise, clear and cohesive. Instead of generating text based solely on either discourse goals, intentions, or the domain, we found a need to combine techniques from each. We have developed an object oriented architecture in which the concepts about which we talk (domain entities), the goals that may be accomplished with them (intentions), end the rhetorical acts through which we express them (discourse goals) are represented as objects with localized knowledge end methods. This paper describes how current text planning methods were insufficient for our needs, and presents our object-oriented method as an alternative.  "
W90-0114 "<NoAbstract>"
W90-0115 " In this paper we argue that current generation methodologies are inadequate for determining the high-level structure characteristic of naturally-occurring extended explanations. Our analysis of such explanations indicates that high-level structure composed of a unifying framework and its associated basic blocks must be determined by bottom-up processes that attempt to satisfy speaker, listener, and compositional goals, after which top-down strategies can be used to organize the material about the selected framework. In addition to a description of this structure, this paper describes three types of repetition whose use is dependent on this highlevel structure; their use not only contributes to the cohesiveness of extended explanations, but supports our thesis of the non-recursive nature of the high-level structure. We conclude with an outline of our computational strategy for generating this structure. "
W90-0116 "Abstract In this paper, I present a model of the local organization of extended text. I show that texts with weak rhetorical structure and strong domain structure, such as descriptions of houses, digital circuits, and families, are best analyzed in terms of local domain structure, and argue that global structures that may be inferred from a domain are not always appropriate for constructing descriptions in the domain. I present a system I am implementing that uses short-raTtge strategies to organize text, and show how part of a description is organized by these strategies. I also briefly discuss a model of incremental text generation that dovetails with the model of local organization presented here. Motivation for local organization The approach to organizing extended text described here has both psychological and computational motivation. It aims both to model how people use language and to provide a flexible architecture for a system's language use. In this section, I describe the empirical data that form the basis of this research, and characterize the local organization of the collected texts. In the next two sections, I describe a computational architecture to implement local text organization and discuss its advantages of generality and flexibility, and give an example of how this architecture works. An extended text has a structure; this structure is a description of how the components relate so that sense can be made of the whole. Two sources of this organization are rhetoricial structure, which describes the way elements of the text fit together, and domaitt structure, which describes relations among domain objects. For this research I chose three domains with strong domain structure, and a task--description--with weak rhetorical structure. I have tape-recorded 29 people giving descriptions of house layouts, electronic circuit layouts, and family relationships. Description fragments of a house and of a family, and the questions asked to obtain the descriptions, are given in figure 1. (Because of space considerations, the fragments are somewhat abbreviated.) Many approaches to text organization 1 are based on analyses of text in terms of rhetorical structure. However, there are few segments of text with interesting rhetorical structure in my corpus. For example, an analysis of the texts using Mann and Thompson's (1987) Rhetorical Structure Theory (RST) would result primarily in the relations sequence and joint and would contain few of the the relations like evidence or justify that give RST its descriptive power. Similarly, it is unclear what work a system like that of Grosz and Sidner (1986) would do in analyzing a description. Since the structure of descriptions cannot be analyzed adequately with rhetorical relations, perhaps it can be explained in terms of the domain. Houses, chips, and families are strongly structured. A family's relationships can be captured in a family tree; one might suppose that a description of the family would also be organized in this way. A house can be encoded in a number of ways; for instance, it has a component hierarchy, being composed of rooms composed of furnishings. Linde (1974) has proposed another comprehensive structure for houses: a phrase structure grammar that determines how the rooms may be visited in a traversal of a house layout. Surprisingly, these global, hierarchical domain structures are not exploited in the organization of descriptions in my corpus. While family trees and composition hierarchies can be inferred from descriptions of families and houses, this does not mean that these structures guide the process of organizing them. For instance, my family informants did not simply construct their descriptions by starting at the root of the appropriate tree and doing a depth-first or breadth-first traversal of it. Instead, to select a next family member to talk about, they would apply one of several criteria. Generally, a sibling, spouse, parent, or child would be the next choice, and this might incidentally constitute part of a tree walk. But that this choice is local is evidenced by the next choice, which may not be construable, in any XWhat I call text organization is usually referred to as text planning. 120 on our righthand side would be a door, which leads to Penni's room and you walk in there... and there are two windows, in the...opposite corner from the one in which you enter one' s on the lefthand wall, and one's on the wall that you would be facing then, on the righthand side...of her room, is the closet Fragment of a house description. In response to the question: \"Could you please describe for me the layout of your house.\" there's my mother Katharine, my father John, my sister Penni, and me it's my mother's relatives that we go to see Margaret and Bill, who are Mommy' s... urn...Margaret's my great-aunt so it must be Mommy's aunt Fragment of a family description. In response to the question: \"Can you tell me how everyone who comes to Thanksgiving is related to each other?\" Figure I: Sample descriptions. principled way, as part of the overall structure of the description that one might have postulated at the previous step. Where to begin the family description also appears to be a locally conditioned choice. Informants begin at various points, such as themselves or long-dead progenitrixes, but the majority start their description of the family by mentioning the hostess and host of the Thanksgiving dinner they are attending; we may suppose that at a different time of year the descriptions are likely to start off differently. Further evidence that people do not structure their descriptions using obvious global domain structures may be adduced from examples in which speakers explicitly deny knowledge of such structures, as in the following fragment. and also...tun... Eleanor and Elizabeth come who are...cousins of...all of us...um... I don't know what generation cousins they are Here, the speaker shows by her description of two family members that she does not know her relationship to them, even though it would be clear if a family tree were being used to organize the description (the women in question are in fact first cousins twice removed of the speaker). Genealogical trees, phrase structure grammars, and component hierarchies are useful for succinctly representing information about houses, chips, and families But there is no a priori reason to suppose that a description of such things are the products of such easily articulable schemas or grammars. When we examine texts of the sort that we wish to generate, we must distinguish the mechanisms that direct the process of choosing what to say next from a retrospective description of its result. The texts I have collected can be best analyzed as locally organized by a process of deciding what to say nezt This decision is based principally on what has already been said and what is currently available to say. For example, if one has just mentioned a large window in the kitchen, one can mention whatever is to the left of the window, whatever is to the right of it, which way it is facing, what it looks like, or how it is similar to a window in another room of the house. If one has mentioned an aunt, one can give her name, say whether she is married, mention her sister, enumerate her children, or talk about how much money she earns. The strong domain structure of subjects like houses, chips, and families ensures that a description can be continued from any point: once a description has been started, there is always something, often many things, that can be said next. In structured domains, there is always a default choice for the next thing to say. In spatial domains like houses, spatial proximity provides this default. Everything in a house, be it a room, a wall, or a kitchen appliance, is next to something else. Spatial proximity does not constrain house descriptions, 121 but it ensures that a description does not come to a premature dead end. Descriptions are finite. Though there may always be more to say, there are points at which a description may stop, when the task may be considered accomplished. Linde (1974) proposed s completeness criterion for house descriptions, which is reflected in my data as well as hers. It states that a description may stop any time after all the rooms have been mentioned, but it is not complete if it stops before. A similar criterion holds in the family descriptions collected: they were given in answer to the question, \"Can yoti tell me how everyone who comes to Thanksgiving is related to each other?\" In this case, then, the criterion is mentioning everyone who attends. Knowing how to continue and knowing when to stop together ensure that a description can be generated depending solely on local organization. The strong domain structure of houses and families makes the working of these mechanisms for continuation and termination particularly clear, and thus these domains are a good site for studying this approach to organizing text. However, the local organization of text is also evident in many other uses of language. People's conversation is often locally organized (Levinson, 1983); some interactive systems are currently being designed with this approach (Frohlich & Luff, 1989). Because I am interested not only in how a program may organize text but also in how people do so, I study people speaking rather than people writing. A written text may be edited and reorganized, and this process often involves explicitly thinking about rhetorical structure. A spoken description is likely to require that the speaker organize her text locally--she cannot plan it out ahead of time. Studying spoken text reveals more of the underlying mechanisms of language, because time constraints and the inability to edit what has already been said make post-processing impossible. Computational architecture I am implementing a system that employs local organization of text as described in this paper. The implementation comprises: a semantic net knowledge base; an organizer composed of strategies and metastrategies; and a generator. Local organization is achieved using short-range strategies, each of which is responsible for organizing only a short segment of text, between a word and a clause in length. Until recently, I have employed Mumble-86 (Meteer et al., 1987) as the generator for this system. Construction is underway, however, on a simpler generator that more accurately implements the principles of generation implied by the structure of the organizer. The system is currently implemented only for descriptions of houses; the examples in this section will thus be drawn from the house domain. The knowledge base is a semantic net that encodes the objects, properties, and relations of a domain. The organizer keeps a pointer to the current node in the description. A strategy describes the current node and others related to it via local connections in the network. Strategies are selected sequentially, based on local conditions; if there is no strategy available or if more than one strategy is appropriate, metastrategies (Davis, 1980) resolve the conflict, using a technique similar to universal subgoaling in Soar (Laird, Newell Rosenbloom, 1987). Metastrategies are responsible for control: they sequence and combine strategies. Like strategies, they can cause the production of text. This architecture has several advantages. First, it is flexible: because there is no fixed priority scheme and because strategies are small, the strategies can be combined in a variety of ways to produce texts that are constrained only by the appropriateness of each strategy as determined by the strategies' interaction with the knowledge base. Second, the architecture is extensible: new strategies can easily be added to extend the organizer to different types of text. Finally, the organizer is mainly domain-independent: while some strategies may be particular to houses, most strategies are not. The strategies are applied to the knowledge base and select the items that make up the description. The strategies find the appropriate lexical items(s) for each knowledge base item that is expressed; these lexical items and the knowledge base items themselves are determined by the domain. While the strategies are simple, complex behavior emerges from their interaction with the knowledge base; this locally organizes the extended text. Each strategy falls into one of four classes, with varying degrees of domain independence: discourse cue strategies; linguistic strategies; parameterizable domainindependent strategies; and semi-domain-independent strategies. Figure 2 gives examples of each. Of the domain-independent strategies, discourse cue strategies focus attention, in a way similar to the clue words described by Reichman (1985), and linguistic strategies mention objects and associated properties. mentlon-sallent-object, used to say \"there is a window,\" may as easily express \"there is a penguin\" or \"there is a policy.\" describe-object is similarly allpurpose, and can produce \"the window with two flanking windows\" or \"the man with one black shoe.\" The parameterizable domain-independent strategies have slightly different textual realizations in different domains, but these differences can be captured by parameters. The example given in figure 2 is typical: the strategy is realized as a prepositional phrase in each domain, and only the preposition changes. The semi-domain-independent strategies accomplish tasks such as a sweep that seem particular to the domain, but are similar to tasks in other domains. A sweep begins at an object and names another bearing some spatial relationship to it, and then another object 122 Discourse Cue Strategies introduce/shift-attention \"then,\" \"and,\" \"now\" refer-back/relnforce \"again,\" \"once more\" Linguistic Strategies mentlon-sallent-object \"there is z\" \"we have z\" describe-object \"the z is ~,\" \"the y z\" \"the z (which) has z\" \"the z with z\" Parameterisable Domain-independent Strategies situate \"in the kitchen\" \"during the morning\" \"about the election\" Semi-domain-independent Strategies sweep Enumerate objects connected each to the next by the same relation. follow a path Traverse a natural connection between parts of the knowledge to be described. Figure 2: Strategies. that bears the same relationship to the just-mentioned one, until there are none left; for example, \"to the left of the window is a stove and then a refrigerator\" is a sweep-left. Similar constructions may be found in other domains. A description of one's day may start at some event and mention the next event and the event after that. In this case, the relationship is temporal, rather than physical. Metastrategies select strategies based on the contezt, which comprises: Local Context  What has just been said.  What is immediately connected to the current item in the knowledge base.  What strategies are applicable. Global Context  Discourse and speaker parameters. (For example, a speaker's propensity to mention objects to the fight before objects to the left.) It is here that anything considered a global goal would be encoded.  The completeness criterion. In future implementations, the context may also involve some model of the hearer. This would be part of the local context: what one knows about one's hearer and about what one's hearer knows changes, particularly under the assumption that hearer and speaker are engaged in a two-way interaction. A strategy conflict set contains whatever strategies are currently applicable. If it contains a single strategy, that one is selected. If more than one is in the set, met\"strategies may resolve the conflict by selecting one or by combining some or all of them. Finally, if no strategy presents itself, the met,strategies apply a default strategy. The met\"strategy find-\"interesting\"-llnk is triggered after the introduction of a new topic, which has links to many other items in the knowledge base. What is \"interesting\" or salient depends on:  The domain: In spatial description, objects that are large or have many features are interesting.  The structure of the domain: Objects that are more connected to other objects are more interesting.  The local context: If a window has just been mentioned, there is reason to mention other windows.  The global context: There may be an inclination to mention furnishings but not structural features of the house, or vice versa; there may be differing levels of detail required. 123 Some metastrategies combine strategies. Such metastrategies apply when several strategies are appropriate at some point in the description and there is a felicitous way to combine them. clrcular-sweep is an example: it combines a number of sweep strategies (sweep-left, sweep-right, and sweep-under), and includes additional orientation strategies to orient hearers between sweeps, kltty-corner is a metastrategy that is used to describe s room in which the most salient feature is diagonally opposite the current location. The object that is \"kitty corner\" from it is mentioned first, and the rest of the room is described in relation to it. 2 The first fragment in figure 1 exemplifies the kltty-corner metastrategy. find-new-topic is the default metastrategy just in case there is nothing \"interesting\" to say. In a spatial domain, the default is to select an object to describe using spatial proximity. Example of description organization In this section, I describe the organization of a fragment of a description in my corpus. While the system is not yet fully implemented to handle all the details, this example is sufficiently complex to show the operation of the architecture in selecting appropriate strategies and metastrategies. Though the strategies used are simple, complex choices, varying with context, are made. On the following page are the text and a sketch of the area being described. In the fragment in figure 3, the speaker is describing the bedroom he shares with his wife Carol. Each line of the fragment, in most cases, is the result of a single strategy. The sketch in figure 4 of the bedroom is provided as an aid to the reader in understanding John's description. There is no corresponding representation in the system. The global context used by this speaker includes parameters that predispose him to mention rather than ignore room furnishings, as well as \"stuff\"--small articles than can be found in, on, or near pieces of furniture. The strategy mention-stuff is a particular form of descrlbe-object, and as such is concerned with mentioning associated properties of the object rather than physical proximity; this is suggested by the speaker's typically using the preposition \"with,\" rather than an obviously spatial one. The global context also of course includes the completeness criterion which is unsatisfied throughout this stretch of text. The fragment starts when the speaker has just finished describing the kitchen and the next spatially proximate thing is the door to the bedroom. When the node to be described is a physical object, 2For a fuller treatment of how the system computes deictic and other spatial terms Uke \"kitty corner,\" \"left,\" and \"right,\" see (Sibun & Huettner, 1989). the mention-object strategy is always available; because this object is a door, mention-room is available to talk about the room that the door leads into. The metastrategies resolve this conflict in favor of the more particular mention-room {1}. Because the last strategy used was mentlon-room, mention-salient-object becomes available; if there is a particularly \"salient\" object in a room, descriptions can then be organized around it. As it happens, salience in this domain tends to depend primarily on size; the kingsize bed fits the bill {2}. There are two objects spatially proximate to the bed; one is selected and the strategy mentlon-object is used to mention the endtable {3}. The endtable is connected to several other items in the knowledge base, but because there is a context parameter to mention \"stuffs\" this is what happens next {4}. Now, there are two unmentioned objects spatially proximate to the endtablemthe window and the \"wall\" (which is actually a covered-up chimney). The \"wall\" is mentioned because, like furnishings, it has extent in the room, and the context disposes the process toward mentioning such objects {5}. The local context keeps track of what has just been mentioned; another feature it records is the direction in which the spatial proximity finks have been followed. The \"wall\" is next along the trajectory from the bed through the endtable. The \"wall\" is spatially linked to three things: the window on the endtable side of it, the window that is along the same trajectory that has been followed, and the chests, which are also along that trajectory. The window along the trajectory is the choice selected from these three for two reasons: there is a tendency to maintain trajectories; 3 and the last thing mentioned is part of the structure of the room, as is the window, but not the chests. But this selection of the window presents a problem (at least, we can infer that it presents a problem to the speaker), because this window is connected via a similarity link to the previous window, which has not been mentioned. So the speaker performs a repair consisting of backing up, mentioning the overlooked window, reiterating mention of the wall, and, finally, mentioning the selected window {6}. While the next window is a promising candidate because it is the same sort of thing as that just mentioned, the parameter for mentioning furniture overrides this, and the cedar chests come next in the description {7}, followed by their associated \"stuff\" {8}. The window is again available but so is the bureau, which is the preferred choice because it is furniture {9}. The bureau has spatial proximity links to two windows and the \"small thing,\" as well as links to its \"stuff,\" so the set of available strategies comprises ones that mention each of these things. A metastrategy can resolve this 3Ullmer-Ehrich (1982) notes a similar tendency in the dorm room descriptions that she collected. 124 and then there's the bedroom {1} and there's a huge kingsize bed {2} and there's an endtable next to it {3} with a lamp and a clock rad/o {4} and some stuff of mine like the Boston University cup and...then there's a wall-{5} and then there's a window behind that {6} and there's a wall, and there's another window and there's some...cedar chests of Carol's {7} that have blankets and sheets in them {8} and...there's her bureau {9} in the middle of two windows on either side {10} with all of her makeup on top of it and clothes and there's...a small thing with all her clothes and there's another great big bookshelf and all her spare books and there's a small end table over on her side um...a small digital clock and more kleenex and...OK Figure 3: John's description. ,J. I window I I on0,a ,o J I bed I end table I I window I [ bookshelf [ Qmall thing R Figure 4: Sketch of bedroom. 125 conflict by realizing that strategies for saying that there is an object of the same sort on two different sides of the current object can be combined by saying that the bureau is between the two windows {10}. 4 The description for the rest of the room continues in a manner similar to that already discussed. Incremental generation The organizer described here is composed of strategies that are often responsible for sub-clausal units of text; furthermore, the strategies have already imposed an organization on the text, obviating the need for the generator to do more than enforce gramma~icalitT/. The model of local text organization I van developing is coupled with a model of incremental generation, in which the increments are often smaller than a sentence. (Gazrett's investigation of speech errors (1975) constitutes early work in this area; De Smedt & Kempen (1987), and Kempen & Hoenkamp (1987) discuss a similar, more fully-developed incremental generation project.) A typical generator produces a sentence at a time. However, spoken text is replete with restarts, fragments, and ungrammaticalities. This suggests that not only do people organize their text incrementally, but generate it in increments as well. Usually, an incremental generator is successful in generating grammatical text. The text then / in the kitchen / there is a large window is the result of three sequentially operating strategies: introduce/shltt-attention, situate, and mentionsallent-object. An incremental generator will be able to produce this text in the increments specified by the strategies. A system that generates in strategy-sized increments can result, in principled ways, in ungrammatical text. A common error, exemplified by and a lot of other people / who / I wasn't quite sure what they did can be explained by the operation of the strategies mention-object, add-clausal-modifier, and a set of strategies to express some additional information, which, because a dependent clause has already been started, happens to result in the ungrammatical resumptive pronoun \"they.\" Locally-organized, occasionally ungrammatical text may be prototypical, but it is certainly not the only sort 'Note that the previous window conflict was not resolved by saying that the \"wall\" was between the two windows. The difference can be explained by the observation that the \"wall,\" despite its extent into the room, is a structural object, wld/e the bureau is furniture. of text we wish to generate. To be comprehensive, my system will require some capability to post-process text after it is organized and before it is generated (Hovy, 1989, suggests a post-processing model). Output of my system might also be appropriate input for a text revision system (e.g., Meteer & McDonald, 1986). Related research There are many other projects whose goal is to organize, or plan, extended text. The main difference between these and mine is flexibility and level of organization: most text planning systems rely on global structures to organize paragraph-sized text. These structures, which are usually schemas or plans, constrain the text to reflect particular rhetorical and domain relations, but at the expense of flexibility. My system builds structure locally with no recourse or reference to an overall structure. Through analysis of written texts, McKeown (1985) has developed a small set of schemas, built from rhetorical predicates, that would provide types of descriptions (for example, constituency) for objects in a knowledge base. ParAs has extended this work to use a process trace which derives its structure in part from the domain (Paris & McKeown, 1987; Paris, 1988). This alternative strategy builds and traverses a path through the knowledge base when it is determined, on the basis of a sparse model of user expertise, that a process description of an object is more appropriate than a declarative one; the two strategies may be interleaved. Dale (1989) similarly organizes text by means of a domain structure, in this case, a recipe plan. Rhetorical Structure Theory (Mann & Thompson, 1987) is also drawn from an analysis of written texts; it differs from McKeown's work in that it is composed of a large number of rhetorical relations, rather than a small number of schemas. RST may thus be more flexible, but is is still assumed that the relations will be combined into a single global tree covering an extended text. While most text planning systems have worked on producing single texts in response to queries, some research has been more particularly concerned with interactive text. Much recent work in this area has been for explanation systems (e.g., Maybury, 1989), and some of this work explicitly addresses allowing a human user to ask follow-up questions (Moore & Swartout, 1989). However, such systems still build and use global structures for extended texts. An argument is sometimes made that global structure is needed to capture high-level rhetorical goals in the output text (see Appelt, 1985); Gricean Maxims (Grice, 1975) are often invoked. Hut, as Hovy (1988) points out, \"be polite\" is not a decomposable goal; the objective of being polite is achieved through local decisions. Such local decisions can comfortably be integrated with a model of local organization of text. 126 "
W90-0117 " To computationalists investigating the structure of coherent discourse, the following questions have become increasingly important over the past few years: Can one describe the structure of discourse using interclausal relations? If so, what interclausal relations are there? How many are required? A fair amount of controversy exists, ranging from the parsimonious position (that two intentional relations suffice) to the profligate position (that an open-ended set of semantic/rhetorical relations is required). This paper outlines the arguments and then summarizes a survey of the conclusions of approximately 25 researchers -from linguists to computational linguists to philosophers to Artificial Intelligence workers. It classifies the more than 350 relations they have proposed into a hierarchy of increasingly semantic relations, and argues that though the hierarchy is open-ended in one dimension, it is bounded in the other and therefore does not give rise to anarchy. Evidence for the hierarchy is mentioned, and its relations (which are rhetorical and semantic in nature) are shown to be complementary to the two intentional relations proposed by the parsimonious position. How Many Interclausal Discourse Coherence Relations? This paper proposes an answer to an issue that keeps surfacing in the computational study of the nature of multisentential discourse. It has been argued fairly generally that multisentence texts (specifically, short texts such *This work was supported by the Rome Air Development Center under RADC contract FQ7619-89-03326-0001. as paragraphs) are coherent by virtue of the rhetorical or semantic relationships that hold among individual clauses or groups of clauses (see, for example, [Aristotle 54, Hobbs 79, Grimes 75, Mann & Thompson 88]. In this view, a text is only coherent when the speaker aids the heater's inferential understanding processes by providing clues, during the discourse, as to how the pieces of the text interrelate. Such clues are often cue words and phrases such as \"in order to\" (signalling a purpose for an action) or \"then\" (signalling the next entity in some temporal or spatial sequence); but they can also be shifts in tense and mode (such as in \"She was gone. Had she been there, all would have been well\"), and even appropriate pronominalizations. Various researchers in various intellectual subfields have produced lists of such relations for English. Typically, their lists contain between seven and thirty relations, though the more detailed the work (which frequently means the closer the work is to actual computer implementation), the more relations tend to be named. I have collected the lists of over 25 researchers -from philosophers (e.g., [Toulmin 58]) to linguists (e.g., [Quirk & Greenbaum 73, Halliday 85]) to computational linguists (e.g., [Mann & Thompson 88, Hobbs 79]) to Artificial Intelligence researchers (e.g., [Schank & Abelson 77, Moore & Paris 89, Dahlgren 88]) -amounting to a total of more than 350 relations. The researchers and their lists appear below. In this paper, I will call the assumption of these researchers, namely that some tens of interclausal relations are required to describe the structure of English discourse, the Profligate Position. 128 Unfortunately, the matter of interclausal relations is not simple, and not everyone agrees with this position. These relations are seldom explicitly signalled in the text, and even when they are, they seem to take various forms particular to their use in context. This fact has led some researchers, notably [Grosz & Sidner 86], to question the wisdom of identifying a specific set of such relations. They argue that trying to identify the \"correct\" set is a doomed enterprise, because there is no closed set; the closer you examine interclausal relationships, the more variability you encounter, until you find yourself on the slippery slope toward the full complexity of semantics proper. Thus though they do not disagree with the idea of relationships between adjacent clauses and blocks of clauses to provide meaning and to enforce coherence, they object to the notion that some small set of interclausal relations can describe English discourse adequately. As a counterproposal, Grosz and Sidner sidestep the issue of the structure of discourse imposed by semantics and define two very basic relations, DOMINANCE and SATISFACTIONPRECEDENCE, which carry purely intentional (that is, goal-oriented, plan-based) import. They use these relations in their theory of the structure of discourse, according to which some pieces of the text are either subordinate to or on the same level as other pieces, with respect to the interlocutors' intentions. I will call this position, namely that two interclausal relations suffice to represent discourse structure, the Parsimonious Position. From the point of view of text analysis, the Parsimonious approach seems satisfactory. Certainly one can analyze discourse using the two intentional relations. However, from the point of view of text generation, this approach is not sufficient. Practical experience has shown that text planners cannot get by on intentional considerations alone, but need considerably more rhetorical and semantic information in order to construct coherent text (there are many examples; see [McKeown 85, Hovy 88a, Moore & Swartout 88, Paris 88, Rankin 89, Cawsey 89]). In practical terms, this means that text planning systems require a rich library of interclausal relations. Questions such as  Does one really need semantic and/or rhetorical discourse structure relations?  Just how many such relations are there?  What is their nature? How do they relate to the two intentional relations? will not go away. Until it is resolved to the satisfaction of the adherents both positions, further work on text planning and discourse analysis is liable to continue getting stranded on the rocks of misunderstanding and disagreement. This paper suggests a compromise that hopefully opens up the way for further development. "
W90-0118 "ions and Planning Mechanisms 1 Daniel D. Suthers Department of Computer and Information Science University of Massachusetts Amherst, Massachusetts 0100 3 suthers@cs.umass.edu The utihty of rhetorical abstractions and certain text planning mechanisms were assessed from the standpoint of accounting for how an explainer chooses and structures content under multiple perspectives to meet knowledge communication goals. This paper discusses ways in which they were found to be inadequate, argues for greater emphasis on an epistemological level of analysis, and proposes a mixed architecture matching computational mechanisms to the explanation planning subtasks they are suited for. Introduction Our research is concerned with explanation in its broad sense, as \"the act or process of making plain or comprehensible; elucidation; clarification\" (American Heritage Dicgionary). In many physical science domains, an explanation can be based on a variety of models of the phenomenon being explained. These models differ on the type of properties and relationships emphasized; what ontology is used and whether the phenomenon is described at a macroscopic, microscopic, or atomic granularity; what factors are ignored or assumed to be constant; the use of general statements vs. concrete examples; and in general on what concepts function as primitives, providing the basis for understanding the topic phenomenon. Selection of an appropriate model of the topic is an important aspect of content selection which impacts on the interlocutor's comprehension of the explanation and on its appropriateness for his or her purposes. We use the term perspeetlve to refer to an abstract characterization of the kind of knowledge provided by a class of models. Our notion of perspective is a composite of distinctions made by Falkenhainer & Forbus (submitted), Stevens & Collins (1980) Stevens & Steinberg (1981) and White & Frederiksen (1989). 1 Copyright @ 1990, Dan iel D. Suthers Permission granted to copy for non-profit academic purposes. Most existing work in explanation and text planning has been directed at other problems, and hence has utilized single-perspective knowledge bases to simplify the research. (Notable exceptions include McCoy, 1989 and McKeown et al. 1985.) A number of such research efforts emphasize rhetorical abstractions for the analysis of natural explanations and for expressing a theory of explanation (Hovy, 1988; Mann & Thompson, 1986; Maybury, 1988; McKeown, 1985; Moore, 1989). A variety of mechanisms for selecting and organizing the content of explanations have also been explored. This includes schema filling (McKeown 1985) and structure matching (Hovy 1988), graph traversal algorithms (Paris & McKeown 1986), and top-down expansion planning (Cawsey, 1989; Moore, 1989). Our own work on choosing explanatory content from multiple-perspective knowledge bases has uncovered some limitations of rhetorical abstractions, and led us to question previous applications of the computational mechanisms listed above. The purpose of this paper is to present our perception of the roles and limitations of these items, and suggest some alternatives. (We do not emphasize our work on perspective and content organization here: see Suthers & Woolf, 1990.) We begin with an example, used to illustrate some of the problems. An Example The following example explanation will be used to illustrate our points. The domain is elementary electricity. We emphasize the communication of an understanding of concepts such as \"charge\", \"current\", and \"capacitance\" within the context of qualitative reasoning about the behavior of simple circuits and their components. The explanation is an edited version of a human protocol. QI: How does a capacilor sore charge? El: A capacitor E2: can be thought of as two fiat metallic plates 137 I jBackground -~ l .'Cnstit ...i I ~Attributive? 1 2 3 4 { ~. Conclusion ? -......~ se-Effect I F, Illustration 5 6 7 8 9 Figure 1: One possible RST analysis (partial) E3: situated close to each other and parallel to each other, E4: with air in between. E5: If you connect a capacitor to a battery, E6: as follows: I I + I I E7: then positive charge builds up on one plate and negative charge on the other, E8: until the voltage across the capacitor is equal to the voltage of the battery. 9: The charge stays there when you disconnect the battery. One possible rhetorical analysis of the first explanation, using relations from McKeown (1985) and Mann & Thompson (1986), is given in Figure 1. (Whether or not this is an optimal analysis is not the point; rather we are concerned with the role of rhetorical abstractions and various characterizations of the content planning task  in accounting for the explanation.) The analysis does point out some features of interest. For example, we note that the explanation starts with Background material (El-E4) before proceeding to the primary explanation (E5-E9). The Background relation in this case describes the high level organization of the explanation. The question is how this relation should be manifest in the mechanism for generating the explanation. Also of interest are the explainer's determination that a process account of how the capacitor carries out its function is appropriate; use of an abstract structural model of the capacitor, simplified to what is needed to support the process account; and use of a concrete situation for the enablement condition of the process. Problems with Rhetorical Abstractions Rhetorical relations were an important development in explanation research, since they provided a first pass at abstractions for a general description of explanatory structure, and in bringing various roles of the parts of explanation into the foreground, pointed out phenomena in need of further study. They are also useful via their \"relational propositions\" (Mann & Thompson 1983), for conveying propositional information implicitly in the structure of the text, hence reducing its length and redundancy. However, we claim that rhetorical abstractions fail to make the necessary distinctions for further advances in a theoretical understanding of explanation. Potpourri. Rhetorical abstractions are descriptive of explanatory tezt, i.e. the end product of some expla138 nation generation process, and so describe with one device structure due to a variety of distinct knowledge sources bearing on such a process. These knowledge sources operate on different levels of information, and hence need to be separated in a theory of explanation generation. For example, (drawing on relations in McKeown, 1985 and Mann & Thompson~ 1986) some correspond directly to the fundamental structure of domain objects and processes (e.g. Constituency and Causality), while others are about derived relations between concepts which may vary according to the context (e.g. Comparison). Relations such as Amplification, Background, Evidence, and Illustration are primarily about relationships between propositions which arise in part out of consideration of what the interlocutor knows and needs to know to better grasp a point. Illocutionary acts are involved in the relations as well (most blatantly, Concession; others are not themselves illocutionary acts but only make sense in the context of certain such acts). Finally, relations such as Topic and Conclusion appear to be due to conventions governing writing style which direct focus of attention. Grosz & Sidner (1986) made similar criticisms from the standpoint of characterizing discourse coherence. They suggested that each rhetorical relation combines domain information with certain general relations between propositions, between actions, and between intentions. Implicit Features Unaccounted For. Rhetorical abstractions are also inappropriate for a theory of content selection because, in describing the final text, they fail to identify important relations between the chosen content and external material, such as what is known about the user, or information left o~$ of the explanation. For example, E5-E6 is more specific than is necessary and includes a concrete example. A more general and accurate way to state the condition for initiation of the charging process would be \"If a voltage is applied to the plates ...'. However, the explainer has opted to replace this with one of the many particular configurations which meet the condition. A rhetorical analysis of the text canno$ even $ell us $ha~ ~his has happened, let alone why, because it does not describe relations between the contents of the text and what is no~ included, viz., other models of the process being described. It can only report that an illustration is being used. Another example is provided by E9. Retention of charge when the voltage is removed is what is meant by \"storage\" in this case, so the fact expressed in E9 is essential to answering the question. Rhetorically, we can only identify relationships E9 has to the rest of the text, e.g. that it is a Conclusion. This does not illuminate the relationship between its content and the goal of the explanation. Epistemological Analysis. The success of an explanation is primarily a function of choice and organization of knowledge. Hencc, to account for how these choices further knowledge communication goals, one must examine explanation in part from an epistemological standpoint. Such an analysis examines how explanations are guided by:  the types of knowledge in a given domain, and its logical and etiological structure (Rissland, 1978);  the types of knowledge which, in principle, could fulfill a given request for information;  the role of an individual's knowledge in understanding new concepts and situations, and hence in understanding a given explanation (Paris, 1987); and  the ways in which individuals are willing or able to undertake conceptual change (Goldstein, 1979; Hewson, 1981; White & Frederiksen, 1989; vanLchn, 1987). As discussed in Suthers (1989), most previous work has offered solutions to the subproblems of explanation in the form of mechanisms and data structures which are in part the result of, rather than the expression of, epistcmological considerations. Epistemological problems have been avoided through direct selection of content based on well-formulated queries; the simplicity of the knowledge bases used, which only permit one way of discussing each topic; and through implicit conflation of epistemological constraints on organization of the explanation with those of rhetorical and linguistic origin. A major goal of our research (Suthers & Woolf, 1990) is an explicit theory of the epistemological structure of the activity of explaining. Roles of Computational Mechanisms In this section we illustrate how inclusion of background material and the use of multiple perspectives pose problems for various mechanisms in the literature, and suggest a mixed architecture solution. Structure Matching. By \"structure matching\" we mean methods where abstract descriptions of the structure of explanations are matched to a collection of propositions (or other content to be expressed) in order to organize this material. This includes bottom-up composition of structural units (Hovy, 1988) and schema filling (McKeown, 1985). We question their adequacy for accounting for the prerequisite structure of explanations, such as the Background relation of the example. In our view, the explanation is organized this way 139 because the explainer recognized in his process model (expressed in E5-Eg) concepts the interlocutor may not be familiar with (the parts of a capacitor), and then added material prerequisite to understanding the process explanation (the structural description expressed in El-E4). The background material is not automatically part of the relevant knowledge pool for this question, and structure matching methods leave choice of content and perspective to other mechanisms. Suppose, then, that some other mechanism accounts for inclusion of the background in the pool. It is included as background by virtue of relationships between the interlocutor's assumed knowledge state and the conceptualizations contained in the first attempt at a knowledge pool. Pattern matching techniques which are ignorant of such relationships and see only the composite pool would be unable to identify the part of the knowledge pool which plays the role of \"background\", and account for placement of background before primary material. Graph Traversal. These are algorithms for selectively following links in a knowledge base, with the path so traced out providing the content and structure of the explanation (Paris & McKeown 1986). Such methods model how an explanation exploits the structure of knowledge. They implicitly embody the heuristic that it will be easier for the interlocutor to reconstruct the knowledge if it is presented such that each unit is introduced in relation to the previous unit. For example, parts of an object are introduced in relation to containing or adjacent parts, and process descriptions organized to follow temporal and causal relations in the forward direction. However, graph traversal is limited to modeling local organization, or global organization which is a composite of local choices. Traversal methods don't naturally extend to global organization which occurs at a higher level of abstraction than the links followed, e.g. the presentation of a coherent structural model before a process account begins. Top-Down Goal Expansion. Top-down expansion of a discourse goal (Cawsey, 1989; Moore, 1989) is a stronger candidate for a uniform mechanism for explanation, integrating content selection and structuring. The background problem can be handled with preconditions on plan operators, as Cawsey does. It simplifies modeling explanations such as our example if one can specify the kind of background knowledge required in preconditions at the highest level of plan operators. To illustrate, consider a rhetorical plan operator which contains an optional satellite for Background but does not specify what constitutes background knowledge. Expansion of the satellite would have to be predicated on a comparison of the content selected by expansion of the nucleus with the user model. This decision could not be made at the time the operator is selected by the planning system, since the knowledge pool for the nucleus would not have been selected yet. Instead, one would have to place the satellite decision on hold, expand the nucleus, collect together the knowledge selected at the leaves of its expansion, and perform the comparison before deciding on the satellite. (One could make the decisions concerning the need for background locally to each leaf, avoiding the need for a high level decision depending on knowledge selected at many localities. But then one could not model the structure of explanations such as this one, where the need for prerequisite material is anticipated and provided in advance as a coherent model, rather than as an interruption to the flow of the process description.) Then, if it was decided that some background was required, expansion of the satellite would have to occur under a binding of some variable in the satellite to the concepts for which background is required. With regards to perspective, some problems emerge. Content selection in top-down expansion can be influenced by the current perspective if some mechanism for sharing perspective decisions across the expansion tree is provided. However, neither the choice of perspective nor the actual mechanism by which it influences content selection are modeled appropriately by topdown expansion. Choice of perspective tries to balance the (sometimes conflicting) constraints of adequacy and comprehensibility. Adequacy constraints come from examination of the informative goals (McKeown et al. 1985). Comprehensibility requires answering the question: given the concepts which have been used in the dialogue so far, and/or which the explainer has evidence the interlocutor is familiar with, what other concepts are also likely to be familiar? This suggests a strengthof-association mechanism (McCoy 1989), which we comment on further in the next section. As Hovy (1990) points out, top-down planning is prescriptive, and does not handle conflicting goals easily. The influence of perspective on content selection involves what Hovy calls restrictive planning: some perspective goals, once generated, remain active throughout a span of discourse, and operate as preferences applied to choice points in content selection. They cannot be erased once satisfied, and they may change dynamically. Finally, McDonald & Pustejovsky (1985) point out that a uniform mechanism may not be desirable, as it incorrectly implies that all information is equally available during each point of the text planning process. A principled match of distinct computational mechanisms to each subtask of the 140 (Query) --> Identify } V Refine < .... t v Retrieve f V Modify ..... t V Order --> (Realize) Figure 2: Content planning tasks explanation planning t&sk is one way to express one's theory of what kind of process explanation is. For these reasons, we postulate a separate level of planning for coordinating perspective decisions. A Mixed Architecture. In our current approach, planning occurs at two granularities: selection and retrieval of coherent packages of knowledge similar to Suthers' (1988a,b) \"views\" or Souther, Acker, Lester, & Porter's (1989) \"viewpoints\"; and editing and ordering the propositions and examples which make up these views. Planning at the granularity of views is concerned with purely epistemological constraints on identification of appropriate content, including choice of perspective and prerequisite explanations. At the finer granularity, further epistemological constraints governing the comprehensibihty of the explanation are apphed, and rhetorical and linguistic constraints play a role as well. The question at hand is: wA~z~ sort of t~zsl~ is cor~ter~ plar~r~ir~g ? We postulate several, and comment on possible computational mechanisms for each. Figure 2 gives the rough relationships between the different tasks. The process is reminiscent of case-based design, where one identifies and refines design specifications, and retrieves and reconfigures a previous solution to fit the circumstances. An explainer must first identify at least some of its goals. These are of two types, as discussed above: prescriptive informative goals, and restrictive goals such as comprehensibihty. We mentioned that the latter suggests a concept association mechanism. This could be done by activating weighted links between concepts, or though intermediate frames of attribute weights, as in McCoy (1989). However, one would have to install a potentially combinatorial number of associations between concepts. Because we wish to make an epistemological theory explicit and implement it with an abstract interface to the knowledge base, we are investigating use of a mechanism akin to prototype induction, to generate an abstract description of the desired perspective from the user model and dialogue history. At the view granularity, content selection may be seen as refinement of explanatory goals into a specification of the appropriate addition to the relevant knowledge pool. This refinement includes consideration of what type of knowledge, in principle, could fulfill the informative goal; of the concepts the interlocutor is likely to understand; and of the models which have already been shared in the dialogue. We are attempting to treat this refinement task as top-down planning, though our work has not progressed fax enough to comment further on this. Retrieval requires a knowledge-base specific mechanism: its only theoretical importance to us is that it correctly operationalize the dimensions used to describe the desired view (Suthers 1988a,b). We are examining a variant of compositional modehng (Falkenhalner & Forbus, submitted) for this purpose. Once a knowledge pool is available, some data driven activities occur at a finer granularity, resulting in modification of the relevant knowledge pool. This includes filtering activities, such as removing particular propositions likely to be famihax to the interlocutor; and augmenting activities, such as illustrating abstract statements with examples. These are oppor~g~istic planning tasks, that can be approached with critics which match to the knowledge pool and user model, and specify replacements or deletions to be made. As shown in figure 2, data driven operators may also reinvoke content planning at the refinement level to access material in a different type of model. For example, we have seen how process propositions may involve use of structural concepts the interlocutor is not likely to understand, causing the explainer to plan a prerequisite explanation. An explicit record of the prerequisite relations between views is created, important for ordering the explanation. Ordering is sensitive to prerequisite and illustration links installed by the modification processes. Hence figure 2 should not be interpreted as a claim that ordering considerations do not arise during content selection. The ordering task involves two kinds of processes. One embodies epistemological constraints by ezploi~ir~g existing structure in the knowledge pool. As discussed previously, techniques for traversing links in the knowledge pool apply here. The result will likely be a partial ordering. Further ordering requires imposition of structure for rhetorical, linguistic, and pictorial reasons 141 during realization (generation of text and graphics). Matching of rhetorical patterns to the knowledge pool may be more appropriate at this later stage. In summary, we have argued for a mixed architecture matching prescriptive, restrictive, and opportunistic mechanisms to explanation subtasks. Coordination of these diverse processes may, at the implementation level, require an agenda control mechanism as in Nirenburg, Lesser, & Nyberg (1989). Conclusions Single-perspective knowledge bases, i.e. those which provide only a single conceptual basis for a given description or explanation, have dominated existing work in planning expository text. This research has overemphasized rhetorical abstractions as the basis for theories of content planning, and used computational formalisms such as top-down goal expansion, traversal algorithms, and opportunistic structure matching which, taken alone, fail to fully account for the search for appropriate conceptualizations during content selection. We suggest a greater emphasis on epistemological abstractions, and viewing content selection in terms of identification, refinement, retrieval, modification, and ordering tasks. Rhetorical abstractions retain a place in initial analyses of explanations, to point out features in need of further study, and in the later stages of text planning, to further constrain a partial ordering of content and implicitly convey content via textual structure. Finally, the devices of top-down goal expansion, traversal algorithms, and structure matching retain potential utility for high level content planning, exploiting the structure of knowledge when ordering an explanation, and further ordering an explanation on a rhetorical basis, respectively. "
W90-0119 "<NoAbstract>"
W90-0120 "<NoAbstract>"
W90-0121 "<NoAbstract>"
W90-0122 "Abstract The paper shows how it is possible, in the framemork of a gygtemle funetlonal g:r~mmat, (SFG) approach to the semantics of aatttral language, to generate an output with intonation that is motivated semantically and discoursally. Most of ~ wurk reported has already been *accuse, fully implem~4 in GF.N~SY$ (the very large generator of the COMMUNAL Pro~eet; see Appendix 1). A major feature is that it does am flint generate a syntax tree and words, and then impose intonational coatogm on them (as is a common aggroaeh in modelling intonation); rather, it generates the various intonational feamrta diteetJy, as it is generating richly labe.llad m:rucratea (as are typical in SFG), and the associated items. ~ claim is not that the model p~ he, solves all the problems of generating intonation, but that it points a way forward that makea natural links with semantics and ditu:ourse. A secondaJty perpoe~ of this paler is to demonstraW,, for one of many possible areas of NLO that could have been choc~n, that there is still much important work to be done in '~nt.e, mev geaeratkm'. I do this tn order to tefut the augge~On, ~easiOnally head at recent eel that the major wod io 'sentence generation' has already been done, and that the main (only'?) area of s/gnifleanee in NLG is ia ltighet level planning. In my expea'ieace the two are inteaxlependen% and we should expect significant developments at eveey level in the years to come. "
W90-0123 " This paper describes the design and functioning of the English generation phase in JETS, a limited transfer, Japanese-English machine translation system that is loosely based on the linguistic framework of relational grammar. To facilitate the development of relational-grammar-based generators, we have built an NL-and-application-independent generator shell and relational grammar rulewriting language. The implemented generator, GENIE, maps abstract canonical structures, representing the basic predicate-argument structures of sentences, into well-formed English sentences via a two-stage plan-and-execute design. This modularity permits the independent development of a very general, deterministic execution grammar that is driven by a set of planning rules sensitive to lexical, syntactic and stylistic constraints. Processing in GENIE is category-driven, i.e., grammatical rules are distributed over a part-of-speech hierarchy and, using an inheritance mechanism, are invoked only ff appropriate for the category being processed. 1Introduction This paper discusses relational-grammar-based generation in the context of JETS, a Japanese-English machine translation (MT) system that is being developed at the IBM Research Tokyo Research Laboratory. To put our work in perspective, we first explain the motivation for basing JETS on relational grammar (RG) and then sketch the processing flow in translation. With this background, we (i) describe and illustrate certain aspects of the rule-writing language, GEAR, in which the GENIE English generator has been written; (ii) comment on key aspects of the generator shell, GENSHELL, in which GENIE has been developed; and (iii) discuss the design and functioning of the GENIE English generator. With few exceptions such as the work being done at CMU (cf. KBMT-89 (1989), Nirenburg (1987), and Nirenburg, et. al. (1988)), in the SEMSYN project at the University of Stuttgart (Rosner (1986)), and the joint work between the ISI Penman project and the University of Saarbrticken (Bateman, et. al. (1989)), generation within the area of machine translation has received very little attention. Typically, MT systems have no independently functioning, linguistically justified generation grammar. In the case of transfer systems, much of the target language grammar is typically built into the transfer component, resulting in a non-modular, rigid and linguistically inadequate system. It is the norm in MT systems for the linguistic complexities inherent in robust generation to be simply ignored, contributing to the inadequacy of MT systems. In contrast, we have sought to shift more of the processing burden from transfer onto generation, allowing our system to incorporate a variety of results coming from theoretical linguistics. GENIE is an application-and-language-independent generator embodying 174 a robust, linguistically justified RG grammar of English. Moreover, GENIE incorporates a syntax planner that applies a set of planning rules determining which rules in the execution grammar should be applied. As long recognized in work on text generators, the incorporation of a syntax planner introduces the kind of flexibility required for robust generation. JETS is a so-called limited transfer system, i.e., a system in which structural transfer is kept to a minimum. The key RG notion in our work is that of canonical (relational) structure (CS), an abstract level of syntactic structure representing the basic predicate-argument structure of clauses in terms of a universal set of primitive (grammatical) relations such as subject, direct object, indirect object, chomeur. 1 Given the basic assumption that one is developing a limited transfer system, implying deep analyses of both the source and target languages which converge on structurally similar internal representations for translation equivalents in a wide range of cases, it is critical to select a linguistic framework which supports the required analyses, enabling one to conceptualize the linguistic processing in a uniform manner. As discussed in Johnson (1988b), with respect to MT, RG is a logical choice of linguistic framework since CSs provide a natural syntactic bridge between languages as diverse in structure as Japanese and English. This is so for two reasons: (1) within one language, the CSs of paraphrases are typically the same or highly similar and (2) translation equivalents often have structurally similar if not isomorphic CSs. One of the key advantages of RG comes from its explicit representation of grammatical relations like subject and direct object, which are argued to be universal. In contrast, structure-based frameworks such as transformational-generative grammar (TG) at best only implicitly represent grammatical relations such as subject and direct object in terms of linear precedence and dominance, which are language particular. If one considers the task of transfer, for instance, it is clear that representing basic clause structure in terms of explicitly marked, order-independent relations rather than in terms of language-dependent structural relations reduces the amount of structure changing to be done in the transfer component. This is especially true for languages like Japanese and English, which differ greatly in superficial structural properties (not to mention the fact that Japanese has very free word order, which arguably makes it even less suited to structure-based frameworks). 2Processing Flow in JETS and GENIE As in all transfer systems, linguistic processing in JETS can be divided into three phases: analysis, which consists of lexical analysis and parsing, transfer and generation. The output of analysis is a Japanese CS, which represents the basic predicate-argument structure of the Japanese sentence. 2 Transfer produces an English CS, which is often, but not always, isomorphic to the Japanese CS. The English CS is passed to the GENIE generator, whose task is to generate a grammatically correct and stylistically appropriate English sentence given a well-formed CS. To illustrate, consider the following Japanese sentence and two of the possible English translations: 1. karera wa Tookyoo e itta rashii they top Tokyo to went seem 2. They seem to have gone to Tokyo. 3. It seems that they went to Tokyo. In translating (1), analysis maps the input string into the Japanese CS shown at the left in Figure 1 on the next page. Transfer then maps the Japanese CS into the English CS shown at the right in Figure 1. I For theoretical background on RG, see the many articles listed in the bibliographic reference Dubinsky and Rosen (1987). Note that the following abbreviations are used in glosses of Japanese examples: top (topic), nm (nominalize), and pp (postposition). 2 For discussion of parsing in JETS, see Maruyama, Watana be and Ogino (1989). 175 Japanese CS for (1) English CS for (2) & (3) rashii seem itta go(tense, past) /loci 1/ loci karera Tookyoo they Tokyo (topic. wa) (pp. e) (topic. T) (prep. to) seem go(past) loci Tokyo(to) TenseSpelling they seem have they go(pastpart) ,< Tokyo(to) Figure 1. Canonical structures for (1), (2) and (3). Note that \"1\" means \"subject\", \"loc\" means \"locative\". Given the English CS, it is up to the GENIE English generator to generate either (2) or (3). Based on the information that they in the English CS is marked as the topic of the sentence, GENIE will map the CS into the superficial (unordered) relational structure shown in Figure 2 via the relational rule of Subject-to-Subject Raising (so-called A-raising). Subsequent rules of Tense-Spelling and Linearization (including the spelling out of verbal forms and prepositions) will result in the string They seem to have gone to Tokyo, as shown in Figure 3. seem seem go(past) go(past) they they Tokyo(to) Tokyo(to) Figure 2. A-Raising Applied to the CS of (2) and (3). Note that \"6\" means \"complement\". --Lineanzation, etc .... > They seem to have gone to Tokyo Figure 3. Rest of the Derivation of (2) As illustrated above, RG, like TG, is a \"multistratal\" theory, i.e., clauses typically have more than one level of syntactic analysis, and these levels/strata are mediated by clause-level rules. In the case of TG, the structures are phrase-structure trees, and transformations map trees into trees; in the case of RG, the structures are edge-labelled trees (called relational structures (RS)), where the edge labels represent primitive relations, and the rules map RSs into RSs. The use of multiple strata sets RG apart from functional frameworks such as FUG (Kay 1979) and LFG (Bresnan 1982), which also use primitive relations (functions), and from all other monostratal frameworks such as GPSG (Gazdar, et. al. 1985), whether functional or not. The manipulation of explicitly marked relations in unordered relational structures sets RG apart from TG. In our work on Japanese-English MT, the RG concept of multiple relational strata has proven to be of significant practical use -facilitating the design and development of a limited transfer component and a robust generation component, enhancing modularity, and allowing the linguistic processing to be conceptualized in a uniform fashion. 176 3The RG Rule Writing Language: GEAR One key aspect of our implementation of an RG generator is the GEAR rule-writing language. GEAR permits a grammar developer to write computationally powerful RG rules in a linguistically natural manner. GEAR rules identify grammatical objects via path specifications, of which there are two types: (1) node-specifier, consisting of a sequence of one or more relation names, and (2) property-specifier, consisting of a node-specifier followed by a property name. For instance, 1:1 indicates a node that is the subject of a node that is the subject of the node currently being processed (the focus) and 2.tense denotes the value of the property tense of a node that is the direct object of the focus. GEAR path expressions are superficially similar to the expressions used in unification-based frameworks such as FUG and PATR (Shieber, et. al. (1983)). However, GEAR is not unification based, rather it provides a number of procedural operations, including node deletion and node creation. Each rule consists of a sequence of statements, of which there are several types, e.g., IF-THEN-ELSE, CALL, ON and restructuring statements. IF-THENELSE statements control the rule internal processing flow. CALL statements are used to invoke rules by name. An ON statement invokes a specified rule on a node reachable from the focus via a node-specifier. There are several types of restructuring statement, e.g., ASSIGN, CREATE, DELETE and COPY. An ASSIGN statement is used to alter the relations of a node identified via a node-specifier; the new relation is also specified by a node-specifier. The core of GENIE's A-raising rule, whose relational changes are illustrated in Figure 2 above, is (using 6 for \"complement\"): (ASSIGN 1 6) \"Assign my subject as my complement\" (ASSIGN 6:1 1) \"Assign my complement's subject as my subject\" The complete rule is shown in Figure 4. % % Define the rule A-raising for intransitive verbs (DEF-RULE A-Raising OF Intransitive-verb % % If the A-raising rule switch is turned on (IF (A-raise is 'yes) % % then assign my subject as my complement THEN (ASSIGN 1 6) % % and assign my complement's subject as my subject (ASSIGN 6:1 1) % % and on my complement call the rule % % which makes infinitives (ON 6 (CALL Make-lnf'mitive)))) Figure 4. GENIE's A-Raising rule Creation, copying and deletion of nodes are also specifiable but space limitations preclude discussion. 4The GENSHELL generator shell Building on our experience with an earlier prototype developed by Schindler (1988), we have developed an NL-independent generator shell, GENSHELL, to facilitate the development of RG generators. For any given generator, grammar developers need only specify the designated grammatical relations, parts of speech, a part-of-speech hierarchy, dictionaries and grammars. GENSHELL takes this information and constructs a runtime generator. One of the distinctive aspects of GENSHELL, due to Sehindler (1988), is the concept of categorydriven processing. In category-driven processing, parts of speech are represented as categories in a category hierarchy (POSH) and nodes in RSs are represented as objects which are instances of categories and thus can inherit properties via the POSH, Among the inheritable properties are grammar rules. For instance, the rules for Passive and Subject-to-Object Raising (so-called B-Raising; discussed later) would be associated with the class Transitive Verb, A-raising would be associated with the class Intransitive Verb, and Subject-Verb Agreement would be associated with the superordinate class Verb. In our implementation, all rules are defined with respect to named rule bundles, and rule bundles are associated either with categories in the POSH, the general/default eases, or with lexical entries, the special cases. Rule definitions have the form: I_77 (DEF-RULE rulename OF rule-bundle-name (rule-body)). (As shown in Figure 4 above, a default rule bundle associated with a POS class is given the same name as that class.) When a node N associated with category C and lexical entry L is being processed, the rule search routine, given a rule named R -the'latter comes from so-called agenda rules which are also associated with C D uses inheritance to first search for R among any rule bundles named in L, then searches for R among C's rules, then C's parent's rules and so on up to the top of the hierarchy until either some rule named R is found or the top category is reached and the process fails. In short, in category-driven processing, the grammar invoked on N is constructed as appropriate at processing time on the basis of lexically activated rules and the rules accessible to N's category using the POSH and inheritance. One example is the ordering of adjectives and nouns. The class Noun is associated with a general/default lineanzation rule which orders adjectives before nouns, generating phrases like tall woman. Nouns like someone, anyone, etc. are associated with a lexically triggered lineafization rule which places the adjective after the head noun. These two rules are both named Linearize. Thus, if the focus is someone and it is modified by tall, the search routine, looking for Linearize, will first find the special rule, correctly generating someone tall. A category-driven system has two advantages over more conventional rule systems: (i) it provides a natural mechanism for dealing with special cases triggered by lexical items, while providing a fail-soft mechanism in the form of the general rules inherited from the POSH and (ii) only rules that in principle could be relevant to processing a given node in an RS will be tested for application. That is, the POSH provides a linguistically motivated means for organizing a large grammar into subgrammars. 3 5GENIE: the English generator Generating from CSs requires a robust generation grammar of the target language, as well as a decisionmaking component that decides which surface form is to be generated. The generation grammar employed in GENIE is a (deterministic) relational grammar having a substantial number of clause-level rules which alter grammatical relations, e.g., Passive, A-raising and B-raising, as well as minor rules such as Tense-Spelling and Linearization (the latter of which does not alter grammatical relations). As illustrated in Figure 1 above, CSs typically do not correspond directly to grammatical sentences. Further, any given CS typically constitutes the basis for the generation of a number of superficial forms, e.g., (2) and (3) above. This control problem has been addressed by splitting generation into two phases: a syntax planning phase and an execution phase. The function of GENIE's planner is quite different from that of other generators. Typically, generator planners decide \"what to say\", constructing some sort of internal representation that is then processed by a realization component. Typical planners will be concerned with chunking into sentences, topic selection and word choice (see, e.g., Appelt(1985), Danlos (1984), Hovy(1985), Kukich (1983), McKeown (1985), McDonald (1984)), and Mann (1983)). In the case of JETS, however, since we are in the domain of transfer-based MT, all of these \"high level\" considerations are decided by the analysis and transfer components. In GENIE's case, the planner must, on the basis of a given CS, deal with a myriad of low-level syntactic conditions and their interactions (most of which have not been discussed or even recognized in the generation literature). Internal to GENIE, this means deciding which of the rules in the deterministic execution grammar should be applied. For instance, CSs with seem have a disjunctive grammatical condition: they must either be raised, yielding the pattern NP seem to VP (as in (2) above) , or extraposed, yielding the pattern It seems that S (as in (3) above). Failure to apply either A-raising or so-called It-Extraposition 3 Earlier work using a lexical hierarchy and inheritance in natural language processing includes Wilensky (1981), Jacobs (1985) and Zernik and Dyer (1987). These works make heavy use of phrasal patterns (so-called pattern-concept pairs) and so the conception of grammar and lexicon and hence the notion of what is inherited in these works differ greatly from ours, which is part of the generative-linguistic tradition. 178 would result in the ungrammatical pattern *That S seems (in the case of Figure 1 above: *That they went to Tokyo seems). The decision to apply A-raising in the above example is stylistic (\"make the topic the main clause subject, if possible\"), but the disjunctive requirement (\"apply either A-raising or It-Extraposition\") is grammatical. Having no control over \"what to say\", GENIE's planner is conceptually part of the realization phase and not part of the typical \"planning phase\". GENIE's planner communicates which rules should be applied to the execution grammar via a set of so-called rule switches, which are simply binary-valued properties whose property names are the names of execution rules, e.g., (A-raise . Yes), (Passive . No). As shown in Figure 4 above, IF statements are often used to test for a rule-switch value, which value is either set by a planning rule or comes from a lexical entry. Rule switches are a generalization of the earlier concept of transformational rule features (cf. Lakoff 1970); the generalization is that rule switches can be dynamically set by planning rules, based on lexicul, syntactic, semantic and stylistic considerations (see Johnson 1988a for more examples and further discussion).'* For example, in (1) above, based on the information that they is the topic (this information comes from transfer), a syntax planning rule which is partly responsible for making topics surface subjects sets the switch (A-raise . Yes), turning on A-raising, and the switch (It-Extra. No), turning off Itextraposition, resulting in (2) rather than (3). GENIE's architecture is shown in Figure 5. Planning rules insure that a multitude of lexicosyntactic and stylistic conditions are met, e.g., that clauses with modals do not undergo A-raising, preventing the generation of, e.g., *They seem to can swim; that clauses with verbs like force have passivized subordinate clauses where required to meet coreferential deletion conditions (cf. She forced him to be examined by the doctor, *She forced him (for) the doctor to examine him); and that verbs like teach undergo dative alternation if there is no specified direct object, generating He taught her rather than *He taught to her (cf. sing, which has the opposite condition He sang to her but *He sang her). It is also the responsibility of the planner to make sure island constraints are not violated. For instance, if a wh-nominal is in a sentential subject, then planning rules turn on execution rules such as A-raising resulting in sentences like Who is likely to win (via A-Raising) rather than *Who is to win likely? or the stylistically marginal ?Who is it likely (that) will win?. This heuristic planning rule also insures that in the case of so-called Tough-Movement sentences, GENIE will generate sentences like Who is easy to please?, (via Tough-Movement) rather than either *Who is to #ease easy? or ?Who is it easy to please?. Engli sh CS (Transfer Output) English CS (dictionary information added) [ Syntax Pl anner I~ ~w~t ~che English CS (rule set) RG Execution Grammar Precycle Cycle Post-cycle kinearization English Sentence Figure 5. GENIE Components. Note that the POSH contains the agenda rules and the default planning and execution rules organized by POS. 4 After completing this work, we discovered that Bates and Ingria (1981) also used a mechanism similar to our \"rule switches\" to control generation within a TG framework. Their transformational constraints, however, were set by a human who wished to test what a given set of constraints would produce. That is, their system had no syntax planner which would evaluate a given base structure via a set of planning rules and set constraints insuring the generation of only grammatical sentences. 179 Execution rules are turned on (or off) either by syntax planning rules or by lexical entries. To illustrate the use of lexical rule-switches, consider the following example from JETS involving verbs of prevention: 4. kanojo wa kare ga iku no o habanda she top he pp go nm pp prevent "
W90-0124 " We present two compilation techniques that, when combined, enable text to be generated from systemic grammars in real-time. The first technique involves representing systemic grammars as C++ class hierarchies--this allows the inheritance in the classification hierarchies to be computed automatically by the C++ compiler at compile-time. The second technique foUows from the stratified/renlizational nature of systemic description that results in a mapping from semantic /contextual features to the grammar--such a mapping means that detailed grammatical features can be inferred directly, without a top-down traversal of the systemic classification hierarchies. When the mapping provides the leaf nodes of an instantiation of the grammar (as might be expected in routine generation), no traversal of the grammar is necessary whatsoever, since all the realization information from higher-level nodes has been inherited by the leaf nodes at compile-time. In such cases the text can be output in less than a second even on relatively slow wolkstatinns; on a 22 MIPS machine the run-time is too small to measure. We have developed a framework for real-time sentence generation that we hope to deploy in future work on realtime applications. Our emphasis has been on the compilation of linguistic inference. We would like to be able to perform generation in real-time even when making adjustments for the occupation of the user, the speed of the output device (short texts for slow devices), whether or not the situation is an emergency, whether the text is spoken or written, and other situational factors that may influence linguistic decisions. A prototype implementation of our framework generates situationadjusted clauses in less than a second on relatively slow workstations, and is too fast to measure on a 22 MIPS machine. The computational slxategy behind this framework is twofold: First, we have developed an object-oriented approach to implementing systemic grammars where much of the grammatical processing is done automatically at compile-time by the C++ compiler. Second, we take advantage of stored (compiled) associations between situations and linguistic choices. Furthermore, there is an interesting synergistic relationship between these two compilation techniques. We will first present our object-oriented implementation of systemic grammar, and provide an example of the grammatical processing. An outline of our approach to storing situation-to-language associations will then be provided. Illustrative examples will then be used to clarify these two ideas. We will then discuss the relationship between these two computational techniques, and compare our framework to other approaches to generation. Finally, some conclusions will be drawn. An implementation of linguistic classification Halliday's theory of Systemic Grammar (for a good introduction see Winograd Chapter 6) is unusual in that the primary descriptive mechanism is classification. The classification hierarchies that appear in the linguistic literature are directly analogous to those found in biology (for instance). 183 While finguistic classification alone may be an interesting theoretical exercise, for any practical propose the grammar must relate these classes to linguistic structures. Just as biological classes can be related to biological properties (e.g. mammals have hair), linguistic classes can be related to structural properties (e.g. declarative clauses have subjects that precede the verb carrying the tense). Economy of description is achieved in each case because instances of a class are not only attributed the properties of that class, but also inherit the properties of all its ancestor classes. In the case of language, these properties are expressed as constraints on the structure of the clause, noun phrase, prepositional phrase or whatever is being classified. These constraints are called realization rules and typically refer to which constituents must appear, the order in which the constituents appear, and so on. The importance of classification hierarchies in systemic grammar led us to consider object-oriented programming as an implementation strategy (for a good discussion of some object-oriented approaches to representing linguistic knowledge, as well as a description of an object-oriented implementation of segment grammar, see ] De Smedt 90). We have chosen to exI plore this idea using C++. The prototype implementation is called SLANG++ (a C++ version of the | Systemic Linguistic Approach to I Natural-language DECLARATIVE Generation). C++ has two advantages: of primary importance for this work is that all inheritance--including multiple inheritance--is I Parent SChi l dren ~ indicative Realization rules: computed at compiletime; an added benefit is that C++ provides a low-overhead run-time envkonment. The objects that we are concerned with are clauses, noun phrases and so on, and each of these categories has a corresponding classification hierarchy. Systemic grammar's classification hierarchies are represented straightforwardly as hierarchies of C++ classes. The realization rules associated with each of the systemic classes are represented in a procedural form to facilitate the inheritance and construction of the appropriate English structures. After the grammar has been compiled, a leaf node in the hierarchy contains a body of code that specifies the construction of English structures according to all the realization roles associated with it and its ancestors. As we will see below, this inheritance can help to avoid traversing the grammar at run-time. There are several steps involved in translating a systemic grammar into C++. Systemic linguists use a graphical notation that is impractical to use as input, and putting the grammar on-line is an important first step in the translation process. To this end we have used m INTERROGATIVE Parents ~ indicative Children yes/no whRealization rules: I a. usB FINITE Parents [[~ clause Children k3_] indicative imperative INDICATIVE Parents ~ finite Children [gL..J de clarafiy~ interrogative Figure 2. Simpfified Hypercard representation Hypercard TM to create a tool that allows a systemic grammar to be entered, browsed, and modified. The card for each grammatical class shows the name, parents, children, and realization rules. Using a mouse to select parents and children, or using the keyboard to type class names, allows the user to move through the grammar to quickly find desired information. Entering a new class typically involves adding a child to an existing card, moving to the new card and entering the relevant information. The tool will not allow the 184 creation of invalid hierarchies. The Hypercard representation of the systemic grammar is then translated, by a simple program, into C++ code. The hierarchies that were represented as links between parent cards and child cards are translated into a C++ class definition hierarchy. The (possibly multiple) inheritance in the grammar is all automatically compiled by C++ before the generation system is given any input. This means that the class description for declarative (for instance) will contain all the realization rules from indicative,finite and clause as well. Since inheritance is computed at compile-time, more work expressed in terms of inheritance means greater run-time efficiency. If we have a text planner--or some other higher-level mechanism--that could select the leaf nodes of the classification hierarchy, then most of the grammatical processing could be done through inheritance. That is, most of the choices in the grammar would be determined by the inheritance, and would not have to be made explicitly at run-time. The problem is that the leaf nodes represent the most detailed and esoteric grammatical classes, which (as Hovy 1985, argues) should not have to he known by the higher level. In the next section we will show that this problem can be solved through the use of knowledge that associates situation classes with grammatical classes. There is no reason that such coarse-grained, compiled knowledge should not associate situations with detailed grammatical classes or even leaf nodes. In these cases the computational benefits of compiled inheritance are remarkable. Guidance from the Situation Our primary goal is to achieve the flexibility of natural-language even in applications where language must be processed in real time. In particular, we are interested in cases where language processing is routine, rather than the difficult special cases. McDonald, Meteer and Pustejovsky (1987) analyze the issue of efficiency in natural-language generation. They observe that: \"The greater the familiarity a speaker has with a situation, the more likely he is to have modeled it in terms of a relatively small number of situational elements which can have been already associated with linguistic counterparts, making possible the highly efficient 'select and execute' style of generation\" (p. 173). We are attempting to address the problem of how these situation-to-language associations can be stored and accessed in an efficient manner. Halliday (1973, 1978) shows that the situation or context (including, to some extent, the information the speaker wishes to convey) can also be described using classification hierarchies. He gives an interesting theory of situation-todanguage associations in his writings on \"register,\" and some of these ideas have been discussed in the computational literature (e.g. Patten 1988a, 1988b; Bateman and Paris 1989). For our present purposes, however, it is sufficient to observe that detailed hierarchical classification schemes can be developed for situations. We represent these Hallidayan situation hierarchies using object-oriented representations in exactly the same manner as we represent the grammar. Situation classes in the hierarchy can be associated with some number of nodes in the grammatical hierarchy. Preferably these grammatical classes wiU be near the bottom of the hierarchy--ideally leaf nodes---because this will minimize the number of decisions that need to be made at run-time. The grammatical associations are prope~es of the situation classes, and are inherited at compile-time in exactly the same way as the realization rules in the grammar. Thus, when a situation class is instantiated, the grammatical classes associated with it are then instantiated. The compile-time inheritance in the grammar ensures that all the relevant realization rules are already contained in the grammatical nodes--the grammar does not have to be traversed to locate realization rules of the ancestors. But the compile-time inheritance also avoids traversal of the situational hierarchy by passing associations down the hierarchy. l i! ii i ili ii i i i iiii iiiii iii i i! !i!ii i !iiiiii ii !iiiii !i i!iii i iiiiiiiiiiii! ! iiiiii ii i i i i !i i iii !i   185 The result is a simple and efficient transduction from situation and meaning to English structures. Examples The run-time operation of SLANG++ is best illustrated through examples. Our first example illustrates the processing of the grammar. Here we assume that the input to the system is a set of situational classes. That is, we assume the existence of a text planner that can pass sets of situational classes to our system--these examples are merely intended to illustrate our approach to realization. Suppose (following an example from Halliday 1978) the situation at hand involves a mother and child at home, and the child is misbehaving. The mother wants to control the behavior of the child by threatening to deprive the child of dessert. Given the situation hierarchy in Figure 4, one input to SLANG++ is the class deprivation (or more precisely threat-of-deprivaaon). Several other situation classes will be input as well (indicating that they are at home and so on), but these are handled in exactly the same way as deprivation. Once deprivation has been chosen, the situation-to-language knowledge indicates that the instantiation of several grammatical classes is in order. The grammatical classes associated with deprivation include declarative, benefactive, and negative. Again, just looking at one of these will suffice. The representation of the class declarative contains not only its own realization rules (to have the subject of the clause precede the finite verb), but also all the realization rules of all its ancestors (indicative, finite and clause) that were inherited at compile-time. Processing these realization roles to build syntactic structures is deterministic and inexpensive (see Patten 1988a, for a detailed description of this type of realization). Other realization rules are similarly inferred from other input situational classes. Thus, in very few steps, and without any expensive search, SLANG++ computes syntactic structures (or at least structural constraints) from situational classes toward the generation of the appropriate threat (e.g. I am not giving you a dessert). A second example will illustrate another important aspect of our approach---compiletime inheritance in the situation hierarchy. Sothcott (1985) describes a system that produces simple plans for building a house, does some text planning, then provides input to a sentence generator. Suppose the input is in the form of situational classes describing the building action and the role of the action in the construction process: Does it enable other actions? Is it enabled by the previous actions? Other relevant choices might include whether or not the addressee is the one responsible for this action. A simple hierarchy for this ::!i; i' :: ii! ili iil;::iii:~!::iiiiiii~!i!!:::/:iiiiiii~!iiiiiii:: !~i !i ~i i ~i ~: i ~i ~: ~, ~: ~!; ~: ~!~: i i i i i i i ~: i ~: ; ~: i i ~i ~i i i i ~i i i !. . ~f~`~: ~. . : . . ~i i i ~!~i ~!i ~: ~: : ~: : i : : i ~i : : i : : i : : ~: i!i/.!!!iiiiiiiiii!ii!ii!!i!i~ ~!iiii!ii!iii!iiiiiiiiliiii!iiiii!ii!/iiiii!ilili!iiiiiiiii~iiiii?! i iiiiiiiiiiiiii! i: i : !i : i : i i ~i i i i !i i ~i !i : : : : i i : i l i ~i : [~i i ~i l i ~i i i ! i: ~ii!i~iiii:ili!~:i!::!iii::i: iiiiiiiiiii: i iii: i ii liiiii !il: iiiiiiiiiiii iii iiiiiiiiiiiiii iiiiiiiiii iiiiii:iii i @i ii ili!i iii !iii !ili ii!iiii! ii ili:i:i.iii iiiiii iiiii i!iii iiii i!iiii iiiiiiiiii iliii i!iiiiiiiii i!!!!:iiiiiiiiii iii:i!ii iiiiiiii : !i i i !i i !i i i i i : i i !: !: ! :i:i :i !ii i i!! ii i !!i !! ~i]iiii a~t~iii!iii!i::ii!!!ii!ilili 186 type of situation is shown in Figure 5. Suppose we want to describe the step of sanding the floors, which has two preconditions--the carpentry and painting must both be completed. Also, suppose we are relating the plan to the person responsible for sanding and painting. The following text is generated using a clause for each action: \"If the carpentry has been finished and you've done the painting, then you can sand the floors.\" The generation is well illustrated by the processing requited for the first clause. Since the carpentry is the first enabling action, the situational class first-enabling is input. Since the addressee is not responsible for the carpentry, the situational class not-addressee-builder is input (the situation requires addressee-builder for the other two clauses, resulting in the second-person subjects). The first step in the generation process is to instanfiate these situational classes. The point of processing the situation hierarchy is to determine the grammatical classes associated with either the input situation classes or their ancestors. But the associations of a node's ancestors are all inherited by that node at compile-time. So, in the case where leaf situational classes are input, we are left with the much simpler problem of determining the grammatical classes associated with the input classes. If we consider the case of first-enabling, we see that it has inherited an association with the grammatical class present from its ancestor enabling, and the class perfective from its ancestor non-enabled. The tense of the clause is therefore immediately constrained to be present perfect. Other situational classes are instanfiated resulting in further grammatical choices (including the grammatical class declarative discussed above) that are processed in the manner described in the previous example. Thus, the situation hierarchy benefits from compile-time inheritance just as the grammar does. "
W94-0301 "<NoAbstract>"
W94-0302 "<NoAbstract>"
W94-0303 "<NoAbstract>"
W94-0304 "Abstract. The paper summarizes an ongoing investigation of the discourse planning tasks concerned with the sequencing of utterances and their parts. Content selection provides some important constraints on sequencing, most notably those derived from the preconditions and effects of planning operators. However these operators underconstraln sequencing, especially below the granularity at which they interface with a domain knowledge source. Further constraints are available from the integrative processes of the heaxer or reader and from working memory ]imlts. Application of these constraints is a matter for discourse planning because the choices relate to one's communicative goals. The planning task is one of translating functionally relevant relationships between units to be ordered into ordering constraints. A collection of strstegies for this task are presented. Some of the strategies were used in an earlier implemented system; many are justified by prior psycholinguistic research. Also discussed include current efforts to extend the work to focus structure in general, and to address the handling of conflicts between strategies. Introduction The extent to which the components of a text or utterance succeed in carrying out their intended function depends in part on the sequence in which they are realized. For example, a critical aspect of understanding an explanation is to integrate the concepts and propositions in the explanation with existing knowledge. Sequencing decisions should attempt to facilitate this integration and otherwise enhance the intended functionality of the segments of the explanation. Superficially, the \"sequential structure = of discourse is simply the order in which its elements are positioned in a linear medium. However, some of the ordering may be arbitrary. In a theoretical analysis, it is more useful to define the sequential structure of discourse as a partial ordering that has specific justifications. This paper provides a collection of such justifications in the form of strategies for translating functionally significant relationships between discourse elements into palrwise ordering I constraints between those elements. We begin with a discussion of the nature of the se1 \"Ordering\" and \"sequencing\" are used interchangeably. quencing task and the advantages of explicit operators for this task. A number of strategies for the coherent ordering of an explanation are then presented, some of which were used in an earlier implemented system [Suthers 1993a], and many of which are justified by prior psycholinguistic research. Directions for further research are also discussed, including current efforts to extend the work to focus structure in general (i.e., subordination structure as well as sequential structure), and to address the handling of conflicts between strategies (e.g., between centering theory and McKeown's focus preferences). Approaches to the Sequencing Task Previous work in generation has handled sequencing decisions in a number of ways. Schematic approaches specify allowable orderings implicitly in terms of the transitions of finite state automata [McKeown 1985]. Nondeterminism in these automata has been addressed using focus preferences for selecting from a content pool, these preferences being embodied in a selection mechanism. Other approaches exploited the structure of domain knowledge with mechanisms for traversing data structures representing this knowledge [Paris & McKeown 1986, Sibun 1992]. Planning approaches initially utilized more local yet still schematic specifications of ordering, expressed as preconditions or optional satellites for plan operators [Cawsey 1989, Hovy 1988, Moore 1989]. More recently, partial order causal link (POCL) planning is being applied to discourse planning, with partial ordering derived in a principled manner from the relationships between preconditions and postconditions of plan steps [Young et gl. 1994]. Content selection processes provide some important constraints on sequential structure, most notably in the form of satisfaction-precedence relations derived from the preconditions and effects of discourse planning operators. However these processes underconstraln sequential structure. This is especially true below the granularity at which the operators interface with a domain knowledge source, because the latter is partially responsible for providing collections of related content that can't be specified by domain-independent opera. tors. For example, a specification that some distinguish29 7th International Generation Workshop  Kennebunkport, Maine * June 21-24, 1994 ing attributes of an entity should be expressed might bind to several nonexclusive alternatives, or a speciflcation for a description of the constituents of an object or process might result in retrieval of a collection of propositions. Some sequencing decisions above the granularity of access to domain knowledge may be underconstrained as well, for example the order in which to express a list of multiple reasons for a conclusion. Further constraints are available, for example from reasoning about the integrative processes of the hearer or reader and the impact of working memory limits on these processes. Application of these constraints is a matter for discourse planning because the choices relate to one's communicative goals, as discussed below. Sequential structure is too important to treat arbitrarily and too context-sensitive to treat in a schematic manner. Overall, ordering heuristics have not typically been made explicit as causally efficacious plan operators. Explicit ordering operators facilitate the expression and study of alternate theories of sequential structure, and have benefits for planning as well. For example, they enable a p|A=~er to tell whether it can achieve a communicative goal by ordering its utterances in a certain way, and they provide a handle for changing strategies in different discourse situations. Suthers [1993a] treated sequencing as a distinct planning task and used sequencing operators that derived ordering constraints from relevant relationships between the elements being ordered. However, prior planning approaches (including that just cited) have not made the effects of ordering decisions explicit. Effects can help choose between conflicting operators, and must be considered in reasoning about what communicative goals are achieved by particular orderings. Choice Between Conflicting Operators. When ordering operators conflict, the operators express principled reasons for choosing between alternate sequential structures. The choice is a matter for discourse planning because it relates to one's goals. The intended effects of conflicting ordering operators can he used to select between them provided that these effects can be related to contextual factors such as superordinate goals and stylistic preferences [DiMarco & Hirst 1993, McCoy & Cheng 1991, Hovy 1990]. For example, a common con~ict is between a sequential structure that makes a single entity salient and a sequential structure that flows smoothly from one entity to another (~dovetailing~). Entity salience might be preferred when the entity in question is a topic of the current segment, while dovetailing might be preferred when communicating a relational structure among equally important entities or to make a transition to a new topic. (A definition of ~topic ~ is forthcoming.) Or consider the ordering of examples with respect to the generality they exemplify. If the examples are presented first, the reader has an opportunity to engage in inductive inference towards the generality, yet may fail to see the relevance of the examples. Presented after a generality, examples provide concrete instances under which the generality may be indexed. The choice depends on whether the speaker or writer is trying to get the hearer or reader to engage in active induction or trying to ease the comprehension process. Achieving Communicative Goals by Sequential Structure. Some planners (e.g., POCL planners) can notice when a goal can be achieved by actions that have already been planned. However, these planners can only take advantage of existing actions when the explicit effects of a single action meets the goal in question. It should be possible to extend these planners to use ordering operators in an opportunistic manner to identify ways in which goals can be achieved by feIicltous ordering of multiple actions, e.g., to achieve communica~ rive goals by implicit relations [Lascarides & Oberlander 1992, Mann & Thompson 1983]. If an operator's effect matches to an active unsatisfied goal and the opera~ tor's constraints match existing utterance components or planning can satisfy these constraints with new utterances, then mere installation of the ordering constraint can be used to achieve the communicative gc~al. For example, suppose a discourse planner has the goals of describing a number of events and the causal relations between them. Communication of the causal relations might be achieved implicitly by describing the events in their causal order. Sequencing as Exploiting Relationships. The sequencing of two elements of a discourse can only be decided if there exists some relationship between the elements which has implications for their order of expression. The sequencing task is relational, not merely selective. Approaches that treat sequencing as a selective task, for example by using predicates that select the most preferred element out of a set of remaining candidates, are forced to generate a full ordering (the sequence of selected items). The structural aspects of sequencing are not a natural consequence of the model. When sequencing is treated as the accumulation of explicit constraints between elements, partial orderings can be constructed. Derivation of these constraints from relationships between the elements to be ordered addresses the structural aspect of sequencing directly. The remainder of this paper will present a number of sequencing strategies that illustrate how relationships 30 7th Intemational Generation Workshop  Kennebunkport, Maine * June 21-24, 1994 between elements yield sequencing constraints. But first some notational preliminaries are required. Notation for Ordering Strategies Two aspects of sequential structure are distinguished: precedence and juxtaposition. Precedence indicates that one segment should occur sometime before another segment. Precedence is significant when prior communication of the contents of one segment facilitate the intended functionality of the contents of another segment. For example, in technical writing definitions of terms usually precede their use. Juxtaposition indicates that one segment should occur nest to another segment in the sequential realization of the explanation. Juxtaposition is significant when the contents of both segments must be in focus of attention at the same time in order for the segments to fulfill their intended communicative function. For example, statements of similarity and difference are usually juxtaposed when making a comparison so that the relative significance of the similarities and differences can be weighed. Any constraint on sequential structure must involve one of precedence or juxtaposition. Succession indicates the simultaneous presence of both constraints (i.e., that one segment should occur immediately before and ~ezt to another segment). Sequential constraints are placed between text plan elements at three granularities. Inter-lntentlonal constraints are placed between intentions to perform rhetorical and communicative acts [Suthers 1993a], and constrain the ordering of the utterance segments that achieve these intentions. For example, an interintentional constraint would be used to ensure that a description of the structure of a device precede an account of how the device carries out its function. Interpropositional constraints are placed between propositions and constrain the ordering of sentences, adjectives, relative clauses, and other surface realizations of the propositions. For example, an inter-propositlonal precedence constraint between (Parallel plates-l) and (Madeof plates-1 metal-l) would allow any of \"parallel metal plates,\" Uparal|el plates made of metal\" or \"The plates are parallel. The plates are made of metal.\" but not \"metal parallel plates,\" \"metal plates that are parallel,\" or ~The plates are made of metal. The plates are parallel.\" (Clearly, this constraint leaves other realization decisions open.) Intra-proposltional constraints, placed between roles of a proposition, controls voice (i.e., which role filler is expressed as the subject of a clause). ~ For example, an intra-propositional precedence constraint 2I.u the implemented system of Sut]aers [1993a], subjects were always surface-initial. Subject and su.rface-inltial are not confiatecl in forthco~.ing revisions. could select between C'The plates are made of metal.\" vs. \"Metal is what the plates are made of.\" Juxtaposition constraints between role fillers discourage the insertion of a subordinate clause between the realization of the role fillers. The ordering strategies areexpressed in the form of rules for translating other kinds of relationships into ordering relations. The general form of the rules is: If $1 bears relation R to $2 then 51 ~'~ $2 where $1 and $2 are segments and S, ~'~ $2 is one of the following: $1 ~ $2 for precedence ($1 occurs sometime before s2), $1 ]~,=t $2 for juxtaposition ($1 and 52 are next to each other in either order), and $1 suc~ 52 for succession ($1 and $2 are juxtaposed and $1 precedes $2). Propositions are denoted by (P rl r2 *) where P is a predicate, the ri are role fillers, and * denotes 0 or more additional role fillers. No ordering of rl, r2, and  is implied by this notation; in particular  may represent other role fillers that can be expressed before or between rl and r~ as well as after them. The notation for intrapropositional ordering is (P r, =d -~ 1, 2 *). This constrains the realizations of the fillers of rl and r~ to be sequenced in a manner respecting relation ord. ~re~ For example, if (P r, r2) is expressed as a clause, then rl will be the surface-initial subject, s Two predicates and a function are needed to express some of the ordering strategies: End-p(P), true when P is a proposition at the end of a chain of ordered propositions: p~ ~'~ ...P. ~'~ p. Toplc-p(t), true when t is the argument of or a constituent of the argument of a rhetorical goal scoping over the propositions to be ordered. For example, in Describe((Structure capacitor-l)) both capacitor-1 and (Structure capacitor-l) are %opics.\" Famillarlty(c) = l when a possibly fallible oracle indicates that concept c is assumed to be familiar to the questioner at level l, a member of a partially ordered set of levels, perhaps using categories such as in Prince's [1981] taxonomy, l = false for unfamiliar c. 8The notation falls to constrain the rea]Jsatlon of the predicate relative to its role fillers, a deficiency that will be acld.ressecl in a future revision. 31 7th International Generation Workshop  Kennebunkport, Maine  June 21-24, 1994 Table 1: Rules for Ordering Strategies Supplemental Con.~traints Antithesis Background Enrichment Evidence a b Exemplification a b Motivation Preview Summary If A is an an~ith.es/J of T If B is backgro=~d for F If E is an enoch.merit of S If E provides evidence for assertions in H (go=l-d~uen t=k) If E provides an ezamp/e of G (illustrative ~e) (inductive use) If M provides a motivation for S If P provides a preuieus of B If S is a s=mmar~d of B jutt then T , J A. then B p,e~ F. then S ,~c~ E. then E =t~c~ H or H '~'=? E. then G a~,e= E or E v\"~ G. then M ,,c~ S. then P ~ B. then 13 P\"~ S. Memor31 and Processing CovJtrain~ Topic Initial Topic as Subject Dovetailing If Topic-p(~), (P1 ~ *) is unordered and (P= *) does not contain t If Topic-p(t) in (Pz t *) If R = (P1 z y ,), End-p(R), and (P2 Y z ,) is unordered then (Pz t *) v,,~ (P2 *). then (P1 t V,e~ ,) then (P1 z g ,) ,==c (P2 Y p,e~ z *). Dora=in Knowledge Constrains Natural Ordering Differentia Context If (N z tt) where N is a Natural-Ordering and z is in the predecessor role of N, If (Subsumption c s) and s is differentiated within c by (P s *) If C is context in which S holds then (N z v,=~ b') then (Subsumption c s) ,ue~ (p s *). then G ,uej= S EpLctemie Cordezt Constrints Familiar First If (P f n *), Famitiarity(P), and Familiarity(f) > Familiarity(n) then (P f P\"~ n *). (See Su~hers [1993a, b] for ezch=nge-leuel co~strair~ts on model cl~oice.) Table 1 lists the strategies. Selected strategies are discussed below. Supplemental Constraints The most obvious constraints on sequential structure are those derived from the inclusion of supplemental material. Supplemental material facilitates the understanding or acceptance of other segments of an explanation in specific ways, the success of which is often affected by order of presentation. Suthers [1993a] used a collection of supplemental relations, these being Urhetorical\" relations that are primarily intentional rather than informational [Moore & Pollack 1992] and in which one can pre-identify a \"nucleus\" that is more essential to the goals of the discourse than the other relata. Ordering strategies are associated with each supplemental relation in Table 1. Some relations give rise to an unambiguous ordering, and thus have only one rule in the table. The ordering implications of others are complicated by possible differences in tutorial strategy and individual differences in learning style. One advantage of separating ordering decisions from supplemental relationships is the ability to model stylistic differences by changing ordering strategies independently of supplemental strategies. Background. Background material is that which functions to enable the comprehenaion of nuclear foreground material. Thus background functions best if it precedes the foreground. Succession is not necessary as long as the delay between background and foreground is small enough that the background will not have been forgotten when the foreground is encountered. 32 7th International Generation Workshop  Kennebunkport, Maine  June 21-24, 1994 Exemplification. Strategic variation is possible in the ordering of examples and illustrations. An explainer can encourage a questioner to engage in inductive inference by giving examples before the generalizations or concepts that they exemplify, as expressed by the E ~-e~ G constraint of version b of the rule. Alternately, the example can be given immediately after the concept or generalization being exemplified, as expressed by the G ,~ef E constraint of version a. Under this strategy, the questioner does not have to guess the generalization and will appreciate why the example was introduced. When multiple examples are present other ordering strategies are available for inter-exemplar ordering: see Mittal & Paris [1993] and Rissla~d [1978]. Motivation. A motivation segment is intended to point out the utility of another segment of an explanation so that the hearer will appreciate the relevance of the motivated segment enough to take it seriously. Motivation to attend doesn't work retroactively, so the motivating segment should occur prior to the motivated segment. Succession is preferred, but not necessary. Previews and Summaries. By definition, a preview precedes the main body of an explanation. To serve the function of preparing the questioner for the sequence of utterances to follow, a previewed text should be the immediate successor of the preview, because a preview sets up an expectation that the subsequent segments will be those mentioned in the preview. Violation of this expectation with intervening material can cause confusion. A summary is similar to a preview in that both provide skeletal characterization of the main body of an explanation, though summaries can refer back to content that was not available at the time of a preview. The pedagogical utility of a summary is in repetition and consolidation. Succession is not as important for summaries. In fact, a summary might be used because there is some extra material between the segments related by the summary relation: the summary functions to refocus on the main points after the digression. Memory and Processing Constraints The ordering operators of this section rely on a few processing assumptions that have been supported in the psycholinguistic literature. Memory (retention and retrieval) is better for \"integrated\" items, i.e. those that the subject can relate to other prior knowledge [Keenan et al. 1984, Kintsch & van Dijk 1978]. New integrative links are constructed in a limited working memory during comprehension [Kintsch & van Dijk 1978]. There is a cost (allocation of attention, probability of error) associated with changing the contents of this working mereory, and the longer an item is kept in working memory the more likely it is to be encoded in long term memory. Finally, subjects attempt to identify thematically central entities and use these as the default locus of integration when attempting to integrate new material [Carpenter & Just 1977]. Comments on \"topic\" and \"theme\" may be helpful at this point. Lavid & Hovy (unpublished working paper) define \"theme\" as \"that element that informs the listener as discourse unfolds how to relate the incoming information to what is already known.\" The topic predicate is not intended as a generally applicable definition of thematic elements outside its use in the ordering rules. It merely provides candidates for being made thematic dements though appropriate ordering and other devices. When a questioner asks a question, the topics of the query are brought into the focus of attention. Their focal status motivates the relevance of assertions made and concepts introduced by the explainer in the response. If assertions or concepts that had no apparent relation to the topics were introduced, the questioner might be unable to integrate them and could become confused due to the conversational implicature of the apparent change in subject. For example: \"What killed the dinosaurs?\" \"Many rocks at the KT-boundary have an unusual concentration of iridium ... (The iridium poisoned tltemf The speaker doesn't want to talk about dinosaur dentisef)\" In contrast, the following explanation changes focus of attention from the question's topic to other concepts and propositions in a well connected manner: \"The dinosaur extinctions may have been caused by a huge meteorite. Evidence for such a meteorite is provided by an unusual concentration of iridium found in KT-boundary rocks .... \" This example illustrates the next three rules: the response starts with a proposition about the topic, the topic is in the subject position, and new concepts are introduced by their relation to prior concepts. Topic Initial and Topic as Subject. The \"topic initial\" rule specifies that propositions involving a topic entity t are to be expressed before propositions not involving a topic. As illustrated above, in situations where the topic has been pre-identified, this rule ensures continuity with the recipient's expectations. In other situations, first-mention aids the subject's identification of the topic [Kieras & Boviar 1981, Gernsbacher & Hargreaves 1988], helping to ensure that facts about a particular entity among the many mentioned are remembered. 33 7th International Generation Workshop * Kennebunkport, Maine * June 21-24, 1994 The \"topic as subject\" rule marks the topic as the surface-initial subject of any clause it occurs in. Repetition in the subject position is another way to facilitate the bearer's or reader's identification of the topic [Kieras & Boviar 1981]. This rule can also help smooth over violations of the following \"dovetailing\" rule by returning to a concept that is presumably easy to reactivate. Dovetailing. This strategy relies on a combination of \"argument overlap\" [Kintsch & van Dijk 1978] and \"given-new\" [Haviland & Clark 1974]. The interpropositional P1 ,~,c~ P2 constraint introduces a proposition P2 when it involves s concept or proposition that has been brought into immediate focus of attention by another ordering decision to express P1 (i.e., there is argument overlap). The intra-propositional y ~re , z constraint makes the surface-initial subject of each proposition be the role flUer by which it was introduced (i.e., proceeds from given to new). Dovetailing is intended to minimize working memory changes and maintain the connectedness of the subject's evolving conceptual model or \"text base.\" Dovetailing can be iterated on z in P2 to produce chains. However, overapplication of dovetailing risks obtaining stream-of-consciousness texts which lack thematic development. Topic salience and dovetailing can be in conflict. One manifestation of this conflict is the conflict between McKeown's [1985] preference of \"change\" over \"maintain\" over \"return\" in order to avoid having to reintroduce entities one has more to say about (based on Sidner [1979]) and Gordon et a/'s [1993] preference of \"continuing\" over \"retaining\" over \"shifting\" in order to maintain local coherence. Further work is required to identify how the choice of one strategy over the other depends on register and task demands, and to examine the interaction with other factors such as location in the discourse. For example, the author expects that topic salience will be preferred in contexts where the theme is being established (e.g., early in a document, paragraph, or other new discourse segment) while dovetailing might be preferred once the theme is established and a transition to a subordinate theme is needed. Domain Knowledge Constraints Now we consider constraints derived from relationships in the domain knowledge being expressed. Natural Orderings. Temporal and causal relations are normally experienced in a particular direction, for example from prior to posterior events or from cause to effect. The assumption that our cognitive apparatus is adapted to more easily use these relations in the ~forward\" direction suggests that predicates categorized as \"natural orderings\" [Bienkowski 1986] be expressed with the prior event or cause as the subject, for example ``X caused y\" is preferred to \"y is caused by z,\" all other things being equal. This strategy is consistent with psycholingulstic evidence indicating that reverse causal and temporal ordering inhibits comprehension [Irwin 1980] and disrupts thematic processing [Townsend 1983]. It also facilitates the hearer's identification of implicitly expressed temporal relations [Lascarides & Oberlander 1992]. Related strategies are available for spatial descriptions [Linde 1974, Sibun 1992]. Differentia and Context. The \"differentia\" relation holds between two propositions when one proposition (P s .) differentiates a subclass s from other subclasses of a class c. An explainer chooses the statement (P s *) from amongst all the possible predicates one could apply to s because P distinguishes s from the other subdivisions of e that the questioner might know about. The questioner cannot recognize or assess this significance of (P s *) unless he or she has been informed of the \"contrast class\" against which the claim (P s *) is being made. This rule suggests that the contrast class c be introduced first, for example: \"An electric field is a kind of force field that applies a force to a charged object.\" is preferred over UAn electric field applies a force to a charged object and is a kind of force field.\" Mere precedence is insufficient because the genus provides the context in which the differentia is meaningful. Succession places them both in focus of attention at once. This strategy can be generalized to the level of sibling communicative acts or rhetorical intentions [Suthers 1993a], and to other context/statement relationships, as suggested by the \"Context\" rule in Table 1. \"Epistemic Context\" Constraints Suthers [1993a,b] discusses how the \"epistemic context\" (the knowledge available to the explainer and questioner and the knowledge shared in prior dialogue) influences the choice between alternate domain models on which to base an explanation. Some of the \"preferences\" presented in these publications address sequential concerns. For example, when preferences to \"say something new,\" \"minimize new propositions,\" and \"elaborate on focal models\" are applied together in a dialogue about some phenomenon, incremental construction of increasingly elaborate domain models of the phenomenon will result [Suthers eta/. 1992]. In general, the epistemic context 34 7th International Generation Workshop * Kennebunkport, Maine * June 21-24, 1994 provides important constraints on sequential structure across multiple exchanges but has less impact on sequential structure within a single utterance. This paper does not discuss constraints across exchanges. Only one constraint originating in assumptions about hearer familiarity is discussed. FAm;llar First. New concepts can be introduced in relation to familiar ones using any domain relation. Suppose concept f is familiar and n is new. Then any proposition (P f n .) will do the job, provided the predicate P itself is familiar. (An unfamiliar predicate won't be much help in integrating an unfamiliar concept.) The strategy assumes an ordering of familiarity levels. If f is more familiar than n, it installs an intra-propositional constraint that fl should be expressed surface-initial, yielding expressions of form \"F is P-related to n.\" (not \"n is P-X-related to .f'). Once familiar-first has been applied, dovetailing can be used to introduce other unfamiliar concepts. The assumption behind familiar-first is that it is easier to retrieve s known concept and integrate a new concept in relation to it than it is to construct a new concept from scratch and subsequently retrieve a known concept to which it can be integrated. However, at and below the clause level the delay between the introduction of r~ and f may be so small that this strategy does not have a discernible effect. Also, if n is a topic, \"topic as subject\" may be more relevant to the speaker's goals. These are questions for empirical work. Closing Comments The sequencing of expository texts and speech should be chosen to enhance the intended functionality of each textual unit and to facilitate the questioner's integration of the communicated information. This aspect of the planning task is usefully seen as one of translating functionally relevant relationships between textual units into ordering constraints. The strategies presented in this paper were derived from examination of example explanations and found to be necessary to enable an automated explanation generator to produce coherently sequenced explanations [Suthers 1993a]. (Space constraints necessitate leaving a full example to the work just cited.) Many of the strategies were subsequently found to correspond to results in the psycholinguistic literature. As discussed previously, the strategies can conflict. Suthers [1993a] handled conflicts with a simple preference ordering. This approach is inadequate because conflict resolution is expected to depend on contextual factors such as the speaker or writer's goals and the relationships of the elements to be ordered with respect to discourse segment boundaries. The author is currently planning psycholinguistic experiments to test the impact of the strategies through reading time and recall studies, with particular concern for how the resolution of conflicts between topic salience and other heuristics should be sensitive to discourse context. Another question for future work is the extent to which the sequencing task fits top-down models of planning such as DPOCL [Young eta/. 1994] vs. requiring s distinct mechanism for the application of conflicting heuristics. The modeling of working memory limits and memory decay in DPOCL operators would require generalizing the POCL notion of a %hreat\" to be a matter of degree rather than absolute. The author suspects that important generalities will be easier to capture if factored out and expressed as explicit sequencing operators rather than manifested in variations of multiple decompositional operators. The foregoing work is being extended to include the subordination structure Of an utterance as well as its sequential structure. \"Focus trees\" [McCoy & Cheng 1991, Hovy & MCCoy 1989] will be used to represent the combined sequential and subordination structure, with partial orderings placed between siblings at each level of the hierarchy. The nodes of a focus tree represent units of a text at various granularities ranging from intentionally defined segments of the sort discussed by Grosz & Sidner [1986] down through clausal propositions to predicates and entity references. \"Focusing operators,\" including versions of the sequencing heuristics of this paper rewritten in the focus tree notation, wiU manipulate the tree structure by subordinating one or more subtree to another or by installing cross-links between siblings to constrain possible traversals. This reformulation is expected to be an improvement for several reasons. It allows the expression of heuristics for the subordination aspects of focus structure, not just sequencing, and thematic development can take place at multiple granularities. Only one ordering relation, precedence, is required. Juxtaposition is handled by grouping within a subordinate structure rather than by a different ordering relation that requires special interpretation. An availability metric can be defined in terms of distance to search back through the tree. Finally, the notation can be used in the analysis of texts, and promises to support application of the sequencing heuristics to text revision as well as text generation. "
W94-0305 "y Clayton, VICTORIA 3168, AUSTRALIA emaU: {ingrid,ricky}@bruce.cs.monash.edu.au phone: 't-61 3 905-5202 fax: --I--61 3 905-5146 Abstract. Discourse planning systems developed to date apply local considerations in order to generate an initial presentation that achieves a given communicative goal. However, they lack a global criterion for selecting among alternative presentations. In this paper, we cast the problem of planning discourse as an optimization problem, which allows the definition of a global optimization criterion. In particular, we consider two such criteria: (1) generating the most concise discourse, and (2) generating the 'shallowest' discourse, i.e., discourse that requires the least prerequisite information. These criteria are embodied in a discourse planning mechanism which considers the following factors: (1) the effect of a user's inferences from planned utterances on his/her beliefs, (2) the amount of prerequisite information a user requires to understand an utterance, and (3) the amount of information that must be included in referring expressions which identify the concepts mentioned in an utterance. This mechanism is part of a discourse planning system called WISHFULII which generates explanations about concepts in technical domains. "
W94-0306 "Brighton BN2 4AT, UK email: { Cecile.Paris,Donia.Scott} @itri.bton.ac.uk Abstract Instructional tex-ts have been the object of many studies recently, motivated by the increased need to produce manuals (especially multilingual manuals) coupled with the cost of translators and technical writers. Because these studies concentrate on aspects other than the linguistic realismion of instructions for example, the integration of text and graphicsthey all generate a sequence of steps required to achieve a task, using imperatives. Our research so flushows, however, that manuals can iu fact have different styles, i.e., not all instructions are stated using a sequence of imperatives, and that, furthermore, different parts of manuals often use different styles. In this paper, we present our preliminary results from an analysis of over 30 user guides/manuals for consumer appliances and discuss some of the implications. Introduction Instructional texts have been the object of many studies recently, with an emphasis on methods for integrating graphics and text, as in wIP (Wahlster et al., 1993) and COMET (Feiner and McKeown, 1990), for tailoring to the user (Peter and R~sner, 1994), for generating purpose expressions in English (Vander Linden, 1993), t~r producing multilingual instructions, (e.g., ROsner and Stede, 1991), and tot planning the appropriate referring expressions, (e.g., Dale, 1992). Most of This work is partially supported by the Commission of the European Union Grant LRE-62009,the Engineering and Physical Sciences Research Council (EPSRC) Grant J19221, and by BC/DAAD ARC Project 293. Dr. Paris also gratefully acknowledges the support of the National Science Foundation Grant IRI-9003087. We would like to thank John Bateman, Richard Power and the anonymous reviewers for their useful comlnenLs. * Dr. Paris is on leave from USC/Infonnation Sciences Institute, 4676 A&niralty Way, Marina del Rey, CA 90292 and the Computer Science Department of the University of Southern California. these systems produce only a sequence of steps necessary to accomplish a t,'tsk (e.g., change the engine oil, replace a radio battery, cook butter bean soup). One of the motivations for generating instructions automatically is the increased need to produce manuals, especially multilingual manuals, coupled with the cost of translators and technical writers. This is important not only for European manufacturers, who are required to produce manuals in all the languages of the European Union, but also for multinational companies, whose international sales are reported to constitute over half of their total sales. Given this motivation, then, producing the sequence of steps required to achieve a task is only part of the job: most user guides and manuals contain more than a simple sequence of steps to achieve a task. In our work, we are attempting to generate a more complete user guide, in several languages. The emphasis of previous research on instructions has led to the almost exclusive use of one type of discourse structure (a sequence, reflecting the sequence of steps needed for a t,'tsk), and one type of realisation (the imperative). We have found, however, that manuals can have different styles: not all instructions are stated in a sequence, using the imperative form. This would indeed lead to rather monotonous texts, texts with potentially the wrong interpersonal force (too many imperatives can be too forceful!), and instructions in which the relative importance of various steps might be lost. Furthermore, different parts of manuals often use different discourse structures and forms of realisation. In our work, 6S~ 1 ,* we are investigating the range of ,'tyros in instructional manuals in different languages, and the relationship between stylistic variants and the global structure of the manual both within and between languages. The work reported here is part of the DRAFTER and file GIST projects at the University of Brighton (Scott, 1993), which aim to generate instructional manuals in several languages 45 : 7th International Generation Workshop  Kennebunkport, Maine  June 21-24, 1994 and in different domains. We have analysed user guides in various languages, attempting to avoid translated manuals. This paper presents our preliminary results concerning the different styles and global structures that frequently occur within English and French manuals for consumer appliances. Although our analysis so far does not provide conclusive results with respect to the relationships between stylistic variants, the global structures, and different languages, they suggest that it is important for a system to provide a range of styles of instructions. Our results also indicate directions for further analysis. Variation in instructions Instructions are aimed at conveying directions to perform a (set of) task(s). For example, we can have instructions for filling out a form, or for operating and repairing a device. We include in our definition of instructions activities related to the actions to be performed, such as installation (preparatory steps), maintenance and trouble-shooting, as well as warnings concerning the safe execution of the actions. Confining instructions solely to central actions to be performed is too restrictive as the above-mentioned aspects are also integral parts of carrying out a task safely and efficiently. The most straightforward way to get a reader to pertorm a k'Lsk is, of course, through the use of a sequence of imperatives. However, our analysis of a corpus of over 30 user manuals for consumer appliances reveals a variety of realisations for such directions, from imperatives to simple statements. Statements may, by some, be considered to be outside the scope of instructions. We do not hold m this: we have found numerous examples to support the view that statements are often implicit instructions. Consider, for example, the following instructions from English and French tbr using a insulated flask, taken from EMSA Thermos. Filter coffee: Simply place the filter on your EMSA vacuum jug to prevent escape of aroma and temperature Filtrage du caf6: Le cal'6 peut-&re filtr6 directement dans le pichet. Le porte-filtres s'adapte p~trfaitement sur l'ouverture du pichet, 6vitant ainsi la perte et la dispersion de l'ar6me du caf6. Loose English Translation: Filtering of Coffee: The co2~.e can be filtered directly into the jug. Filter holders .lit perfectly on the opening of the jug, thus preventing loosing the aroma of the coffee Here we see that the English instructions to filter coffee is given as an imperative, whereas, in the French version, the user is provided with a description of the utensil, from which the directions can be derived. The issue of style is of course not a new one: indeed, there are whole subfields of theoretical and computational linguistics devoted to it. Sometimes it is referred to as stylistics (e.g., Crystal and Davy, 1969; Ager, 1970; DiMarco, 1992), special languages (e.g., Sager et aL, 1980), sublanguages (e.g., Kittredge and Lehrberger, 1982), conative function (e.g., Jakobs~na, 1960), registers (e.g., Halliday, 1973; Ghadessy, 1988; Bateman and Paris, 1989; Martin, 1992), or pragmatic eftects (e.g., Hovy, 1988). It is thus not surprising that this linguistic feature also applies to instructions, although it has not been explicitly addressed so far. The different styles in instructional manuals appear to be closely allied to the stance the writer takes towards the reader. For example, different stances are usually taken when providing a description and a warning. Although, in general, such stances can be a decision on the part of the writer (or speaker), in which case it can be referred to as a conative intention (Sager et al., 1980), in the case of instructional manuals, it is usually an institutional decision on the part of the product manufacturer or the technical writing company (this is often referred to as \"house style\"). Stances can indeed be exploited in manuals to project a specific company image (for example, your friendly local nuclear power pianO. These stances are factors that professional technical authors and translators are taught to pay particular attention to when writing or translating instructions, since failure to do so will lead to instructions which, although technically and grammatically correct, have an unintended pragmatic force. Following Systemic Linguistic Theory (Halliday, 1978), we wiU refer to these stances as semantic meanings to be expressed, at the interpersonal level. Semantic meanings lead to the inclusion of different types of information, different organisations of a text, and different expressions of the actions to be performed. For instance, with indirect commands \"the addressee is treated as if they have the fight to demur\"; a choice of a direct command, on the other hand, \"grants the addressee little or no discretion\" (Hasan, 1988, p 24). They can even result in different typographical devices. This is why not all instructions are written as a sequence of steps in the imperative mode. Depending on the semantic meanings to be conveyed, the text can be quite different at both the discourse and realisation levels. From our text analysis, we have began to identify different meanings that can be conveyed in user manuals and their preferred realisations. We have noted that these meanings are not constant throughout an entire manual, but vary across sections. It is thus necessary to identify the different parts that make up manuals, the semantic meanings that can be expressed in the different sections, and the linguistic means available to express them. Furthermore, given our multilingual framework, we are also investigating whether the preferred linguistic means of realising these meanings differ across languages, and whether the preferred stance is likely to change across various languages, as suggested by Hervey and Higgins (1992), and for different target audiences. The work on DRAFTER and GIST is directly related to these issues. Semantic Meanings We have so t~ analysed over 30 manuals given to users when they buy consumer-ofiented goods, such ms coffee machines and camping stoves, in both French and English when available. These manuals are at least one page long and can be up to 10 to 20 pages. In conducting our analysis, we tried to identify the different attitudes expressed in the texts, their potential realisations, and the global structure of the texts. We have identified four different stances that a manual can adopt and give examples of them in Figure 1: 46 7th International Generation Workshop  Kennebunkport, Maine  June 21-24, 1994 (1) Information Provision from Dietrich self-cleaning enamel oven: L'~mail auto-nettoyant est de couleur brun fonc6 mouchet~ de blanc pour certaines pitces. Loose English Translation: The self-cleaning enamel is dark brown, speckled with white on some parts. (2) Information Provisionfrom HP Laser Jet 4 Printer Installation Guide: The optional 500-sheet paper tray assembly comes equipped with a tray housing and either a legal, letter, A4, and executive multi-size paper tray or a letter, A4, and executive multi-size paper tray. You may purchase replacement trays from your authorized HP dealer. The part number of the letter, A4 and executive multi-size paper tray is C2084B... (3) Eulogy from Sennheisser Headphones: To wish to convince you of the superior quality of the dynamic open air headphone HI) 40 would be something of a paradox as you are by now akeady in possession of this product. However, the arguments for the HD 40 are in fact very convincing: Hw  \"oh quahty reproduction.  Exa'emely corafortable thanks to very low weight.  Problem-free connection by means of universal connector.  Very flat storage space thanks to turntable driver elements. (4) "
W94-0307 "<NoAbstract>"
W94-0308 "Brighton BN2 4AT, UK email: { Tony.Hartley, Cecile.Paris,Donim Scott,I~eith.Vander-Linden } @it ri.bton.ac.uk Abstract In this paper we discuss a study of the expression of procedural relations in multilingual user instructions~ in particular the relations of Gene,ution and Enablement. These procedural relations are defined in terms of a plan representation model, and applied in a corpus study of English, French, and Portuguese instructions. The results of our analysis indicate specific guidelines for the tactical realisation of expressions of these relations in multilingual instructional text. Introduction In user instructions, it is common to find expressions like: (1) Pull down and remove the white plastic tray that holds the video cable and unpack the cable.(Apple) Here we have what appears to be a simple expression of sequential actions. The reader is expected to pull down the white plastic tray, remove that tray: and then unpack the cable. Current studies of instructional text (e.g.. RSsner and Stede, 1992; Vander Linden, 1993) typically represent the relationship found here in a simple multi-dependent structure such as that provided by the Sequence schenla in Mann and Thompson:s Rhetorical Structure Theory (RST) (1988) (see figure 1A). This rhetorical structure, which represents three sequential actidns, directly matches the grammatical forms used in the actual text. Indeed: this sequential execution is precisely what the instruction writer desires the reader to \"This work is supported by the Commission of the European Union Grant LRE-62009, the Engineering and Physical Sciences Research Council Grant 319221, and the BC/DAAD ARC Project 293. tAuthors are presented in alphabetical or(ler. do in this context. The complication comes when one finds, sometimes even in the same instruction mamlal, an alternate form of expression for the very same user action; such as the one shown in the following: (2) Pull down and remove to unpack the video cable. (Apple) R ST analysts would most likely represent this with a purpose relation, as in figure lB. Clearly, the representation formalism fails to capture the common semantics of these two examples. This problem may be only rarely encountered in a monolingual context, but is exceedingly common in a multilingual enviromnent (Delin et al., 1993). Consider the following example, taken from a trilingual, translated instruction nlanual for a step-aerobics machine: (3) The stepping load can be altered by loosening the locking lever and changing the position of the cylinder foot. (Liftmaster) (4) Pour modifier la charge d;appui, desserrer To modify les leviers puis the levers then v~rins.(Liftmaster) cylinder foot (5) Nach Lockern After loosening the load stepping loosen d6placer le pied des change the foot of the der Klemmhebel kann of the levers can durch Verschieben des ZylinderfuBes die by pvzhing of the cylinder foot the Tretbelastung ver~ndert werden.(Liftmaster) load changed be. Here we find the same information being conveyed through expressions that exhibit rather different RST 61 7th International Generation Workshop  Kennebunkport, Maine  June 21-24, 1994 "
W94-0309 " This paper discusses the principles that should govern the construction of two components of a system for natural language generation (NLG): (1) the ontology or, rather, as the paper argues, the 'ontological' aspects of a belief system -and (2) the semantic representation of noun senses. It is an interesting fact that many ontologies bear a striking resemblance to a system network, as used in systemic functional grammar (SFG). Furthermore, two major current research efforts in the field of ontologybuilding are designed to run with a SFG generator: Pangloss, where the generator is Penman, and COMMUNAL, where the generator is GENESYS. It is therefore important to establish a principled approach to the 'division of labour' between the ontology and the equivalent aspects of the model of language here a system network for the 'meaning potential' of English nouns. (However, the general principles should be relevant to ANY model of language.) The paper summafises (a)the purposes and (b)the structure of (1) a system network for noun senses and (2) the equivalent ontology (based on what we in the COMMUNAL Project judge is required in the next generation of belief systems for NLG). Examples are given of current work on the relevant system network and, more briefly, of the equivalent ontological aspects of the belief system. In particular, reasons are given why it would be inappropriate to give a primary place to the 'mass' vs. 'count' distinction in an 'interlingua' ontology and even, surprising though it may seem, in a language-specific semantics for English. Finally, it turns out that, in the new perspective presented here, there is no 'component' of the belief system that is 'the ontology', and the reasons for this apparently anomalous position are given. Keywords: ontology, system network, belief system, knowledge base, semantics, noun senses, natural language generation 1 Some  "
W94-0310 " We introduce the problem of referential creativity: how it is that a person can give a word or phrase a new denotation even though she has never heard or used in that way before. Using real examples that we have collected, we focus on the case of semantic type coercion, where a phrase of a given type is used in a textual context that requires a completely different type yet the intended message is perfectly well understood. We frame our account as the problem of what form the linguistic resources available to a speaker must have such that she can appreciate the opportunity for creative phrasings. What systematic relationships exist in the speaker's lexicon that enable a phrase to convey something quite different than it normally would, and why tshould his ever occur during the generation process? We draw on a new theory of how information is associated with a wor6-Pustejovsky's Generative Lexicon--and we embed our account in a theory of generation as an incremental process that makes use of a rich model of the situation in which the utterance occurs. Keywords: creativity, lexical choice, content determination, incremental generation 1  "
W94-0311 " In this paper, we address the issue of integrating semantic lexicons into NLG systems and argue that the problem of lexical choice in generation can be approached only by such an integration. We take the approach of Generative Lexicon Theory (GLT) (Pnstejovsky, 1991, 1994c) which provides a system involving four levels of representation connected by a set of generative devices accounting for a compositional interpretation of words in context. We are interested in showing that we can reduce the set of collocations listed in the lexicon by introducing the notion of \"semantic collofations\" which can be predicted within GLT framework. We argue that the lack of semantic welldefined calculi in previous approaches, whether linguistic or conceptual, renders them unable to account for semantic collocations. 1 Intro  "
W94-0312 " The SAGE system (Simulation and Generation Environment) was developed to address issues at the interface between conceptual modelling and natural language generation. In this paper, I describe SAGE and its components in the context of event descriptions. I show how kinds of information, such as the Reichenbachian temporal points and event structure, which are usually treated as unified systems, are often best represented at multiple levels in the overall system. SAGE is composed of a knowledge representation language and simulator, which form the underlying model and constitute the \"speaker\"; a graphics component, which displays the actions of the simulator and provides an anchor for locative and deictic relations; and the generator SPOKESMAN, which produces a textual narration of events.  "
W94-0313 "Abstract: Causation is a very pervasive phenomenom in natural language which can be expressed by numerous linguistic alternatives. Any language user or natural language generation system is thus confronted with the .problem of choosing one alternative over another. In this paper, I analyze the semantic constraints determining the selection of analytic causatives in Dutch and how this can be accounted for in a systemic functional generation system. Keywords: Multilingual Text Generation, Systexiaic Functional Linguistics, Analytic causatives, Dutch. "
W94-0314 "<NoAbstract>"
W94-0315 " A natural language generation system is typically constituted by two main components: a content planning component (e.g., text planner or dialogue act planner) and a linguistic realization component. But, this is not sufficient since, on the one hand, the message built by the content pldnning component is generally not adequately detailed in order to control the many possibilities for its expression and, on the other hand, the content planner cannot influence the way in which the message will be verbalized. Generation systems require a third component, called the micro-planning (or sentence planning or phrasing) component, which acts as an intermediary between the pragmatico-semantic level and the purely syntactic level. The micro-planner is responsible for transforming the message into a textual structure. For this transformation to be achieved, grammatical and lexical resources must be selected. 1  "
W94-0316 " In this paper, we address one of the central problems in text generation: the missing link (\"the generation gap\" in Meteer's terms) between the global discourse organization as often provided by text planning modules and the linguistic realization of this organization. We argue that the link should be established by the lexical choice proces s using resources derived from Mel'~uk's Lezical Functions (LFs). In particular, we demonstrate that sequences of LFs may well serve as lexical discourse structure relations which link up to global discourse relations in the output of a Rhetorical Structure Theory style text planner. 1 I  "
W94-0317 "Xiaorong Huang Fachbereich Informatik, Universitt des Saarlandes Postfach 15 11 50, D-66041 Saarbr/icken, Germany Email: huangQcs.uni-sb.de Abstract This paper deals with the reference choices involved in the generation of argumentative text. A piece of argumentative text such as the proof of a mathematical theorem conveys a sequence of derivations. For each step of derivation, the premises (previously conveyed intermediate results) and the inference method (such as the application of a particular theorem or definition) must be made clear. The appropriateness of these references crucially affects the quality of the text produced. Although hot restricted to nominal phrases, our reference decisions are similar to those concerning nominal subsequent referring expressions: they depend on the availability of the object referred to within a context and are sensitive to its attentional hierarchy. In this paper, we show how the current context can be appropriately segmented into an attentional hierarchy by viewing text generation as a combination of planned and unplanned behavior, and how the discourse theory of Reichmann can be adapted to handle our special reference problem. "
W94-0318 "(GMD) Dolivostrat3e 15 D-64293 Darmstadt, FRG e-mail:{ reich ,bateman} ~daxTastadt. grad. de Abstract We describe the application of multilingual text generation in a system for assisting the process of publication. This system is an editor's workbench for preparation of the publication of an art history encyclopedia (the Macmillan Dictionary of Art), which is itself part of an integrated pub!ication environment being developed at GMD-IPSI. We show how an editor's tasks can be facilitated by the use of NLP (natural language processing) systems and suggest the important role of text generation in future electronic publications as products. In both cases, we focus on text generation as providing an essential new mode of information presentation. Text generation provides a quality gain in which the flexibility of the electronic product is augmented; in particular, views on knowledge expressed as text, possibly in different languages are incorporated. The major prerequisite for making this possible is an explicit and systematic representation of genres or text types combined with a general interfacing method for specific domain knowledge. "
W94-0319 " I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other. I also compare this 'consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems. 1 Introductio  "
W94-0320 " A discourse planner for (task-oriented) dialogue must be able to make choices about whether relevant, but optional information (for example, the \"satellites\" in an RST-based planner) Should be communicated. We claim that effective text planners must explicitly model aspects of the Hearer's cognitive state, such as what the hearer is attending to and what inferences the hearer can draw, in ord@r to make these choices. We argue that a mere representation of the Hearer's knowledge is inadequate. We support this claim by (1) an analysis of naturally occurring dialogue, and (2) by simulating the generation of discourses in a situation in which we can vary the cognitive parameters of the hearer. Our results show that modeling cognitive state can lead to more effective discourses (measured with respect to a simple task).  "
W94-0321 " In expository discourse, people sometimes ask questions that digress from the purpose of the discussion. A system that provides interactive explanations and advice must be able to distinguish pertinent questions from questions that digress. It must also be able .to recognize questions that are incoherent. These types of questions require different treatment. Pertinent questions must be answered to achieve the discourse ~ phr~ ~ pose. If the user asks a digressive question, the system may need to shift the focus of the discussion back ' to the original-purpose. Incoherent questions signal a more serious misunderstanding that requires clarification and repair. The Interactive Discourse Planner (IDP) is designed to plan text to describe and/or justify a domain plan interactively. As a testbed, IDP plans text to discuss driving routes. IDP uses questions from the user to recognize how to extend its own text plan in a way that both satisfies its listener and achieves the system's discourse goal. In the process of recognizing ways to expand its own text plan, IDP can detect three types of digressions that the user can initiate with a question. 1 Chara  "
W94-0322 " An indirect answer to! a Yes-No question conversationally implicates the speaker's evaluation of the truth of the questioned proposition. We present the approach to generation used in our implemented system for generating and interpret~ing indirect answers to Yes-No questions in English. Generation of a discourse plan is performed in two phases: content planning and plan pruning. During content planning, stimulus conditions are used to trigger speaker goals to include appropriate extra information wit h the direct answer. Plan pruning determines what parts of this full response do not need to be stated explicitly resulting in, in appropriate discourse contexts, the generation of an indirect answer. 1. Introd  "
W94-0323 " NL-Soar is a computer system that performs language comprehension and generation within the framework of the Soar architecture [New90]. NL-Soar provides language capabilities for systems working in a real-time environment. Responding in real time to changing situations requires a flexible way to shift control between language and task operations. To provide this flexibility, NL-Soar organizes generation as a sequence of incremental steps that can be interleaved with task actions as the situation requires. This capability has been demonstrated via the integration of NL-Soar with two different independentlydeveloped Soar-based systems. 1 Rea  "
W94-0324 " This paper describes the Corinna system which integrates a theoretical approach to dialogue modeling with text generation techniques to conduct cooperative dialogues in natural language. It is shown how the dialogue model COR can be augmented by adding discourse relations as an additional level of description which is particularly valuable for the generation of dialogue acts.  "
W94-0325 "<NoAbstract>"
W94-0326 "<NoAbstract>"
W94-0327 " We describe an approach to surface generation designed for a \"pragmatics-based\" dialogue system. The implementation has been extended to deal with certain well-known difficulties with the underlying linguistic formalism (Categorial Grammar) at the same time yielding a system capable of supporting incremental generation as well as interpretation. Aspects of the formalism used for the initial description that constitutes the interface with the planning component are also discussed.  "
W94-0328 "<NoAbstract>"
W94-0329 " In the CORECT project, we are building a computerbased requirements capture tool for custom-built electronic testing systems. The requirements capture process involves the participation of a wide range of different types of people the customer, the salesperson, systems engineers, quality assurance, marketing, and so on. Our aim is to build a Computer-Supported Cooperative Working (CSCW) system which will allow these participants to define an Automatic Test System (ATS) collaboratively by adding data and making changes to an evolving design. The collected information about the design will form a large knowledge pool, all of which is pertinent to the design as a whole, but most of which is irrelevant to any particular person engaged in the design process. We will therefore be using natural language generation (NLG) technology to create documents from the central knowledge pool which are tailored to the particular information needs of the participants. These documents will give the users a snapshot of the developing design and will enable them to see how it can be improved and further developed. This paper gives an introduction to the problem we are tackling and how we are trying to solve it, and argues that combining CSCW for input with NLG for output in this way solves some of the problems which are encountered when trying to use either technology on its own. 1 Introduction  "
W94-0330 "<NoAbstract>"
W94-0331 "<NoAbstract>"
W94-0332 "<NoAbstract>"
W96-0401 " This paper describes the Sentence Planner (sP) in the HealthDoc project, which is concerned with the production of customized patienteducation material from a source encoded in terms of plans. The task of the sP is to transform selected, not necessarily consecutive, plans (which may vary in detail, from text plans specifying only content and discourse organization to fine-grained but incohesive, sentence plans) into completely specified specifications for the surface generator. The paper identifies the sentence planning tasks, which are highly interdependent and partially parallel, and argues, in accordance with [Nirenburg et al., 1989], that' a blackboard architecture with several independent modules is most suitable to deal with them. The architecture is presented, and the interaction of the sentence planning modules within this architecture is shown. The first implementation of the sP is discussed; examples illustrate the planning process in action. 1 Sentence Plan  "
W96-0402 " Building text planning resources by hand is timeconsuming and difficult. Certainly, a number of planning architectures and their accompanying plan libraries have been implemented, but while the architectures themselves may be reused in a new domain, the library of plans typically cannot. One way to address this problem is to use machine learning techniques to automate the derivation of planning resources for new domains. In this paper, we apply this technique to build microplanning rules for preventative expressions in instructional text. 1 Introdu  "
W96-0403 "<NoAbstract>"
W96-0404 "<NoAbstract>"
W96-0405 " This paper describes the input specification language of the WAG Sentence Generation system. The input is described in terms of Halliday's (1978) three meaning components, ideational meaning (the propositional content to be expressed), interactional meaning (what the speaker intends the listener to do in making the utterance), and textual meaning (how the content is structured as a message, in terms of theme, reference, etc.). 1 Int  "
W96-0406 " Graphics and text have to be well integrated in order to achieve their full potential. A picture shows but a text describes. In a statistical report, graphics show the data that is analyzed in the text. This paper describes a system, called PostGraphe, which generates a report integrating graphics and text from a single set of writer's intentions. The system is given the data in tabular form as might be found in a spreadsheet; also input is a declaration of the types of values in the columns of the table. The user chooses the intentions to be conveyed in the graphics (e.g. compare two variables, show the evolution of a set of variables ...) and the system generates a report in IgTEX with the appropriate PostScript graphic files. 1 Introdu  "
W96-0407 " Patent claims are the subject of legal protection. They must be formulated according to a set of precise syntactic, lexical and stylistic guidelines. Composing patent claims is a complex task, even for experts. In this paper we report about an tmplemented system for supporting authoring claims for patents describing apparatuses. The system generates claim texts from the input specified partly by the stored conceptual text schemata and partly by the input from the user. The result of the interactive content acquisition stage is a shaUow-level representation which can be considered a draft to be automatically revised into the final text of the claim. Subject Keywords: interactive, automatic, generation, conceptual schema, template, patent claim 61  "
W96-0408 " In this paper we discuss how generation issues affect the design of a computer-assisted language learning tool designed to teach written English as a second language to deaf users of American Sign Language. We discuss a dual-component linguistic model that attempts to reflect the generation process of the learners. The first model component captures the influence of the first language on the acquisition of the second. The second model component captures the process of second language acquisition itself. The linguistic model helps the system identify errors along with their probable source(s). This information is crucial for effective correction. It is also useful in the response phase of the system to focus tutoring on the errors that are most beneficial to correct. In addition, the linguistic model can be used to tailor the system's realization of its response. In this way, the syntactic constructions generated by the system will provide understandable and positive exemplars of the language features currently being acquired by the leamer. Keywords tailoring response generation, user modeling, computer-assisted language learning 1 Introduct  "
W96-0409 " This paper describes tactical generation in Turkish, a free con~stituent order language, in which the order of the constituents may change according to the information structure of the sentences to be generated. In the absence of any information regarding the information structure of a sentence (i.e., topic, focus, background, etc.), the constituents of the sentence obey a default order, but the order is almost freely changeable, depending on the constraints of the text flow or discourse. We have used a recursively structured finite state machine for handling the changes in constituent order, implemented as a right-linear grammar backbone. Our implementation environment is the GenKit system, developed at Carnegie Mellon University-Center for Machine Translation. Morphological realization has been implemented using an external morpholggical analysis/generation component which performs concrete morpheme selection and handles morphographemic processes. Introduction Natural Language Generation is the operation of producing natural language sentences using specified communicative goals. This process consists of three main kinds of activities (McDonald, 1987):  the goals the utterance is to obtain must be determined,  the way the goals may be obtained must be planned,  the plans should be realized as text. Tactical generation is the realization, as linear text, of the contents specified usually using some kind of a feature structure that is generated by a higher level process such as text planning, or transfer in machine translation applications. In this process, a generation grammar and a generation lexicon are used. As a component of a large scale project on natural language processing for Turkish, we have undertaken the development of a generator for Turkish sentences. In order to implement the variations in the constituent order dictated by various information structure constraints, we have used a recursively structured finite state machine instead of enumerating grammar rules for all possible word orders. A second reason for this approach is that many constituents, especially the arguments of verbs are typically optional and dealing with such optionality within rules proved to be rather problematic. Our implementation is based on the GenKit environment developed at Carnegie Mellon University-Center for Machine Translation. GenKit provides writing a context-free backbone grammar along with feature structure constraints on the non-terminals. The paper is organized as follows: The next section presents relevant aspects of constituent order in Turkish sentences and factors that determine it. We then present an overview of the feature structures for representing the contents and the information structure of these sentences, along with the recursive finite state machine that generates the proper order required by the grammatical and information structure constraints. Later, we give the highlights of the generation grammar architecture along with some example rules and sample outputs. We then present a discussion comparing our approach with similar work, on Turkish generation and conclude with some final comments. 81 Turkish In terms of word order, Turkish can be characterized as a subject-object-verb (SOV) language in which constituents at certain phrase levels can change order rather freely, depending on the constraints of text flow or discourse. The morphology of Turkish enables morphological markings on the constituents to signal their grammatical roles without relying on their order. This, however, does not mean that word order is immaterial. Sentences with different word orders reflect different pragmatic conditions, in that, topic, focus and background information conveyed by such sentences differ, t Information conveyed through intonation, stress and/or clefting in fixed word order languages such as English, is expressed in Turkish by changing the order of the constituents. Obviously, there are certain constraints on constituent order, especially, inside noun and postpositional phrases. There are also certain constraints at sentence level when explicit case marking is not used (e.g., with indefinite direct objects). In Turkish, the information which links the sentence to the previous context, the topic, is in the first position. The information which is new or emphasized, the focus, is in the immediately preverbal position, and the extra information which may be given to help the hearer understand the sentence, the background, is in the post verbal position (Erguvanh, 1979). The topic, focus and background information, when available, alter the order of constituents of Turkish sentences. In the absence of any such control information, the constituents of Turkish sentences have the default order: subject, ezpression of time, ezpression of place, direct object, beneficiary, source, goal, location, instrument, value designator, path, duration, expression of manner, verb. All of these constituents except the verb are optional unless the verb obligatorily subcategorizes for a specific lexical item as an object in order to convey a certain (usually idiomatic) sense. The definiteness of the direct object adds a minor twist to the default order. If the direct object is an indefinite noun phrase, it has to be immediately preverbal. This is due to the fact that, both the subject and the indefinite 1See Erguvanh (1979) for a discussion of the function of word order in Turkish grammar. direct object have no surface case-marking that distinguishes them, so word order constraints come into play to force this distinction. In order to present the flavor of word order variations in Turkish, we provide the following examples. These two sentences are used to describe the same event (i.e., have the same logical form), but they are used in different discourse situations. The first sentence presents constituents in a neutral default order, while in the second sentence 'bugiin' (today) is the topic and 'Ahmet' is the focus: 2 (1) a. Ahmet bug{in evden okula Ahmet today home+ABL school+DAT \"Ahmet went from home to school otob{isle 3 dakikada git~i. bus+WITH 3 minute+LOC go+PAST+aSG by bus in 3 minutes today.' b. Bug{in evden okula otobiisle today home+ABL school+DAT bus+WITH 'It was Ahmet who went from home to 3 dakikada Ahmet gitti. 3 minute+LOC Ahmet go+PAST+3SG school in 3 minutes by bus today.' Although, sentences (b) and (c), in the following example, are both grammatical, (c) is not acceptable as a response to the question (a): (2) a. b. C. All nereye gitti? All where+DAT go+PAST+3SG 'Where did All go?' All okula gitti. All school+DAT go+PAST+3SG 'All went to school.' * Okula All gitti. school+DAT All go+PAST+3SG 'It was All who went to school.' 2In the glosses, 3SG denotes third person singular verbal agreement, P1PL and P3SG denote first person plural and third person singular possessive agreement, WITH denotes a derivational marker making adjectives from nouns, L0C, ABL, DAT, GEtl denote locative, ablative, dative, and genitive case markers, PAST denotes past tense, and INF denotes a marker that derives an infinitive form from a verb. 82 The word order variations exemplified by (2) are very common in Turkish, especially in discourse. Generation of Free Word Order Sentences The generation process gets as input a feature structure representing the content of the sentence where all the lexical choices have been made, then produces as output the surface form of the sentence. The feature structures for sentences are represented using a case-frame representation. Sentential arguments of verbs adhere to the same morphosyntactic constraints as the nominal arguments (e.g., the participle of, say, a clause that acts as a direct object is case-marked accusative, just as the nominal one would be). This enables a nice recursire embedding of case-frames of similar general structure to be used to represent sentential arguments. In the next sections, we will highlight relevant aspects of our feature structures for sentences and their constituents. Simple Sentences We use the case-frame feature structure in Figure 1 to encode the contents of a sentence. 3 We use the information given in the CONTROL feature to guide our grammar in generating the appropriate sentential constituent order. This information is exploited by a right linear grammar (recursively structured nevertheless) to generate the proper order of constituents at every sentential level (including embedded sentential clauses with their own information structure). The simplified outline of this right linear grammar is given as a finite state machine in Figure 2. Here, transitions are labeled by constraints and constituents (shown in bold face along a transition arc) which are generated when those constraints are satisfied. If any transition has a NIL label, then no surface form is generated for that transition. The recursive behavior of this finite state machine comes from the fact that the individual argument or adjunct constituents can also embed sentential clauses. Sentential clauses 3Here, c-name denotes a feature structure for representing noun phrases or case-frames representing embedded sentential forms which can be used as nominal or adverbial constituents. \"S-FORM CLAUSE-TYPE VOICE SPEECH-ACT QUES VERB ARGS ADJN CONTROL infinitive/adverbial/participle/finite existential/attributive/predicative active/reflexive/reciprocal/passive/causative imperative/opt ative/necessit ative/wish/ interrogative/declarative TYPE yes-no/wh ] CONST list-of(subject/dir-obj/et,)J ROOT verb POLARITY negative/positive TENSE present/past/future ASPECT progressive/habitual/etc. MODALITY potentiality DIR-OBJ c-name SOURCE c.name GOAL c-name LOCATION c.name BENEFICIARY c-narne INSTRUMENT c-narne VALUE c-name TIME c-namePLACE e-name MANNER c-name PATH c-narne DURATION c-name \"TOPIC constituent\" FOCUS constituent BACKGR constituent Figure 1: The case-frame for Turkish sentences. correspond to either full sentences with nonfinite or participle verb forms which act as noun phrases in either argument or adjunct roles, or gapped sentences with participle verb forms which function as modifiers of noun phrases (the filler of the gap). The former non-gapped forms can in Turkish be further classified into those representing acts, facts and adverbials. The latter (gapped form) is linked to the filler noun phrase by the ROLES feature in the structure for noun phrase (which will be presented in the following sections): this feature encodes the (semantic) role filled by the filler noun phrase and the case-frame of the sentential clause. The details of the feature structures for sentential clauses are very similar to the structure for the case-frame. Thus, when an argument or adjunct, which is a sentential clause, is to be realized, the clause is recursively generated by using the same set of transitions. For example, the verb 'g6r' (see) takes a direct object which can be a sentential clause: 83 (3) Aye'nin geliini Ay~e+GEN come+INF+P3SG 'I did not see Ay~e's coming.' g6rmedim. see+NEG+PAST+ISG Similarly, the subject or any other constituent of a sentence can also be a sentential clause: (4) Ali'nin buraya gelmesi AIi+GEN here come+INF+P3SG 'Ali's coming here made us bizim i~i bitirmemizi we+GEN the_job finish+INF+P1PL+ACC finish the job easier.' kolayl~tlr&. make_easy+PAST+3SG In all these cases, the main sentence generator also generates the sentential subjects and objects, in addition to generating the main sentence. Complex Sentences Complex sentences are combinations of simple sentences (or complex sentences themselves) which are linked by either conjoining or various relationships like conditional dependence, cause-result, etc. The generator works on a feature structure representing a complex sentence which may be in one of the following forms:  a simple sentence. In this case the sentence has the case-frame as its argument feature structure. TYPE simple ] ARG caseframe]  a series of simple or complex sentences connected by coordinating or bracketing conjunctions. Such sentences have feature structures which have the individual case-frames as the values of their ELEMENTS features: eonj [TYPE and/or/etc. ] [CONJ ]..ELEMENTS list-of(compl .... entence)J  sentences linked with a certain relationship. Such sentences have the feature structure: TYPE linked LINK-RELATION [el ARG 1 complex-sentence ARG2 complex-sentence Issues in Representing Noun Phrases In this section we will briefly touch on relevant aspects of the representation of noun phrases. We use the following feature structure (simplified by leaving out irrelevant details) to describe the structure of a noun phrase: [ARO ba ......... 1] REF CONTROL [DROP +/(default -) CLASS classifier ROLES role-type list-off mod. relatzon) 1 MOD-REL [POSITION pos.] ORDINAL [INTENSIFIER +/-J MODF [QUANT-MOD quant2fier QUALY-MOD list-of[strnple-property) I LCONTROL t/EMPHASIS n TMquant / ]/ ]j DEFINITE +/DET REFERENTIAL +/SPEC [.SPECIFIC +/|SET-SPEC list-of(c-name) |SPEC-REL list-of(spec, relation') LDEMONS demonstrative L/ARGUMENT c-name ]] POSS ICONTROL [DROP +//MOVE +/The order of constituents in noun phrases is rather strict at a gross level, i.e., speficiers almost always precede modifiers and modifiers almost always precede classifiers, 4 which precede the head noun, although there are numerous exceptions. Also, within each group, word order variation is possible due to a number of reasons:  The order of quantitative and qualitative modifiers may change: the aspect that is emphasized is closer to the head noun. The indefinite singular determiner may also follow 4A classifier in Turkish is a nominal modifier which forms a noun-noun noun phrase, essentially the equivalent of book in forms fike book cover in Engfish. 84 z ~ z ~ ) ~, e ~ .' ~ ~ ~-:~ ~.,~ ~ 2.~  N ,, ~i] ( )ii~, Figure 2: The finite state machine for generating the proper order of constituents in Turkish sentences. 85 any qualitative modifier and immediately precede any classifier and/or head noun.  Depending on the determiner used, the position of the demonstrative specifier may be different. This is a strictly lexical issue and not explicitly controlled by the feature structure, but by the information (stored in the lexicon) about the determiner used.  The order of lexieal and phrasal modifiers (e.g., corresponding to a postpositional phrase on the surface) may change, if positioning the lexical modifier before the phrasal one causes unnecessary ambiguity (i.e.. the lexical modifier in that case can also be interpreted as a modifier of some internal constituent of the phrasal modifier). So, phrasal modifiers always precede lexical modifiers and phrasal specifiers precede lexical specifiers, unless otherwise specified, in which case punctuation needs to be used.  Tile possessor may scramble to a position past the head or even outside the phrase (to a background position), or allow some adverbial adjunct intervene between it and the rest of the noun phrase, causing a discontinuous constituent. Although we have included control information for scrambling the possessor to post head position, we have opted not to deal with either discontinuous constituents or long(er) distance scrambling as these are mainly used in spoken discourse. Furthermore, since the possessor information is explicitly marked on the head noun, if the discourse does not require an overt possessor 5 it may be dropped by suitable setting of the DROP feature. Interfacing with Morphology As Turkish has complex agglutinative word forms with productive inflectional and derivational morphological processes, we handle morphology outside our system using the generation component of a full-scale morphological SFor example, (c) cannot be used as an answer to (a) in the following discourse, where the owner of the book should be emphasized: a. Kimin kitabl kahn? whose book+P3SG thick \"Whose book is thick?' b. Benim kitablm kahn. I+GEN book+P1SG thick 'My book is thick.' c. * Kitablm kahn. book+P1SG thick analyzer of Turkish (Oflazer, 1993). Within GenKit, we generate relevant abstract morphological features such as agreement and possessive markers and case marker for nominals and voice, polarity, tense, aspect, mood and agreement markers for verbal forms. This information is properly ordered at the interface and sent to the morphological generator, which then: 1. performs concrete morpheme selection, dictated by the morphotactic constraints and morphophonological context, 2. handles morphographemic phenomena such as vowel harmony, and vowel and consonant ellipsis, and 3. produces an agglutinative surface form. Grammar Architecture and Output Our generation grammar is written in a formalism called Pseudo Unification Grammar implemented by the GenKit generation system (Tomita and Nyberg, 1988). Each rule consists of a context-free phrase structure description and a set of feature constraint equations, which are used to express constraints on feature values. Non-terminals in the phrase structure part of a rule are referenced as x0 ..... xn in the equations, where x0 corresponds to the nonterminal in the left hand side, and xn is the n th non-terminal in the right hand side. Since the context-free rules are directly compiled into tables, the performance of the system is essentially independent of the number of rules, but depends on the complexity of the feature constraint equations (which are compiled into LISP code). Currently, our grammar has 273 rules each with very simple constraint checks. Of these 273 rules, 133 are for sentences and 107 are for noun phrases. To implement the sentence level generator (described by the finite state machine presented earlier), we use rules of the form: Si ----~ XP Sj where the Si and Sj denote some state in the finite state machine and the XP denotes the constituent to be realized while taking this transition. If this XP corresponds to a sentential clause, the same set of rules are reeursively applied. This is a variation of the method suggested by Takeda et al. (1991). 86 Tile following are rule examples that implement some of the transitions from state 0 to state 1: ( <==> ( ) ( ((x0 control topic) =c *undefined*) (xl = x0) )) ( <==> (  ) ( ((x0 control topic) =c subject) (x2 = x0) ((x2 arguments subject) = *remove*) (xl = (x0 arguments subject)) )) ( <==> (  ) ( ((xO control topic) =c time) (x2 = x0) ((x2 adjuncts time) = *remove*) (xl = (x0 adjuncts time)) )) The grammar also has rules for realizing a constituent like or (which may eventually call the same rules if the argument is sentential) and rules like above for traversing the finite state machine from state 1 on. Examples In this section, we provide feature structures tbr three example sentences which only differ in their information structures. Although the following feature structures seem very similar, they correspond to different surface formsJ (5) Ahmet diin kitabl masada Ahmet yesterday book+ACC table+LOC 'Ahmet left the book on the table btraktl. leave+PAST+3SG yesterday.' \"SFO RM CLAUSE-TYPE VOICE SPEECH-ACT VERB ARGUMENTS ADJUNCTS finite predicative active declarative \"ROOT #birak ] SENSE positive[ TENSE past [ ASPECT perfect J SUBJECT { Ahmet}] Dm-OBJ {ki~ap} | LOCATION {masa} J [TIME { di.in }] (6) Dfin kitabl masada Ahmet yesterday book+ACC table+LOC Ahmet 'It was Ahmet who left the book on blrakU. leave+PAST+3SG the table yesterday.' \"S-FORM finite CLAUSE-TYPE predicative VOICE SPEECHACT VERB ARGUMENTS ADJUNCTS CONTROL active declarative \"ROOT SENSE TENSE ASPECT \"SUBJECT DIR-OBJ ~blrak positive past perfect {Ahmet}\" {kitap} LOCATION {m~a} TIME {dUn}[ 'TOPIC time ] FOCUS subject (7) Diin kitabl Ahmet yesterday book+ACC Ahmet 'It was Ahmet who left the book 6The feature values in curly brackets indicate that, that feature has as value a c-name structure for the noun phrase inside the curly brackets. blraktl masada. Ieave+PAST+3SG table+LOC yesterday on the table.' 87 \"S-FORM CLAUSE-TYPE VOICE SPELCH-ACT\" VERB ARGUMENTS ADJUNCTS CONTROL finite predicative active declarative \"ROOT #blrak SENSE positive TENSE past ASPECT perfect I SUBJECT {Ahmet}\" Om-OBJ {k,tap} LOCATION {m~a} [~,~ {~0n}] TOPIC time ] FOCUS subject | BACKGROUND IocationJ Figure 3 shows the path the generator follows while generating sentence 7. The solid lines show the transitions that the generator makes in its right linear backbone. Comparison with Related Work Dick (1993) has worked on a classification based language generator for Turkish. His goal was to generate Turkish sentences of varying complexity, from input semantic representations in Penman's Sentence Planning Language (SPL). However, his generator is not complete, in that, noun phrase structures in their entirety, postpositional phrases, word order variations, and many morphological phenomena are not implemented. Our generator differs from his in various aspects: We use a caseframe based input representation which we feel is more suitable for languages with free constituent order. Our coverage of the grammar is substantially higher than the coverage presented in his thesis and we also use a full-scale external morphological generator to deal with complex morphological phenomena of agglutinative lexical forms of Turkish, which he has attempted embedding into the sentence generator itself. Hoffman, in her thesis (Hoffman, 1995a, Hoffman, 1995b), has used the MultisetCombinatory Categorial Grammar formalism (Hoffman, 1992), an extension of Combinatory Categorial Grammar to handle free word order languages, to develop a generator for Turkish. Her generator also uses relevant features of the information structure of the input and can handle word order variations within embedded clauses. She can also deal with scrambling out of a clause dictated by information structure constraints, as her formalism allows this in a very convenient manner. The word order information is lexically kept as multisets associated with each verb. She has demonstrated the capabilities of her system as a component of a prototype database query system. We have been influenced by her approach to incorporate information structure in generation, but, since our aim is to build a wide-coverage generator for Turkish for use in a machine translation application, we have opted to use a simpler formalism and a very robust implementation environment. Conclusions We have presented the highlights of our work on tactical generation in Turkish a free constituent order language with agglutinative word structures. In addition to the content information, our generator takes as input the information structure of the sentence (topic, focus and background) and uses these to select the appropriate word order. Our grammar uses a right-linear rule backbone which implements a (recursive) finite state machine for dealing with alternative word orders. We have also provided for constituent order and stylistic variations within noun phrases based on certain emphasis and formality features. We plan to use this generator in a prototype transfer-based human assisted machine translation system from English to Turkish. Acknowledgments We would like to thank Carnegie Mellon University-Center for Machine Translation for providing us the GenKit environment. This work was supported by a NATO Science for Stability Project Grant TU-LANGUAGE. "
W96-0410 " In this paper, we introduce a system, Sentence Planning Using Description, which generates collocations within the paradigm of sentence planning. SPUD simultaneously constructs the semantics and syntax of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG). This approach captures naturally and elegantly the interaction between pragmatic and syntactic constraints on descriptions in a sentence, and the inferential and lexical interactions between multiple descriptions in a sentence, At the same time, it exploits linguistically motivated, declarative specifications of the discourse functions of syntactic constructions to make contextually appropriate syntactic choices. 1 Introductio  "
W96-0411 " Current work in surface realization concentrates on the use of general, abstract algorithms that interpret large, reversible grammars. Only little attention has been paid so far to the many small and simple applications that require coverage of a small sublanguage at different degrees of sophistication. The system TG/2 described in this paper can be smoothly integrated with deep generation processes, it integrates canned text, templates, and context-free rules into a single formalism, it allows for both textual and tabular output, and it can be parameterized according to linguistic preferences. These features are based on suitably restricted production system techniques and on a generic backtracking regime. 1 Motivation Current work in surface realization concentrates on the use of general, abstract algorithms that interpret declaratively defined, non-directional grammars. It is claimed that this way, a grammar can be reused for parsing *This work has been supported by a grant from The Federal Ministry for Research and Technology (FKZ ITW 9402). I am grateful to Michael Wein, who implemented the interpreter, and to Jan Alexandersson for influential work on a previous version of the system. Finally, I wish to thank two anonymous reviewers for useful suggestions. All errors contained in this paper are my own. and generation, or a generator can interpret different grammars (e.g. in machine translation). A prominent example for this type of abstract algorithm is semantic-head-driven generation [Shieber et al., 1990] that has been used with HPSG, CUG, DCG and several other formalisms. In practice, this type of surface realization has several drawbacks. First, many existing grammars have been developed with parsing as the primary type of processing in mind. Adapting their semantics layer to a generation algorithm, and thus achieving reversibility, can turn out to be a difficult enterprise [Russell et al., 1990]. Second, many linguistically motivated grammars do not cover common means of information presentation, such as filling in a table, bulletized lists, or semifrozen formulae used for greetings in letters. Finally, the grammar-based logical form representation hardly serves as a suitable interface to deep generation processes. Grammarbased semantics is, to a large extent, a compositional reflex of the syntactic structure and hence corresponds too closely to the surface form to be generated. As a consequence, only little attention has been paid to interfacing this type of realizers adequately to deep generation processes, e.g. by allowing the latter to influence the order of results of the former. The system TG/2, which is presented in this contribution, overcomes many flaws of grammar-based surface realization systems that arise in concrete applications. In particular, TG/2 101  can be smoothly integrated with 'deep' generation processes,  integrates canned text, templates, and context-free rules into a single formalism,  allows for both textual and tabular output,  efficiently reuses generated substrings for additional solutions, and  can be parameterized according to linguistic properties (regarding style, grammar, fine-grained rhetorics etc.). TG/2 is based on restricted production system techniques that preserve modularity of processing and linguistic knowledge, hence making the system transparent and reusable for various applications. Production systems have been used both for modeling human thought (e.g. [Newell, 1973]) and for the construction of knowledge-based expert systems (e.g. [Shortliffe, 1976]). In spite of the modularity gained by separating the rule basis from the interpreter, production systems have disappeared from the focus of current research because of their limited transparency caused by various types of side effects. In particular, side effects could modify the data base in such a way that other rules become applicable [Davis and King, 1977]. However, precondition-action pairs can be used in a more restricted way, preserving transparency by disallowing side effects that affect the database. In TG/2 preconditions are tests over the database contents (the generator's input structure), and actions typically lead to a new subset of rules the applicabilitv of which would be tested on some selected portion of the database. By constraining the effects of production rules in such a way, the disadvantages of early production systems are avoided. At the same time, considerable flexibility is maintained with regard to linguistic knowledge used. A production rule may  involve a direct mapping to surface forms (canned text),  require to fill in some missing portion from a surface text (template), or  induce the application of other rules (classical grammar rules) Early template-based generation methods have correctly been criticized for beeing too inflexible to account adequately for the communicative and rhetorical demands of many applications. On the other hand, templates have been successfully used when these demands could be hard-wired into the rules. In TG/2 the rule writer can choose her degree of abstraction according to the task at hand. She can freely intermix all kinds of rules. The rest of the paper is organized as follows. TG/2 assumes as its input a predicateargument st  "
W96-0412 " In this paper, we present an evaluation of anaphors generated by a Chinese natural language generation system. In the evaluation work, the anaphors in five test texts generated by three test systems employing generation rules with different complexities ~vere compared with the ones in the same texts created by twelve native speakers of Chinese. We took the average number of anaphors matching between the machine and human texts as a measure of the quality of anaphors generated by the test systems. The results suggest that the one we have chosen and which has the most complex rule is better than the other two. There axe, however, real difficulties in establishing the significance of the results because of the degree of disagreement among the native speakers. 1 Introductio  "
W96-0413 "E-mail.\" n. creaney@ulst, ac. uk Tel: +44 (0)1265 324502 Fax: +44 (0)1265 324916 Abstract: Quantifiers, and their associated scoping phenomena are ubiquitous in English and other natural languages, and a great deal of attention has been paid to their treatment in the context of natural language analysis. Rather less attention, however, has been paid to their treatment in the context of language generation. This paper describes an algorithm for generating quantifiers in English sentences which describe small models containing collections of individuals which are inter-related in various ways. The input to the algorithm is, i) a model represented as a collection of facts and ii) an abstract description of the target sentence with gaps where the quantifiers should be. Keywords: lexical choice, realisation, quantifiers 121 An Algorithm for Generating Quantifiers Quantifiers, and their associated scoping phenomena are ubiquitous in English and other natural languages, and a great deal of attention has been paid to their treatment in the context of natural language analysis (Alshawi 1990, Creaney 1995, Grosz et al. 1987, Hobbs and Shieber 1987, Park 1995, Saint-Dizier 1984). Rather less attention, however, has been paid to their treatment in the context of language generation. This paper describes an algorithm for generating quantifiers in English sentences which describe small models containing collections of individuals which are inter-related in various ways. A model is represented as a collection of facts like the following: rep( rl ). sample( sl ). rep( r2 ). sample( s2 ). rep( r3 ). sample( s3 ). rep( r4 ). saw( rl, sl ). saw( r2, sl ). saw( r2, s2 ). saw( r3, s2 ). saw( r3, s3 ). saw( r4, sl ). saw( r4, s2 ). saw( r4, s3 ). In model (1) there are four representatives and three samples and some of the representatives saw some of the samples. The algorithm generates suitable quantifiers to complete sentences of the form: QR representative(s) saw Qs sample(s) where QR and Qs can be arbitrary quantifiers like; some, two, all, both, one of the, most, etc. The algorithm also handles models containing more relationships than model (I) to generate sentences of the form: QR representative(s) of Q c company(s) saw Qs sample(s) QR representative(s) saw Qs sample(s) of Qp product(s) QR representative(s) of Qc company(s) saw Qs sample(s) of Qp product(s) One of the most striking things about the problem is that there are generally a great many sentences which provide reasonable descriptions of any given model. For example, the following are all acceptable for model (1). 2 Every representative saw a sample 3 Every representative saw at least one sample 4 Most representatives saw at least two samples 5 A representative saw every sample 6 At least one representative saw every sample 7 At least two representatives saw most samples It turns out that there are three distinct sources of this variation and they correspond to three different kinds of choices which are made in the generation algorithm. They are: o quantifier scoping choices o choice of focus sets o choice of individual quantifiers constrained by the above two choices A great deal has been written about the quantifier scoping problem for natural language analysis (Hobbs & Shieber 1987) and much of this is applicable to the generation problem in the sense that any particular description must assume some particular quantifier scoping arrangement. For example, sentence (2) assumes that \"Every representative\" has wide scope while sentence (5) assumes that \"every sample\" has wide scope, and the sentences are only satisfied in the model under these assumptions. In fact sentences (2), (3) and (4) all assume wide scope for \"representative\" while sentences (5), (6) and (7) all assume wide scope for \"sample\". The gencration algorithm, of necessity, incorporates quantifier scoping. 122 The concept of a focus set has no correlate in the language analysis literature although it is similar to what Barwise and Cooper (Barwise and Cooper 1981) call a witness set. It has to do with choosing some particular subset of the model on which to base the description. Sentence (2) talks about all three representatives while sentence (4) talks only about the subset {r2,r3,r4}. This subset stands in the realtion most to the entire set of representatives {rl,r2,r3,r4} and is the focus set for R in sentence (4). The concept of a focus set will be made more precise below where dependency functions are discussed. saw( QI(R, rep(R)), Q2(S, sample(S)) ) saw( Q3(R, rep(R) ^ of(R, Q4(C,com(C))), Qs(S, sample(S)) ) and the purpose of the algorithm is to assign values to the Oi's given some suitable model. It works by processing the PAS recursively and non-deterministically selecting quantifier scopings and focus sets at each level. Quantifiers are then generated based on the cosen focus set. Inputs to the algorithm The inputs to the algorithm are: a model like (1) a predicate argument structure for the target sentence A predicate argument structure (PAS) is essentially an unscoped logical form of the form taken as input to Hobs and Shieber's algorithm (Hobbs & Shieber 1987). It makes explicit the relationships between predicates and there arguments but does not express any quantifier scope relationships. Sentence (2) has the following PAS: saw(every(R,rep(R)), a(S,sample(S)) ) Quantifier scoping Since the particular scoping framework underlying the generation algorithm is novel a brief explanation is appropriate. The orthodox approach to quantifier scoping is embodied in Hobbs and Sheiber's algorithm and it permits all permutations of quantifiers such that there are no unbound variables in the resulting logical form. For example, Hobbs and Shieber's algorithm produces the scopings (8b-f) for sentence (8a)t: 8 a Every representative of a company saw some samples b R>C>S c C>R>S d S>R>C e C>S>R f S>C>R and sentence (8a) has the following one. saw( every(R, rep(R)Aof(R,a(C,com(C))), some(S, sample(S)) ) Each variable in a PAS has a quantifier and a restriction which restricts the values which it may take. O O S's restriction is sample(S) in both cases R's restriction is rep(R) in the first PAS and rep(R)Aof(R,a(C,company(C)) in the second Since the algorithm generates quantifiers its input PASs are not exactly like these. Instead they have gaps where quantifiers should be: where R > C > S indicates that \"Every representative\" outscopes \"a company\" which in turn outscopes \"some samples\". The missing permutation, R > S > C, is not permitted because it violates what has become known as the unbound variable constraint. The scoping framework which underlies the generation algorithm recognises fewer scopings than (8). The relative scope of two quantifiers is only considered for variables We adopt the convention of naming variables with the first letter of the head noun with which they are associated (R=representative, C=company, S=sample) and using the symbol '>' to denote relative scope. 123 which are arguments to the same predicate. For example, R must be scoped relative to C because they are both arguments to the predicate of The possibilities are R > C and t3 > R. Similarly, R and S are arguments to the predicate saw and may be scoped either R > S or S > R. The relative scoping of C and S is never considered directly because they do not participate directly in any single predication in the sentence. They may however end up with a relative scoping as a result of taking the transitive closure of other scoping relationships. For example, if it is decided that C > R and R > S, then clearly, since scope is transitive, C > R. In this framework the following scopings are allowed for (8a). 9 a R>S,R>C b S>R>C c C>R>S d S>R,C>R It is clear from (8) and (9) that this framework undergenerates in comparison with Hobbs and Shieber's. However, in the context of language generation, undergeneration is not necessarily a serious problem, provided that there is the ability to adequately describe any model. In fact there is an argument to be made in favour ofa scoping framework which undergenerates with respect to Hobbs and Shieber's as a general approach to quantifier scoping (Park 1995). This is the subject of a future paper. Dependency functions, partitions and focus sets Each variable in a PAS has a candidate set which is defined by its restriction and the model under consideration. Definition 1: candidate set A variables candidate set is the set of individuals from the model which satisfy the variables restriction. For the PAS: saw(every(R,rep(R)), a(S,sample(S)) ) and model (1), R's candidate set is {rl,r2,r3,r4} and S's is {sl,s2,s3}. When we say that \"Every representative\" has wide scope we are saying that there is a function which maps R's candidate set onto the power set of S's candidate set. This dependencyffunction is computed from model (1) and is exhaustively listed in (lO) below. 10 saw: {rl,r2,r3,r4} ~ power({sl,s2,s3}) rl ~ { sl } r2 ~ {sl, s2} r3 --~ { s2, s3 } r4 ~ {sl,s2, s3} Alternatively, ifS is given wide scope the following dependency function is computed. 11 saw: {sl,s2,s3} -, power({rl,r2,r3,r4}) sl --~ {rl, r2, r4} s2 --~ { r2, r3, r4 } s3 ~ {r3, r4} Focus sets were discussed briefly above and are made more precise now in the context of dependency function partitions. Any dependency function can be partitioned by choosing a arbitrary subset of the mappings it contains as its focus, the remainder being its complement. Of course, the domains and ranges of these sub-functions are appropriately adjusted. The partitions (12a-c) are among the possible partitions of dependency function ( I 0). 12 a focus: saw: {rl} ~ power({sl}) rl -+ { sl } compt: saw: {r2,r3,r4} power({sl ,s2,s3}) r2 ~ {sl, s2} r3 ~ {s2, s3} r4 -~ { sl, s2, s3 } focus: saw: {r2,r3} --~ power({sl,s2,s3}) r2 --~ {sl, s2} r3 --~ { s2, s3 } compt: saw: {rl,r4} ~ power({sl,s2,s3}) rl ~ { sl } r4 --~ {sl, s2, s3} 124 focus:saw:{r2,r3,r4} --~ power({sl ,s2,s3}) r2 --~ {sl,s2} r3 ~ {s2, s3} r4 ~ {sl, s2, s3} The mapping from partitioned dependency f,mction to quantifiers is non-deterministic as (13) shows. For instance, partition (12b) gives, at least, the three sentences (13bi,ii,iii). compt: saw: {rl} ~ power({sl}) rl -~ {sl} Definition 2: focus set The focus set for a variable, given a particular partition is either: o the domain of the partition's focus o the union of the range of the partition's focus depending on the variable of interest. Not all sentences provide equally good descriptions of the model but they are all true in it. For example, (13ai) is true in (1), assuming \"a\" means at least one, but is not very informative. Bigger focus sets tend to give more information and sound more natural however the generation algorithm is concerned only with presenting alternatives and not with selecting between them. For example, (12a-c) define the following candidate sets for R and S. {rl} {sl} {r2, r3} {sl, s2, s3} { r2, r3, r4 } { sl, s2, s3 } It is useful to note that a variable's candidate set is related to an unpartititioned dependency function in exactly the same way that its focus set is related to the focus of the partitioned function. These relationships are illustrated in appendix 1. Individual quantifiers are selected for generation on the basis of dependency function partitions. For example, the descriptions (13a-c) are licensed by the partitions (12a-c) respectively. 13a i A representative saw a sample ii Exactly one representative saw exactly one sample III At least hal[the representatives saw exactly two samples Exactly two representatives saw exactly two samples Two. representatives saw most samples Exactly three representatives saw at least twq samples Three representatives saw some samples Generating quantifiers The process of generating quantifiers takes place after a scoping has been chosen and a dependency function has been constructed and partitioned, so that all decisions are made in the context of a particular partitioning of a particular dependency function. Generation consists of going through the list of all possible quantifiers and checking whether or not each one is consistent with the appropriate variable in the current dependency function partition. Those which are consistent are then generated and those which are inconsistent are rejected. To check the consistency of a particular quantifier with a particular variable it is first necessary to compute the variable's candidate set, focus set, and focus maximum and focus minimum. Definition 3: Focus maximum and minimum For a variable with wide scope the focus maximum (Fmax) and focus minimum (Fmin) are the same. They are simply the size of the focus set or, equivalently, the number of mappings in the focus of the dependency function. e.g. R in (12a): Fmax=Fmin= I{rl}l = 1 R in (12c): Fmax=Fmin= I {r2,r3,r4} I = 3 For a variable with narrow scope the focus maximum (Fmax) is equal to the size of the 125 biggest member of the range of the focus of the dependency function. e.g. S in (12a): Fmax = max(l{sl}i) = 1 S in (12c): Fmax= max( I {sl ,s2} I, I {s2,s3} ], I{sl ,s2,s3} I ) =3 The focus minimum is defined along the same lines as he focus maximum except that the minimum set size is taken. e.g. S in (12a): Fmin= min([{sl}l)= 1 S in (12c): Fmin= min(I {s1,s2}l, I {s2,s3}l, I {sl,s2,s3}l ) =2 q_inc( 1,_, [a] ). q_inc( 1,_, [somesing] ). q_inc( 1,_, [at, least,one] ). q_inc( 2, _, [at, least,two] ). q_inc( N, _, [some_plur] ):N > 1. q_inc( N, M, [most] ) :M < 2*N. q_inc( 1, 1, [the] ). q_inc( 2, 2, [both] ). q_inc( 3, 3, [all,three] ). q_inc( N, N, [all] ). q_inc( N, N, [each] ). q_inc( N, N, [every] ). For R in (12a) the appropriate call is therefore: The checking procedure varies according to the type of quantifier under consideration where quantifiers are classified as one of three types monotone increasing, monotone decreasing or cardinal (Barwise and Cooper 1981). o Monotone increasing quantifiers are those with an at least N interpretation. They include; a, some_sing, some_plur, the, both, many, at least four o Monotone decreasing quantifiers are ones with an at most N interpretation. They include; no, few, at most three, less than three quarters o Cardinal quantifiers are of the form exactly N ?q_inc( 1, 4, QR ). which returns the following quantifiers: a, some_sing, at least one. Similarly, for 9 in (12a) the appropriate call is: ?q_inc( 1, 3, QS ). which returns the same set ofquantifiers. Hence sentence (13ai) is generated, as is: Some representatives saw _a sample At least one representative saw a sample A representative saw some samples and other similar sentences formed by selecting from the above quantifiers. The check for monotone increasing quantifiers is simplest. The acceptability of each quantifier is as defined by a call to the following Prolog goal: ?q_inc( Fmin, Nc, QUANT ). For a monotone decreasing quantifier the check depends on whether it is in wide scope position or narrow scope position. In narrow scope position the check is similar to the one for monotone increasing quantifiers except that: where; Fmin = the focus minimum, Nc = I candidate set], and the q_inc/3 relation is defined along the following lines. 14 % q_inc( +N1, +N2, ?Q ) defines Q as % \"at least N1 out of a possible N2\" % e.g. \"a man\" means % at least 1 man out of any number % \"some men\" means % at least 2 men out of any number % \"both men\" means % at least 2 men out of a possible 2 o a different collection of quantifiers is checked the monotone decreasing ones o the focus maximum is input rather than the focus minimum. ?q_dec( Fmax, Nc, QUANT ). where; Fmax = the focus maximum, Nc = I candidate set[, and the q_dec/3 relation is defined along the following lines. 126 15 % q_dec( +N1, +N2, ?Q ) defines Q as % \"at most N1 out of a possible N2\" % e.g. \"no man\" means % at most 0 men out of any number % \"few men\" means % at most half of all the men % \"neither man\" means % at most 0 men out of 2 q_dec( 0, _, [no] ). q_dec( 2, _, [at, most, two] ). q_dec( N, M, [few] ) :M < 2*N. q_dec( 0, 2, [neither] ). The check for monotone decreasing quantifiers in wide scope position is a little bit trickier. For example, to check the consistency of the quantifier at most two in \"At most two representatives saw a sample\", assuming R > S, the following checks need to be made. O There must be a set of at most two of R's who may or may not have seen a sample. This entails checking that R's focus set contains exactly two members. All other R's outside this set must certainly not have seen a sample. This entails checking the complement of the dependency function to make sure that the quantifier a fails to be consistent with the variable S. These checks are carried by calls to q_dec/3 an q_inc/3 with appropriate input values. The check for cardinal quantifiers is defined in terms of two sub checks: one for a monotone increasing quantifier and one for a monotone decreasing. This follows from the observation that exactly N meant the same as (at least N)A(at most N). Embedded quantifiers The preceding discussion concentrated on simp!e linguistic structures like (2-7) which contain one main verb and noun phrases with no recursive structure. The processing of a more complex structure like: 16 saw( QR(R, rep(R) ^ of(R, Qc(C,com(C))), Qs(S, sample(S)) ) is done by breaking it down into substructures (17) which are processed almost independently. 17 a saw( QR(R .... ), Qs(S, sample(S))) b of( QR,(R, rep(R)), Qc(C, com(C)) ) The variable R is assigned the quantifier OR in (1 7a) and the quantifier OR, in ( 1 7b) but clearly only one of these will ultimately be generated and some special treatment is required. Thes are called R's outer and inner quantifiers respectively. PAS (17b) is processed first. A scoping is chosen for R and C and a dependency function is constructed in the normal way but when it comes to partitioning the function and generating a quantifier for C some care must be taken. Some choices of partition and quantifier must be excluded. What is required is that the resulting focus set for R is the set of all representatives who satisfy restriction (17b) under the chosen partition and quantifier. Consider the following dependency function and associated partition. 18a of: {rl,r2,r3} ~ power( {cl,c2,c3} ) rl --~ { cl } r2 --~ {c2} r3 --~ {c3} b focus: of: {rl,r2} ~ power({cl,c2}) rl ~ { cl } r2 ~ {c2} compt: of: {r3} ~ power({c3}) r3 ~ {b3} Based on the focus in (18b) the quantifier exactly one might be generated for C. The corresponding candidate set for R is {rl,r2} but this is not the set of all representatives who satisfy the restriction since r3 also satisfies it. It is to avoid this anomaly that the following constraint on the acceptability of dependency function partitions in this context. 127 19 If variable R is in wide scope position in (17b) then QR' must be of the form exactly N but is not generated in the final output.  "
W96-0414 "<NoAbstract>"
W96-0415 " We sketch the architecture of a sentence generation module that maps a language-neutral \"deep\" representation to a language-specific sentence-semantic specification, which is given to a front-end generator. Lexicalizat, ion is tlm main instrument tbr the mapl~ing step, and we examine the role of verb semantics in the process. In particular, we propose a set of rules that derive a range of verb alternations from a single base form, which is one source of lexical paraphrasing in the system. 1 Overv  "
W96-0416 " 'Dynamic hypertext' is hypertext which is automatically generated at the point of need. A number of NLG systems have now been developed to operate within a hypertext environment; and now that these systems are becoming widely available on the World Wide Web, it is useful to take stock of how well-equipped NLG technology is to work in this new domain. A generation system in a hypertext environment faces a specific set of requirements; here, we discuss those requirements, and the resources that can be provided to help meet them. Examples are drawn from a number of systems, including our own prototype, ILEX0. We conclude by indicating that the major benefit of such systems could be in the way that they combine flexibility with the illusion of user control. Keywords: content selection, text planning, applications, hypertext 1 NLG in a hype rtext environment "
W96-0417 " Comparisons are typically employed to distinguish similar entities, or to illustrate a property of an entity by referring to another com monly known entity which shares that property. Based on an analysis of a corpus of encyclopaedia texts, we define three types of comparisons and outline some strategies for applying these in the generation of entity descriptions. We describe how these comparison strategies are used within the PEBA-II hypertext generation system to generate descriptions of animals. 1 Introducti  "
W96-0418 "<NoAbstract>"
W96-0501 "<NoAbstract>"
W96-0502 "SPLAT (Sentence Plan Language Authoring Tool) is an authoring tool intended to facilitate the creation of sentence-plan specifications for the Penman natural language generation system. SPLAT uses an examplebased approach in the form of sentence.plan templates to aid the user in creating and maintaining sentence plans. SPLAT also contains a sentence bank, a user-extensible collection of sentence plans annotated in various ways. The sentence bank can be searched for candidate plans that can then be used in the creation of new sentence plans specific to the domain of interest. SPLAT's graphical environment provides additional support to the user in the form of menu-driven access to Penman's linguistic resources and management of partially built sentence plans. "
W96-0503 "<NoAbstract>"
W96-0504 "<NoAbstract>"
W96-0505 " We will demonstrate the GIST system, which generates social security forms in English, Italian and German. The system is intended for use by the technical authors and translators who design forms. A knowledge specification tool allows the author to build a model of the form in the knowledge representation language LOOM. From the LOOM model, a text drafter generates equivalent texts in the three supported languages, guided by some broad stylistic parameters which the author can control. The output texts serve as drafts which the authors and translators can modify or extend. Keywords: multilingual generation, applications. Type of submission: demonstration. in Europe have multiple languages: GIST focusses on the Trentino Alto-Adige region of Northern Italy, in which all official documentation has to be produced in two languages, Italian and German, laid out side by side on the page. The GIST consortium includes two organizations that have to implement this requirement: the Italian social security institute (INPS), and the local government agency for the Bolzano province (PAB). 2 Requi  "
W96-0506 " This paper 1 introduces a new line of research which ensures soundness and completeness in Natural Language text planners on top of an efficient control strategy. The work builds on the HUNTER-GATItERER analysis system (Beale. 96; Beale & Nirenburg, 96). That system employs constraint satisfaction, branch-and-bound and solution synthesis techniques to produce near linear-time processing for knowledge-based semantic analysis. PICARD enables similar results for the field of text planning by recasting localized means-end planning instances into abstractions connected by usage constraints that allow HUNTER-GATHERER to process the global problem as a simple constraint satisfaction problem. PICARD is currently being used to plan Enghsh and Spanish text in the Mikrokosmos Machine Translation Project. HUNTER-GATHERER Ove  "
W96-0507 "<NoAbstract>"
W96-0508 "<NoAbstract>"
W96-0509 "<NoAbstract>"
W96-0510 "<NoAbstract>"
W96-0511 "<NoAbstract>"
W96-0512 " We present a system that incorporates agentbased technology and natural language generation to address the problem of natural language summarization of live sources of data. The input to the system includes newswire and on-fine databases and ontologies. The output consists of short summaries that convey information selected to fit the user's interests, the most recent news updates, and historical information. The system is under development. 1 Intro  "
W96-0513 " In this paper, we address the issue of generating multilingua.1 computational semantic lexicons from analysis lexicons, showing the necessity of relying on a conceptual lexicon. We first discuss the type of information which should be found in NLP lexicons, whatever their use (analysis, generation, speech, robotics). We claim that we should take advantage of the existing large-scale analysis lexicons and use tliem as the starting point in the process of building large-scale generation lexicons, by first reversing tliem and then enhancing them. Tliis implies having access to a conceptual lexicon, which will serve as a pivot point between the analysis and the generation lexicons. We implemented the work reported here for Spanish and English MT projects, within the knowledge-based paradigm. From a theoretical point of view, regenerating the source text with the reversed analysis lexicon enabled us to enhance several issues as diverse as: evaluating analysis lexicons, testing the semantic analyser, evaluating which information should be added to the generation lexicon; and testing the grain-size of the pivot point between analysis and generation. Introductio  "
W96-0514 "<NoAbstract>"
W97-0601 " This paper discusses the range of ways in which spoken dialogue system components have been evaluated and discusses approaches to evaluation that attempt to integrate component evaluation into an overall view of system performance. We will argue that the PARADISE (PARAdigm for Dialogue System Evaluation) framework has several advantages over other proposals. I Int  "
W97-0602 "<NoAbstract>"
W97-0603 " This paper presents a first set of test results on the generality and objectivity of the Dialogue Evaluation Tool DET. Building on the assumption that most, if not all, dialogue design errors can be viewed as problems of non-cooperative system behaviour, DET has two closely related aspects to its use. Firstly, it may be used for the diagnostic evaluation of spoken human-machine dialogue. Following the detection of miscommunication, DET enables in-depth classification of miscornmunication problems that are caused by flawed dialogue design and supports the repair of those problems, preventing their future occurrence. Secondly, DET can be used to guide early dialogue design in order to prevent dialogue design errors from occurring in the implemented system. We describe the development and inhouse testing of the tool, and present the results of ongoing work on testing its generality and objectivity on an external corpus, i.e. an early corpus from the Sundial project in spoken language dialogue systems development. 1. Introducti  "
W97-0604 " Our approach to speech-based dialogue modelling aims to exploit, in the context of an object-oriented architecture, dialogue processing abilities that are common to many application domains. The coded objects that comprise the system contribute both recognition rules and processing rules (heuristics). A Domain Spotter supports the ability to move between domains and between individual skillsets. A Dialogue Model records individual concepts as they occur; notes the extent to which concepts have been confirmed; populates request templates; and fulfils a remembering and reminding role as the system attempts to gather coherent information from an imperfect speech recognition component. Our work will aim to confirm the extent to which the potential strengths of an object-oriented-paradigm (system extensibility, component reuse, etc.) can be realised in a natural language dialogue system, and the extent to which a functionally rich suite of collaborating and inheriting objects can support purposeful humancomputer conversations that are adaptable in structure, and wide ranging in subject matter and skillsets. "
W97-0605 " Using specialised text corpus to automatically enhance a general lexicon is the aim of this study. Indeed, having lexicons which offer maximal cover on a specific topic is an important benefit in many applications of Automatic Speech and Natural Language Processing. The enhancement of these lexicons can be made automatic as big corpora of specialised texts are available. A syntactic tagging process, based on 3class and 3-gram language models, allows us to automatically allocate possible syntactic categories to the Out-Of-Vocabulary (OOV) words which are found in the corpus processed. These OOV words generally occur several times in the corpus, and a number of these occurrences can be important. By taking into account all the occurrences of an OOV word in a given text as a whole, we propose here a method for automatically extracting a specialised lexicon from a text corpus which is representative of a specific topic. 1 Introduction  "
W97-0606 "Stuhlsatzenhausweg 3 D-66123 Saarbriicken, Germany {maier, reithinger, alexanders son}@dfki, uni-sb, de Abstract A number of methods are implemented in the face-to-face translation system VERBMOBIL to improve its robustness. In this paper, we describe clarification dialogues as one method to deal with incomplete or inconsistent information. Currently, three  types of clarification dialogues are realized: subdialogues concerning phonological ambiguities, unknown words and semantic inconsistencies. For each clarification type we discuss the detection of situations and system states which lead to their initialization and explain the information flow during processing. "
W97-0607 "<NoAbstract>"
W97-0608 "7, B-8900 Ieper, Belgium tel.: 32-57-22.88.88, fax: 32-57-20.84.89 (2) E.L.I.S., University of Gent, Sint-Pietersnieuwstraat 41, B-9000 Gent, Belgium tel.: 32-9-264.33.95, fax: 32-9-264.35.94 { Peter.Spyns,Filip.Deprez,Luc.VanTichelen,Bert.VanCoile} @lhs.be Abstract In this paper, we present a Message-toSpeech system for Natural Language Generation that is to be integrated in a dialogue system. As the system has to function in a very restrictive environment with respect to computational resources, a compromise between concept based and template based generation systems had to be found. Still, the approach aims at achieving linguistic flexibility for the utterances and attaining a natural sounding prosody. "
W97-0609 "<NoAbstract>"
W97-0610 "<NoAbstract>"
W97-0611 "<NoAbstract>"
W97-0612 "<NoAbstract>"
W97-0613 " We describe a new technology for using small collections of example sentences to automatically restrict a speech recognition grammar to allow only the more plausible subset of the sentences it would otherwise admit. This technology is unusual because it bridges the gap between hand-built grammars (used with no training data) and statistical approaches (which require significant data). 1 Motiv  "
W97-0614 " We argue that grammatical processing is a viable alternative to concept spotting for processing spoken input in a practical dialogue system. We discuss the structure of the grammar, the properties of the parser, and a method for achieving robustness. We discuss test results suggesting that grammatical processing allows fast and accurate processing of spoken input. 1 I  "
W97-0615 "t Thomson-CSF Laboratoire Central de Recherches, F-91404 Orsay Cedex, France emaih (roussel,ariane}@thomson-lcr.fr Abstract Our work addresses the integration of speech recognition and language processing for whole spoken dialogue systems. To filter ill-recognized words, we design an on-line computing of word confidence scores based on the recognizer output hypothesis. To infer as much information as possible from the retained sequence of words, we propose a bottom-up syntacticosemantic robust parsing relying on a lexicalized tree grammar and on integrated repairing strategies. "
W97-0616 " We describe the design and implementation of the dialogue management module in a voice operated car-driver information system. The literature on designing 'good' user interfaces involving natural language dialogue in general and speech in particular is abundant with useful guidelines for actual development. We have tried to summarize these guidelines in 7 'metaguidelines', or commandments. Even though state-of-the-art Speech Recognition modules perform well, speech recognition errors cannot be precluded. For'the current application, the fact that the car is an acoustically hostile environment is an extra complication. This means that special attention should be paid to effective methods to compensate for speech recognition errors. Moreover, this should be done in a way which is not disturbing for the driver. In this paper, we show how these constraints influence the design and subsequent implementation of the Dialogue Manager module, and how the additional requirements fit in with the 7 commandments. keywords: spoken dialogue management, errorprevention, error-recovery, design issues 1 Introduct  "
W97-0617 " With the rapid explosion of the World Wide Web, it is becoming increasingly possible to easily acquire a wide variety of information such as flight schedules, yellow pages, used car prices, current stock prices, entertainment event schedules, account balances, etc. It would be very useful to have spoken dialogue interfaces for such information access tasks. We identify portability, usability, robustness, and extensibility as the four primary design objectives for such systems. In other words, the objective is to develop a PURE (Portable, Usable, Robust, Extensible) system. A two-layered dialogue architecture for spoken dialogue systems is presented where the upper layer is domainindependent and the lower layer is domainspecific. We are implementing this architecture in a mixed-initiative system that accesses flight arrival/departure information from the World Wide Web. "
W97-0618 "<NoAbstract>"
W97-0619 " The Alparon project aims to improve Vxos, Openbaar Vervoer Reisinformatie's (OVa) automated speech processing system for public transport information, by using a corpus-based approach. The shortcomings of the current system have been investigated, and a study is made of how dialogues in the OVR domain usually occur between a human operator and a client. While centering our attention on the presentation of information by the Automated Speech Processing (ASP) system, we describe the implications of this corpus-based approach on the implementation of our prototype. 1 Intro  "
W97-0620 " Interactions with spoken language systems may present breakdowns that are due to errors in the acoustic decoding of user utterances. Some of these errors have important consequences in reducing the naturalness of human-machine dialogues. In this paper we identify some typologies of recognition errors that cannot be recovered during the syntactico-semantic analysis, but that may be effectively approached at the dialogue level. We will describe how nonunderstanding and the effects of misrecognition are dealt with by Dialogos, a realtime spoken dialogue system that allows users to access a database of railway information by telephone. We will discuss the importance of supporting confirmation turns, and clarification and correction subdialogues. We will show the positive effects of robust dialogue management and dialogue state dependent language modeling, by taking into account both the recognition and understanding performance, and the success rate of dialogue transactions. 1 Introduction During the last few years the recognition of spontaneous speech in telephone applications \"has greatly improved; nevertheless spoken dialogue between computers and inexperienced users still presents some problematic issues that reduce the user satisfaction in interacting with spoken language systems. The occurrence of errors in the acoustic decoding of users' utterance is the potential cause of miscommunication in oral interaction with spoken language systems. Some of these errors have important consequences in reducing the naturalness of humanmachine dialogues. Sometimes a robust approach in parsing and the use of language models during recognition are not sufficient to avoid recognition breakdowns. The recognition performance has a direct impact on the requirements that the dialogue modules of spoken language systems have to meet. In order to increase the usability of the applications, dialogue management modules have to deal with partial or total breakdowns of the lower levels of analysis by preventing and detecting miscommunication sources. In this paper we identify some typologies of recognition errors that cannot be recovered during the syntactico-semantic analysis, but that may be effectively approached at the dialogue level. Our analysis and the methodologies we describe have been tested in a task-oriented telephone application, but we deem that some considerations may also be useful for other display-less human-machine communication applications. We will describe how nonunderstanding and the effects of misrecognition are dealt with in Dialogos, a real-time spoken language system that allows/users to access a database of railway information by using the telephone. A detailed description of the different modules of Dialogos may be found in (Albesano et al., 1997). In this paper we will discuss the importance of supporting confirmation turns, and clarification and correction subdialogues. The dialogue module of Dialogos makes an extensive use of context knowledge: contextual information is used not only for validating or rejecting semantic interpretations, but it is also sent to the lower levels of input analysis for helping the recognizer. We will show that the positive effects of robust dialogue management and dialogue state dependent language modeling may be evaluated by taking into account both the recognition and understanding performance, and the success rate of dialogue transactions. From our experience we may conclude that if we provide robust behaviour in our dialogue systems, speech is a viable interface even with relatively low word accuracy rates. Nevertheless we believe that some important issues are still unexplored, and one of these is related to the weight that recognition errors have with respect to the degree of co-operativeness of the users. These open issues and some experimental data that emphasize their I14 urgency will be discussed in the section on experimental data. 2 Recognition errors and naturalness of dialogue State-of-the-art systems that receive their input by high-quality microphone have word accuracy scores above 90%. However, the recognition of spontaneous speech in telephone environment is below that rate. Actually, the telephone input of the recognizer may greatly differ from the uttered acoustic signal, due to the noisy environment of the call, and to the quality of the telephone microphone and propagation network. Most current task-oriented applications of telephone human-machine dialogue are developed for being used by a large population of potential users. These applications require speaker independent realtime systems, and the opportunity of having training sessions with the system cannot be provided. The speaker independent recognizers designed for such applications assure the coverage of a great number of speakers, but some aspects of the speech modality of some users can induce the recognizer to make mistakes, especially in recognizing long sentences. The adverse recognition environment and the variability in user dependent features are the most frequent reasons of three kinds of recognition errors. The speech community usually classifies these errors into deletions, insertions, and substitutions. Some of them may be prevented by using language models during the recognition. At present, some approaches to language modeling take advantage of contextual information sent by the dialogue model. However, the task of the dialogue state dependent language modeling is more difficult in some application domains. For example, some of the task-oriented systems that give information about railway timetable, or flight scheduling, have large vocabularies that contain an huge number of words belonging to the same class: for example, Dialogos vocabulary is 3,500 words, including 2,983 proper names of places; another example is the LIMSI Arise Railway Information system (Lamel et al., 1996) that has 1,500 words, including more than 680 station names. This peculiarity directly impacts on the performance of the language models, that is in these applications, language modeling predictions are weaker: when the dialogue prediction says that next user's utterance is likely to be about a departure place, this does not exclude that the recognizer substitutes the actually uttered name with a phonetically similar one. Only the user is able to detect such kinds of errors. In this situation the dialogue system should identify the user's detection of miscommunication and provide appropriate repairs. All the problems described above lead to the decrease of the recognition performance and of the usability of spoken language systems. More specifically, they identify some severe requirements that spoken dialogue modules have to meet. In particular, dialogue systems for telephone applications have to rely not only on an adequate model of the human user, but they should also implement particular techniques for preventing and recovering communication breakdowns. "
W97-1201 "<NoAbstract>"
W97-1202 "7, B-8900 Ieper, Belgium tel.: 32-57-22.88.88, fax: 32-57-20.84.89 (2) E.L.I.S., University of Gent, Sint-Pietersnieuwstraat 41, B-9000 Gent, Belgium tel.: 32-9-264.33.95, fax: 32-9-264.35.94 {Peter.Spyns,Filip.Deprez,Luc.VanTichelen,Bert.VanCoile}@lhs.be Abstract In this paper, we present a Message-toSpeech (MTS) system that offers the linguistic flexibility desired for spoken dialogue and message generating systems. The use of prosody transplantation and special purpose prosody models results in highly natural prosody for the synthesised speech. "
W97-1203 "German lZesearch Center for Artifical Intelligence (DFKI GmbH) Stuhlsatzenhausweg 3 66123 Saarbriicken, Germany po ller@dfki, uni-sb, de Abstract The acceptance of speech dialogue systems by the user is critically dependent on the degree of \"naturalness\" realized. The speech generation and synthesis modules have to be able to run in real time and to produce high-quality speech output. To produce naturally sounding speech, the synthesizer has to have not only the knowledge of the words to utter and the order in which they appear but also information about their structural relationship. The latter is expressed acoustically in the form of prosody, i.e. how the voice raises and falls during an utterance, the rhythm, where pauses are set, etc. Prosody is also influenced by the properties associated with given words in the context of an utterance, e.g. the focus of a sentence or certain emphatic elements. This article describes a compact representation for conveying this type of information from the generator to the synthesizer in a modular system and describes how (parts of) this information is (are) derived in the EFFENDI system, the generation module for a speech dialogue system for train inquiries. "
W97-1204 "New York, N.Y. 10027 {pan, kathy} @cs. columbia, edu Abstract Concept To Speech (CTS) systems are closely related to two other types of systems: Natural Language Generation (NLG) and Speech Synthesis (SS). In this paper, we propose a new architecture for a CTS system. A Speech Integrating Markup Language (SIML) is designed as an general interface for integrating NLG and SS. We also present a CTS system for a multimedia presentation generation application. We discuss how to extend the current CTS system based on the new architecture. Currently, only limited semantic, syntactic and prosodic features are covered inour prototype system. "
W97-1205 " This paper analyses the intonation of polar questions extracted from a corpus of taskoriented dialogues in the Bari variety of Italian. They are classified using a system developed for similar dialogues in English where each question is regarded as an initiating move in a conversational game (Carletta et al 1995). It was found that there was no one-to-one correspondence between move-type and intonation pattern. An alternative classification was carried out taking into account information status, that is, whether or not the information requested by the speaker is recoverable from the previous dialogue context. It is found that the degree of confidence with which the speaker believes the information to be shared with the interlocutor is reflected in the choice of pitch accent and postfocal accentual pattern. Low confidence polar questions contain a L+H* focal pitch accent and allow for accents to follow it, whereas high confidence ones contain a H*+L focal pitch accent, followed by deaccenting or suppression of accents. 1 Introduct  "
W97-1206 " We propose a set of rules for the computation of prosody which are implemented in an existing generic Data-to-Speech system. The rules make crucial use of both sentence-internal and sentence-external semantic and syntactic information provided by the system. In a Textto-Speech system, this information would have to be obtained through text analysis, but in Data-to-Speech it is readily available, and its reliable and detailed character makes it possible to compute the prosodic properties of generated sentences in a sophisticated way. This in turn allows for a close control of prosodic realization, resulting in natural-sounding intonation. 1 Introdu  "
W97-1207 " Concept-to-Speech (CTS) systems, which aim to synthesize speech from semantic information and discourse context, have succeeded in producing more appropriate and naturalsounding prosody than text-to-speech (TTS) systems, which rely mostly on syntactic and orthographic information. In this paper, we show how recent advances in CTS systems can be used to improve intonation in text reading systems for English. Specifically, following (Prevost, 1995; Prevost, 1996), we show how information structure is used by our program to produce intonational patterns with context-appropriate variation in pitch accent type and prominence. Following (Cahn, 1994; Cahn, 1997), we also show how some of the semantic information used by such CTS systems can be drawn from WordNet (Miller et al., 1993), a large-scale semantic lexicon. 1 Introduction  "
W98-0801 " Current speech recognition systems are capable of performing complex tasks for co-operative users by determining their requirements through a conversation. Most systems have been constructed without attempting to accurately model spontaneous speech. Some components, such as the parser, can be easily made robust to some of the artifacts of conversational speech. Others, such as the pronunciation models, simply ignore the possibility that incomplete words can occur. This results in some recognition errors, and may cause the application to begin to perform the wrong the action. Typically, however, the next several conversation turns can identify and correct the error. This talk gives a brief overview of state-of-the-art of spoken language systems and describes how some of the components are affected by artifacts of spontaneous speech. Large bodies of accurately transcribed spontaneous speech are required to learn the properties of spontaneous events. : ;:i :! fi~!~i i ,~i O Utli n e N~RTEL NORTHERN TELECOM  Components of a speech recognition system.  Modeling speech: acoustic models. pronunciation models. language models.  Natural language understanding.  Discourse management.  Effects of spontaneous speech. Norte[ OpenSpeech I ~ode]~ I\"~ I \\ i.I Soarc ! \\ \\, ' , I Manager l I / \"~ Under~ , '' / [ standing] ] i /  i J Appli]1 / ; ] cation If/ : , ,i/ N~RTEL NORTHERN TELECOM Norte[ OpenSpeech The search engine matches the user's speech to the most likely path through the search graph. The search graph is specified by the acoustic models, and the language model (where the language model includes pronunciation). The most likely path corresponds to a word sequence. The matching is usually accomplished using a coarse quantization of the speech spectrum and some variation of the Viterbi algorithm (dynamic programming). This word sequence is passed to the discourse manager who in turn passes it to the understanding component. The meaning is extracted and returned to the discourse manager. The discourse manager takes the appropriate action and tells the user the result. X 3 ib;delmcj Speech lag~ ~i~';i ~i~;~i~i!~;i;:~ii ~ !~::!~: ~:,,: ~!, ; :::.: .... N~RTEL NORTHERN TELECOM  Acoustic models. Model speech sounds, typically phone-based models.  Pronunciation. How words are pronounced, typically concatenations of acoustic models as defined by the lexicon.  Language model. How words may be connected together, typically statistical for large applications. NorteL OpenSpeech The acoustic models are usually some variation of hidden Markov models (HMMs). The state sequence helps to model the quasi-stationarity of speech with discrete jumps from one type of statistics to another (such as a transition from a fricative to a vowel). Each acoustic model typically corresponds to a phone in a particular context. Acoustic models are trained from a large corpus of transcribed data. Words models are constructed by determining the pronunciation of each word in a lexicon. The string of phonemes is mapped to a sequence of acoustic models the resulting chain of models becomes the model for the word. With this approach, models can be constructed for each word without actually having training data specifically for that word. The language model describes how words may be connected together. The most common language models for large applications are purely statitistical, with the most common ones being the defined by the previous several words. Bigram models give the probability of each word depending on the previous word and trigram models give the probability of each word depending on the previous two words. Language models are trained on large corpora of text as well as the transcribed data used to train acoustic models. ~ ' ii.~ .U ~ . . . . ~g' ~; ~: ~ :natural N~RTEL NORTHERN TELECOM Extract meaning from natural language in a normalized manner. Typically some variation of CFG rules. Robust parsers do not require complete parses. Robust to ungrammatical speech and recognition errors. ilnm tl Norte[ OpenSpeech The NLU component attempts to extract the meaning from the recognized word string. This is typically accomplished by matching CFG rules to build parse trees. For example, a number can be represented by a digit followed by another number or simply a digit by itself. Recursively applying this rule can represent any number. In most cases, a robust parser is used. A robust parser is similar to a normal parser, but does not require a complete parse to succeed. Instead, a forest of parse trees is found that represent the highest level rules that could be found. With this approach, words that do not fit any rule (because of ungrammatical speech or misrecognitions) are simply left out of the parse, and the parser returns what it can find. For the above example, since the parser only has a rule for a number, it will extract only \"one five six four\" and ignore \"I'll,\" \"take,\" and \"please.\" 15 ~.~ ~!~i!~i}}i~~ilaln a g Q me n t N~RTEL NOHHERH TELECOM ~Hutomated Broker. ~____~pen-ended I ow may I help y u~ L_ prompt I ~wen~-fi,,e ~e~.L.~ d hree thousand~ ollars. s made you rich~ l Action of \"find oowat' s the pric~ idgets Inc.~ ~ Nortel? jJ ~Wow. Sell al~ Qmy shares. J / remembered, remembered. I Norte[ OpenSpeech The discourse manager is responsible for carrying on the conversation with the user. Ambiguous or unclear information is clarified by the DM throughout the dialogue by:  asking the user what they meant or prompting for the missing information.  confirming the most likely interpretation. In a spontaneous dialogue, the discourse manager is required to infer things based on the history of the conversation. For example, if the previous request was for the price of a stock and the subsequent request gives only a stock, the most likely interpretation is that the user also wants the price of the subsequent stock. The discourse manager also interacts with the actual application, such as a data base, voice mail/e-mail server, etc. Thus, for each application, the discourse manager must understand the requirements of the application, how to express the user's request to the application, and how to interpret the response from the application. 6 "
W98-0802 " This paper argues for the usefulness of multimodal spoken language corpora and specifies components of a platform for the creation, maintenance and exploitation of such corpora. Two of the components, which have already been implemented as prototypes, are described in more detail: TransTool and SyncTool. TransTool is a transcription editor meant to facilitate and partially automate the task of a human transcriber, while SyncTool is a tool for aligning the resulting transcriptions with a digitized audio and video recording in order to allow synchronized presentation of different representations (e.g., text, audio, video, acoustic analysis). Finally, a brief comparison is made between these tools and other programs developed for similar purposes. 1. In  "
W98-0803 "<NoAbstract>"
W98-0804 " Large phonetic corpora including both standard and variant transcriptions are available for many languages. However, applications requiring the use of dynamic vocabularies make necessary to transcribe words not present in the dictionary. Also, additional alternative pronunciations to standard forms have shown to improve recognition accuracy. Therefore, new techniques to automatically generate variants in pronunciations have been investigated and proven to be very effective. However, rule-based systems still remain useful to generate standard transcriptions not previously available or to build new corpora, oriented chiefly to synthesis applications. The present paper describes a letter-to-phone conversion system for Spanish designed to supply transcriptions to the flexible vocabulary speech recogniser and to the synthesiser, both developed at CSELT (Centro Studi e Laboratori relecomunicazioni), Turin, Italy. Different sets of rules are designed for the two applications. Symbols inventories also differ, although the IPA alphabet is the reference system for both. Rules have been written in ANSI C and implemented on DOS and Windows 95 and can be selectively applied. Two speech corpora have been transcribed by means of these grapheme-to-phoneme conversion rules: a) the SpeechDat Spanish corpus which includes 4444 words extracted from the phonetically balanced sentences of the database b) a corpus designed to train an automatic aligner to segment units for synthesis, composed of 303 sentences (3240 words) and 38 isolated words; rule-based transcriptions of this corpus were manually corrected. The phonetic forms obtained by the rules matched satisfactorily the reference transcriptions: most mistakes on the first corpus were caused by the presence of secondary stresses in the SpeechDat transcriptions, which were not assigned by the rules, whereas errors on the synthesis corpus appeared mostly on hiatuses and on words of foreign origin. Further developments oriented to recognition can imply addition of rules to account for Latin American pronunciations (especially Mexican, Argentinian and Paraguayan); for synthesis, on the other hand, rules to represent coarticulatory phenomena at word boundaries can be implemented, in order to transcribe whole sentences. 33 Introduction Grapheme-to-phoneme conversion is an important prerequisite for many applicalions involving speech synthesis and recognition [I]. Large corpora used for these applications (e.g. WSJ, CMU, Oxford Pronunciation Dictionary, ONOMASTICA, SpeechDat) include phonetic transcriptions for both standard pronunciations and for variants, which can represent either differences in dialectal or individual realisation of single words (intra-word variants) [2, 3] or variations in the standard form produced by coarticulation between words (inter-word variants) [4]. These alternative pronuciations have been shown to improve recognition accuracy [5] and they need to be present in large phonetic database: the variants can either be realised manually on the basis of expert phonetic knowledge, or by a rulebased system. However, maintenace of such systems is complex, because insertion of new rules often causes to change the overall performance of the module. Therefore, new techniques to derive automatically rules for vapheme-to-phoneme conversion from training data have been investigated. Generally rules are obtained through forced recognition, according to the following procedure: 1) aligning of the canonical pronunciation to the alternative ones by means of a dynamic programming algorithm, in order to generate an aligned database 2) use this database to train a statistical model or a binary decision tree to generate variants of words or proper names [1] [3] [6] [5] or to model contextdependent variations at word boundary [4]; neural networks can also be used to generate variants in pronunciation of words [2] or of surnames [7], on the basis of pre-aligned or nonaligned training data [8]. Finally, a mixed approach combining knowledge obtained fi'om training data and a priori phonetic expertise has also been experimented to derive possible nonnative pronunciations of English and Italian words [9]. All these techniques have proven to be very effective to generate plausible alternatives to canonical ones. However, rule-based approaches can still represent an effective tool to automatically obtain standard transcriptions of large corpora built ad hoc for special applications, in particular oriented to synthesis: a letter-tophone rules component is very suitable to represent allophonic and allomorphic variations [10] [11] [12] which are essential to allow segmentation and diphone extraction from an acoustic database [ 13]. The rule system described in the present paper was developed on the basis of phonetic knowledge [14] [15] and has two different application domains, which imply different transcription requirements: the recogniser for Spanish uses sub-word units [16] [17] linked to the phonetic representation of isolated words; the units have been trained on the corpus of words extracted from the phonetically balanced sentences included in the SpeechDat database. Therefore, the SpeechDat corpus has been considered as the reference set of words that the conversion rules minimally had to correctly transcribe. Only isolated words were used, with the same phoneme inventory employed in the original SpeechDat transcriptions, including no allophones. On the other hand, the corpus for synthesis was selected to collect speech material to train the automatic phonetic aligner, in order to extract diphones for a concatenative synthesis system [18] and had to meet different requirements: a) units were to be pronounced both in isolated words and in sentences b) the phoneme inventory had to include the maximum number of allophones so to allow to build a representative acoustic dictionary containing occurences of all units and sequences in every appropriate segmental and prosodic context (stressed and unstressed syllables; initial and final position in the syllable; initial, internal and final position in the sentence; short and long sentences). Therefore, two partially different sets of rules have been designed for synthesis and recognition, which can be alternatively activated: the latter are a subset of the former. Both systems provide only 34 one variant in output, i.e. the standard Castilian (Madrid) Spanish pronunciation. 1. Orthog  "
W98-1401 "Journeys to Interactive 3D Worlds* Invited Talk Extended Abstract James C. Lester and William H. Bares Charles B , Callaway and Stuart G. Towns Multimedia Laborator  Department of Computer Scienc  North Carolina State Universit y Raleigh, NC 27695 {lester, whbsres, cbcallaw , sgtowns}~os.ncsu.edu http://multimedia.ncsu.edu/imedia/ Abstract Interactive 3D worlds offer an intriguing testbed for the natural language generation community. To complement interactive 3D worlds' rich visualizations, they ~equire significant linguistic flexibility and communicative  power. We explore the major functionalities and ~rchitectural implications of natural language generation for three key classes of interactive 3D worlds: self. .\" explaining 3D environments, habitable 3D learning environments, and interactive 3D narrative worlds. These are illustrated with .empirical investigations underway in our laboratory with severalsuch systems.    . .  Introduction Natural language generation (NLG) has witnessed great strides over the past decade. Our theoretical Underpinnings are firming up, our systems building activities are proceeding quickly, and we are beginning to see significant empirical results. As a result of this maturation, the field is now well positioned to attack the  \"challenges pose'd by a new family of computing envi ronments: interactive 3D worlds, which continuously  render the activities playing out in rich 3D scenes in realtime. Because of these worlds' compelling visual properties and their promise of a high degree of multimodal interactivity, they will soon form the basis for applications ranging from learning environments for ed, ucation and training to interactive fiction systems for entertainment. Interactive 3D worlds offer an intriguing testbed for the NLG community for several reasons. They may portray scenes with complicated spatial relationships, \" * Support for this work was provided by the followlag organizations: the National Science Foundation under grants CDA-9720395 (Learning and Intelligent Systems Initiative) and IRI-9701503 (CAREER Award Program); the North Carolina State University Intelli/Cledia Initiative; the William S: Kenan Institute for Engineering, Technology and Science; and a Corporate gift from Novell, Inc.  such as those found in the domain of electricity and magnetism in physics. They may include multiple dynamic objects tracing out complex motion paths, such as water particles traveling through xylem tissue in Virtual plants. They might be inhabited by user-directed avatars that manipulate objects in the world and lifelike agents that will need to coordinate speech, gesture, and locomotion as they explain and demonstrate complex phenomena. In 3D interactive fiction systems, userdirected avatars and lifelike autonomous agents may navigate through complex cityscapes and interact with users and with one another to create new forms of theater. As the visual complexities of interactive 3D worlds grow, they will place increasingly heavy demands on the visual channel. To complement their rich visualizations, interactive 3D worlds will require the linguistic flexibility and communicative power that only NLG can provide. In interactive learning environments, the spatial complexities and dynamic phenomena that characterize physical devices must be clearly explained. NLG delivered with speech synthesis will need to be carefully coordinated with 3D graphics generation to create interactive presentations that are both coherent and interesting. In a similar fashion, lifelike agents roaming  around the same 3D worlds through which users guide their avatars will require sophisticated NLG capabilities, and 3D interacti.ve fiction systems will-benefit considerably from virtual narrators that are articulate and can generate interesting commentary in realtime. In this talk, we will explore the major issues, functionalities, and architectural implications of natural language generation for interactive 3D worlds. Our discussion will examine NLG issues for three interesting classes of interactive 3D worlds:  Self-Explaining 3D Environments: In response to users' questions, Self-explaining environments dynamically generate spoken natural language and 3D animated visualizations and produce vivid explana.   : Figure 1: The PHYSVIZ Self-Explaining 3D Environment tions of complex phenomena.  H a b i t a b l e 3D Learning E n v i r o n m e n t s : In habitable learning environments, lifelike pedagogical agents generate advice combining speech and gesture as users solve problems by guiding avatars through 3D worlds and manipulating devices housed in the worlds.  I n t e r a c t i v e 3D N a r r a t i v e Worlds: Virtual narrators generate fluid descriptions of lifelike characters' interaction with one another in response to incremental specifications produced by narrative planners and interactively-issued user directives. To begin mapping out the very large and complex space of NLG phenomena in 3D interactive worlds, it is informative to examine the issues empirically. These issues are being studied in the context of several projects currently under development in our laboratory. First, self-explaining 3D environments must coordinate NLG with 3D graphics generation. These requirements will be discussed with regard to the PHYSVIZ (Towns, Callaway, & Lester 1998) and the PLANTWORLD (Bares & Lester 1997) self-explaining 3D environments for the domains of physics and plant physiology, respectively. Second, in habitable 3D learning environments, lifelike agents must be able to generate clear language that is carefully coordinated with agents' gestures and movements as they interact with users in problem-solving episodes. We examine these issues in the VIRTUAL COMPUTER (Bares el al. 1998; Bares, Zettlemoyer, & Lester 1998), a habitable 3D learning environment for the domain of introductory computer architecture. Third, virtual narrators for 3D interactive fiction should be able to generate compelling realtime descriptions of multiple characters' behaviors. These issues are illustrated with examples from the COPS~ROBBERS world (Bares, Gr~goire, & Lester 1998), a 3D interactive fiction testbed. In the talk, we discuss current efforts to introduce NLG capabilities into these worlds at several levels. This includes (1) discourse planning, as provided by the KNIGHT explanation planner (Lester & Porter 1997), (2) sentence construction, as provided by the the FARE sentence planner (Callaway & Lester 1995) and the REv i s o r clause aggregator (Callaway & Lester 1997), and (3) surface generation, as provided by FUF (Elhadad 1991). Below we briefly summarize the requirements and issues of NLG for self-explaining 3D environments, habitable 3D learning environments, and interactive 3D narrative worlds. These will be discussed in some detail in the talk. G e n e r a t i o n in S e l f E x p l a i n i n g 3 D E n v i r o n m e n t s As graphics technologies reach ever higher levels of sophistication, knowledge-based learning environments and intelligent training systems can create increasingly Figure 2: The PLANTWORLD Self-Explaining 3D Environment effective educational experiences. A critical functionality required in many such systems is the ability to unambiguously communicate spatial knowledge. Learning environments for the basic sciences frequently focus on physical structures and the fundamental forces that act on them in the world, and training systems for technical domains often revolve around the structure and function of complex devices. Explanations of electromagnetism, for example, must effectively communicate the complex spatiM relationships governing the directions and magnitudes of multiple vectors representing currents and electromagnetic fields, many of which are orthogonal to one another. Because text-only explanations are inadequate for expressing complex spatial relationships and describing dynamic phenomena, realtime explanation generation combining natural language and 3D graphics could contribute significantly to a broad range of learning environments and training systems. This calls for a computational model of 3D multimodal explanation generation for complex spatial and dynamic phenomena. Unfortunately, planning the integrated creation of 3D animation and spatial/behavior linguistic utterances in realtime requires coordinating the visual presentation of 3D objects and generating appropriate referring expressions that accurately reflect the relative position, orientation, direction, and motion paths of the objects presented with respect to the virtual camera's view of the scene. To address this problem, we are developing the visuolinguistic ezplanation planning framework for generating multimodal spatial and behavioral explanations combining 3D animation and speech that complement one another. Because 3D animation planners require spatial knowledge in a geometric form and natural language generators require spatial knowledge in a linguistic form, a realtime multimodal planner interposed between the visual and linguistic components serves as a mediator. This framework has been implemented in CINESP~.AK, a multimodal generator consisting of a media-independent explanation planner, a visuolinguistic mediator, a 3D animation planner, and a realtime natural language generator with a speech synthesizer. Experimentation with CINESPEAK is underway in conjunction with self-explaining environments that are being designed to produce language of spatial and dynamic phenomena:  Complex spatial explanations: PHYSVIZ (Towns, Callaway, & Lester 1998) is a self-explaining 3D environment in the domain of physics that generates multimodal explanations of three dimensional electromagnetic fields, force, and electric currents in realtime (Figure I).  Complex dynamic behavior explanations: PLANTWORbD (Bares & Lester 1997) is a self-explaining 3D environment in the domain of plant anatomy and 4 ! ! i| !. Figure 3: The VIRTUAL COMPUTER Habitable 3D Learning Environment physiology that generates multimodal explanations of dynamic three dimensional physiological phenomena such as nutrient transport (Figure 2). G e n e r a t i o n i n H a b i t a b l e 3 D L e a r n i n g E n v i r o n m e n t s Engaging 3D learning environments in which users guide avatars through virtual worlds hold great promise for learner-centered education. By enabling users to participate in immersive experiences, 3D learning environments could help them come to develop accurate mental models of highly complex biological, electronic, or mechanical systems. In particular, 3D learning environments could permit learners to actively participate in the very systems about which they are learning and interact with lifelike agents that could effectively communicate the knowledge relevant to the user's task. For example, users could study computer architecture in a virtual computer where they might be advised by a lifelike agent about how to help a CPU carry data from RAM to the hard disk, or they could study the human immune system by helping a T-cell traverse a virtual lymph system. Properly designed, 3D learning environments that blur the distinction between education and entertainment could produce engrossing learning experiences that are intrinsically motivating and are solidly grounded in problem solving. Lifelike agents that are to interact with users in habitable 3D learning environments should be able to generate language that enables them to provide clear problem-solving advice. Rather than operating in isolation, generation decisions must be carefully coordinated with decisions about gesture, locomotion, and eventually prosody. In collaboration with the STEVE virtual environments tutor project at USC/ISI (Rickel &: Johnson 1998), we have begun to design NLG techniques for embodied explanation generation in which the avatar/agent generates coordinated utterances (delivered with a speech synthesizer) and gestural and locomotive behaviors as it manipulates various devices in the world. Embodied explanation generation poses particularly interesting challenges in the following areas:  Deictic believability: Lifelike agents must be able to employ referring expressions and gestures that together are both unambiguous and natural (Lester et al. 1998).  Socially motivated generation: Lifelike agents must not only express concepts clearly but also create utterances that are properly situated in the current socio-linguistic context.  Embodied discourse planning: Media allocation issues must be considered in adjudicating between expressing advice verbally or through agents' demonstrative actions. Over the past two years, we have constructed a habitable learning environment for the domain of computer Figure 4: The COPSe:ROBBERS Interactive 3D Narrative World architecture. The VIRTUAL COMPUTER (Bares et al. 1998; Bares, Zettlemoyer, & Lester 1998) (Figure 3) is a habitable 3D learning environment that teaches novices the fundamentals of computer architecture and system algorithms, e.g., the fetch-execute cycle. To learn the basics of computation, users direct an avatar in the form of a friendly robot courier as they execute instructions and transport data packets to appropriate locations in a 3D \"town\" whose buildings represent the CPU, RAM, and hard disk. We are beginning to investigate deictic believability, socially motivated generation, and embodied discourse planning in an lifelike agent that provides advice in the VIRTUAL COMPUTER. I n t e r a c t i v e 3 D N a r r a t i v e W o r l d s While story generation has been an NLG goal that dates back more than. a quarter century and text-based interactive fiction systems have been the subject of increasing attention, it is the prospect of coupling sophisticated NLG with 3D believable characters that offers the potential of achieving interactive fiction generation in a visually compelling environment. One can imagine different genres of 3D interactive fiction, many of which will involve a virtual narrator who comments on the events unfolding in the world. In much the same manner that sports announcers come in two varieties, play-by-play and color commentary, virtual narrators can provide both a descriptive account of the world's activities as well as a running analysis on their significance. To stress test NLG, we adopt three constraints on generation for 3D narrative worlds: 1  Realtime: World events play out in realtime and can be modified by users. Consequently, the relevance of utterances is time-bound; generators must construct their utterances in realtime and cannot know in advance how the actions in the world will play out.  Non-interference: Generators cannot themselves enact modifications on objects or characters in the world. As a result, they must cope with what they are dealt by world simulators and users' actions. * Multiple, simultaneous events: Multiple activities occur in the world at the same time. Consequently, generators must make time-bounded moment-bymoment content determination decisions that necessarily omit mention of many actions. We have recently begun to study these issues in COPS&ROBBERs (Bares, Gr~goire, & Lester 1998), a 3D interactive fiction testbed with multiple characters interacting with each other in an intricate cityscape. In COPS&ROBBERS (Figure 4), three autonomous characters, a policeman and two robbers, attempt to capture a lost money bag dropped by a careless bank teller. I f l Efisabeth Andrfi and colleagues at DFKI are addressing similar issues in their realtime generator for the ROBOCUP competition. ! i i I I I ! ! the policeman finds the money bag first, he dutifully returns it to the bank, but if either of the two miscreants find the unclaimed money, they will scurry off to Joe's Bar to spend their new found loot. If the cop catches either robber carrying the money, he will immobilize him and return the money bag to the bank. When the narrative begins, the three characters meander randomly through the town searching for the lost money bag. At any time, users may affect the narrative by modifying characters' physical abilities such as their speed or eyesight. Despite the relative simplicity of the testbed, it poses significant NLG challenges. Of particular interest are problems in the virtual narrator's expressing time sequence relations, concisely describing locations where particular events are occurring, and linking characters' actions to their intentions. Because events occur simultaneously, tense issues are problematic in accurately describing the temporal relations between events in sequential utterances. Especially difficult are generating precise disambiguating locative descriptions involving relative locations, direction of movement, and proximity of characters and structures in the world. Because it is often important to identify where a specific action has occurred, generators must be able to formulate locatives that are precise. Frequently, they must also be concise, because utterances that are too verbose will require excessive speaking times, causing tile narration to miss other important events. Finally, generators must be able to communicate about, characters' goals, actions, and the relation between the two. For example, if the cop is scurrying toward one of the robbers, rather than merely reporting the action, the generator should sometimes comment on the causal link between the cop's desire to obtain the money bag and and his accosting the targeted robber. "
W98-1402 " In this paper we presen t a system for automatically producing multimedia pages of information that draws both from results in data-driven aggregation in information visualization and from results in communicative-goal oriented natural language generation. Our system constitutes an architectural synthesis of these two directions, allowing a beneficial cross-fertilization of research methods. We suggest that data-driven visualization provides a general approach to aggregation in NLG, and that text planning allows higher user-responsiveness in visualization via automatic diagram design.  "
W98-1403 "<NoAbstract>"
W98-1404 "rgh 1 Abstract We describe the architecture of the ILEX system,  which supports opportunistic text generation. In  web-based text generation, the SYstem cannot plan the entire multi-page discourse because the user's browsing path is unpredictable. For this reason,  the system must be ready opportunistically to take  advantage of whatever path the user chooses. We describe both the nature of opportunism in ILEX's museum domain, and then show how ILEX has been designed to function in this environment. The architecture presented addresses opportunism in both content determination and sentenceplanning. "
W98-1405 " This paper Is astudy in the tactics of content selection and realization at the micro-planning level. It presents a .technique for controlling the content and phrasing of complex sentences through the use of data derived from a parser that has read through a corpus and taken note of which variations do and do not occur in the realization of the concepts in the genre the corpus is taken from. These findings are entered as annotations on a new representational device, a 'saturation lattice', that provides a systematic way to define partial information and is the jumping off point for the micro-planner. The generator and parser are both based on a declarative, bi-directional representation of realization relationship between concepts and text. Keywords:  generation, bi-directional, realization, model-drive n "
W98-1406 "<NoAbstract>"
W98-1407 "nada lapalme@ iro. umontreal, ca \" ' Michael Zock LIMSI-CNRS, BP 133 F-91403 Orsay Cedex, France zock@l imsi. fr Abstract A frequently encountered problem in urban life is navigation. In order to get to some place we use private means or public transportation, and if we lack clear directions we tend to ask for help. We will deal in this paper with the descriptions of subway routes and their automatic generation. In particular, we will try to show how the relative importance of a given piece of information can effect not only the message but also the form. "
W98-1408 " This work describes a method for text planning that is suitable to small domains like train table information. Our aim is to introduce maximal variation in the packaging of  information and in the linear order of its presentation. To this end, we regard text planning as a goal-driven process that dynamically constructs a text plan. The goal is a state where all information in the input is shared with the user; the means to achieve this goal are utterances. The application of utterances is limited by constraints that refer to the user's current state of knowledge. This approach to text planning can be conven!ently implemented as a Functional Unification Grammar. In addition, we show how optional or inferable information can be accountedfor, how focus can be distributed, and how the generation of anaphoric expressions can be constrained by looking at the form and content of a previous utterance . "
W98-1409 "<NoAbstract>"
W98-1410 " In order to generate high quality explanations in technical or mathematical domains, the presentation must be adapted to the knowledge of the intended audience. Current proof presentation systems only communicate proofs on a fixed degree of abstraction independently of the addressee's knowledge. in this paper that describes ongoing research, we propose an architecture for an interactive proof explanation system, called P. rex. Based on the theory of human cognition AcT-R, its dialog planner exploits a cognitive model, in which both the user's knowledge and his cognitive processes are modeled. By this means, his cognitive states are traced during the explanation. The explicit representation of the user's cognitive states in AcT-R allows the dialog planner to choose a degree of abstraction tailored to the user for each proof step to be explained. 1  "
W98-1411 " Marcu has characterised an important and difficult problem in text planning: given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how can one arrange this material so as to yield the best possible text? We describe experiments with a number of heuristic Search methods for this task. ' 1  Introduction: Text Planning "
W98-1412 "<NoAbstract>"
W98-1413 " We present a system for the generation of natural language instructions, as are found in instruction manuals for household appliances; that is able to automatically generate safety warnings tO the user at appropriate points. Situations in which accidents and injuries to the user can occur are considered at every step in the planning of the normal operation of the device, and these \"'injury sub-plans, are then used to instruct the user to avoM these situations. 1 Introduction We present a system for the generation of natural language instructions, as are found in instruction manuals for household appliances  , that is able to automatically generate safety Warnings to the user at appropriate points. Situations in which accidents and injuries to the user can occur are considered at every step in the planning of the normal operation of the device, and these \"injury sub-plans\" are then used to instruct the user to avoid these situations. Thus, unlike other instruction generation systems, our system tells the user what not to do as well as what to do. We will show how knowledge about a device that is assumed to already exist as part of the engineering effort, together with adequate, domain-independent knowledge about .the environment, can be used for this. We also put forth the notion that actions are performed on the materials that thedevice operates upon, that the states of these materials may change as a result of these actions, and that the goal of the system should be defined in  terms of the final states of the materials. We take the stand that a complete natural language instruction generation system for a device should have, at the top level, knowledge of the device (as suggested by Delin et al. (1993)). This is one facet of instruction generation that many NLG systems have largely ignored by instead incorporating the knowledge of the task at their top level, i.e., the basic content of the instructions is assumed to already exist and does not need to be planned for. In our approach, all the knowledge necessary for the planning stage of a system  "
W98-1414 "* Otto-von-Guericke Universit~it Magdeburg Institut fiir Wissensund Sprachverarbeitung -Abstract Manfred Stede t Technische Universit~t Berlin Projektgruppe KIT In text, discourse markers signal the kind of coherence relation holding between adjacent text spans; for example, because, since; and for this reason are different markers for causal relations. For any but the most simple applications of text generation, marker Selection is an important aspect of producing cohesive text. However, present systems use markers in fairly simplistic ways andcannot make use of the full potential of markers that language offers for a given relation.  To improve this situation, we propose a specialized lexicon for discourse markers, which holds the relevant constraints and preferences associated with the markers, and which can be used by a text generator to make an informed choice among alternative ways of expressing a relation in the given context, we demonstrate how .the lexicon can be employed in the generation  process and propose to perform discourse marker choice in the sentence planning stage, where the interactions with other generation decisions can be accounted for. "
W98-1415 "Abstract By combining multiple clauses into one single sentence, a text generation system can express the same amount of information in fewer words and at the same time, produce a great variety of complex constructions. In this paper, we describe hypotactic and paratactic operators for generating complex sentences from clause-sized semantic representations. These two types of operators are portable and reusable because they are based on general resources such as the lexicon and the grammar. "
W98-1416 "LIA email: {ingrid,ricky, korb} @csse.monash.edu.au Abstract We describe the operation of our argumentation system, and discuss its use of attentional focus during both content planning and argument presentation. During content planning, attentional focus guides an abductive process used to build-up arguments. This process is applied to a model of a user's  beliefs and a normative model. During argument presentation, attentional focus supports the generation of enthymematic arguments. "
W98-1417 " The paper discusses a framework for planning contributions in a spoken dialogue system, and focuses especially on the three/'s: Incrementality, Immediacy, and Interactivity. The emphasis is on communicative principles and the notion of NewInfo, or the information focus of the utterance. NewInfo provides a natural way of to conceptualize the planning process and to generate utterances on the level of granularity required in spoken interaction.  "
W98-1418 "Department of Mathematics and Computer Science, Beer Sheva, 84105, Israel (yaeln [ elhadad) @cs. bgu. ac. il Abstract Hebrew includes a very productive noun-compounding construction called smixut. Because smixut is marked morphologically and is restricted by many syntactic constraints, it has been the focus of many descriptive studies in Hebrew grammar. We present the treatment of smixut in HUGG, a FUF-based syntactic realization system capable of producing complex noun phrases in Hebrew. We contrast the treatment of smixut with noun-compounding in English and illustrate the potential for paraphrasing it introduces. We Specifically address the issue of determining when a smixut construction can be generated as opposed to other semantically equivalent constructs. We investigate several competing hypotheses smixut is lexically, semantically and/or pragmatically determined. For each hy: pothesis, we explain why the decision to produce a smixut construction cannot be reduced to a computation Over features produced by an outside module that Would not need to know about the smixut phenomenon. We conclude that smixut provides yet another theoretical example where the interface that a syntactic realization component presents to the other components of a generation architecture cannot be made as isolated as we would hope. While the syntactic constraints on smixut are encapsulated within HUGG, the input Specification language to HUGG must contain a feature that specifies that smixut is requested if possible.  However, because smixut accounts for close to half the cases of NP modifiers observed on a corpus of complex NPs, and it appears to be the unmarked realization form for some frequent semantic relations, we empirically evaluate a default setting strategy for the feature use-smixut based on a simple semantic Classification of the relations head-modifier in the NP. This study provides a Solid ground for the definition of a small set of predicates in the input specification language to HUGG, that has applications beyond the selection of smixut -for the determination of the order of modifiers in the NP and the use of stacking vs. conjunction --and for the definition of a bilingual input specification language. "
W98-1419 "<NoAbstract>"
W98-1420 "<NoAbstract>"
W98-1421 " : This paper presents a novel multi-lingual progress protocol generation module. The module is used within the speech-to--speech translation system VERBMOBIL. The task of the protocol is to give the dialogue partners a brief description of the content of their dialogue. We utilize an .abstract representation describing, for instance, thematic information and dialogue acts of the dialogue utterances. From this representation we generate simplified paraphrases of the individual turns of the dialogue which together make up the protocol. Instead of writing completely new software, the protocol generation component is almost exclusively composed of already existing modules in the system which are extended by planning and formatting routines for protocol formulations. We describe how the abstract information is extracted from user utterances in different languages and how the abstract thematic representation is used to generate a protocol in one specific language. Future directions are given.  "
W98-1422 "German Research Center for Artificial Intelligence (DFKI GmbH) Stuhlsatzenhausweg 3, 66123 Saarbriicken, Germany becker@dfki, de Abstract We describe a new approach to syntactic generation with Head-Driven Phrase Structure Grammars (HPSG) that uses an extensive off-line preprocessing step. Direct generation algo rithms apply the phra~se-structure rules (schemata) of the grammar on:line which is an computationally expensive step. Instead, we collect off-line for every lexical type of the HPSG grammar all minimally complete projections (called elementary trees) that can be derived with the schemata. This process is known as 'compiling HPSG to TAG' and derives a Lexicalized Tree-Adjoining Grammar (LTAG). The representation as an LTAG is 'fully lexicalized' in the sense that all grammatical information is directly encoded with the lexical item (as a set of elementary trees) and the combination operations are reduced from schema applications to the TAG primitives of adjunction and substitution. Given this LTAG, the generation task has a very different search space that Can be traversed very efficiently, avoiding the costly on-line applications of HPSG unification. The entire generation task from a semantic representation to a surface string is split into two tasks, a microplanner and a syntactic realizer. This paper discusses the syntactic generator and the preprocessing steps as implemented in the Verbmobil system. "
W98-1423 "881 Manchester M60 1QD, United Kingdom graham~ccl, umist, ac. u_k : Abstract HPSG is widely Used in theoretical and computational linguistics, but rarely in natural language generation. The paper describes some approaches to surface realization in which HPSG can be used. The implementation of all the approaches combines generation algorithms in Prolog and HPSG grammars in ProFIT. It is natural to combine a head-driven HPSG grammar with a headdriven generation algorithm. We show how a simple head-driven generator can easily be adapted for use with HPSG. This works well with simplified semantics, but if we implement the full HPSG textbook semantics this approach does not work. In a second approach to head-driven generation, we implement some recent revisions of HPSG, and show that head-driven generation with HPSG  is in fact possible. We then switch to non-head-driven approaches. We show how a bag generation algorithm, developed for use with categorial grammar and indexed QLF, can be used with HPSG and MinimalRecursion Semantics. We describe an approach to incremental generation with HPSG, noting a difficulty for highly incremental generation with HPSG and proposing a solution. Finally we briefly mention a few other plausible approaches. "
W98-1424 "<NoAbstract>"
W98-1425 "<NoAbstract>"
W98-1426 "<NoAbstract>"
W98-1427 " Natural language generation technology is now ripe for commercial exploitation, but one of the remaining bottlenecks is that of providing NLG systems with user-friendly interfaces for Specifying the content of documents to be generated. We present here a new technique we have developed for providing such interfaces: WYSIWYM editing. WYSIWYM (What You See Is What You Meant) makes novel use of the system's generator to provide a natural language input device which requires no NL interpretation . . . . -only NL generation. 1  "
W98-1428 "nc. {mike, ted} @cogentex.. com Abstract In this paper, we present EXEMPLARS, an object-oriented, rule-based framework designed to support practical, dynamic text generation, emphasizing its novel features compared to .existing hybrid systems that mix template-style and more sophisticated techniques. These features-.include an extensible classification-based text planning mechanism, a definition language that is a superset of the Java language, and advanced support for HTMIdSGML templates. "
W98-1429 " We present a system for Natural Language Generation based on an Abstract Machine approach. Our abstract machine operates on grammar s encoded in a unification-based Typed Feature Structure formalism, and is capable of both generation and parsing. For efficient generation, grammars are first inverted to a suitable form, and then compiled into abstract machine instructions. A dual compiler translates the same input grammar into an abstract machine program for parsing. Both generation and parsing programs are executed under the same (chart-based) evaluation strategy: This results in an efficient, bidirectional (parsing/generation) System for Natural Language Processing. Moreover, the system possesses ample debugging features, and thus can serve as a user-friendly environment for bidirectional grammar design and development. "
W98-1430 "<NoAbstract>"
W98-1431 "<NoAbstract>"
W98-1432 "<NoAbstract>"
W98-1433 "SYSTEM DEMONSTRATION GOALGETTER: GENERATION OF SPOKEN SOCCER REPORTS Mari~t Theune and Esther Klabbers IPO, Center for Research on User-System Interaction, Eindhoven University of Technology. Abstract In this paper we describe a demonstration Of the GoalGetter system, which generates spoken soccer reports (in Dutch) on the basis of tabular data. Two types of speech output are available. The demo runs via the web. It includes the possibility of !creating your own match' and having GoalGetter generate a report on this match.  . ._ "
W98-1434 " The MLWFA (Multilingual Weather Forecasts Assistant) system will be demonstrated. It is developed to generate the multilingual text of the weather forecasts automatically. The raw data from the weather observation can be used to generate the weather element chart. According to the weather change trend, the forecasters can directly modify the value Of the element on the chart, such as the center .point value, the isoline and the isocircle. After that, the modified data are stored as the input for the system. The system can select a schema depending on the input or the requirement from the users. The schema library can be conveniently maintained, such as the schema modification or extension. Through optimizing and mapping the schema tree, the microplanner constructs the brief and coherent internal text structure for the surface generator. After the processing of the generator, the muitilingual weather forecasts used for the broadcast program are generated. Keywords: Multilingual Generation, Weather Forecast Assistant. ., 1 Introduction The MLWFA ~system is developed as the first application of the project ACNLG [Huang et al. 97a & b] which is an international cooperation between the German Research Center for Artificial Intelligence (DFKI) and ShanghaiJiao Tong University (SJTU). The system mainly consists of four components: the graphic processor, the macroplanner, the microplanner and the surface generator. The graphic, processor is used to adjust weather forecasts data by the forecasters. The technique adopted for the macroplanner is based on the schema approach [McKeown 85], but we expand the operator of schema. The microplanner is based on the sentence structure optimizing which is independent of the language and language resource mapping -which is associated with the language. On the basis of the FB-LTAG (Feature-based Lexicalized -Tree Adjoining Grammar) [Joshi 85, XTAGRG 95], the surface generator identifies the feature of the nodes, compounds the grammar-trees and finally generates Chinese, English and German weather forecasts.  "
W98-1435 "<NoAbstract>"
W98-1436 "street, 3400 Cluj-Napoca, Romania Abstract. The ROMVOX Text-toaSpeech synthesis system developed by our team is the first one that allowed the synthesis of any unrestricted Romanian text with intonation facilities on 1BM-PC compatible computers. During the last years of research several version of text-to-speech systems were achieved, trying to enhance their facilities. Our paper describes the present stage of our experiments performed in order to improve the naturalness of the generated voice. "
W99-0802 "<NoAbstract>"
W99-0803 " We introduce a notion of training methodology space (TM space) for specifying training methodologies in tile different disciplines and teaching traditions associated with computational linguistics and the human language technologies, and pin our approach to the concept of operational model; we also discuss different general levels of interactivity. A number of operational models are introduced, with web interfaces for lexical databases, DFSA matrices, finitestate phonotactics development, and DATR lexica. 1 Why tools f  "
W99-0804 " This paper describes experience with the developed of tools for CL education using Java. Some are standalone Java applets and others are clients which connect to a parsing server using a LISP-based backend. The principal benefits are platform independence and reusability rather than world-wide web access, although intranet technology reduces the need for special purpose labs. Introduction Networked computers can be used to support learning in various ways. In computational linguistics, the predominant pattern of use is twofold: Learning materials are distributed using hypertext, and laboratories are conducted in which students work directly with computational linguistics processors such as parsers and generators. The 'authorware' approach to developing learning materials has not been popular in the teaching of computational linguistics because of the extensive labour involved in encoding content. Since CL is all about the use of powerful general mechanisms and expressive formalisms, the idea of writing learning materials using less expressive tools has little appeal. However, the new technologies of the internet make it easier to combine media to produce integrated learning environments in which pedagogical materials can be intimately connected to mechanisms and resources. Using such approaches can produce payoffs whether or not distance learning is involved. A better integrated set of resources for laboratory activities makes fewer demands on support staff such as graduate demonstrators. The ability to encapsulate mechanisms and tools in applets also means that the need to maintain special purpose laboratories is diminished, and it is also possible to promote CL to potential students in schools. This paper reports experience with the use of web browsers to provide practical activities to an introductory class of computational linguistics students. We concentrate on the tools developed locally, although we make use of others where appropriate. Much of the discussion focuses on what is possible with the constraints imposed by current network software.  "
W99-0805 " The subject matter of speech and hearing is packed full of phenomena and processes which lend themselves to or require auditory demonstration. In the past, this has been achieved through passive media such as tape or CD (e.g. Houtsma et ai, 1987; Bregman & Ahad, 1995). The advent of languages such as MATLAB which suppor!s sound handling, modern interface elements and powerful signal processing routines, coupled with the availability of fast processors and ubiquitous soundcards allows tbr a more interactive style of demonstration. A significant effort is now underway in the speech and hearing community to exploit these favourable conditions (see the MATISSE proceedings (1999), for instance). Excitingly, it is now possible to allow exploratory access to part or all of the parameter space underlying each phenomenon. Over the past 18 months, more than 20 interactive auditory demonstrations have been produced at Sheffield as part of an ongoing project to provide teaching material for the diverse disciplines which contribute to speech and hearing. Many of the demonstrations are suitable for undergraduate courses, while others encode phenomena which are primarily of interest to researchers. The motivation for and design ethos behind this project has been described previously in Cooke & Brown (1999) and Wrigley, Cooke & Brown (1999). In this extended abstract, a gallery of screenshots which focus on the auditory (as opposed to speech) demonstrations is provided. The aim is to show the breadth of what is possible in a relatively short time and to encourage others to produce similar tools. The demonstrations can be freely downloaded via http://www.dcs.shef.ac.uk/-martin . Acknowledgements  "
W99-0806 "<NoAbstract>"
W99-0807 "<NoAbstract>"
